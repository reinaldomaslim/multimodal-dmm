1
0:00:12.535,000 --> 0:00:14,000
[This talk contains mature content]

2
0:00:17.762,000 --> 0:00:19,000
Rana Ayyub is a journalist in India

3
0:00:20.778,000 --> 0:00:22,000
whose work has exposed government corruption

4
0:00:24.411,000 --> 0:00:26,000
and human rights violations.

5
0:00:26.99,000 --> 0:00:27,000
And over the years,

6
0:00:28.181,000 --> 0:00:31,000
she's gotten used to vitriol and controversy around her work.

7
0:00:32.149,000 --> 0:00:37,000
But none of it could have prepared her for what she faced in April 2018.

8
0:00:38.125,000 --> 0:00:41,000
She was sitting in a café with a friend when she first saw it:

9
0:00:41.8,000 --> 0:00:45,000
a two-minute, 20-second video of her engaged in a sex act.

10
0:00:47.188,000 --> 0:00:49,000
And she couldn't believe her eyes.

11
0:00:49.561,000 --> 0:00:51,000
She had never made a sex video.

12
0:00:52.506,000 --> 0:00:55,000
But unfortunately, thousands upon thousands of people

13
0:00:55.995,000 --> 0:00:56,000
would believe it was her.

14
0:00:58.673,000 --> 0:01:,000
I interviewed Ms. Ayyub about three months ago,

15
0:01:01.641,000 --> 0:01:03,000
in connection with my book on sexual privacy.

16
0:01:04.681,000 --> 0:01:07,000
I'm a law professor, lawyer and civil rights advocate.

17
0:01:08.204,000 --> 0:01:12,000
So it's incredibly frustrating knowing that right now,

18
0:01:12.839,000 --> 0:01:14,000
law could do very little to help her.

19
0:01:15.458,000 --> 0:01:16,000
And as we talked,

20
0:01:17.029,000 --> 0:01:21,000
she explained that she should have seen the fake sex video coming.

21
0:01:22.038,000 --> 0:01:27,000
She said, "After all, sex is so often used to demean and to shame women,

22
0:01:27.658,000 --> 0:01:29,000
especially minority women,

23
0:01:30.11,000 --> 0:01:34,000
and especially minority women who dare to challenge powerful men,"

24
0:01:34.446,000 --> 0:01:35,000
as she had in her work.

25
0:01:37.191,000 --> 0:01:4,000
The fake sex video went viral in 48 hours.

26
0:01:42.064,000 --> 0:01:47,000
All of her online accounts were flooded with screenshots of the video,

27
0:01:47.395,000 --> 0:01:49,000
with graphic rape and death threats

28
0:01:50.046,000 --> 0:01:52,000
and with slurs about her Muslim faith.

29
0:01:53.426,000 --> 0:01:57,000
Online posts suggested that she was "available" for sex.

30
0:01:58.014,000 --> 0:01:59,000
And she was doxed,

31
0:01:59.648,000 --> 0:02:01,000
which means that her home address and her cell phone number

32
0:02:02.45,000 --> 0:02:03,000
were spread across the internet.

33
0:02:04.879,000 --> 0:02:08,000
The video was shared more than 40,000 times.

34
0:02:09.76,000 --> 0:02:12,000
Now, when someone is targeted with this kind of cybermob attack,

35
0:02:13.72,000 --> 0:02:15,000
the harm is profound.

36
0:02:16.482,000 --> 0:02:19,000
Rana Ayyub's life was turned upside down.

37
0:02:20.211,000 --> 0:02:23,000
For weeks, she could hardly eat or speak.

38
0:02:23.919,000 --> 0:02:26,000
She stopped writing and closed all of her social media accounts,

39
0:02:27.632,000 --> 0:02:3,000
which is, you know, a tough thing to do when you're a journalist.

40
0:02:31.188,000 --> 0:02:34,000
And she was afraid to go outside her family's home.

41
0:02:34.696,000 --> 0:02:37,000
What if the posters made good on their threats?

42
0:02:38.395,000 --> 0:02:42,000
The UN Council on Human Rights confirmed that she wasn't being crazy.

43
0:02:42.784,000 --> 0:02:46,000
It issued a public statement saying that they were worried about her safety.

44
0:02:48.776,000 --> 0:02:52,000
What Rana Ayyub faced was a deepfake:

45
0:02:53.029,000 --> 0:02:55,000
machine-learning technology

46
0:02:55.593,000 --> 0:02:59,000
that manipulates or fabricates audio and video recordings

47
0:02:59.728,000 --> 0:03:01,000
to show people doing and saying things

48
0:03:02.475,000 --> 0:03:03,000
that they never did or said.

49
0:03:04.807,000 --> 0:03:07,000
Deepfakes appear authentic and realistic, but they're not;

50
0:03:08.192,000 --> 0:03:09,000
they're total falsehoods.

51
0:03:11.228,000 --> 0:03:14,000
Although the technology is still developing in its sophistication,

52
0:03:15.046,000 --> 0:03:16,000
it is widely available.

53
0:03:17.371,000 --> 0:03:2,000
Now, the most recent attention to deepfakes arose,

54
0:03:20.467,000 --> 0:03:22,000
as so many things do online,

55
0:03:22.652,000 --> 0:03:23,000
with pornography.

56
0:03:24.498,000 --> 0:03:26,000
In early 2018,

57
0:03:26.633,000 --> 0:03:28,000
someone posted a tool on Reddit

58
0:03:29.125,000 --> 0:03:33,000
to allow users to insert faces into porn videos.

59
0:03:33.561,000 --> 0:03:36,000
And what followed was a cascade of fake porn videos

60
0:03:37.025,000 --> 0:03:39,000
featuring people's favorite female celebrities.

61
0:03:40.712,000 --> 0:03:43,000
And today, you can go on YouTube and pull up countless tutorials

62
0:03:44.213,000 --> 0:03:46,000
with step-by-step instructions

63
0:03:46.523,000 --> 0:03:49,000
on how to make a deepfake on your desktop application.

64
0:03:50.26,000 --> 0:03:53,000
And soon we may be even able to make them on our cell phones.

65
0:03:55.072,000 --> 0:04:,000
Now, it's the interaction of some of our most basic human frailties

66
0:04:00.478,000 --> 0:04:01,000
and network tools

67
0:04:02.184,000 --> 0:04:04,000
that can turn deepfakes into weapons.

68
0:04:04.874,000 --> 0:04:05,000
So let me explain.

69
0:04:06.875,000 --> 0:04:1,000
As human beings, we have a visceral reaction to audio and video.

70
0:04:11.86,000 --> 0:04:12,000
We believe they're true,

71
0:04:13.372,000 --> 0:04:15,000
on the notion that of course you can believe

72
0:04:15.474,000 --> 0:04:17,000
what your eyes and ears are telling you.

73
0:04:18.476,000 --> 0:04:19,000
And it's that mechanism

74
0:04:20.199,000 --> 0:04:23,000
that might undermine our shared sense of reality.

75
0:04:23.921,000 --> 0:04:26,000
Although we believe deepfakes to be true, they're not.

76
0:04:27.604,000 --> 0:04:31,000
And we're attracted to the salacious, the provocative.

77
0:04:32.365,000 --> 0:04:35,000
We tend to believe and to share information

78
0:04:35.436,000 --> 0:04:37,000
that's negative and novel.

79
0:04:37.809,000 --> 0:04:42,000
And researchers have found that online hoaxes spread 10 times faster

80
0:04:42.852,000 --> 0:04:43,000
than accurate stories.

81
0:04:46.015,000 --> 0:04:5,000
Now, we're also drawn to information

82
0:04:50.419,000 --> 0:04:51,000
that aligns with our viewpoints.

83
0:04:52.95,000 --> 0:04:55,000
Psychologists call that tendency "confirmation bias."

84
0:04:57.3,000 --> 0:05:01,000
And social media platforms supercharge that tendency,

85
0:05:01.711,000 --> 0:05:04,000
by allowing us to instantly and widely share information

86
0:05:05.616,000 --> 0:05:06,000
that accords with our viewpoints.

87
0:05:08.735,000 --> 0:05:13,000
Now, deepfakes have the potential to cause grave individual and societal harm.

88
0:05:15.204,000 --> 0:05:17,000
So, imagine a deepfake

89
0:05:17.252,000 --> 0:05:21,000
that shows American soldiers in Afganistan burning a Koran.

90
0:05:22.807,000 --> 0:05:25,000
You can imagine that that deepfake would provoke violence

91
0:05:25.855,000 --> 0:05:26,000
against those soldiers.

92
0:05:27.847,000 --> 0:05:29,000
And what if the very next day

93
0:05:30.744,000 --> 0:05:32,000
there's another deepfake that drops,

94
0:05:33.022,000 --> 0:05:36,000
that shows a well-known imam based in London

95
0:05:36.363,000 --> 0:05:38,000
praising the attack on those soldiers?

96
0:05:39.617,000 --> 0:05:42,000
We might see violence and civil unrest,

97
0:05:42.804,000 --> 0:05:45,000
not only in Afganistan and the United Kingdom,

98
0:05:46.077,000 --> 0:05:47,000
but across the globe.

99
0:05:48.251,000 --> 0:05:49,000
And you might say to me,

100
0:05:49.433,000 --> 0:05:51,000
"Come on, Danielle, that's far-fetched."

101
0:05:51.704,000 --> 0:05:52,000
But it's not.

102
0:05:53.293,000 --> 0:05:55,000
We've seen falsehoods spread

103
0:05:55.508,000 --> 0:05:57,000
on WhatsApp and other online message services

104
0:05:58.254,000 --> 0:06:,000
lead to violence against ethnic minorities.

105
0:06:01.039,000 --> 0:06:02,000
And that was just text --

106
0:06:02.95,000 --> 0:06:04,000
imagine if it were video.

107
0:06:06.593,000 --> 0:06:11,000
Now, deepfakes have the potential to corrode the trust that we have

108
0:06:11.974,000 --> 0:06:12,000
in democratic institutions.

109
0:06:15.006,000 --> 0:06:17,000
So, imagine the night before an election.

110
0:06:17.996,000 --> 0:06:2,000
There's a deepfake showing one of the major party candidates

111
0:06:21.258,000 --> 0:06:22,000
gravely sick.

112
0:06:23.202,000 --> 0:06:25,000
The deepfake could tip the election

113
0:06:25.559,000 --> 0:06:28,000
and shake our sense that elections are legitimate.

114
0:06:30.515,000 --> 0:06:33,000
Imagine if the night before an initial public offering

115
0:06:33.865,000 --> 0:06:35,000
of a major global bank,

116
0:06:36.222,000 --> 0:06:39,000
there was a deepfake showing the bank's CEO

117
0:06:39.395,000 --> 0:06:41,000
drunkenly spouting conspiracy theories.

118
0:06:42.887,000 --> 0:06:45,000
The deepfake could tank the IPO,

119
0:06:45.958,000 --> 0:06:49,000
and worse, shake our sense that financial markets are stable.

120
0:06:51.385,000 --> 0:06:57,000
So deepfakes can exploit and magnify the deep distrust that we already have

121
0:06:58.398,000 --> 0:07:02,000
in politicians, business leaders and other influential leaders.

122
0:07:02.945,000 --> 0:07:05,000
They find an audience primed to believe them.

123
0:07:07.287,000 --> 0:07:09,000
And the pursuit of truth is on the line as well.

124
0:07:11.077,000 --> 0:07:14,000
Technologists expect that with advances in AI,

125
0:07:14.665,000 --> 0:07:17,000
soon it may be difficult if not impossible

126
0:07:18.371,000 --> 0:07:21,000
to tell the difference between a real video and a fake one.

127
0:07:23.022,000 --> 0:07:28,000
So how can the truth emerge in a deepfake-ridden marketplace of ideas?

128
0:07:28.752,000 --> 0:07:31,000
Will we just proceed along the path of least resistance

129
0:07:32.196,000 --> 0:07:34,000
and believe what we want to believe,

130
0:07:34.657,000 --> 0:07:35,000
truth be damned?

131
0:07:36.831,000 --> 0:07:39,000
And not only might we believe the fakery,

132
0:07:40.03,000 --> 0:07:43,000
we might start disbelieving the truth.

133
0:07:43.887,000 --> 0:07:47,000
We've already seen people invoke the phenomenon of deepfakes

134
0:07:47.99,000 --> 0:07:5,000
to cast doubt on real evidence of their wrongdoing.

135
0:07:51.934,000 --> 0:07:56,000
We've heard politicians say of audio of their disturbing comments,

136
0:07:57.927,000 --> 0:07:58,000
"Come on, that's fake news.

137
0:07:59.697,000 --> 0:08:02,000
You can't believe what your eyes and ears are telling you."

138
0:08:04.402,000 --> 0:08:05,000
And it's that risk

139
0:08:06.157,000 --> 0:08:11,000
that professor Robert Chesney and I call the "liar's dividend":

140
0:08:11.617,000 --> 0:08:14,000
the risk that liars will invoke deepfakes

141
0:08:14.998,000 --> 0:08:16,000
to escape accountability for their wrongdoing.

142
0:08:18.963,000 --> 0:08:21,000
So we've got our work cut out for us, there's no doubt about it.

143
0:08:22.606,000 --> 0:08:25,000
And we're going to need a proactive solution

144
0:08:25.955,000 --> 0:08:28,000
from tech companies, from lawmakers,

145
0:08:29.49,000 --> 0:08:3,000
law enforcers and the media.

146
0:08:32.093,000 --> 0:08:36,000
And we're going to need a healthy dose of societal resilience.

147
0:08:37.506,000 --> 0:08:4,000
So now, we're right now engaged in a very public conversation

148
0:08:41.426,000 --> 0:08:43,000
about the responsibility of tech companies.

149
0:08:44.926,000 --> 0:08:47,000
And my advice to social media platforms

150
0:08:47.982,000 --> 0:08:5,000
has been to change their terms of service and community guidelines

151
0:08:51.879,000 --> 0:08:53,000
to ban deepfakes that cause harm.

152
0:08:54.712,000 --> 0:08:57,000
That determination, that's going to require human judgment,

153
0:08:58.696,000 --> 0:08:59,000
and it's expensive.

154
0:09:00.673,000 --> 0:09:02,000
But we need human beings

155
0:09:02.982,000 --> 0:09:05,000
to look at the content and context of a deepfake

156
0:09:06.879,000 --> 0:09:09,000
to figure out if it is a harmful impersonation

157
0:09:10.585,000 --> 0:09:14,000
or instead, if it's valuable satire, art or education.

158
0:09:16.118,000 --> 0:09:17,000
So now, what about the law?

159
0:09:18.666,000 --> 0:09:2,000
Law is our educator.

160
0:09:21.515,000 --> 0:09:25,000
It teaches us about what's harmful and what's wrong.

161
0:09:25.577,000 --> 0:09:29,000
And it shapes behavior it deters by punishing perpetrators

162
0:09:30.156,000 --> 0:09:32,000
and securing remedies for victims.

163
0:09:33.148,000 --> 0:09:37,000
Right now, law is not up to the challenge of deepfakes.

164
0:09:38.116,000 --> 0:09:39,000
Across the globe,

165
0:09:39.53,000 --> 0:09:41,000
we lack well-tailored laws

166
0:09:41.998,000 --> 0:09:44,000
that would be designed to tackle digital impersonations

167
0:09:45.592,000 --> 0:09:47,000
that invade sexual privacy,

168
0:09:47.847,000 --> 0:09:48,000
that damage reputations

169
0:09:49.258,000 --> 0:09:5,000
and that cause emotional distress.

170
0:09:51.725,000 --> 0:09:54,000
What happened to Rana Ayyub is increasingly commonplace.

171
0:09:56.074,000 --> 0:09:58,000
Yet, when she went to law enforcement in Delhi,

172
0:09:58.312,000 --> 0:10:,000
she was told nothing could be done.

173
0:10:01.101,000 --> 0:10:04,000
And the sad truth is that the same would be true

174
0:10:04.308,000 --> 0:10:06,000
in the United States and in Europe.

175
0:10:07.3,000 --> 0:10:11,000
So we have a legal vacuum that needs to be filled.

176
0:10:12.292,000 --> 0:10:16,000
My colleague Dr. Mary Anne Franks and I are working with US lawmakers

177
0:10:16.408,000 --> 0:10:2,000
to devise legislation that would ban harmful digital impersonations

178
0:10:21.236,000 --> 0:10:23,000
that are tantamount to identity theft.

179
0:10:24.252,000 --> 0:10:26,000
And we've seen similar moves

180
0:10:26.402,000 --> 0:10:29,000
in Iceland, the UK and Australia.

181
0:10:30.157,000 --> 0:10:33,000
But of course, that's just a small piece of the regulatory puzzle.

182
0:10:34.911,000 --> 0:10:37,000
Now, I know law is not a cure-all. Right?

183
0:10:38.104,000 --> 0:10:39,000
It's a blunt instrument.

184
0:10:40.346,000 --> 0:10:41,000
And we've got to use it wisely.

185
0:10:42.411,000 --> 0:10:44,000
It also has some practical impediments.

186
0:10:45.657,000 --> 0:10:5,000
You can't leverage law against people you can't identify and find.

187
0:10:51.463,000 --> 0:10:54,000
And if a perpetrator lives outside the country

188
0:10:54.773,000 --> 0:10:55,000
where a victim lives,

189
0:10:56.551,000 --> 0:10:57,000
then you may not be able to insist

190
0:10:58.204,000 --> 0:11:,000
that the perpetrator come into local courts

191
0:11:00.577,000 --> 0:11:01,000
to face justice.

192
0:11:02.236,000 --> 0:11:06,000
And so we're going to need a coordinated international response.

193
0:11:07.819,000 --> 0:11:1,000
Education has to be part of our response as well.

194
0:11:11.803,000 --> 0:11:14,000
Law enforcers are not going to enforce laws

195
0:11:15.558,000 --> 0:11:16,000
they don't know about

196
0:11:17.04,000 --> 0:11:19,000
and proffer problems they don't understand.

197
0:11:20.376,000 --> 0:11:22,000
In my research on cyberstalking,

198
0:11:22.591,000 --> 0:11:25,000
I found that law enforcement lacked the training

199
0:11:26.114,000 --> 0:11:28,000
to understand the laws available to them

200
0:11:28.72,000 --> 0:11:3,000
and the problem of online abuse.

201
0:11:31.093,000 --> 0:11:33,000
And so often they told victims,

202
0:11:33.799,000 --> 0:11:36,000
"Just turn your computer off. Ignore it. It'll go away."

203
0:11:38.261,000 --> 0:11:4,000
And we saw that in Rana Ayyub's case.

204
0:11:41.102,000 --> 0:11:44,000
She was told, "Come on, you're making such a big deal about this.

205
0:11:44.594,000 --> 0:11:45,000
It's boys being boys."

206
0:11:47.268,000 --> 0:11:52,000
And so we need to pair new legislation with efforts at training.

207
0:11:54.053,000 --> 0:11:57,000
And education has to be aimed on the media as well.

208
0:11:58.18,000 --> 0:12:02,000
Journalists need educating about the phenomenon of deepfakes

209
0:12:02.464,000 --> 0:12:05,000
so they don't amplify and spread them.

210
0:12:06.583,000 --> 0:12:08,000
And this is the part where we're all involved.

211
0:12:08.775,000 --> 0:12:11,000
Each and every one of us needs educating.

212
0:12:13.375,000 --> 0:12:16,000
We click, we share, we like, and we don't even think about it.

213
0:12:17.551,000 --> 0:12:18,000
We need to do better.

214
0:12:19.726,000 --> 0:12:21,000
We need far better radar for fakery.

215
0:12:25.744,000 --> 0:12:28,000
So as we're working through these solutions,

216
0:12:29.609,000 --> 0:12:31,000
there's going to be a lot of suffering to go around.

217
0:12:33.093,000 --> 0:12:35,000
Rana Ayyub is still wrestling with the fallout.

218
0:12:36.669,000 --> 0:12:4,000
She still doesn't feel free to express herself on- and offline.

219
0:12:41.566,000 --> 0:12:42,000
And as she told me,

220
0:12:42.955,000 --> 0:12:47,000
she still feels like there are thousands of eyes on her naked body,

221
0:12:48.053,000 --> 0:12:51,000
even though, intellectually, she knows it wasn't her body.

222
0:12:52.371,000 --> 0:12:54,000
And she has frequent panic attacks,

223
0:12:54.744,000 --> 0:12:58,000
especially when someone she doesn't know tries to take her picture.

224
0:12:58.868,000 --> 0:13:01,000
"What if they're going to make another deepfake?" she thinks to herself.

225
0:13:03.082,000 --> 0:13:06,000
And so for the sake of individuals like Rana Ayyub

226
0:13:07.027,000 --> 0:13:09,000
and the sake of our democracy,

227
0:13:09.357,000 --> 0:13:11,000
we need to do something right now.

228
0:13:11.563,000 --> 0:13:12,000
Thank you.

229
0:13:12.738,000 --> 0:13:14,000
(Applause)

