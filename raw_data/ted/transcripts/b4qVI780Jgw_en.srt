1
0:00:12.802,000 --> 0:00:16,000
I'm here, because I've spent far too many nights lying awake,

2
0:00:17.397,000 --> 0:00:19,000
worrying and wondering who wins in the end.

3
0:00:19.907,000 --> 0:00:21,000
Is it humans or is it robots?

4
0:00:22.918,000 --> 0:00:23,000
You see, as a technology strategist,

5
0:00:24.665,000 --> 0:00:25,000
my job involves behavior change:

6
0:00:26.593,000 --> 0:00:29,000
understanding why and how people adopt new technologies.

7
0:00:30.101,000 --> 0:00:32,000
And that means I'm really frustrated

8
0:00:32.498,000 --> 0:00:35,000
that I know I won't live to see how this all ends up.

9
0:00:36.577,000 --> 0:00:39,000
And in fact, if the youngest person watching this is 14

10
0:00:39.874,000 --> 0:00:41,000
and the oldest, a robust 99,

11
0:00:42.711,000 --> 0:00:43,000
then together,

12
0:00:43.887,000 --> 0:00:47,000
our collective consciousnesses span just 185 years.

13
0:00:48.593,000 --> 0:00:51,000
That is a myopic pinprick of time

14
0:00:51.983,000 --> 0:00:54,000
when you think of the evolution and the story of life on this planet.

15
0:00:55.912,000 --> 0:00:56,000
Turns out we're all in the cheap seats

16
0:00:57.761,000 --> 0:00:59,000
and none of us will live to see how it all pans out.

17
0:01:00.777,000 --> 0:01:02,000
So at my company, we wanted a way around this.

18
0:01:03.149,000 --> 0:01:05,000
We wanted to see if there was a way to cantilever out,

19
0:01:05.911,000 --> 0:01:08,000
beyond our fixed temporal vantage point,

20
0:01:08.943,000 --> 0:01:1,000
to get a sense of how it all shakes up.

21
0:01:11.237,000 --> 0:01:14,000
And to do this, we conducted a study amongst 1,200 Americans

22
0:01:15.238,000 --> 0:01:17,000
representative of the US census,

23
0:01:17.412,000 --> 0:01:19,000
in which we asked a battery of attitudinal questions

24
0:01:19.96,000 --> 0:01:2,000
around robotics and AI

25
0:01:21.785,000 --> 0:01:24,000
and also captured behavioral ones around technology adoption.

26
0:01:26.011,000 --> 0:01:27,000
We had a big study

27
0:01:27.32,000 --> 0:01:29,000
so that we could analyze differences in gender and generations,

28
0:01:30.321,000 --> 0:01:31,000
between religious and political beliefs,

29
0:01:32.313,000 --> 0:01:34,000
even job function and personality trait.

30
0:01:35.106,000 --> 0:01:38,000
It is a fascinating, time-bound time capsule

31
0:01:38.633,000 --> 0:01:39,000
of our human frailty

32
0:01:40.323,000 --> 0:01:42,000
in this predawn of the robotic era.

33
0:01:43.22,000 --> 0:01:45,000
And I have five minutes to tell you about it.

34
0:01:46.26,000 --> 0:01:48,000
The first thing you should know is that we brainstormed

35
0:01:48.863,000 --> 0:01:54,000
a list of scenarios of current and potential AI robotics.

36
0:01:55.693,000 --> 0:01:57,000
They ran the spectrum from the mundane,

37
0:01:58.259,000 --> 0:01:59,000
so, a robot house cleaner, anyone?

38
0:02:00.259,000 --> 0:02:01,000
Through to the mischievous,

39
0:02:01.585,000 --> 0:02:04,000
the idea of a robot pet sitter, or maybe a robot lawyer,

40
0:02:04.704,000 --> 0:02:05,000
or maybe a sex partner.

41
0:02:06.553,000 --> 0:02:08,000
Through to the downright macabre, the idea of being a cyborg,

42
0:02:09.498,000 --> 0:02:1,000
blending human and robot,

43
0:02:10.815,000 --> 0:02:13,000
or uploading your brain so it could live on after your death.

44
0:02:15.045,000 --> 0:02:18,000
And we plotted people's comfort levels with these various scenarios.

45
0:02:18.966,000 --> 0:02:19,000
There were actually 31 in the study,

46
0:02:20.72,000 --> 0:02:23,000
but for ease, I'm going to show you just a few of them here.

47
0:02:24.355,000 --> 0:02:26,000
The first thing you'll notice, of course, is the sea of red.

48
0:02:27.212,000 --> 0:02:29,000
America is very uncomfortable with this stuff.

49
0:02:30.901,000 --> 0:02:32,000
That's why we call it the discomfort index,

50
0:02:33.76,000 --> 0:02:34,000
not the comfort index.

51
0:02:35.204,000 --> 0:02:38,000
There were only two things the majority of America is OK with.

52
0:02:38.919,000 --> 0:02:4,000
And that's the idea of a robot AI house cleaner

53
0:02:41.817,000 --> 0:02:42,000
and a robot AI package deliverer,

54
0:02:43.833,000 --> 0:02:45,000
so Dyson and Amazon, you guys should talk.

55
0:02:46.848,000 --> 0:02:47,000
There's an opportunity there.

56
0:02:48.42,000 --> 0:02:51,000
It seems we're ready to off-load our chores to our robot friends.

57
0:02:52.404,000 --> 0:02:55,000
We're kind of definitely on the fence when it comes to services,

58
0:02:55.443,000 --> 0:02:58,000
so robot AI lawyer or a financial adviser, maybe.

59
0:02:59.237,000 --> 0:03:01,000
But we're firmly closed to the idea of robot care,

60
0:03:02.046,000 --> 0:03:04,000
whether it be a nurse, a doctor, child care.

61
0:03:04.509,000 --> 0:03:05,000
So from this, you'd go,

62
0:03:05.886,000 --> 0:03:06,000
"It's OK, Lucy, you know what?

63
0:03:07.363,000 --> 0:03:1,000
Go back to sleep, stop worrying, the humans win in the end."

64
0:03:10.442,000 --> 0:03:11,000
But actually not so fast.

65
0:03:11.688,000 --> 0:03:12,000
If you look at my data very closely,

66
0:03:13.434,000 --> 0:03:15,000
you can see we're more vulnerable than we think.

67
0:03:15.704,000 --> 0:03:16,000
AI has a branding problem.

68
0:03:17.013,000 --> 0:03:19,000
So of those folks who said

69
0:03:19.194,000 --> 0:03:22,000
that they would absolutely reject the idea of a personal assistant,

70
0:03:22.361,000 --> 0:03:24,000
45 percent of them had, in fact, one in their pockets,

71
0:03:25.271,000 --> 0:03:28,000
in terms of a device with Alexa, Google or Siri.

72
0:03:28.898,000 --> 0:03:31,000
One in five of those who were against the idea of AI matchmaking

73
0:03:31.994,000 --> 0:03:33,000
had of course, you guessed it, done online dating.

74
0:03:34.673,000 --> 0:03:36,000
And 80 percent of those of us who refuse the idea

75
0:03:36.986,000 --> 0:03:38,000
of boarding an autonomous plane with a pilot backup

76
0:03:39.466,000 --> 0:03:41,000
had in fact, just like me to get here to Vancouver,

77
0:03:42.045,000 --> 0:03:43,000
flown commercial.

78
0:03:43.236,000 --> 0:03:45,000
Lest you think everybody was scared, though,

79
0:03:45.315,000 --> 0:03:47,000
here are the marvelous folk in the middle.

80
0:03:47.474,000 --> 0:03:48,000
These are the neutrals.

81
0:03:48.745,000 --> 0:03:49,000
These are people for whom you say,

82
0:03:50.411,000 --> 0:03:51,000
"OK, robot friend,"

83
0:03:51.704,000 --> 0:03:53,000
and they're like, "Hm, robot friend. Maybe."

84
0:03:54.663,000 --> 0:03:55,000
Or, "AI pet,"

85
0:03:56.308,000 --> 0:03:58,000
and they go, "Never say never."

86
0:03:58.843,000 --> 0:04:,000
And as any decent political operative knows,

87
0:04:01.533,000 --> 0:04:03,000
flipping the ambivalent middle can change the game.

88
0:04:04.644,000 --> 0:04:06,000
Another reason I know we're vulnerable is men --

89
0:04:06.987,000 --> 0:04:08,000
I'm sorry, but men, you are twice as likely than women

90
0:04:09.587,000 --> 0:04:12,000
to believe that getting into an autonomous car is a good idea,

91
0:04:13.022,000 --> 0:04:15,000
that uploading your brain for posterity is fun,

92
0:04:15.863,000 --> 0:04:18,000
and two and a half times more likely to believe that becoming a cyborg is cool,

93
0:04:19.609,000 --> 0:04:2,000
and for this, I blame Hollywood.

94
0:04:21.312,000 --> 0:04:22,000
(Laughter)

95
0:04:22.632,000 --> 0:04:24,000
And this is where I want you to look around the theater

96
0:04:25.252,000 --> 0:04:28,000
and know that one in four men are OK with the idea of sex with a robot.

97
0:04:28.633,000 --> 0:04:31,000
That goes up to 44 percent of millennial men

98
0:04:31.794,000 --> 0:04:32,000
compared to just one in 10 women,

99
0:04:33.412,000 --> 0:04:36,000
which I think puts a whole new twist on the complaint of mechanical sex.

100
0:04:36.825,000 --> 0:04:37,000
(Laughter)

101
0:04:38.817,000 --> 0:04:4,000
Even more astounding than that though, to be honest,

102
0:04:41.27,000 --> 0:04:42,000
is this behavioral difference.

103
0:04:42.864,000 --> 0:04:46,000
So here we have people who have a device with a voice assistant in it,

104
0:04:47.101,000 --> 0:04:49,000
so a smart speaker, a home hub or a smart phone,

105
0:04:49.871,000 --> 0:04:5,000
versus those who don't.

106
0:04:51.617,000 --> 0:04:52,000
And you can see from this graph

107
0:04:53.141,000 --> 0:04:56,000
that the Trojan horse is already in our living room.

108
0:04:56.888,000 --> 0:04:58,000
And as these devices proliferate

109
0:04:58.999,000 --> 0:05:01,000
and our collective defenses soften,

110
0:05:02.24,000 --> 0:05:03,000
we all see how it can end.

111
0:05:04.261,000 --> 0:05:06,000
In fact, this may be as good a time as any to admit

112
0:05:06.674,000 --> 0:05:08,000
I did take my Alexa Dot on vacation with me.

113
0:05:10.192,000 --> 0:05:12,000
Final finding I have time for is generational.

114
0:05:12.367,000 --> 0:05:14,000
So look at the difference just three generations make.

115
0:05:14.946,000 --> 0:05:17,000
This is the leap from silent to boomer to millennial.

116
0:05:18.125,000 --> 0:05:21,000
And what's more fascinating than this is if you extrapolate this out,

117
0:05:21.394,000 --> 0:05:22,000
the same rate of change,

118
0:05:22.633,000 --> 0:05:23,000
just the same pace,

119
0:05:23.847,000 --> 0:05:25,000
not the accelerated one I actually believe will be the case,

120
0:05:26.706,000 --> 0:05:27,000
the same pace,

121
0:05:27.905,000 --> 0:05:28,000
then it is eight generations away

122
0:05:29.626,000 --> 0:05:31,000
when we hear every single American

123
0:05:31.99,000 --> 0:05:34,000
thinking the majority of these things here are normal.

124
0:05:35.395,000 --> 0:05:38,000
So the year 2222 is an astounding place

125
0:05:38.458,000 --> 0:05:4,000
where everything here is mainstream.

126
0:05:40.776,000 --> 0:05:41,000
And lest you needed any more convincing,

127
0:05:42.745,000 --> 0:05:44,000
here is the generation's "excitement level with AI."

128
0:05:45.577,000 --> 0:05:48,000
So not surprisingly, the youngest of us are more excited.

129
0:05:49.331,000 --> 0:05:52,000
But, and possibly the most paradoxical finding of my career,

130
0:05:53.331,000 --> 0:05:56,000
when I asked these people my 3am question,

131
0:05:56.355,000 --> 0:05:57,000
"Who wins in the end?"

132
0:05:58.188,000 --> 0:05:59,000
Guess what.

133
0:05:59.736,000 --> 0:06:01,000
The more excited you are about AI and robotics,

134
0:06:01.998,000 --> 0:06:03,000
the more likely you are to say it's the robots.

135
0:06:05.347,000 --> 0:06:08,000
And I don't think we need a neural net running pattern-recognition software

136
0:06:08.903,000 --> 0:06:09,000
to see where this is all headed.

137
0:06:10.632,000 --> 0:06:12,000
We are the proverbial frogs in boiling water.

138
0:06:13.361,000 --> 0:06:18,000
So if the robots at TED2222 are watching this for posterity,

139
0:06:18.538,000 --> 0:06:2,000
could you send a cyborg, dig me up and tell me if I was right?

140
0:06:21.538,000 --> 0:06:22,000
(Laughter)

141
0:06:22.727,000 --> 0:06:23,000
Thank you.

142
0:06:23.928,000 --> 0:06:24,000
(Applause)

