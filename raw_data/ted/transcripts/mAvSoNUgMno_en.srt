1
0:00:12.984,000 --> 0:00:13,000
My job at Twitter

2
0:00:14.275,000 --> 0:00:15,000
is to ensure user trust,

3
0:00:16.253,000 --> 0:00:18,000
protect user rights and keep users safe,

4
0:00:19.09,000 --> 0:00:2,000
both from each other

5
0:00:20.35,000 --> 0:00:23,000
and, at times, from themselves.

6
0:00:24.249,000 --> 0:00:28,000
Let's talk about what scale looks like at Twitter.

7
0:00:28.524,000 --> 0:00:3,000
Back in January 2009,

8
0:00:31.394,000 --> 0:00:34,000
we saw more than two million new tweets each day

9
0:00:34.725,000 --> 0:00:35,000
on the platform.

10
0:00:36.489,000 --> 0:00:41,000
January 2014, more than 500 million.

11
0:00:42.397,000 --> 0:00:44,000
We were seeing two million tweets

12
0:00:44.889,000 --> 0:00:46,000
in less than six minutes.

13
0:00:47.065,000 --> 0:00:53,000
That's a 24,900-percent increase.

14
0:00:54.049,000 --> 0:00:57,000
Now, the vast majority of activity on Twitter

15
0:00:57.302,000 --> 0:00:58,000
puts no one in harm's way.

16
0:00:58.805,000 --> 0:00:59,000
There's no risk involved.

17
0:01:00.74,000 --> 0:01:05,000
My job is to root out and prevent activity that might.

18
0:01:06.493,000 --> 0:01:07,000
Sounds straightforward, right?

19
0:01:08.466,000 --> 0:01:09,000
You might even think it'd be easy,

20
0:01:09.618,000 --> 0:01:11,000
given that I just said the vast majority

21
0:01:11.788,000 --> 0:01:14,000
of activity on Twitter puts no one in harm's way.

22
0:01:15.598,000 --> 0:01:17,000
Why spend so much time

23
0:01:17.767,000 --> 0:01:19,000
searching for potential calamities

24
0:01:20.51,000 --> 0:01:22,000
in innocuous activities?

25
0:01:23.41,000 --> 0:01:25,000
Given the scale that Twitter is at,

26
0:01:26.35,000 --> 0:01:28,000
a one-in-a-million chance happens

27
0:01:28.707,000 --> 0:01:32,000
500 times a day.

28
0:01:33.583,000 --> 0:01:34,000
It's the same for other companies

29
0:01:35.028,000 --> 0:01:36,000
dealing at this sort of scale.

30
0:01:36.499,000 --> 0:01:37,000
For us, edge cases,

31
0:01:38.207,000 --> 0:01:41,000
those rare situations that are unlikely to occur,

32
0:01:41.832,000 --> 0:01:43,000
are more like norms.

33
0:01:44.454,000 --> 0:01:47,000
Say 99.999 percent of tweets

34
0:01:48.396,000 --> 0:01:49,000
pose no risk to anyone.

35
0:01:50.284,000 --> 0:01:51,000
There's no threat involved.

36
0:01:51.35,000 --> 0:01:53,000
Maybe people are documenting travel landmarks

37
0:01:54.304,000 --> 0:01:55,000
like Australia's Heart Reef,

38
0:01:56.267,000 --> 0:01:58,000
or tweeting about a concert they're attending,

39
0:01:59.188,000 --> 0:02:03,000
or sharing pictures of cute baby animals.

40
0:02:03.935,000 --> 0:02:07,000
After you take out that 99.999 percent,

41
0:02:08.444,000 --> 0:02:11,000
that tiny percentage of tweets remaining

42
0:02:11.973,000 --> 0:02:13,000
works out to roughly

43
0:02:14.362,000 --> 0:02:17,000
150,000 per month.

44
0:02:17.837,000 --> 0:02:19,000
The sheer scale of what we're dealing with

45
0:02:20.293,000 --> 0:02:22,000
makes for a challenge.

46
0:02:22.605,000 --> 0:02:23,000
You know what else makes my role

47
0:02:23.783,000 --> 0:02:26,000
particularly challenging?

48
0:02:26.89,000 --> 0:02:31,000
People do weird things.

49
0:02:32.013,000 --> 0:02:33,000
(Laughter)

50
0:02:33.842,000 --> 0:02:35,000
And I have to figure out what they're doing,

51
0:02:36.233,000 --> 0:02:38,000
why, and whether or not there's risk involved,

52
0:02:38.482,000 --> 0:02:4,000
often without much in terms of context

53
0:02:40.65,000 --> 0:02:41,000
or background.

54
0:02:42.497,000 --> 0:02:44,000
I'm going to show you some examples

55
0:02:44.574,000 --> 0:02:46,000
that I've run into during my time at Twitter --

56
0:02:46.579,000 --> 0:02:47,000
these are all real examples —

57
0:02:48.199,000 --> 0:02:5,000
of situations that at first seemed cut and dried,

58
0:02:50.852,000 --> 0:02:51,000
but the truth of the matter was something

59
0:02:52.495,000 --> 0:02:53,000
altogether different.

60
0:02:54.045,000 --> 0:02:55,000
The details have been changed

61
0:02:56.022,000 --> 0:02:57,000
to protect the innocent

62
0:02:57.279,000 --> 0:03:,000
and sometimes the guilty.

63
0:03:00.512,000 --> 0:03:03,000
We'll start off easy.

64
0:03:03.517,000 --> 0:03:04,000
["Yo bitch"]

65
0:03:05.31,000 --> 0:03:08,000
If you saw a Tweet that only said this,

66
0:03:08.538,000 --> 0:03:09,000
you might think to yourself,

67
0:03:10.232,000 --> 0:03:11,000
"That looks like abuse."

68
0:03:11.885,000 --> 0:03:14,000
After all, why would you want to receive the message,

69
0:03:14.992,000 --> 0:03:16,000
"Yo, bitch."

70
0:03:17.21,000 --> 0:03:21,000
Now, I try to stay relatively hip

71
0:03:21.873,000 --> 0:03:23,000
to the latest trends and memes,

72
0:03:24.385,000 --> 0:03:26,000
so I knew that "yo, bitch"

73
0:03:27.089,000 --> 0:03:3,000
was also often a common greeting between friends,

74
0:03:30.243,000 --> 0:03:34,000
as well as being a popular "Breaking Bad" reference.

75
0:03:34.505,000 --> 0:03:36,000
I will admit that I did not expect

76
0:03:36.992,000 --> 0:03:38,000
to encounter a fourth use case.

77
0:03:39.833,000 --> 0:03:42,000
It turns out it is also used on Twitter

78
0:03:42.937,000 --> 0:03:45,000
when people are role-playing as dogs.

79
0:03:45.999,000 --> 0:03:5,000
(Laughter)

80
0:03:51.278,000 --> 0:03:52,000
And in fact, in that case,

81
0:03:52.944,000 --> 0:03:53,000
it's not only not abusive,

82
0:03:54.553,000 --> 0:03:57,000
it's technically just an accurate greeting.

83
0:03:57.692,000 --> 0:03:59,000
(Laughter)

84
0:04:00.581,000 --> 0:04:02,000
So okay, determining whether or not

85
0:04:02.652,000 --> 0:04:03,000
something is abusive without context,

86
0:04:04.5,000 --> 0:04:05,000
definitely hard.

87
0:04:06.092,000 --> 0:04:08,000
Let's look at spam.

88
0:04:08.809,000 --> 0:04:09,000
Here's an example of an account engaged

89
0:04:10.769,000 --> 0:04:11,000
in classic spammer behavior,

90
0:04:12.437,000 --> 0:04:13,000
sending the exact same message

91
0:04:13.996,000 --> 0:04:14,000
to thousands of people.

92
0:04:15.8,000 --> 0:04:17,000
While this is a mockup I put together using my account,

93
0:04:18.593,000 --> 0:04:21,000
we see accounts doing this all the time.

94
0:04:21.594,000 --> 0:04:22,000
Seems pretty straightforward.

95
0:04:23.573,000 --> 0:04:25,000
We should just automatically suspend accounts

96
0:04:25.626,000 --> 0:04:28,000
engaging in this kind of behavior.

97
0:04:28.933,000 --> 0:04:31,000
Turns out there's some exceptions to that rule.

98
0:04:32.143,000 --> 0:04:34,000
Turns out that that message could also be a notification

99
0:04:35.026,000 --> 0:04:38,000
you signed up for that the International Space Station is passing overhead

100
0:04:38.915,000 --> 0:04:39,000
because you wanted to go outside

101
0:04:40.761,000 --> 0:04:41,000
and see if you could see it.

102
0:04:42.709,000 --> 0:04:43,000
You're not going to get that chance

103
0:04:43.934,000 --> 0:04:44,000
if we mistakenly suspend the account

104
0:04:45.781,000 --> 0:04:47,000
thinking it's spam.

105
0:04:48.047,000 --> 0:04:51,000
Okay. Let's make the stakes higher.

106
0:04:51.573,000 --> 0:04:52,000
Back to my account,

107
0:04:53.489,000 --> 0:04:56,000
again exhibiting classic behavior.

108
0:04:56.994,000 --> 0:04:58,000
This time it's sending the same message and link.

109
0:04:59.637,000 --> 0:05:01,000
This is often indicative of something called phishing,

110
0:05:02.411,000 --> 0:05:05,000
somebody trying to steal another person's account information

111
0:05:05.589,000 --> 0:05:07,000
by directing them to another website.

112
0:05:07.792,000 --> 0:05:11,000
That's pretty clearly not a good thing.

113
0:05:11.986,000 --> 0:05:12,000
We want to, and do, suspend accounts

114
0:05:13.916,000 --> 0:05:15,000
engaging in that kind of behavior.

115
0:05:16.54,000 --> 0:05:19,000
So why are the stakes higher for this?

116
0:05:19.787,000 --> 0:05:21,000
Well, this could also be a bystander at a rally

117
0:05:22.786,000 --> 0:05:23,000
who managed to record a video

118
0:05:24.696,000 --> 0:05:27,000
of a police officer beating a non-violent protester

119
0:05:27.966,000 --> 0:05:29,000
who's trying to let the world know what's happening.

120
0:05:30.941,000 --> 0:05:31,000
We don't want to gamble

121
0:05:32.584,000 --> 0:05:34,000
on potentially silencing that crucial speech

122
0:05:35.101,000 --> 0:05:37,000
by classifying it as spam and suspending it.

123
0:05:38.03,000 --> 0:05:4,000
That means we evaluate hundreds of parameters

124
0:05:40.909,000 --> 0:05:41,000
when looking at account behaviors,

125
0:05:42.597,000 --> 0:05:44,000
and even then, we can still get it wrong

126
0:05:44.613,000 --> 0:05:46,000
and have to reevaluate.

127
0:05:46.849,000 --> 0:05:49,000
Now, given the sorts of challenges I'm up against,

128
0:05:50.557,000 --> 0:05:52,000
it's crucial that I not only predict

129
0:05:53.253,000 --> 0:05:56,000
but also design protections for the unexpected.

130
0:05:57.037,000 --> 0:05:59,000
And that's not just an issue for me,

131
0:05:59.379,000 --> 0:06:01,000
or for Twitter, it's an issue for you.

132
0:06:01.466,000 --> 0:06:03,000
It's an issue for anybody who's building or creating

133
0:06:03.872,000 --> 0:06:04,000
something that you think is going to be amazing

134
0:06:05.797,000 --> 0:06:07,000
and will let people do awesome things.

135
0:06:08.586,000 --> 0:06:1,000
So what do I do?

136
0:06:11.452,000 --> 0:06:14,000
I pause and I think,

137
0:06:14.77,000 --> 0:06:16,000
how could all of this

138
0:06:16.865,000 --> 0:06:19,000
go horribly wrong?

139
0:06:20.658,000 --> 0:06:24,000
I visualize catastrophe.

140
0:06:25.111,000 --> 0:06:27,000
And that's hard. There's a sort of

141
0:06:27.574,000 --> 0:06:29,000
inherent cognitive dissonance in doing that,

142
0:06:30.422,000 --> 0:06:31,000
like when you're writing your wedding vows

143
0:06:32.234,000 --> 0:06:34,000
at the same time as your prenuptial agreement.

144
0:06:34.88,000 --> 0:06:35,000
(Laughter)

145
0:06:36.576,000 --> 0:06:38,000
But you still have to do it,

146
0:06:38.949,000 --> 0:06:42,000
particularly if you're marrying 500 million tweets per day.

147
0:06:43.395,000 --> 0:06:46,000
What do I mean by "visualize catastrophe?"

148
0:06:46.492,000 --> 0:06:48,000
I try to think of how something as

149
0:06:49.254,000 --> 0:06:52,000
benign and innocuous as a picture of a cat

150
0:06:52.482,000 --> 0:06:53,000
could lead to death,

151
0:06:53.586,000 --> 0:06:55,000
and what to do to prevent that.

152
0:06:55.912,000 --> 0:06:57,000
Which happens to be my next example.

153
0:06:58.295,000 --> 0:07:01,000
This is my cat, Eli.

154
0:07:01.405,000 --> 0:07:02,000
We wanted to give users the ability

155
0:07:03.386,000 --> 0:07:05,000
to add photos to their tweets.

156
0:07:05.459,000 --> 0:07:06,000
A picture is worth a thousand words.

157
0:07:07.056,000 --> 0:07:09,000
You only get 140 characters.

158
0:07:09.065,000 --> 0:07:1,000
You add a photo to your tweet,

159
0:07:10.265,000 --> 0:07:13,000
look at how much more content you've got now.

160
0:07:13.303,000 --> 0:07:14,000
There's all sorts of great things you can do

161
0:07:14.98,000 --> 0:07:16,000
by adding a photo to a tweet.

162
0:07:16.987,000 --> 0:07:18,000
My job isn't to think of those.

163
0:07:19.267,000 --> 0:07:21,000
It's to think of what could go wrong.

164
0:07:22.014,000 --> 0:07:23,000
How could this picture

165
0:07:23.906,000 --> 0:07:26,000
lead to my death?

166
0:07:27.445,000 --> 0:07:3,000
Well, here's one possibility.

167
0:07:30.605,000 --> 0:07:33,000
There's more in that picture than just a cat.

168
0:07:33.691,000 --> 0:07:35,000
There's geodata.

169
0:07:35.783,000 --> 0:07:37,000
When you take a picture with your smartphone

170
0:07:37.995,000 --> 0:07:38,000
or digital camera,

171
0:07:39.294,000 --> 0:07:4,000
there's a lot of additional information

172
0:07:40.948,000 --> 0:07:41,000
saved along in that image.

173
0:07:42.564,000 --> 0:07:43,000
In fact, this image also contains

174
0:07:44.496,000 --> 0:07:45,000
the equivalent of this,

175
0:07:46.301,000 --> 0:07:49,000
more specifically, this.

176
0:07:49.38,000 --> 0:07:5,000
Sure, it's not likely that someone's going to try

177
0:07:51.336,000 --> 0:07:53,000
to track me down and do me harm

178
0:07:53.621,000 --> 0:07:54,000
based upon image data associated

179
0:07:55.405,000 --> 0:07:56,000
with a picture I took of my cat,

180
0:07:57.353,000 --> 0:08:,000
but I start by assuming the worst will happen.

181
0:08:01.004,000 --> 0:08:03,000
That's why, when we launched photos on Twitter,

182
0:08:03.342,000 --> 0:08:06,000
we made the decision to strip that geodata out.

183
0:08:07.163,000 --> 0:08:12,000
(Applause)

184
0:08:13.01,000 --> 0:08:15,000
If I start by assuming the worst

185
0:08:15.623,000 --> 0:08:15,000
and work backwards,

186
0:08:16.57,000 --> 0:08:18,000
I can make sure that the protections we build

187
0:08:19.123,000 --> 0:08:2,000
work for both expected

188
0:08:20.891,000 --> 0:08:22,000
and unexpected use cases.

189
0:08:22.969,000 --> 0:08:24,000
Given that I spend my days and nights

190
0:08:25.914,000 --> 0:08:27,000
imagining the worst that could happen,

191
0:08:28.455,000 --> 0:08:32,000
it wouldn't be surprising if my worldview was gloomy.

192
0:08:32.712,000 --> 0:08:33,000
(Laughter)

193
0:08:34.495,000 --> 0:08:35,000
It's not.

194
0:08:35.912,000 --> 0:08:38,000
The vast majority of interactions I see --

195
0:08:39.788,000 --> 0:08:42,000
and I see a lot, believe me -- are positive,

196
0:08:43.689,000 --> 0:08:44,000
people reaching out to help

197
0:08:45.613,000 --> 0:08:48,000
or to connect or share information with each other.

198
0:08:49.061,000 --> 0:08:52,000
It's just that for those of us dealing with scale,

199
0:08:52.384,000 --> 0:08:55,000
for those of us tasked with keeping people safe,

200
0:08:56.184,000 --> 0:08:58,000
we have to assume the worst will happen,

201
0:08:58.73,000 --> 0:09:02,000
because for us, a one-in-a-million chance

202
0:09:02.957,000 --> 0:09:04,000
is pretty good odds.

203
0:09:05.706,000 --> 0:09:06,000
Thank you.

204
0:09:07.57,000 --> 0:09:11,000
(Applause)

