1
0:00:,000 --> 0:00:07,000
Traductor: Ruth Alonso Revisor: Patricia Tatis

2
0:00:13.16,000 --> 0:00:17,000
¿Saben?, me sorprende que uno de los temas implícitos de TED

3
0:00:17.16,000 --> 0:00:2,000
sea la compasión. Acabamos de ver demostraciones muy emotivas:

4
0:00:21.16,000 --> 0:00:25,000
como la de anoche del Presidente Clinton, sobre el VIH en África.

5
0:00:25.16,000 --> 0:00:3,000
Y me gustaría hacer un poco de pensamiento colateral, por así llamarlo,

6
0:00:30.16,000 --> 0:00:35,000
sobre la compasión, y llevarlo de lo global a lo personal.

7
0:00:35.16,000 --> 0:00:37,000
Soy psicólogo, pero tranquilos,

8
0:00:37.16,000 --> 0:00:38,000
que no lo llevaré al plano sexual.

9
0:00:39.16,000 --> 0:00:43,000
(Risas)

10
0:00:44.16,000 --> 0:00:46,000
Hace tiempo se realizó un estudio muy importante

11
0:00:46.16,000 --> 0:00:5,000
en el Semirario Teológico de Princeton que nos habla de por qué

12
0:00:51.16,000 --> 0:00:54,000
cuando todos nosotros tenemos tantas oportunidades de ayudar,

13
0:00:54.16,000 --> 0:00:57,000
a veces lo hacemos y a veces no.

14
0:00:58.16,000 --> 0:01:01,000
A un grupo de estudiantes de teología en el Seminario de Teología en Princeton

15
0:01:02.16,000 --> 0:01:06,000
se les dijo que iban a dar un sermón de práctica,

16
0:01:06.16,000 --> 0:01:09,000
y a cada uno se le dio un tema para el sermón.

17
0:01:09.16,000 --> 0:01:12,000
A la mitad de esos alumnos se les dio como tema,

18
0:01:12.16,000 --> 0:01:14,000
la parábola del buen samaritano:

19
0:01:14.16,000 --> 0:01:16,000
el hombre que se paró para ayudar al desconocido --

20
0:01:17.16,000 --> 0:01:19,000
al necesitado que estaba en un lado de la carretera.

21
0:01:19.16,000 --> 0:01:22,000
A la otra mitad se les dio temas bíblicos aleatorios.

22
0:01:22.16,000 --> 0:01:25,000
Luego, uno a uno, se les dijo que tenían que ir a otro edificio

23
0:01:26.16,000 --> 0:01:27,000
y dar el sermón.

24
0:01:27.16,000 --> 0:01:3,000
Mientras iban desde el primer edificio hasta el segundo,

25
0:01:30.16,000 --> 0:01:33,000
todos se cruzaron con un hombre que estaba encogido, se quejaba y estaba

26
0:01:34.16,000 --> 0:01:38,000
claramente necesitado. La pregunta es: ¿se detuvieron a ayudarlo?

27
0:01:38.16,000 --> 0:01:39,000
La pregunta más interesante es:

28
0:01:40.16,000 --> 0:01:43,000
¿Importó que estuvieran pensando en la parábola

29
0:01:43.16,000 --> 0:01:47,000
del buen samaritano? Respuesta: no, en lo absoluto.

30
0:01:48.16,000 --> 0:01:51,000
Lo que resultó determinar si alguien se detendría

31
0:01:51.16,000 --> 0:01:52,000
a ayudar a un desconocido necesitado

32
0:01:52.16,000 --> 0:01:55,000
fue cuánta prisa creían que tenían --

33
0:01:56.16,000 --> 0:02:,000
¿pensaban que llegaban tarde, o estaban absortos

34
0:02:00.16,000 --> 0:02:01,000
pensando en lo que iban a hablar?

35
0:02:02.16,000 --> 0:02:04,000
Y este es, según creo, el problema de nuestras vidas:

36
0:02:05.16,000 --> 0:02:09,000
que no aprovechamos todas las oportunidades para ayudar,

37
0:02:09.16,000 --> 0:02:12,000
porque nuestro foco apunta en la dirección equivocada.

38
0:02:12.16,000 --> 0:02:15,000
Hay una nueva disciplina en neurociencia, la neurociencia social.

39
0:02:16.16,000 --> 0:02:2,000
Estudia los circuitos en los cerebros de dos personas

40
0:02:20.16,000 --> 0:02:22,000
que se activan cuando éstas interactúan.

41
0:02:22.16,000 --> 0:02:26,000
Y la nueva manera de pensar sobre la compasión en neurociencia social

42
0:02:26.16,000 --> 0:02:3,000
es que nuestra reacción, por defecto, es ayudar.

43
0:02:30.16,000 --> 0:02:34,000
Es decir, si prestamos atención a la otra persona,

44
0:02:35.16,000 --> 0:02:38,000
automáticamente nos identificamos, automáticamente sentimos como él.

45
0:02:39.16,000 --> 0:02:41,000
Existen unas neuronas identificadas recientemente, neuronas espejo,

46
0:02:41.16,000 --> 0:02:45,000
que actúan como una conexión inhálambrica neuronal, al activar en nuestro cerebro

47
0:02:45.16,000 --> 0:02:49,000
exactamente las áreas activadas en el cerebro de otro. Nos identificamos automáticamente.

48
0:02:49.16,000 --> 0:02:53,000
Y si esa persona está necesitada, si esa persona está sufriendo,

49
0:02:54.16,000 --> 0:02:58,000
automáticamente estamos listos para ayudar. Por lo menos esa es la idea.

50
0:02:58.16,000 --> 0:03:01,000
Pero entonces la pregunta es: ¿por qué no lo hacemos?

51
0:03:01.16,000 --> 0:03:03,000
Y creo que esto tiene que ver con un espectro

52
0:03:04.16,000 --> 0:03:06,000
que va desde el ensimismamiento absoluto,

53
0:03:07.16,000 --> 0:03:09,000
hasta el hecho de darse cuenta y tener empatía y compasión.

54
0:03:09.16,000 --> 0:03:13,000
Y el simple hecho es que, si nos centramos en nosotros mismos,

55
0:03:14.16,000 --> 0:03:17,000
si estamos preocupados, como tantas veces lo estamos a lo largo del día,

56
0:03:17.16,000 --> 0:03:2,000
realmente no percibimos al otro del todo.

57
0:03:20.16,000 --> 0:03:22,000
Y esta diferencia entre centrarse en uno mismo o en el otro

58
0:03:22.16,000 --> 0:03:23,000
puede ser muy sutil.

59
0:03:23.16,000 --> 0:03:27,000
Estaba haciendo la declaración de los impuestos hace unos días, y llegué al punto

60
0:03:27.16,000 --> 0:03:29,000
en que tenía que declarar todas las donaciones que había hecho,

61
0:03:30.16,000 --> 0:03:33,000
y tuve una epifanía, fue -- llegué al cheque que mandé

62
0:03:33.16,000 --> 0:03:36,000
a la Fundación Seva y me di cuenta de que pensé,

63
0:03:36.16,000 --> 0:03:38,000
vaya, mi amigo Larry Brilliant estaría muy contento

64
0:03:39.16,000 --> 0:03:4,000
de que yo haya dado dinero a Seva.

65
0:03:40.16,000 --> 0:03:43,000
Entonces me di cuenta de que lo que yo estaba recibiendo por dar

66
0:03:43.16,000 --> 0:03:47,000
era una dosis de narcisismo -- de sentirme bien conmigo mismo.

67
0:03:47.16,000 --> 0:03:52,000
Entonces empecé a pensar en la gente en el Himalaya

68
0:03:52.16,000 --> 0:03:54,000
cuyas cataratas mejorarían, y me di cuenta

69
0:03:55.16,000 --> 0:03:58,000
de que fui de una clase de ensimismamiento narcisista

70
0:03:59.16,000 --> 0:04:02,000
a una alegría altruísta, a sentirme bien

71
0:04:02.16,000 --> 0:04:06,000
por la gente a la que estaba ayudando. Creo que eso motiva.

72
0:04:06.16,000 --> 0:04:09,000
Pero esta distinción entre centrarnos en nosotros mismos

73
0:04:09.16,000 --> 0:04:1,000
y centrarnos en otros,

74
0:04:10.16,000 --> 0:04:13,000
es a la que los animo a prestar atención.

75
0:04:13.16,000 --> 0:04:16,000
Lo pueden ver de manera generalizada en el mundo de las citas amorosas.

76
0:04:17.16,000 --> 0:04:2,000
Hace un tiempo estaba en un restaurante de sushi

77
0:04:20.16,000 --> 0:04:23,000
y escuché a dos mujeres hablar sobre el hermano de una de ellas,

78
0:04:24.16,000 --> 0:04:27,000
que estaba soltero. Y una mujer dice:

79
0:04:27.16,000 --> 0:04:29,000
"A mi hermano le está resultando difícil salir con alguien,

80
0:04:29.16,000 --> 0:04:31,000
así que está intentándolo con citas rápidas". No sé si conocen las citas rápidas.

81
0:04:31.16,000 --> 0:04:35,000
Las mujeres se sientan en mesas y los hombres van de mesa en mesa,

82
0:04:35.16,000 --> 0:04:38,000
y hay un reloj y una campana, y a los cinco minutos, bingo,

83
0:04:39.16,000 --> 0:04:41,000
la conversación termina y la mujer decide

84
0:04:41.16,000 --> 0:04:45,000
si darle su tarjeta o su correo electrónico al hombre

85
0:04:45.16,000 --> 0:04:47,000
para continuar. Y la mujer dice:

86
0:04:47.16,000 --> 0:04:51,000
"A mi hermano nunca le han dado ninguna tarjeta. Y sé exactamente por qué.

87
0:04:51.16,000 --> 0:04:56,000
En cuanto se sienta, empieza a hablar sin parar sobre sí mismo,

88
0:04:56.16,000 --> 0:04:57,000
y nunca le pregunta sobre ella".

89
0:04:58.16,000 --> 0:05:03,000
Yo estaba estudiando la sección "Sunday Styles" (Estilos de domingo) del New York Times,

90
0:05:03.16,000 --> 0:05:06,000
mirando la historia detrás de algunos matrimonios

91
0:05:06.16,000 --> 0:05:09,000
porque son muy interesantes -- y llegué al matrimonio de

92
0:05:09.16,000 --> 0:05:12,000
Alice Charney Epstein. Y decía

93
0:05:12.16,000 --> 0:05:14,000
que cuando ella estaba buscando pareja,

94
0:05:15.16,000 --> 0:05:17,000
tenía una prueba sencilla que ella aplicaba.

95
0:05:18.16,000 --> 0:05:2,000
La prueba era: desde el momento en el que se conocieran,

96
0:05:20.16,000 --> 0:05:23,000
¿cuánto tiempo le llevaría al hombre hacerle una pregunta

97
0:05:23.16,000 --> 0:05:25,000
con la palabra "tú" en ella?

98
0:05:25.16,000 --> 0:05:29,000
Y aparentemente Epstein lo hizo muy bien, de ahí el artículo.

99
0:05:29.16,000 --> 0:05:3,000
(Risas)

100
0:05:30.16,000 --> 0:05:32,000
Ahora éste es un-- es un pequeño test

101
0:05:32.16,000 --> 0:05:34,000
que les animo a que usen en una fiesta.

102
0:05:34.16,000 --> 0:05:36,000
En TED hay grandes oportunidades.

103
0:05:38.16,000 --> 0:05:41,000
La Harvard Business Review tenía un artículo hace poco titulado

104
0:05:41.16,000 --> 0:05:44,000
"El momento humano", sobre cómo crear contacto real

105
0:05:44.16,000 --> 0:05:47,000
con una persona en el trabajo. Y decía que

106
0:05:47.16,000 --> 0:05:5,000
lo más importante que tienes que hacer es apagar tu Blackberry,

107
0:05:51.16,000 --> 0:05:54,000
cerrar tu computadora portátil, dejar de soñar despierto

108
0:05:55.16,000 --> 0:05:57,000
y prestar toda tu atención a la persona.

109
0:05:58.16,000 --> 0:06:02,000
Hay una palabra acuñada recientemente en el idioma inglés

110
0:06:03.16,000 --> 0:06:06,000
para describir el momento en el que la persona con la que estamos saca su Blackberry

111
0:06:06.16,000 --> 0:06:09,000
o responde a una llamada en el móvil y de repente no existimos.

112
0:06:10.16,000 --> 0:06:14,000
La palabra es "pizzled": una combinación entre confundido y enojado.

113
0:06:14.16,000 --> 0:06:17,000
(Risas)

114
0:06:17.16,000 --> 0:06:23,000
Me parece bastante apropiada. Es nuestra empatía, nuestra capacidad para conectar

115
0:06:24.16,000 --> 0:06:27,000
lo que nos separa de la gente maquiavélica o de los sociópatas.

116
0:06:27.16,000 --> 0:06:32,000
Tengo un cuñado que es experto en horror y terror-

117
0:06:32.16,000 --> 0:06:35,000
escribió "Drácula Anotado", "Frankenstein Esencial" --

118
0:06:35.16,000 --> 0:06:36,000
fue entrenado como especialista en Chaucer,

119
0:06:36.16,000 --> 0:06:38,000
pero nació en Transilvania

120
0:06:38.16,000 --> 0:06:4,000
y creo que eso le afectó un poco.

121
0:06:40.16,000 --> 0:06:44,000
De todos modos, en cierto momento mi cuñado, Leonardo,

122
0:06:44.16,000 --> 0:06:46,000
decidió escribir un libro sobre un asesino en serie.

123
0:06:46.16,000 --> 0:06:49,000
Se trata de un hombre que hace muchos años sembró el pánico en esta zona.

124
0:06:50.16,000 --> 0:06:52,000
Se le conocía como el estrangulador de Santa Cruz.

125
0:06:53.16,000 --> 0:06:57,000
Y antes de que fuera arrestado, había asesinado a sus abuelos,

126
0:06:57.16,000 --> 0:07:,000
a su madre y a cinco chicas en la universidad de UC Santa Cruz.

127
0:07:01.16,000 --> 0:07:03,000
Así que mi cuñado va a entrevistar a este asesino

128
0:07:04.16,000 --> 0:07:06,000
y se da cuenta cuando lo conoce que

129
0:07:06.16,000 --> 0:07:07,000
el hombre es absolutamente aterrador.

130
0:07:08.16,000 --> 0:07:1,000
Por un lado, mide casi siete pies de alto.

131
0:07:10.16,000 --> 0:07:13,000
Pero eso no es lo peor.

132
0:07:13.16,000 --> 0:07:18,000
Lo que más miedo da es que su coeficiente intelectual es de 160: un genio acreditado.

133
0:07:19.16,000 --> 0:07:23,000
Pero la correlación entre el coeficiente intelectual y la empatía emocional,

134
0:07:23.16,000 --> 0:07:24,000
el sentir con la otra persona, es nula.

135
0:07:25.16,000 --> 0:07:27,000
Están controlados por diferentes partes del cerebro.

136
0:07:28.16,000 --> 0:07:3,000
Así que en un momento determinado, mi cuñado se arma de valor

137
0:07:31.16,000 --> 0:07:33,000
y le hace una pregunta cuya respuesta realmente quiere saber.

138
0:07:33.16,000 --> 0:07:36,000
Y la pregunta es: ¿cómo pudo hacerlo?

139
0:07:36.16,000 --> 0:07:38,000
¿No sintió lástima alguna por sus víctimas?

140
0:07:38.16,000 --> 0:07:41,000
Fueron asesinatos muy íntimos -- extranguló a sus víctimas.

141
0:07:42.16,000 --> 0:07:44,000
Y el extrangulador le dice impasible:

142
0:07:44.16,000 --> 0:07:49,000
"Oh, no. Si me hubiera afligido, no podría haberlo hecho.

143
0:07:49.16,000 --> 0:07:55,000
Tuve que desconectar esa parte de mí. Tuve que desconectar esa parte de mí".

144
0:07:55.16,000 --> 0:08:,000
Y creo que eso es muy preocupante.

145
0:08:01.16,000 --> 0:08:05,000
Y, en cierto sentido, he estado reflexionando sobre el hecho de desconectar esa parte de nosotros.

146
0:08:05.16,000 --> 0:08:07,000
Cuando nos centramos en nosotros, en cualquier actividad,

147
0:08:08.16,000 --> 0:08:11,000
desconectamos esa parte de nosotros si hay otra persona.

148
0:08:12.16,000 --> 0:08:17,000
Piensen en ir de compras y piensen en las posibilidades

149
0:08:17.16,000 --> 0:08:19,000
de un consumismo compasivo.

150
0:08:20.16,000 --> 0:08:22,000
Ahora mismo, como señaló Bill McDonough,

151
0:08:24.16,000 --> 0:08:28,000
los objetos que compramos y usamos esconden consecuencias.

152
0:08:28.16,000 --> 0:08:31,000
Todos somos víctimas involuntarias de un talón de Aquiles colectivo.

153
0:08:32.16,000 --> 0:08:34,000
No percibimos y no nos damos cuenta de que no percibimos

154
0:08:35.16,000 --> 0:08:41,000
las moléculas tóxicas emitidas por una alfombra o por el tejido de los asientos.

155
0:08:42.16,000 --> 0:08:47,000
O no sabemos si ese tejido es un nutriente tecnológico

156
0:08:47.16,000 --> 0:08:51,000
o de manufactura; ¿puede reutilizarse

157
0:08:51.16,000 --> 0:08:53,000
o se va directamente a un vertedero? En otras palabras,

158
0:08:53.16,000 --> 0:08:58,000
somos ajenos a las consecuencias ecológicas, de salud pública,

159
0:08:59.16,000 --> 0:09:02,000
sociales y de justicia económica

160
0:09:02.16,000 --> 0:09:04,000
de las cosas que compramos y usamos.

161
0:09:06.16,000 --> 0:09:1,000
En cierto sentido, lo tenemos a la vista,

162
0:09:10.16,000 --> 0:09:14,000
pero no lo vemos. Y nos hemos convertido en víctimas

163
0:09:14.16,000 --> 0:09:17,000
de un sistema que nos distrae. Consideren esto.

164
0:09:18.16,000 --> 0:09:21,000
Hay un libro maravilloso llamado:

165
0:09:22.16,000 --> 0:09:24,000
"Cosas: la vida oculta de los objetos cotidianos".

166
0:09:25.16,000 --> 0:09:28,000
Y habla de la historia detrás de una camiseta.

167
0:09:28.16,000 --> 0:09:31,000
Y habla de dónde se cultivó el algodón,

168
0:09:31.16,000 --> 0:09:33,000
y de los fertilizantes que se usaron y de las consecuencias

169
0:09:33.16,000 --> 0:09:37,000
de ese fertilizante para la tierra. Y menciona, por ejemplo,

170
0:09:37.16,000 --> 0:09:4,000
que el algodón es muy resistente a los tintes textiles;

171
0:09:40.16,000 --> 0:09:43,000
alrededor del 60 por ciento se convierte en agua residual.

172
0:09:43.16,000 --> 0:09:46,000
Y los epidemiólogos saben bien que los niños

173
0:09:46.16,000 --> 0:09:51,000
que viven cerca de fábricas textiles son más propensos a la leucemia.

174
0:09:52.16,000 --> 0:09:56,000
Hay una compañía, Bennett and Company, que abastece a Polo.com,

175
0:09:57.16,000 --> 0:10:02,000
a Victoria's Secret -- ellos, gracias a su director ejecutivo, que es consciente de esto,

176
0:10:03.16,000 --> 0:10:07,000
hicieron una alianza estratégica en China, para trabajar conjuntamente sus trabajos con tintes

177
0:10:07.16,000 --> 0:10:09,000
para asegurarse que sus aguas residuales

178
0:10:09.16,000 --> 0:10:13,000
serían depuradas antes de volver a los canales subterráneos.

179
0:10:13.16,000 --> 0:10:17,000
Ahora mismo, no tenemos la opción de decidir entre la camiseta elaborada con consciencia social

180
0:10:18.16,000 --> 0:10:22,000
y la que no lo ha sido. ¿Qué se requeriría para tener esa opción?

181
0:10:25.16,000 --> 0:10:28,000
Bueno, he estado pensando. Por un lado,

182
0:10:28.16,000 --> 0:10:33,000
hay una nueva tecnología de etiquetado electrónico que permite que cualquier tienda

183
0:10:33.16,000 --> 0:10:37,000
sepa la historia completa de cualquier objeto en los estantes de esa tienda.

184
0:10:38.16,000 --> 0:10:4,000
Puedes rastrearlo hasta la fábrica. Una vez que lo has rastreado hasta la fábrica

185
0:10:40.16,000 --> 0:10:44,000
puedes fijarte en los procesos de manufactura

186
0:10:44.16,000 --> 0:10:48,000
que se usaron para hacerlo, y si se ha confeccionado con compasión

187
0:10:48.16,000 --> 0:10:52,000
lo puedes etiquetar de ese modo. O, si no,

188
0:10:52.16,000 --> 0:10:56,000
puedes entrar -- hoy, entrar en cualquier tienda,

189
0:10:56.16,000 --> 0:10:59,000
poner tu escáner en la palma de la mano y aplicarlo a un código de barras

190
0:10:59.16,000 --> 0:11:01,000
que te llevará a una página web.

191
0:11:01.16,000 --> 0:11:03,000
Lo tienen para gente con alergia a los cacahuates.

192
0:11:04.16,000 --> 0:11:06,000
Esa página podría decirte cosas sobre ese objeto.

193
0:11:07.16,000 --> 0:11:08,000
En otras palabras, al momento de comprar,

194
0:11:08.16,000 --> 0:11:12,000
quizás podamos hacer una elección compasiva.

195
0:11:12.16,000 --> 0:11:18,000
Hay un dicho en el mundo de la ciencia de la información:

196
0:11:18.16,000 --> 0:11:21,000
Al final, todos sabrán todo.

197
0:11:21.16,000 --> 0:11:23,000
Y la pregunta es: ¿hará esto una diferencia?

198
0:11:25.16,000 --> 0:11:28,000
Hace algún tiempo cuando trabajaba para el New York Times,

199
0:11:29.16,000 --> 0:11:31,000
en los años 80, escribí un artículo

200
0:11:31.16,000 --> 0:11:33,000
sobre lo que entonces era un nuevo problema en Nueva York --

201
0:11:33.16,000 --> 0:11:35,000
las personas sin hogar que estaban en la calle.

202
0:11:35.16,000 --> 0:11:39,000
Y pasé un par de semanas dando vueltas por ahí con una agencia de trabajo social

203
0:11:39.16,000 --> 0:11:42,000
que se dedicaba a los desamparados. Y me di cuenta al mirar a los desamparados a los ojos,

204
0:11:42.16,000 --> 0:11:47,000
de que casi todos eran pacientes psiquiátricos

205
0:11:47.16,000 --> 0:11:51,000
sin un lugar adonde ir. Estaban diagnosticados. Me hizo --

206
0:11:52.16,000 --> 0:11:55,000
lo que hizo fue sacarme del trance urbano por el cual,

207
0:11:56.16,000 --> 0:11:59,000
cuando miramos, cuando pasamos al lado de una persona sin hogar

208
0:11:59.16,000 --> 0:12:02,000
en la periferia de nuestra visión, se queda en la periferia.

209
0:12:04.16,000 --> 0:12:06,000
No nos fijamos, y, en consecuencia, no actuamos.

210
0:12:09.16,000 --> 0:12:14,000
Un día próximo a eso -- era un viernes -- al final del día,

211
0:12:14.16,000 --> 0:12:17,000
bajé -- iba al tren subterráneo. Era una hora crucial

212
0:12:17.16,000 --> 0:12:19,000
y miles de personas bajaban las escaleras como una corriente.

213
0:12:19.16,000 --> 0:12:21,000
Y de repente al yo bajar las escaleras

214
0:12:21.16,000 --> 0:12:24,000
me fijé en que había un hombre inclinado hacia un costado,

215
0:12:24.16,000 --> 0:12:28,000
sin camisa, sin moverse, y la gente estaba pasando por encima de él --

216
0:12:29.16,000 --> 0:12:3,000
cientos y cientos de personas.

217
0:12:31.16,000 --> 0:12:34,000
Y como mi trance urbano se había delibitado de alguna manera,

218
0:12:35.16,000 --> 0:12:38,000
me vi a mí mismo deteniéndome para averiguar qué le pasaba.

219
0:12:39.16,000 --> 0:12:41,000
En cuanto me detuve, media docena de personas más

220
0:12:42.16,000 --> 0:12:43,000
rodearon al tipo inmediatamente.

221
0:12:44.16,000 --> 0:12:46,000
Y averiguamos que era hispano, que no hablaba nada de inglés,

222
0:12:46.16,000 --> 0:12:51,000
que no tenía dinero, que llevaba días deambulando por las calles, hambriento,

223
0:12:51.16,000 --> 0:12:52,000
y que se había desmayado de hambre.

224
0:12:52.16,000 --> 0:12:54,000
Inmediatamente alguien fue a conseguir un jugo de naranja,

225
0:12:54.16,000 --> 0:12:56,000
alguien le consiguió un "hotdog", alguien trajo a un policía del tren.

226
0:12:57.16,000 --> 0:13:,000
El tipo estaba en pie inmediatamente.

227
0:13:00.16,000 --> 0:13:04,000
Pero todo lo que hizo falta fue el simple hecho de fijarse.

228
0:13:05.16,000 --> 0:13:06,000
Así que soy optimista.

229
0:13:06.16,000 --> 0:13:07,000
Muchas gracias.

230
0:13:07.16,000 --> 0:13:09,000
(Aplausos)

