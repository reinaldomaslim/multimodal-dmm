1
0:00:12.787,000 --> 0:00:15,000
America's favorite pie is?

2
0:00:16.632,000 --> 0:00:19,000
Audience: Apple. Kenneth Cukier: Apple. Of course it is.

3
0:00:20.138,000 --> 0:00:21,000
How do we know it?

4
0:00:21.369,000 --> 0:00:23,000
Because of data.

5
0:00:24.122,000 --> 0:00:26,000
You look at supermarket sales.

6
0:00:26.188,000 --> 0:00:28,000
You look at supermarket sales of 30-centimeter pies

7
0:00:29.054,000 --> 0:00:33,000
that are frozen, and apple wins, no contest.

8
0:00:33.129,000 --> 0:00:38,000
The majority of the sales are apple.

9
0:00:38.309,000 --> 0:00:4,000
But then supermarkets started selling

10
0:00:41.273,000 --> 0:00:43,000
smaller, 11-centimeter pies,

11
0:00:43.856,000 --> 0:00:47,000
and suddenly, apple fell to fourth or fifth place.

12
0:00:48.03,000 --> 0:00:5,000
Why? What happened?

13
0:00:50.905,000 --> 0:00:52,000
Okay, think about it.

14
0:00:53.723,000 --> 0:00:56,000
When you buy a 30-centimeter pie,

15
0:00:57.571,000 --> 0:00:59,000
the whole family has to agree,

16
0:00:59.832,000 --> 0:01:02,000
and apple is everyone's second favorite.

17
0:01:03.623,000 --> 0:01:04,000
(Laughter)

18
0:01:05.558,000 --> 0:01:08,000
But when you buy an individual 11-centimeter pie,

19
0:01:09.173,000 --> 0:01:12,000
you can buy the one that you want.

20
0:01:12.918,000 --> 0:01:16,000
You can get your first choice.

21
0:01:16.933,000 --> 0:01:17,000
You have more data.

22
0:01:18.574,000 --> 0:01:19,000
You can see something

23
0:01:20.128,000 --> 0:01:21,000
that you couldn't see

24
0:01:21.26,000 --> 0:01:24,000
when you only had smaller amounts of it.

25
0:01:25.213,000 --> 0:01:27,000
Now, the point here is that more data

26
0:01:27.688,000 --> 0:01:29,000
doesn't just let us see more,

27
0:01:29.971,000 --> 0:01:3,000
more of the same thing we were looking at.

28
0:01:31.825,000 --> 0:01:34,000
More data allows us to see new.

29
0:01:35.438,000 --> 0:01:38,000
It allows us to see better.

30
0:01:38.532,000 --> 0:01:41,000
It allows us to see different.

31
0:01:42.188,000 --> 0:01:45,000
In this case, it allows us to see

32
0:01:45.361,000 --> 0:01:47,000
what America's favorite pie is:

33
0:01:48.274,000 --> 0:01:5,000
not apple.

34
0:01:50.816,000 --> 0:01:53,000
Now, you probably all have heard the term big data.

35
0:01:54.43,000 --> 0:01:56,000
In fact, you're probably sick of hearing the term

36
0:01:56.487,000 --> 0:01:57,000
big data.

37
0:01:58.117,000 --> 0:02:01,000
It is true that there is a lot of hype around the term,

38
0:02:01.447,000 --> 0:02:03,000
and that is very unfortunate,

39
0:02:03.779,000 --> 0:02:06,000
because big data is an extremely important tool

40
0:02:06.825,000 --> 0:02:09,000
by which society is going to advance.

41
0:02:10.559,000 --> 0:02:13,000
In the past, we used to look at small data

42
0:02:14.12,000 --> 0:02:15,000
and think about what it would mean

43
0:02:15.824,000 --> 0:02:16,000
to try to understand the world,

44
0:02:17.32,000 --> 0:02:18,000
and now we have a lot more of it,

45
0:02:19.311,000 --> 0:02:21,000
more than we ever could before.

46
0:02:22.033,000 --> 0:02:23,000
What we find is that when we have

47
0:02:23.91,000 --> 0:02:25,000
a large body of data, we can fundamentally do things

48
0:02:26.634,000 --> 0:02:29,000
that we couldn't do when we only had smaller amounts.

49
0:02:29.91,000 --> 0:02:31,000
Big data is important, and big data is new,

50
0:02:32.551,000 --> 0:02:33,000
and when you think about it,

51
0:02:34.328,000 --> 0:02:36,000
the only way this planet is going to deal

52
0:02:36.544,000 --> 0:02:37,000
with its global challenges —

53
0:02:38.333,000 --> 0:02:41,000
to feed people, supply them with medical care,

54
0:02:41.87,000 --> 0:02:43,000
supply them with energy, electricity,

55
0:02:44.68,000 --> 0:02:45,000
and to make sure they're not burnt to a crisp

56
0:02:46.469,000 --> 0:02:47,000
because of global warming —

57
0:02:47.707,000 --> 0:02:51,000
is because of the effective use of data.

58
0:02:51.902,000 --> 0:02:54,000
So what is new about big data? What is the big deal?

59
0:02:55.772,000 --> 0:02:57,000
Well, to answer that question, let's think about

60
0:02:58.289,000 --> 0:02:59,000
what information looked like,

61
0:03:00.185,000 --> 0:03:03,000
physically looked like in the past.

62
0:03:03.219,000 --> 0:03:06,000
In 1908, on the island of Crete,

63
0:03:06.83,000 --> 0:03:1,000
archaeologists discovered a clay disc.

64
0:03:11.565,000 --> 0:03:15,000
They dated it from 2000 B.C., so it's 4,000 years old.

65
0:03:15.624,000 --> 0:03:17,000
Now, there's inscriptions on this disc,

66
0:03:17.628,000 --> 0:03:18,000
but we actually don't know what it means.

67
0:03:18.955,000 --> 0:03:2,000
It's a complete mystery, but the point is that

68
0:03:21.053,000 --> 0:03:22,000
this is what information used to look like

69
0:03:22.981,000 --> 0:03:24,000
4,000 years ago.

70
0:03:25.07,000 --> 0:03:27,000
This is how society stored

71
0:03:27.618,000 --> 0:03:3,000
and transmitted information.

72
0:03:31.142,000 --> 0:03:35,000
Now, society hasn't advanced all that much.

73
0:03:35.302,000 --> 0:03:38,000
We still store information on discs,

74
0:03:38.776,000 --> 0:03:41,000
but now we can store a lot more information,

75
0:03:41.96,000 --> 0:03:42,000
more than ever before.

76
0:03:43.22,000 --> 0:03:46,000
Searching it is easier. Copying it easier.

77
0:03:46.313,000 --> 0:03:49,000
Sharing it is easier. Processing it is easier.

78
0:03:49.813,000 --> 0:03:51,000
And what we can do is we can reuse this information

79
0:03:52.579,000 --> 0:03:53,000
for uses that we never even imagined

80
0:03:54.413,000 --> 0:03:57,000
when we first collected the data.

81
0:03:57.608,000 --> 0:03:59,000
In this respect, the data has gone

82
0:03:59.86,000 --> 0:04:02,000
from a stock to a flow,

83
0:04:03.392,000 --> 0:04:06,000
from something that is stationary and static

84
0:04:07.33,000 --> 0:04:1,000
to something that is fluid and dynamic.

85
0:04:10.939,000 --> 0:04:14,000
There is, if you will, a liquidity to information.

86
0:04:14.962,000 --> 0:04:17,000
The disc that was discovered off of Crete

87
0:04:18.436,000 --> 0:04:21,000
that's 4,000 years old, is heavy,

88
0:04:22.2,000 --> 0:04:23,000
it doesn't store a lot of information,

89
0:04:24.162,000 --> 0:04:27,000
and that information is unchangeable.

90
0:04:27.278,000 --> 0:04:31,000
By contrast, all of the files

91
0:04:31.289,000 --> 0:04:32,000
that Edward Snowden took

92
0:04:33.15,000 --> 0:04:35,000
from the National Security Agency in the United States

93
0:04:35.771,000 --> 0:04:37,000
fits on a memory stick

94
0:04:38.19,000 --> 0:04:41,000
the size of a fingernail,

95
0:04:41.2,000 --> 0:04:45,000
and it can be shared at the speed of light.

96
0:04:45.945,000 --> 0:04:5,000
More data. More.

97
0:04:51.2,000 --> 0:04:52,000
Now, one reason why we have so much data in the world today

98
0:04:53.174,000 --> 0:04:54,000
is we are collecting things

99
0:04:54.606,000 --> 0:04:57,000
that we've always collected information on,

100
0:04:57.886,000 --> 0:04:59,000
but another reason why is we're taking things

101
0:05:00.542,000 --> 0:05:02,000
that have always been informational

102
0:05:03.354,000 --> 0:05:05,000
but have never been rendered into a data format

103
0:05:05.84,000 --> 0:05:07,000
and we are putting it into data.

104
0:05:08.259,000 --> 0:05:11,000
Think, for example, the question of location.

105
0:05:11.567,000 --> 0:05:13,000
Take, for example, Martin Luther.

106
0:05:13.816,000 --> 0:05:14,000
If we wanted to know in the 1500s

107
0:05:15.413,000 --> 0:05:17,000
where Martin Luther was,

108
0:05:18.08,000 --> 0:05:2,000
we would have to follow him at all times,

109
0:05:20.172,000 --> 0:05:22,000
maybe with a feathery quill and an inkwell,

110
0:05:22.309,000 --> 0:05:23,000
and record it,

111
0:05:23.985,000 --> 0:05:25,000
but now think about what it looks like today.

112
0:05:26.168,000 --> 0:05:28,000
You know that somewhere,

113
0:05:28.29,000 --> 0:05:3,000
probably in a telecommunications carrier's database,

114
0:05:30.736,000 --> 0:05:33,000
there is a spreadsheet or at least a database entry

115
0:05:33.772,000 --> 0:05:35,000
that records your information

116
0:05:35.86,000 --> 0:05:37,000
of where you've been at all times.

117
0:05:37.923,000 --> 0:05:38,000
If you have a cell phone,

118
0:05:39.283,000 --> 0:05:41,000
and that cell phone has GPS, but even if it doesn't have GPS,

119
0:05:42.13,000 --> 0:05:44,000
it can record your information.

120
0:05:44.515,000 --> 0:05:48,000
In this respect, location has been datafied.

121
0:05:48.599,000 --> 0:05:52,000
Now think, for example, of the issue of posture,

122
0:05:53.2,000 --> 0:05:54,000
the way that you are all sitting right now,

123
0:05:54.485,000 --> 0:05:56,000
the way that you sit,

124
0:05:56.515,000 --> 0:05:58,000
the way that you sit, the way that you sit.

125
0:05:59.286,000 --> 0:06:01,000
It's all different, and it's a function of your leg length

126
0:06:01.363,000 --> 0:06:03,000
and your back and the contours of your back,

127
0:06:03.456,000 --> 0:06:05,000
and if I were to put sensors, maybe 100 sensors

128
0:06:05.987,000 --> 0:06:06,000
into all of your chairs right now,

129
0:06:07.753,000 --> 0:06:1,000
I could create an index that's fairly unique to you,

130
0:06:11.353,000 --> 0:06:15,000
sort of like a fingerprint, but it's not your finger.

131
0:06:15.762,000 --> 0:06:17,000
So what could we do with this?

132
0:06:18.731,000 --> 0:06:2,000
Researchers in Tokyo are using it

133
0:06:21.128,000 --> 0:06:25,000
as a potential anti-theft device in cars.

134
0:06:25.516,000 --> 0:06:27,000
The idea is that the carjacker sits behind the wheel,

135
0:06:28.44,000 --> 0:06:3,000
tries to stream off, but the car recognizes

136
0:06:30.544,000 --> 0:06:32,000
that a non-approved driver is behind the wheel,

137
0:06:32.906,000 --> 0:06:34,000
and maybe the engine just stops, unless you

138
0:06:35.07,000 --> 0:06:38,000
type in a password into the dashboard

139
0:06:38.247,000 --> 0:06:42,000
to say, "Hey, I have authorization to drive." Great.

140
0:06:42.905,000 --> 0:06:44,000
What if every single car in Europe

141
0:06:45.458,000 --> 0:06:46,000
had this technology in it?

142
0:06:46.915,000 --> 0:06:49,000
What could we do then?

143
0:06:50.08,000 --> 0:06:52,000
Maybe, if we aggregated the data,

144
0:06:52.32,000 --> 0:06:55,000
maybe we could identify telltale signs

145
0:06:56.134,000 --> 0:06:58,000
that best predict that a car accident

146
0:06:58.843,000 --> 0:07:03,000
is going to take place in the next five seconds.

147
0:07:04.736,000 --> 0:07:06,000
And then what we will have datafied

148
0:07:07.293,000 --> 0:07:08,000
is driver fatigue,

149
0:07:09.076,000 --> 0:07:11,000
and the service would be when the car senses

150
0:07:11.41,000 --> 0:07:14,000
that the person slumps into that position,

151
0:07:14.847,000 --> 0:07:17,000
automatically knows, hey, set an internal alarm

152
0:07:18.841,000 --> 0:07:2,000
that would vibrate the steering wheel, honk inside

153
0:07:20.866,000 --> 0:07:21,000
to say, "Hey, wake up,

154
0:07:22.587,000 --> 0:07:23,000
pay more attention to the road."

155
0:07:24.491,000 --> 0:07:25,000
These are the sorts of things we can do

156
0:07:26.344,000 --> 0:07:28,000
when we datafy more aspects of our lives.

157
0:07:29.165,000 --> 0:07:32,000
So what is the value of big data?

158
0:07:32.84,000 --> 0:07:34,000
Well, think about it.

159
0:07:35.03,000 --> 0:07:37,000
You have more information.

160
0:07:37.442,000 --> 0:07:4,000
You can do things that you couldn't do before.

161
0:07:40.783,000 --> 0:07:41,000
One of the most impressive areas

162
0:07:42.459,000 --> 0:07:43,000
where this concept is taking place

163
0:07:44.188,000 --> 0:07:47,000
is in the area of machine learning.

164
0:07:47.495,000 --> 0:07:5,000
Machine learning is a branch of artificial intelligence,

165
0:07:50.572,000 --> 0:07:53,000
which itself is a branch of computer science.

166
0:07:53.95,000 --> 0:07:54,000
The general idea is that instead of

167
0:07:55.493,000 --> 0:07:57,000
instructing a computer what do do,

168
0:07:57.61,000 --> 0:07:59,000
we are going to simply throw data at the problem

169
0:08:00.23,000 --> 0:08:03,000
and tell the computer to figure it out for itself.

170
0:08:03.436,000 --> 0:08:04,000
And it will help you understand it

171
0:08:05.213,000 --> 0:08:08,000
by seeing its origins.

172
0:08:08.765,000 --> 0:08:1,000
In the 1950s, a computer scientist

173
0:08:11.153,000 --> 0:08:14,000
at IBM named Arthur Samuel liked to play checkers,

174
0:08:14.745,000 --> 0:08:15,000
so he wrote a computer program

175
0:08:16.147,000 --> 0:08:18,000
so he could play against the computer.

176
0:08:18.96,000 --> 0:08:2,000
He played. He won.

177
0:08:21.671,000 --> 0:08:23,000
He played. He won.

178
0:08:23.774,000 --> 0:08:26,000
He played. He won,

179
0:08:26.789,000 --> 0:08:27,000
because the computer only knew

180
0:08:28.567,000 --> 0:08:3,000
what a legal move was.

181
0:08:30.794,000 --> 0:08:32,000
Arthur Samuel knew something else.

182
0:08:32.881,000 --> 0:08:36,000
Arthur Samuel knew strategy.

183
0:08:37.51,000 --> 0:08:39,000
So he wrote a small sub-program alongside it

184
0:08:39.906,000 --> 0:08:4,000
operating in the background, and all it did

185
0:08:41.88,000 --> 0:08:42,000
was score the probability

186
0:08:43.697,000 --> 0:08:45,000
that a given board configuration would likely lead

187
0:08:46.26,000 --> 0:08:48,000
to a winning board versus a losing board

188
0:08:49.17,000 --> 0:08:51,000
after every move.

189
0:08:51.678,000 --> 0:08:54,000
He plays the computer. He wins.

190
0:08:54.828,000 --> 0:08:56,000
He plays the computer. He wins.

191
0:08:57.336,000 --> 0:09:,000
He plays the computer. He wins.

192
0:09:01.067,000 --> 0:09:03,000
And then Arthur Samuel leaves the computer

193
0:09:03.344,000 --> 0:09:05,000
to play itself.

194
0:09:05.571,000 --> 0:09:08,000
It plays itself. It collects more data.

195
0:09:09.08,000 --> 0:09:13,000
It collects more data. It increases the accuracy of its prediction.

196
0:09:13.389,000 --> 0:09:15,000
And then Arthur Samuel goes back to the computer

197
0:09:15.493,000 --> 0:09:17,000
and he plays it, and he loses,

198
0:09:17.811,000 --> 0:09:19,000
and he plays it, and he loses,

199
0:09:19.88,000 --> 0:09:21,000
and he plays it, and he loses,

200
0:09:21.927,000 --> 0:09:23,000
and Arthur Samuel has created a machine

201
0:09:24.526,000 --> 0:09:3,000
that surpasses his ability in a task that he taught it.

202
0:09:30.814,000 --> 0:09:32,000
And this idea of machine learning

203
0:09:33.312,000 --> 0:09:36,000
is going everywhere.

204
0:09:37.239,000 --> 0:09:4,000
How do you think we have self-driving cars?

205
0:09:40.388,000 --> 0:09:42,000
Are we any better off as a society

206
0:09:42.525,000 --> 0:09:45,000
enshrining all the rules of the road into software?

207
0:09:45.81,000 --> 0:09:47,000
No. Memory is cheaper. No.

208
0:09:48.408,000 --> 0:09:51,000
Algorithms are faster. No. Processors are better. No.

209
0:09:52.402,000 --> 0:09:54,000
All of those things matter, but that's not why.

210
0:09:55.174,000 --> 0:09:58,000
It's because we changed the nature of the problem.

211
0:09:58.315,000 --> 0:09:59,000
We changed the nature of the problem from one

212
0:09:59.845,000 --> 0:10:01,000
in which we tried to overtly and explicitly

213
0:10:02.09,000 --> 0:10:04,000
explain to the computer how to drive

214
0:10:04.671,000 --> 0:10:05,000
to one in which we say,

215
0:10:05.987,000 --> 0:10:06,000
"Here's a lot of data around the vehicle.

216
0:10:07.863,000 --> 0:10:08,000
You figure it out.

217
0:10:09.396,000 --> 0:10:1,000
You figure it out that that is a traffic light,

218
0:10:11.263,000 --> 0:10:13,000
that that traffic light is red and not green,

219
0:10:13.344,000 --> 0:10:15,000
that that means that you need to stop

220
0:10:15.358,000 --> 0:10:18,000
and not go forward."

221
0:10:18.441,000 --> 0:10:19,000
Machine learning is at the basis

222
0:10:19.959,000 --> 0:10:2,000
of many of the things that we do online:

223
0:10:21.95,000 --> 0:10:22,000
search engines,

224
0:10:23.807,000 --> 0:10:26,000
Amazon's personalization algorithm,

225
0:10:27.608,000 --> 0:10:29,000
computer translation,

226
0:10:29.82,000 --> 0:10:33,000
voice recognition systems.

227
0:10:34.11,000 --> 0:10:36,000
Researchers recently have looked at

228
0:10:36.945,000 --> 0:10:39,000
the question of biopsies,

229
0:10:40.14,000 --> 0:10:42,000
cancerous biopsies,

230
0:10:42.907,000 --> 0:10:44,000
and they've asked the computer to identify

231
0:10:45.222,000 --> 0:10:47,000
by looking at the data and survival rates

232
0:10:47.693,000 --> 0:10:51,000
to determine whether cells are actually

233
0:10:52.36,000 --> 0:10:54,000
cancerous or not,

234
0:10:54.904,000 --> 0:10:55,000
and sure enough, when you throw the data at it,

235
0:10:56.682,000 --> 0:10:58,000
through a machine-learning algorithm,

236
0:10:58.729,000 --> 0:10:59,000
the machine was able to identify

237
0:11:00.606,000 --> 0:11:02,000
the 12 telltale signs that best predict

238
0:11:02.868,000 --> 0:11:05,000
that this biopsy of the breast cancer cells

239
0:11:06.167,000 --> 0:11:09,000
are indeed cancerous.

240
0:11:09.385,000 --> 0:11:11,000
The problem: The medical literature

241
0:11:11.883,000 --> 0:11:13,000
only knew nine of them.

242
0:11:14.672,000 --> 0:11:15,000
Three of the traits were ones

243
0:11:16.472,000 --> 0:11:18,000
that people didn't need to look for,

244
0:11:19.447,000 --> 0:11:24,000
but that the machine spotted.

245
0:11:24.978,000 --> 0:11:29,000
Now, there are dark sides to big data as well.

246
0:11:30.903,000 --> 0:11:32,000
It will improve our lives, but there are problems

247
0:11:32.977,000 --> 0:11:34,000
that we need to be conscious of,

248
0:11:35.617,000 --> 0:11:37,000
and the first one is the idea

249
0:11:38.24,000 --> 0:11:4,000
that we may be punished for predictions,

250
0:11:40.926,000 --> 0:11:43,000
that the police may use big data for their purposes,

251
0:11:44.796,000 --> 0:11:46,000
a little bit like "Minority Report."

252
0:11:47.147,000 --> 0:11:49,000
Now, it's a term called predictive policing,

253
0:11:49.588,000 --> 0:11:51,000
or algorithmic criminology,

254
0:11:51.951,000 --> 0:11:53,000
and the idea is that if we take a lot of data,

255
0:11:53.987,000 --> 0:11:55,000
for example where past crimes have been,

256
0:11:56.146,000 --> 0:11:58,000
we know where to send the patrols.

257
0:11:58.689,000 --> 0:12:,000
That makes sense, but the problem, of course,

258
0:12:00.804,000 --> 0:12:04,000
is that it's not simply going to stop on location data,

259
0:12:05.348,000 --> 0:12:07,000
it's going to go down to the level of the individual.

260
0:12:08.307,000 --> 0:12:1,000
Why don't we use data about the person's

261
0:12:10.557,000 --> 0:12:12,000
high school transcript?

262
0:12:12.785,000 --> 0:12:13,000
Maybe we should use the fact that

263
0:12:14.346,000 --> 0:12:16,000
they're unemployed or not, their credit score,

264
0:12:16.374,000 --> 0:12:17,000
their web-surfing behavior,

265
0:12:17.926,000 --> 0:12:18,000
whether they're up late at night.

266
0:12:19.804,000 --> 0:12:22,000
Their Fitbit, when it's able to identify biochemistries,

267
0:12:22.965,000 --> 0:12:26,000
will show that they have aggressive thoughts.

268
0:12:27.201,000 --> 0:12:29,000
We may have algorithms that are likely to predict

269
0:12:29.422,000 --> 0:12:3,000
what we are about to do,

270
0:12:31.055,000 --> 0:12:32,000
and we may be held accountable

271
0:12:32.299,000 --> 0:12:34,000
before we've actually acted.

272
0:12:34.889,000 --> 0:12:35,000
Privacy was the central challenge

273
0:12:36.621,000 --> 0:12:38,000
in a small data era.

274
0:12:39.501,000 --> 0:12:41,000
In the big data age,

275
0:12:41.65,000 --> 0:12:45,000
the challenge will be safeguarding free will,

276
0:12:46.173,000 --> 0:12:49,000
moral choice, human volition,

277
0:12:49.952,000 --> 0:12:52,000
human agency.

278
0:12:54.54,000 --> 0:12:56,000
There is another problem:

279
0:12:56.765,000 --> 0:12:59,000
Big data is going to steal our jobs.

280
0:13:00.321,000 --> 0:13:03,000
Big data and algorithms are going to challenge

281
0:13:03.833,000 --> 0:13:06,000
white collar, professional knowledge work

282
0:13:06.894,000 --> 0:13:07,000
in the 21st century

283
0:13:08.547,000 --> 0:13:1,000
in the same way that factory automation

284
0:13:10.981,000 --> 0:13:12,000
and the assembly line

285
0:13:13.17,000 --> 0:13:16,000
challenged blue collar labor in the 20th century.

286
0:13:16.196,000 --> 0:13:18,000
Think about a lab technician

287
0:13:18.288,000 --> 0:13:19,000
who is looking through a microscope

288
0:13:19.697,000 --> 0:13:2,000
at a cancer biopsy

289
0:13:21.321,000 --> 0:13:23,000
and determining whether it's cancerous or not.

290
0:13:23.958,000 --> 0:13:24,000
The person went to university.

291
0:13:25.93,000 --> 0:13:26,000
The person buys property.

292
0:13:27.36,000 --> 0:13:28,000
He or she votes.

293
0:13:29.101,000 --> 0:13:32,000
He or she is a stakeholder in society.

294
0:13:32.767,000 --> 0:13:33,000
And that person's job,

295
0:13:34.161,000 --> 0:13:35,000
as well as an entire fleet

296
0:13:35.77,000 --> 0:13:36,000
of professionals like that person,

297
0:13:37.739,000 --> 0:13:4,000
is going to find that their jobs are radically changed

298
0:13:40.889,000 --> 0:13:42,000
or actually completely eliminated.

299
0:13:43.246,000 --> 0:13:44,000
Now, we like to think

300
0:13:44.53,000 --> 0:13:47,000
that technology creates jobs over a period of time

301
0:13:47.717,000 --> 0:13:5,000
after a short, temporary period of dislocation,

302
0:13:51.182,000 --> 0:13:52,000
and that is true for the frame of reference

303
0:13:53.123,000 --> 0:13:55,000
with which we all live, the Industrial Revolution,

304
0:13:55.265,000 --> 0:13:57,000
because that's precisely what happened.

305
0:13:57.593,000 --> 0:13:59,000
But we forget something in that analysis:

306
0:13:59.926,000 --> 0:14:,000
There are some categories of jobs

307
0:14:01.756,000 --> 0:14:04,000
that simply get eliminated and never come back.

308
0:14:05.176,000 --> 0:14:07,000
The Industrial Revolution wasn't very good

309
0:14:07.18,000 --> 0:14:11,000
if you were a horse.

310
0:14:11.182,000 --> 0:14:13,000
So we're going to need to be careful

311
0:14:13.237,000 --> 0:14:16,000
and take big data and adjust it for our needs,

312
0:14:16.751,000 --> 0:14:19,000
our very human needs.

313
0:14:19.936,000 --> 0:14:2,000
We have to be the master of this technology,

314
0:14:21.89,000 --> 0:14:22,000
not its servant.

315
0:14:23.546,000 --> 0:14:25,000
We are just at the outset of the big data era,

316
0:14:26.504,000 --> 0:14:29,000
and honestly, we are not very good

317
0:14:29.654,000 --> 0:14:33,000
at handling all the data that we can now collect.

318
0:14:33.861,000 --> 0:14:36,000
It's not just a problem for the National Security Agency.

319
0:14:37.191,000 --> 0:14:4,000
Businesses collect lots of data, and they misuse it too,

320
0:14:40.229,000 --> 0:14:43,000
and we need to get better at this, and this will take time.

321
0:14:43.896,000 --> 0:14:44,000
It's a little bit like the challenge that was faced

322
0:14:45.718,000 --> 0:14:47,000
by primitive man and fire.

323
0:14:48.125,000 --> 0:14:49,000
This is a tool, but this is a tool that,

324
0:14:50.01,000 --> 0:14:53,000
unless we're careful, will burn us.

325
0:14:56.008,000 --> 0:14:59,000
Big data is going to transform how we live,

326
0:14:59.128,000 --> 0:15:01,000
how we work and how we think.

327
0:15:01.929,000 --> 0:15:02,000
It is going to help us manage our careers

328
0:15:03.818,000 --> 0:15:06,000
and lead lives of satisfaction and hope

329
0:15:07.452,000 --> 0:15:09,000
and happiness and health,

330
0:15:10.444,000 --> 0:15:13,000
but in the past, we've often looked at information technology

331
0:15:13.75,000 --> 0:15:15,000
and our eyes have only seen the T,

332
0:15:15.958,000 --> 0:15:16,000
the technology, the hardware,

333
0:15:17.644,000 --> 0:15:19,000
because that's what was physical.

334
0:15:19.906,000 --> 0:15:21,000
We now need to recast our gaze at the I,

335
0:15:22.83,000 --> 0:15:23,000
the information,

336
0:15:24.21,000 --> 0:15:25,000
which is less apparent,

337
0:15:25.583,000 --> 0:15:29,000
but in some ways a lot more important.

338
0:15:29.692,000 --> 0:15:32,000
Humanity can finally learn from the information

339
0:15:33.157,000 --> 0:15:35,000
that it can collect,

340
0:15:35.575,000 --> 0:15:37,000
as part of our timeless quest

341
0:15:37.69,000 --> 0:15:4,000
to understand the world and our place in it,

342
0:15:40.849,000 --> 0:15:45,000
and that's why big data is a big deal.

343
0:15:46.48,000 --> 0:15:49,000
(Applause)

