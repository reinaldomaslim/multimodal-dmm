1
0:00:12.76,000 --> 0:00:16,000
After 13.8 billion years of cosmic history,

2
0:00:17.2,000 --> 0:00:19,000
our universe has woken up

3
0:00:19.32,000 --> 0:00:2,000
and become aware of itself.

4
0:00:21.48,000 --> 0:00:22,000
From a small blue planet,

5
0:00:23.44,000 --> 0:00:27,000
tiny, conscious parts of our universe have begun gazing out into the cosmos

6
0:00:27.6,000 --> 0:00:28,000
with telescopes,

7
0:00:29,000 --> 0:00:3,000
discovering something humbling.

8
0:00:31.32,000 --> 0:00:33,000
We've discovered that our universe is vastly grander

9
0:00:34.24,000 --> 0:00:35,000
than our ancestors imagined

10
0:00:35.6,000 --> 0:00:39,000
and that life seems to be an almost imperceptibly small perturbation

11
0:00:39.88,000 --> 0:00:4,000
on an otherwise dead universe.

12
0:00:42.32,000 --> 0:00:45,000
But we've also discovered something inspiring,

13
0:00:45.36,000 --> 0:00:47,000
which is that the technology we're developing has the potential

14
0:00:48.36,000 --> 0:00:5,000
to help life flourish like never before,

15
0:00:51.24,000 --> 0:00:54,000
not just for centuries but for billions of years,

16
0:00:54.36,000 --> 0:00:58,000
and not just on earth but throughout much of this amazing cosmos.

17
0:00:59.68,000 --> 0:01:02,000
I think of the earliest life as "Life 1.0"

18
0:01:03.04,000 --> 0:01:04,000
because it was really dumb,

19
0:01:04.44,000 --> 0:01:08,000
like bacteria, unable to learn anything during its lifetime.

20
0:01:08.76,000 --> 0:01:11,000
I think of us humans as "Life 2.0" because we can learn,

21
0:01:12.16,000 --> 0:01:13,000
which we in nerdy, geek speak,

22
0:01:13.68,000 --> 0:01:16,000
might think of as installing new software into our brains,

23
0:01:16.92,000 --> 0:01:18,000
like languages and job skills.

24
0:01:19.68,000 --> 0:01:23,000
"Life 3.0," which can design not only its software but also its hardware

25
0:01:24,000 --> 0:01:25,000
of course doesn't exist yet.

26
0:01:25.68,000 --> 0:01:28,000
But perhaps our technology has already made us "Life 2.1,"

27
0:01:29.48,000 --> 0:01:33,000
with our artificial knees, pacemakers and cochlear implants.

28
0:01:33.84,000 --> 0:01:36,000
So let's take a closer look at our relationship with technology, OK?

29
0:01:38.8,000 --> 0:01:39,000
As an example,

30
0:01:40.04,000 --> 0:01:45,000
the Apollo 11 moon mission was both successful and inspiring,

31
0:01:45.36,000 --> 0:01:48,000
showing that when we humans use technology wisely,

32
0:01:48.4,000 --> 0:01:51,000
we can accomplish things that our ancestors could only dream of.

33
0:01:52.36,000 --> 0:01:54,000
But there's an even more inspiring journey

34
0:01:55.36,000 --> 0:01:57,000
propelled by something more powerful than rocket engines,

35
0:01:59.2,000 --> 0:02:01,000
where the passengers aren't just three astronauts

36
0:02:01.56,000 --> 0:02:02,000
but all of humanity.

37
0:02:03.36,000 --> 0:02:05,000
Let's talk about our collective journey into the future

38
0:02:06.32,000 --> 0:02:08,000
with artificial intelligence.

39
0:02:08.96,000 --> 0:02:12,000
My friend Jaan Tallinn likes to point out that just as with rocketry,

40
0:02:13.52,000 --> 0:02:16,000
it's not enough to make our technology powerful.

41
0:02:17.56,000 --> 0:02:2,000
We also have to figure out, if we're going to be really ambitious,

42
0:02:20.759,000 --> 0:02:21,000
how to steer it

43
0:02:22.199,000 --> 0:02:23,000
and where we want to go with it.

44
0:02:24.88,000 --> 0:02:26,000
So let's talk about all three for artificial intelligence:

45
0:02:28.44,000 --> 0:02:31,000
the power, the steering and the destination.

46
0:02:31.52,000 --> 0:02:32,000
Let's start with the power.

47
0:02:33.6,000 --> 0:02:36,000
I define intelligence very inclusively --

48
0:02:36.72,000 --> 0:02:4,000
simply as our ability to accomplish complex goals,

49
0:02:41.08,000 --> 0:02:44,000
because I want to include both biological and artificial intelligence.

50
0:02:44.92,000 --> 0:02:48,000
And I want to avoid the silly carbon-chauvinism idea

51
0:02:48.96,000 --> 0:02:5,000
that you can only be smart if you're made of meat.

52
0:02:52.88,000 --> 0:02:56,000
It's really amazing how the power of AI has grown recently.

53
0:02:57.08,000 --> 0:02:58,000
Just think about it.

54
0:02:58.36,000 --> 0:03:01,000
Not long ago, robots couldn't walk.

55
0:03:03.04,000 --> 0:03:04,000
Now, they can do backflips.

56
0:03:06.08,000 --> 0:03:07,000
Not long ago,

57
0:03:07.92,000 --> 0:03:08,000
we didn't have self-driving cars.

58
0:03:10.92,000 --> 0:03:12,000
Now, we have self-flying rockets.

59
0:03:15.96,000 --> 0:03:16,000
Not long ago,

60
0:03:17.4,000 --> 0:03:19,000
AI couldn't do face recognition.

61
0:03:20.04,000 --> 0:03:22,000
Now, AI can generate fake faces

62
0:03:23.04,000 --> 0:03:27,000
and simulate your face saying stuff that you never said.

63
0:03:28.4,000 --> 0:03:29,000
Not long ago,

64
0:03:3,000 --> 0:03:31,000
AI couldn't beat us at the game of Go.

65
0:03:32.4,000 --> 0:03:37,000
Then, Google DeepMind's AlphaZero AI took 3,000 years of human Go games

66
0:03:37.52,000 --> 0:03:38,000
and Go wisdom,

67
0:03:38.8,000 --> 0:03:42,000
ignored it all and became the world's best player by just playing against itself.

68
0:03:43.8,000 --> 0:03:46,000
And the most impressive feat here wasn't that it crushed human gamers,

69
0:03:47.52,000 --> 0:03:49,000
but that it crushed human AI researchers

70
0:03:50.12,000 --> 0:03:53,000
who had spent decades handcrafting game-playing software.

71
0:03:54.2,000 --> 0:03:58,000
And AlphaZero crushed human AI researchers not just in Go but even at chess,

72
0:03:58.88,000 --> 0:04:,000
which we have been working on since 1950.

73
0:04:02,000 --> 0:04:06,000
So all this amazing recent progress in AI really begs the question:

74
0:04:07.28,000 --> 0:04:08,000
How far will it go?

75
0:04:09.8,000 --> 0:04:1,000
I like to think about this question

76
0:04:11.52,000 --> 0:04:13,000
in terms of this abstract landscape of tasks,

77
0:04:14.52,000 --> 0:04:17,000
where the elevation represents how hard it is for AI to do each task

78
0:04:18,000 --> 0:04:19,000
at human level,

79
0:04:19.24,000 --> 0:04:21,000
and the sea level represents what AI can do today.

80
0:04:23.12,000 --> 0:04:25,000
The sea level is rising as AI improves,

81
0:04:25.2,000 --> 0:04:28,000
so there's a kind of global warming going on here in the task landscape.

82
0:04:30.04,000 --> 0:04:33,000
And the obvious takeaway is to avoid careers at the waterfront --

83
0:04:33.399,000 --> 0:04:34,000
(Laughter)

84
0:04:34.68,000 --> 0:04:36,000
which will soon be automated and disrupted.

85
0:04:37.56,000 --> 0:04:39,000
But there's a much bigger question as well.

86
0:04:40.56,000 --> 0:04:41,000
How high will the water end up rising?

87
0:04:43.44,000 --> 0:04:46,000
Will it eventually rise to flood everything,

88
0:04:47.84,000 --> 0:04:49,000
matching human intelligence at all tasks.

89
0:04:50.36,000 --> 0:04:53,000
This is the definition of artificial general intelligence --

90
0:04:54.12,000 --> 0:04:55,000
AGI,

91
0:04:55.44,000 --> 0:04:58,000
which has been the holy grail of AI research since its inception.

92
0:04:59,000 --> 0:05:,000
By this definition, people who say,

93
0:05:00.8,000 --> 0:05:03,000
"Ah, there will always be jobs that humans can do better than machines,"

94
0:05:04.24,000 --> 0:05:06,000
are simply saying that we'll never get AGI.

95
0:05:07.68,000 --> 0:05:1,000
Sure, we might still choose to have some human jobs

96
0:05:11.28,000 --> 0:05:14,000
or to give humans income and purpose with our jobs,

97
0:05:14.4,000 --> 0:05:17,000
but AGI will in any case transform life as we know it

98
0:05:18.16,000 --> 0:05:2,000
with humans no longer being the most intelligent.

99
0:05:20.92,000 --> 0:05:23,000
Now, if the water level does reach AGI,

100
0:05:24.64,000 --> 0:05:29,000
then further AI progress will be driven mainly not by humans but by AI,

101
0:05:29.96,000 --> 0:05:3,000
which means that there's a possibility

102
0:05:31.84,000 --> 0:05:33,000
that further AI progress could be way faster

103
0:05:34.2,000 --> 0:05:37,000
than the typical human research and development timescale of years,

104
0:05:37.6,000 --> 0:05:41,000
raising the controversial possibility of an intelligence explosion

105
0:05:41.64,000 --> 0:05:43,000
where recursively self-improving AI

106
0:05:43.96,000 --> 0:05:46,000
rapidly leaves human intelligence far behind,

107
0:05:47.4,000 --> 0:05:49,000
creating what's known as superintelligence.

108
0:05:51.8,000 --> 0:05:53,000
Alright, reality check:

109
0:05:55.12,000 --> 0:05:57,000
Are we going to get AGI any time soon?

110
0:05:58.36,000 --> 0:06:,000
Some famous AI researchers, like Rodney Brooks,

111
0:06:01.08,000 --> 0:06:03,000
think it won't happen for hundreds of years.

112
0:06:03.6,000 --> 0:06:06,000
But others, like Google DeepMind founder Demis Hassabis,

113
0:06:07.52,000 --> 0:06:08,000
are more optimistic

114
0:06:08.8,000 --> 0:06:1,000
and are working to try to make it happen much sooner.

115
0:06:11.4,000 --> 0:06:14,000
And recent surveys have shown that most AI researchers

116
0:06:14.72,000 --> 0:06:16,000
actually share Demis's optimism,

117
0:06:17.6,000 --> 0:06:2,000
expecting that we will get AGI within decades,

118
0:06:21.64,000 --> 0:06:23,000
so within the lifetime of many of us,

119
0:06:23.92,000 --> 0:06:24,000
which begs the question -- and then what?

120
0:06:27.04,000 --> 0:06:29,000
What do we want the role of humans to be

121
0:06:29.28,000 --> 0:06:31,000
if machines can do everything better and cheaper than us?

122
0:06:35,000 --> 0:06:37,000
The way I see it, we face a choice.

123
0:06:38,000 --> 0:06:39,000
One option is to be complacent.

124
0:06:39.6,000 --> 0:06:42,000
We can say, "Oh, let's just build machines that can do everything we can do

125
0:06:43.4,000 --> 0:06:44,000
and not worry about the consequences.

126
0:06:45.24,000 --> 0:06:48,000
Come on, if we build technology that makes all humans obsolete,

127
0:06:48.52,000 --> 0:06:5,000
what could possibly go wrong?"

128
0:06:50.64,000 --> 0:06:51,000
(Laughter)

129
0:06:52.32,000 --> 0:06:54,000
But I think that would be embarrassingly lame.

130
0:06:56.08,000 --> 0:06:59,000
I think we should be more ambitious -- in the spirit of TED.

131
0:06:59.6,000 --> 0:07:02,000
Let's envision a truly inspiring high-tech future

132
0:07:03.12,000 --> 0:07:04,000
and try to steer towards it.

133
0:07:05.72,000 --> 0:07:08,000
This brings us to the second part of our rocket metaphor: the steering.

134
0:07:09.28,000 --> 0:07:1,000
We're making AI more powerful,

135
0:07:11.2,000 --> 0:07:14,000
but how can we steer towards a future

136
0:07:15.04,000 --> 0:07:18,000
where AI helps humanity flourish rather than flounder?

137
0:07:18.76,000 --> 0:07:19,000
To help with this,

138
0:07:20.04,000 --> 0:07:21,000
I cofounded the Future of Life Institute.

139
0:07:22.04,000 --> 0:07:24,000
It's a small nonprofit promoting beneficial technology use,

140
0:07:24.84,000 --> 0:07:26,000
and our goal is simply for the future of life to exist

141
0:07:27.6,000 --> 0:07:29,000
and to be as inspiring as possible.

142
0:07:29.68,000 --> 0:07:32,000
You know, I love technology.

143
0:07:32.88,000 --> 0:07:34,000
Technology is why today is better than the Stone Age.

144
0:07:36.6,000 --> 0:07:4,000
And I'm optimistic that we can create a really inspiring high-tech future ...

145
0:07:41.68,000 --> 0:07:42,000
if -- and this is a big if --

146
0:07:43.16,000 --> 0:07:45,000
if we win the wisdom race --

147
0:07:45.64,000 --> 0:07:47,000
the race between the growing power of our technology

148
0:07:48.52,000 --> 0:07:5,000
and the growing wisdom with which we manage it.

149
0:07:51.24,000 --> 0:07:53,000
But this is going to require a change of strategy

150
0:07:53.56,000 --> 0:07:56,000
because our old strategy has been learning from mistakes.

151
0:07:57.28,000 --> 0:07:58,000
We invented fire,

152
0:07:58.84,000 --> 0:07:59,000
screwed up a bunch of times --

153
0:08:00.4,000 --> 0:08:01,000
invented the fire extinguisher.

154
0:08:02.24,000 --> 0:08:03,000
(Laughter)

155
0:08:03.6,000 --> 0:08:05,000
We invented the car, screwed up a bunch of times --

156
0:08:06.04,000 --> 0:08:08,000
invented the traffic light, the seat belt and the airbag,

157
0:08:08.731,000 --> 0:08:11,000
but with more powerful technology like nuclear weapons and AGI,

158
0:08:12.6,000 --> 0:08:15,000
learning from mistakes is a lousy strategy,

159
0:08:16,000 --> 0:08:17,000
don't you think?

160
0:08:17.24,000 --> 0:08:18,000
(Laughter)

161
0:08:18.28,000 --> 0:08:2,000
It's much better to be proactive rather than reactive;

162
0:08:20.88,000 --> 0:08:22,000
plan ahead and get things right the first time

163
0:08:23.2,000 --> 0:08:25,000
because that might be the only time we'll get.

164
0:08:25.72,000 --> 0:08:27,000
But it is funny because sometimes people tell me,

165
0:08:28.08,000 --> 0:08:3,000
"Max, shhh, don't talk like that.

166
0:08:30.84,000 --> 0:08:31,000
That's Luddite scaremongering."

167
0:08:34.04,000 --> 0:08:35,000
But it's not scaremongering.

168
0:08:35.6,000 --> 0:08:37,000
It's what we at MIT call safety engineering.

169
0:08:39.2,000 --> 0:08:4,000
Think about it:

170
0:08:40.44,000 --> 0:08:42,000
before NASA launched the Apollo 11 mission,

171
0:08:42.68,000 --> 0:08:45,000
they systematically thought through everything that could go wrong

172
0:08:45.84,000 --> 0:08:47,000
when you put people on top of explosive fuel tanks

173
0:08:48.24,000 --> 0:08:5,000
and launch them somewhere where no one could help them.

174
0:08:50.88,000 --> 0:08:51,000
And there was a lot that could go wrong.

175
0:08:52.84,000 --> 0:08:53,000
Was that scaremongering?

176
0:08:55.159,000 --> 0:08:56,000
No.

177
0:08:56.4,000 --> 0:08:58,000
That's was precisely the safety engineering

178
0:08:58.44,000 --> 0:08:59,000
that ensured the success of the mission,

179
0:09:00.4,000 --> 0:09:04,000
and that is precisely the strategy I think we should take with AGI.

180
0:09:04.6,000 --> 0:09:08,000
Think through what can go wrong to make sure it goes right.

181
0:09:08.68,000 --> 0:09:1,000
So in this spirit, we've organized conferences,

182
0:09:11.24,000 --> 0:09:13,000
bringing together leading AI researchers and other thinkers

183
0:09:14.08,000 --> 0:09:17,000
to discuss how to grow this wisdom we need to keep AI beneficial.

184
0:09:17.84,000 --> 0:09:2,000
Our last conference was in Asilomar, California last year

185
0:09:21.16,000 --> 0:09:24,000
and produced this list of 23 principles

186
0:09:24.24,000 --> 0:09:26,000
which have since been signed by over 1,000 AI researchers

187
0:09:27.16,000 --> 0:09:28,000
and key industry leaders,

188
0:09:28.48,000 --> 0:09:31,000
and I want to tell you about three of these principles.

189
0:09:31.68,000 --> 0:09:35,000
One is that we should avoid an arms race and lethal autonomous weapons.

190
0:09:37.48,000 --> 0:09:4,000
The idea here is that any science can be used for new ways of helping people

191
0:09:41.12,000 --> 0:09:42,000
or new ways of harming people.

192
0:09:42.68,000 --> 0:09:45,000
For example, biology and chemistry are much more likely to be used

193
0:09:46.64,000 --> 0:09:5,000
for new medicines or new cures than for new ways of killing people,

194
0:09:51.52,000 --> 0:09:53,000
because biologists and chemists pushed hard --

195
0:09:53.72,000 --> 0:09:54,000
and successfully --

196
0:09:55,000 --> 0:09:57,000
for bans on biological and chemical weapons.

197
0:09:57.2,000 --> 0:09:58,000
And in the same spirit,

198
0:09:58.48,000 --> 0:10:02,000
most AI researchers want to stigmatize and ban lethal autonomous weapons.

199
0:10:03.6,000 --> 0:10:04,000
Another Asilomar AI principle

200
0:10:05.44,000 --> 0:10:08,000
is that we should mitigate AI-fueled income inequality.

201
0:10:09.16,000 --> 0:10:13,000
I think that if we can grow the economic pie dramatically with AI

202
0:10:13.64,000 --> 0:10:15,000
and we still can't figure out how to divide this pie

203
0:10:16.12,000 --> 0:10:17,000
so that everyone is better off,

204
0:10:17.72,000 --> 0:10:18,000
then shame on us.

205
0:10:19,000 --> 0:10:23,000
(Applause)

206
0:10:23.12,000 --> 0:10:26,000
Alright, now raise your hand if your computer has ever crashed.

207
0:10:27.48,000 --> 0:10:28,000
(Laughter)

208
0:10:28.76,000 --> 0:10:29,000
Wow, that's a lot of hands.

209
0:10:30.44,000 --> 0:10:32,000
Well, then you'll appreciate this principle

210
0:10:32.64,000 --> 0:10:35,000
that we should invest much more in AI safety research,

211
0:10:35.8,000 --> 0:10:38,000
because as we put AI in charge of even more decisions and infrastructure,

212
0:10:39.48,000 --> 0:10:42,000
we need to figure out how to transform today's buggy and hackable computers

213
0:10:43.12,000 --> 0:10:45,000
into robust AI systems that we can really trust,

214
0:10:45.56,000 --> 0:10:46,000
because otherwise,

215
0:10:46.8,000 --> 0:10:48,000
all this awesome new technology can malfunction and harm us,

216
0:10:49.64,000 --> 0:10:5,000
or get hacked and be turned against us.

217
0:10:51.64,000 --> 0:10:56,000
And this AI safety work has to include work on AI value alignment,

218
0:10:57.36,000 --> 0:10:59,000
because the real threat from AGI isn't malice,

219
0:11:00.2,000 --> 0:11:01,000
like in silly Hollywood movies,

220
0:11:01.88,000 --> 0:11:02,000
but competence --

221
0:11:03.64,000 --> 0:11:06,000
AGI accomplishing goals that just aren't aligned with ours.

222
0:11:07.08,000 --> 0:11:11,000
For example, when we humans drove the West African black rhino extinct,

223
0:11:11.84,000 --> 0:11:14,000
we didn't do it because we were a bunch of evil rhinoceros haters, did we?

224
0:11:15.76,000 --> 0:11:17,000
We did it because we were smarter than them

225
0:11:17.84,000 --> 0:11:19,000
and our goals weren't aligned with theirs.

226
0:11:20.44,000 --> 0:11:22,000
But AGI is by definition smarter than us,

227
0:11:23.12,000 --> 0:11:26,000
so to make sure that we don't put ourselves in the position of those rhinos

228
0:11:26.72,000 --> 0:11:27,000
if we create AGI,

229
0:11:28.72,000 --> 0:11:32,000
we need to figure out how to make machines understand our goals,

230
0:11:32.92,000 --> 0:11:35,000
adopt our goals and retain our goals.

231
0:11:37.32,000 --> 0:11:39,000
And whose goals should these be, anyway?

232
0:11:40.2,000 --> 0:11:41,000
Which goals should they be?

233
0:11:42.12,000 --> 0:11:45,000
This brings us to the third part of our rocket metaphor: the destination.

234
0:11:47.16,000 --> 0:11:48,000
We're making AI more powerful,

235
0:11:49.04,000 --> 0:11:5,000
trying to figure out how to steer it,

236
0:11:50.88,000 --> 0:11:51,000
but where do we want to go with it?

237
0:11:53.76,000 --> 0:11:56,000
This is the elephant in the room that almost nobody talks about --

238
0:11:57.44,000 --> 0:11:58,000
not even here at TED --

239
0:11:59.32,000 --> 0:12:03,000
because we're so fixated on short-term AI challenges.

240
0:12:04.08,000 --> 0:12:08,000
Look, our species is trying to build AGI,

241
0:12:08.76,000 --> 0:12:11,000
motivated by curiosity and economics,

242
0:12:12.28,000 --> 0:12:15,000
but what sort of future society are we hoping for if we succeed?

243
0:12:16.68,000 --> 0:12:17,000
We did an opinion poll on this recently,

244
0:12:18.64,000 --> 0:12:19,000
and I was struck to see

245
0:12:19.88,000 --> 0:12:21,000
that most people actually want us to build superintelligence:

246
0:12:22.8,000 --> 0:12:25,000
AI that's vastly smarter than us in all ways.

247
0:12:27.12,000 --> 0:12:3,000
What there was the greatest agreement on was that we should be ambitious

248
0:12:30.56,000 --> 0:12:32,000
and help life spread into the cosmos,

249
0:12:32.6,000 --> 0:12:36,000
but there was much less agreement about who or what should be in charge.

250
0:12:37.12,000 --> 0:12:38,000
And I was actually quite amused

251
0:12:38.88,000 --> 0:12:41,000
to see that there's some some people who want it to be just machines.

252
0:12:42.36,000 --> 0:12:43,000
(Laughter)

253
0:12:44.08,000 --> 0:12:47,000
And there was total disagreement about what the role of humans should be,

254
0:12:47.96,000 --> 0:12:48,000
even at the most basic level,

255
0:12:49.96,000 --> 0:12:51,000
so let's take a closer look at possible futures

256
0:12:52.8,000 --> 0:12:54,000
that we might choose to steer toward, alright?

257
0:12:55.56,000 --> 0:12:56,000
So don't get me wrong here.

258
0:12:56.92,000 --> 0:12:58,000
I'm not talking about space travel,

259
0:12:59,000 --> 0:13:02,000
merely about humanity's metaphorical journey into the future.

260
0:13:02.92,000 --> 0:13:05,000
So one option that some of my AI colleagues like

261
0:13:06.44,000 --> 0:13:09,000
is to build superintelligence and keep it under human control,

262
0:13:10.08,000 --> 0:13:11,000
like an enslaved god,

263
0:13:11.84,000 --> 0:13:12,000
disconnected from the internet

264
0:13:13.44,000 --> 0:13:16,000
and used to create unimaginable technology and wealth

265
0:13:16.72,000 --> 0:13:17,000
for whoever controls it.

266
0:13:18.8,000 --> 0:13:19,000
But Lord Acton warned us

267
0:13:20.28,000 --> 0:13:23,000
that power corrupts, and absolute power corrupts absolutely,

268
0:13:23.92,000 --> 0:13:27,000
so you might worry that maybe we humans just aren't smart enough,

269
0:13:28,000 --> 0:13:29,000
or wise enough rather,

270
0:13:29.56,000 --> 0:13:3,000
to handle this much power.

271
0:13:31.64,000 --> 0:13:33,000
Also, aside from any moral qualms you might have

272
0:13:34.2,000 --> 0:13:36,000
about enslaving superior minds,

273
0:13:36.52,000 --> 0:13:39,000
you might worry that maybe the superintelligence could outsmart us,

274
0:13:40.52,000 --> 0:13:42,000
break out and take over.

275
0:13:43.56,000 --> 0:13:46,000
But I also have colleagues who are fine with AI taking over

276
0:13:47,000 --> 0:13:49,000
and even causing human extinction,

277
0:13:49.32,000 --> 0:13:52,000
as long as we feel the the AIs are our worthy descendants,

278
0:13:52.92,000 --> 0:13:53,000
like our children.

279
0:13:54.68,000 --> 0:13:59,000
But how would we know that the AIs have adopted our best values

280
0:14:00.32,000 --> 0:14:04,000
and aren't just unconscious zombies tricking us into anthropomorphizing them?

281
0:14:04.72,000 --> 0:14:06,000
Also, shouldn't those people who don't want human extinction

282
0:14:07.6,000 --> 0:14:08,000
have a say in the matter, too?

283
0:14:10.2,000 --> 0:14:13,000
Now, if you didn't like either of those two high-tech options,

284
0:14:13.6,000 --> 0:14:16,000
it's important to remember that low-tech is suicide

285
0:14:16.8,000 --> 0:14:17,000
from a cosmic perspective,

286
0:14:18.08,000 --> 0:14:2,000
because if we don't go far beyond today's technology,

287
0:14:20.6,000 --> 0:14:22,000
the question isn't whether humanity is going to go extinct,

288
0:14:23.44,000 --> 0:14:25,000
merely whether we're going to get taken out

289
0:14:25.48,000 --> 0:14:27,000
by the next killer asteroid, supervolcano

290
0:14:27.64,000 --> 0:14:3,000
or some other problem that better technology could have solved.

291
0:14:30.76,000 --> 0:14:33,000
So, how about having our cake and eating it ...

292
0:14:34.36,000 --> 0:14:35,000
with AGI that's not enslaved

293
0:14:37.12,000 --> 0:14:4,000
but treats us well because its values are aligned with ours?

294
0:14:40.32,000 --> 0:14:44,000
This is the gist of what Eliezer Yudkowsky has called "friendly AI,"

295
0:14:44.52,000 --> 0:14:46,000
and if we can do this, it could be awesome.

296
0:14:47.84,000 --> 0:14:51,000
It could not only eliminate negative experiences like disease, poverty,

297
0:14:52.68,000 --> 0:14:53,000
crime and other suffering,

298
0:14:54.16,000 --> 0:14:56,000
but it could also give us the freedom to choose

299
0:14:57,000 --> 0:15:01,000
from a fantastic new diversity of positive experiences --

300
0:15:01.08,000 --> 0:15:04,000
basically making us the masters of our own destiny.

301
0:15:06.28,000 --> 0:15:07,000
So in summary,

302
0:15:07.68,000 --> 0:15:1,000
our situation with technology is complicated,

303
0:15:10.8,000 --> 0:15:12,000
but the big picture is rather simple.

304
0:15:13.24,000 --> 0:15:16,000
Most AI researchers expect AGI within decades,

305
0:15:16.72,000 --> 0:15:19,000
and if we just bumble into this unprepared,

306
0:15:19.88,000 --> 0:15:22,000
it will probably be the biggest mistake in human history --

307
0:15:23.24,000 --> 0:15:24,000
let's face it.

308
0:15:24.68,000 --> 0:15:26,000
It could enable brutal, global dictatorship

309
0:15:27.28,000 --> 0:15:3,000
with unprecedented inequality, surveillance and suffering,

310
0:15:30.84,000 --> 0:15:31,000
and maybe even human extinction.

311
0:15:32.84,000 --> 0:15:34,000
But if we steer carefully,

312
0:15:36.04,000 --> 0:15:39,000
we could end up in a fantastic future where everybody's better off:

313
0:15:39.96,000 --> 0:15:41,000
the poor are richer, the rich are richer,

314
0:15:42.36,000 --> 0:15:45,000
everybody is healthy and free to live out their dreams.

315
0:15:47,000 --> 0:15:48,000
Now, hang on.

316
0:15:48.56,000 --> 0:15:52,000
Do you folks want the future that's politically right or left?

317
0:15:53.16,000 --> 0:15:55,000
Do you want the pious society with strict moral rules,

318
0:15:56.04,000 --> 0:15:57,000
or do you an hedonistic free-for-all,

319
0:15:57.88,000 --> 0:15:59,000
more like Burning Man 24/7?

320
0:16:00.12,000 --> 0:16:02,000
Do you want beautiful beaches, forests and lakes,

321
0:16:02.56,000 --> 0:16:05,000
or would you prefer to rearrange some of those atoms with the computers,

322
0:16:06,000 --> 0:16:07,000
enabling virtual experiences?

323
0:16:07.739,000 --> 0:16:1,000
With friendly AI, we could simply build all of these societies

324
0:16:10.92,000 --> 0:16:13,000
and give people the freedom to choose which one they want to live in

325
0:16:14.16,000 --> 0:16:17,000
because we would no longer be limited by our intelligence,

326
0:16:17.28,000 --> 0:16:18,000
merely by the laws of physics.

327
0:16:18.76,000 --> 0:16:22,000
So the resources and space for this would be astronomical --

328
0:16:23.4,000 --> 0:16:24,000
literally.

329
0:16:25.32,000 --> 0:16:26,000
So here's our choice.

330
0:16:27.88,000 --> 0:16:29,000
We can either be complacent about our future,

331
0:16:31.44,000 --> 0:16:33,000
taking as an article of blind faith

332
0:16:34.12,000 --> 0:16:38,000
that any new technology is guaranteed to be beneficial,

333
0:16:38.16,000 --> 0:16:42,000
and just repeat that to ourselves as a mantra over and over and over again

334
0:16:42.32,000 --> 0:16:45,000
as we drift like a rudderless ship towards our own obsolescence.

335
0:16:46.92,000 --> 0:16:47,000
Or we can be ambitious --

336
0:16:49.84,000 --> 0:16:51,000
thinking hard about how to steer our technology

337
0:16:52.32,000 --> 0:16:53,000
and where we want to go with it

338
0:16:54.28,000 --> 0:16:55,000
to create the age of amazement.

339
0:16:57,000 --> 0:16:59,000
We're all here to celebrate the age of amazement,

340
0:16:59.88,000 --> 0:17:03,000
and I feel that its essence should lie in becoming not overpowered

341
0:17:05.24,000 --> 0:17:07,000
but empowered by our technology.

342
0:17:07.88,000 --> 0:17:08,000
Thank you.

343
0:17:09.28,000 --> 0:17:12,000
(Applause)

