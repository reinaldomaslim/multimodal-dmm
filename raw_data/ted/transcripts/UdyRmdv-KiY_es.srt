1
0:00:,000 --> 0:00:07,000
Traductor: Reynaldo Valerio Revisor: William Martinez

2
0:00:18.33,000 --> 0:00:23,000
De lo que quiero hablarles hoy es de cómo veo a los robots invadiendo nuestras vidas

3
0:00:23.33,000 --> 0:00:26,000
en múltiples niveles y líneas de tiempo.

4
0:00:26.33,000 --> 0:00:3,000
Y cuando miro hacia el futuro, no puedo imaginarme un mundo, 500 años a partir de ahora,

5
0:00:30.33,000 --> 0:00:32,000
en el cual no tengamos robots por todas partes,

6
0:00:32.33,000 --> 0:00:37,000
suponiendo que -- a pesar de todas las espantosas predicciones que mucha gente hace sobre nuestro futuro --

7
0:00:37.33,000 --> 0:00:41,000
suponiendo que todavía estemos por aquí, no puedo imaginar el mundo no estando poblado por robots.

8
0:00:41.33,000 --> 0:00:44,000
Y luego la pregunta es, bueno, si ellos van a estar por aquí en 500 años,

9
0:00:44.33,000 --> 0:00:46,000
¿estarán por todas partes antes de ese tiempo?

10
0:00:46.33,000 --> 0:00:48,000
¿Estarán por aquí dentro de 50 años?

11
0:00:48.33,000 --> 0:00:51,000
Sí, pienso que es muy probable -- habrá muchos robots por todas partes.

12
0:00:51.33,000 --> 0:00:54,000
Y, de hecho, pienso que eso ocurrirá mucho antes.

13
0:00:54.33,000 --> 0:00:58,000
Pienso que estamos a punto de que los robots se vuelvan cosa común

14
0:00:58.33,000 --> 0:01:04,000
y pienso que estamos como en 1978 ó 1980 en años de las computadoras personales

15
0:01:04.33,000 --> 0:01:07,000
cuando los primeros robots están comenzando a aparecer.

16
0:01:07.33,000 --> 0:01:11,000
Las computadoras comenzaron a llegar a través de juegos de video y de juguetes.

17
0:01:11.33,000 --> 0:01:14,000
Y, saben, la primera computadora que muchas personas tuvieron en sus casas

18
0:01:14.33,000 --> 0:01:16,000
puede haber sido una para jugar Pong,

19
0:01:16.33,000 --> 0:01:18,000
con un pequeño microprocesador integrado,

20
0:01:18.33,000 --> 0:01:21,000
y luego algunos otros juegos de video que vinieron después de ese.

21
0:01:21.33,000 --> 0:01:24,000
Y estamos comenzado a ver el mismo tipo de cosas con los robots:

22
0:01:24.33,000 --> 0:01:28,000
LEGO Mindstorms, Furbies -- los cuales -- ¿alguien aquí tuvo un Furby?

23
0:01:28.33,000 --> 0:01:31,000
Claro, hay 38 millones vendidos a nivel mundial.

24
0:01:31.33,000 --> 0:01:33,000
Son muy comunes, y son un pequeño robot,

25
0:01:33.33,000 --> 0:01:35,000
un robot simple con algunos sensores.

26
0:01:35.33,000 --> 0:01:37,000
Es un poco de procesamiento causal.

27
0:01:37.33,000 --> 0:01:4,000
A la derecha vemos otra muñeca robot, la que podían obtener hace un par de años.

28
0:01:40.33,000 --> 0:01:42,000
Y tal como en los primeros días,

29
0:01:42.33,000 --> 0:01:47,000
cuando había mucha interacción de aficionados a las computadoras,

30
0:01:47.33,000 --> 0:01:51,000
pueden obtener ahora muchos juegos para armar robots, y libros de cómo armar robots.

31
0:01:51.33,000 --> 0:01:55,000
Y a la izquierda hay una plataforma de Evolution Robotics,

32
0:01:55.33,000 --> 0:01:58,000
a la cual conectas una PC, y programas esto con una Interfaz Gráfica

33
0:01:58.33,000 --> 0:02:01,000
para moverse por tu casa y hacer diversas cosas.

34
0:02:01.33,000 --> 0:02:04,000
Y luego hay un punto de más alto precio, una especie de robots de juguete --

35
0:02:04.33,000 --> 0:02:08,000
el Aibo de Sony. Y a la derecha ahí, hay uno desarrollado por NEC,

36
0:02:08.33,000 --> 0:02:11,000
el PaPeRo, el cual no creo que vayan a lanzar.

37
0:02:11.33,000 --> 0:02:14,000
Pero, sin embargo, ese tipo de cosas están ahí afuera.

38
0:02:14.33,000 --> 0:02:18,000
Y hemos visto, durante los últimos dos o tres años, robots para cortar el césped,

39
0:02:18.33,000 --> 0:02:24,000
Husqvarna en la parte de abajo, Friendly Robotics aquí arriba, una empresa israrelí.

40
0:02:24.33,000 --> 0:02:26,000
Y, luego, en los últimos 12 meses, aproximadamente,

41
0:02:26.33,000 --> 0:02:3,000
hemos comenzado a ver aparecer muchos robots para limpiar la casa.

42
0:02:30.33,000 --> 0:02:33,000
Arriba y a la derecha hay un robot muy bueno para limpiar la casa

43
0:02:33.33,000 --> 0:02:37,000
de una empresa llamada Dyson, de Reino Unido. Excepto que era tan caro --

44
0:02:37.33,000 --> 0:02:39,000
US$3,500 dólares -- que no lo lanzaron al mercado.

45
0:02:39.33,000 --> 0:02:42,000
Pero abajo a la izquierda ven el Electrolux, que sí está de venta.

46
0:02:42.33,000 --> 0:02:44,000
Otro de Karcher.

47
0:02:44.33,000 --> 0:02:46,000
Y abajo a la derecha está uno que yo construí en mi laboratorio

48
0:02:46.33,000 --> 0:02:49,000
hace unos 10 años, y que finalmente convertimos en un producto.

49
0:02:49.33,000 --> 0:02:51,000
Y permítanme mostrarles eso.

50
0:02:51.33,000 --> 0:02:55,000
Vamos a regalar éste, creo que Chris dijo, al final de la charla.

51
0:02:55.33,000 --> 0:03:01,000
Este es un robot que puedes salir y comprar, y que limpiará tu piso.

52
0:03:05.33,000 --> 0:03:1,000
Y arranca moviéndose en círculos cada vez más amplios.

53
0:03:10.33,000 --> 0:03:14,000
Si golpea con algo -- ¿vieron eso?

54
0:03:14.33,000 --> 0:03:17,000
Ahora está comenzando a seguir la pared, está siguiendo el contorno de mis pies

55
0:03:17.33,000 --> 0:03:21,000
para limpiar alrededor de mí. Veamos, vamos a --

56
0:03:21.33,000 --> 0:03:26,000
oh, ¿quién se llevó mis Rice Krispies? ¡Se robaron mis Rice Krispies!

57
0:03:26.33,000 --> 0:03:32,000
(Risas)

58
0:03:32.33,000 --> 0:03:35,000
¡No se preocupen, relájense, no, relájense, es un robot , es inteligente!

59
0:03:35.33,000 --> 0:03:38,000
(Risas)

60
0:03:38.33,000 --> 0:03:42,000
Ven, los niños de tres años no se preocupan por él.

61
0:03:42.33,000 --> 0:03:44,000
Son los adultos los que de verdad se sienten incómodos.

62
0:03:44.33,000 --> 0:03:45,000
(Risas)

63
0:03:45.33,000 --> 0:03:47,000
Vamos a poner algo de basura aquí.

64
0:03:47.33,000 --> 0:03:51,000
(Risas)

65
0:03:51.33,000 --> 0:03:53,000
Bien.

66
0:03:53.33,000 --> 0:03:57,000
(Risas)

67
0:03:57.33,000 --> 0:04:,000
No sé si lo pueden ver -- bueno, puse un poco de Rice Krispies allí,

68
0:04:00.33,000 --> 0:04:07,000
puse algunas monedas, movámos el robot hacia allá, a ver si lo limpia.

69
0:04:10.33,000 --> 0:04:12,000
Sí, bien. Así que --

70
0:04:12.33,000 --> 0:04:16,000
dejaremos esto para luego.

71
0:04:16.33,000 --> 0:04:21,000
(Aplausos)

72
0:04:22.33,000 --> 0:04:26,000
De hecho, parte del truco fue construir un mejor mecanismo de limpieza;

73
0:04:26.33,000 --> 0:04:3,000
la inteligencia a bordo era bastante simple.

74
0:04:30.33,000 --> 0:04:32,000
Y eso es igual con muchos robots.

75
0:04:32.33,000 --> 0:04:36,000
Nos hemos convertido todos, pienso, en una especie de chauvinistas computacionales,

76
0:04:36.33,000 --> 0:04:38,000
y piensan que la computación lo es todo,

77
0:04:38.33,000 --> 0:04:4,000
pero, la mecánica todavía es importante.

78
0:04:40.33,000 --> 0:04:43,000
Aquí hay otro robot, el PackBot,

79
0:04:43.33,000 --> 0:04:45,000
que hemos estado construyendo durante algunos años.

80
0:04:45.33,000 --> 0:04:51,000
Es un robot militar de vigilancia, para ir delante de las tropas,

81
0:04:51.33,000 --> 0:04:54,000
y examinar cavernas, por ejemplo.

82
0:04:54.33,000 --> 0:04:56,000
Pero, tuvimos que hacerlo muy robusto,

83
0:04:56.33,000 --> 0:05:03,000
mucho más robusto que los robots que construimos en nuestros laboratorios.

84
0:05:03.33,000 --> 0:05:06,000
(Risas)

85
0:05:12.33,000 --> 0:05:16,000
A bordo de ese robot hay una PC que corre Linux.

86
0:05:16.33,000 --> 0:05:22,000
Esto puede soportar un golpe de 400G. El robot tienen inteligencia local:

87
0:05:22.33,000 --> 0:05:28,000
puede voltearse boca arriba él solo, y puede ubicarse dentro del rango de alcance de comunicaciones,

88
0:05:28.33,000 --> 0:05:31,000
puede subir escaleras por sí solo, etcétera.

89
0:05:38.33,000 --> 0:05:42,000
Bien, aquí está haciendo navegación local.

90
0:05:42.33,000 --> 0:05:48,000
Un soldado le da un comando de subir las escaleras y lo hace.

91
0:05:49.33,000 --> 0:05:52,000
¡Ese no fue un descenso controlado!

92
0:05:52.33,000 --> 0:05:54,000
(Risas)

93
0:05:54.33,000 --> 0:05:56,000
Ahora se va.

94
0:05:56.33,000 --> 0:06:01,000
Y la gran prueba para estos robots, realmente, fue el 11 de Septiembre.

95
0:06:01.33,000 --> 0:06:05,000
Llevamos los robots hasta el World Trade Center tarde esa noche.

96
0:06:06.33,000 --> 0:06:08,000
No pudieron hacer mucho en la montaña de escombros,

97
0:06:08.33,000 --> 0:06:11,000
todo estaba muy -- no había nada que hacer.

98
0:06:11.33,000 --> 0:06:16,000
Pero, sí penetramos en todos lo edificios circundantes que habían sido evacuados,

99
0:06:16.33,000 --> 0:06:19,000
y buscamos posibles sobrevivientes en esos edificios

100
0:06:19.33,000 --> 0:06:21,000
en los cuales era peligroso entrar.

101
0:06:21.33,000 --> 0:06:23,000
Veamos este video.

102
0:06:23.33,000 --> 0:06:26,000
Reportera: ... unos compañeros del campo de batalla están ayudando a reducir los riesgos del combate.

103
0:06:26.33,000 --> 0:06:29,000
Nick Robertson tiene esa historia.

104
0:06:31.33,000 --> 0:06:33,000
Rodney Brooks: ¿Podemos tener otro de estos?

105
0:06:38.33,000 --> 0:06:4,000
OK, bien.

106
0:06:43.33,000 --> 0:06:46,000
Bien, este es un soldado que ha visto un robot hace dos semanas.

107
0:06:48.33,000 --> 0:06:52,000
Está enviando robots dentro de cuevas, viendo lo que ocurre.

108
0:06:52.33,000 --> 0:06:54,000
El robot es totalmente autónomo.

109
0:06:54.33,000 --> 0:06:58,000
Lo peor que ha ocurrido dentro de la cueva, hasta ahora,

110
0:06:58.33,000 --> 0:07:01,000
es que uno de los robots cayó diez metros.

111
0:07:08.33,000 --> 0:07:11,000
Así que, hace un año, el ejército de los Estados Unidos no tenía estos robots.

112
0:07:11.33,000 --> 0:07:13,000
Ahora están en servicio activo en Afganistán todos los días.

113
0:07:13.33,000 --> 0:07:16,000
Y esa es una de las razones por las que dicen que se está dando una invasión de robots.

114
0:07:16.33,000 --> 0:07:2,000
Hay un gran cambio ahora en cómo -- hacia dónde la tecnología está caminando.

115
0:07:20.33,000 --> 0:07:22,000
Gracias

116
0:07:23.33,000 --> 0:07:25,000
Y en los próximos meses,

117
0:07:25.33,000 --> 0:07:28,000
vamos a estar enviando robots que están en producción

118
0:07:28.33,000 --> 0:07:32,000
a perforar pozos de petróleo para sacar de la tierra esos pocos años de petróleo que quedan.

119
0:07:32.33,000 --> 0:07:36,000
Son ambientes muy hostiles, 150 grados centígrados, 10,000 PSI.

120
0:07:36.33,000 --> 0:07:4,000
Robots autónomos descendiendo y haciendo este tipo de trabajo.

121
0:07:40.33,000 --> 0:07:43,000
Pero, robots como éstos, son un poco difíciles de programar.

122
0:07:43.33,000 --> 0:07:45,000
¿Cómo, en el futuro, vamos a programar nuestros robots

123
0:07:45.33,000 --> 0:07:47,000
y a hacerlos más fáciles de usar?

124
0:07:47.33,000 --> 0:07:5,000
Y, de hecho, quiero usar un robot en este momento --

125
0:07:50.33,000 --> 0:07:55,000
un robot llamado Chris -- ponte de pie. Sí. ¡Bien!

126
0:07:57.33,000 --> 0:08:01,000
Ven aquí. Ahora fíjense, él cree que los robots tienen que ser un poco tiesos.

127
0:08:01.33,000 --> 0:08:04,000
Él hace un poco eso. Pero voy a --

128
0:08:04.33,000 --> 0:08:06,000
Chris Anderson: Es sólo que soy inglés. RB: Oh.

129
0:08:06.33,000 --> 0:08:08,000
(Risas)

130
0:08:08.33,000 --> 0:08:1,000
(Aplauso)

131
0:08:10.33,000 --> 0:08:13,000
Voy a enseñarle a este robot una tarea. Es una tarea muy compleja.

132
0:08:13.33,000 --> 0:08:16,000
Ahora, fíjense, él asintió con la cabeza, dándome una indicación de que

133
0:08:16.33,000 --> 0:08:19,000
estaba comprendiendo el flujo de comunicación.

134
0:08:19.33,000 --> 0:08:21,000
Y, si hubiese dicho algo totalmente extraño

135
0:08:21.33,000 --> 0:08:24,000
él me habría mirado de lado, con dudas, y habría regulado la conversación.

136
0:08:24.33,000 --> 0:08:27,000
Bueno, ahora traje esto frente a él.

137
0:08:27.33,000 --> 0:08:31,000
Miré sus ojos y ví que sus ojos miraron la tapa de la botella.

138
0:08:31.33,000 --> 0:08:33,000
Y estoy realizando esta tarea aquí, y él está observando.

139
0:08:33.33,000 --> 0:08:36,000
Sus ojos van y vienen hacia mi para ver qué estoy mirando,

140
0:08:36.33,000 --> 0:08:38,000
así que tenemos atención compartida.

141
0:08:38.33,000 --> 0:08:41,000
Y entonces, realizo esta tarea, y él observa, y me mira

142
0:08:41.33,000 --> 0:08:45,000
para ver qué ocurrirá luego. Y ahora le pasaré la botella,

143
0:08:45.33,000 --> 0:08:47,000
y veremos si puede realizar la tarea. ¿Puedes hacer eso?

144
0:08:47.33,000 --> 0:08:5,000
(Risas)

145
0:08:50.33,000 --> 0:08:54,000
Bien. Lo hace muy bien. Sí. Bien, bien, bien.

146
0:08:54.33,000 --> 0:08:56,000
No te enseñé a hacer eso.

147
0:08:56.33,000 --> 0:08:58,000
Ahora veamos si lo puedes armar todo de nuevo.

148
0:08:58.33,000 --> 0:09:,000
(Risas)

149
0:09:00.33,000 --> 0:09:01,000
Y piensa que un robot tiene que ser muy lento.

150
0:09:01.33,000 --> 0:09:03,000
¡Buen robot, bien hecho!

151
0:09:03.33,000 --> 0:09:05,000
Bien, aquí vimos unas cuantas cosas.

152
0:09:06.33,000 --> 0:09:09,000
Vimos que cuando interactuamos,

153
0:09:09.33,000 --> 0:09:13,000
tratamos de enseñarle a alguien cómo hacer algo, dirigimos su atención visual.

154
0:09:13.33,000 --> 0:09:17,000
Lo otro que nos comunica es su estado interno,

155
0:09:17.33,000 --> 0:09:2,000
si nos está comprendiendo o no, regula nuestra interacción social.

156
0:09:20.33,000 --> 0:09:22,000
Hubo atención compartida al mirar el mismo tipo de cosas,

157
0:09:22.33,000 --> 0:09:26,000
y reconocimiento del refuerzo social al final.

158
0:09:26.33,000 --> 0:09:29,000
Y hemos estado tratando de incluir eso en los robots de nuestro laboratorio

159
0:09:29.33,000 --> 0:09:33,000
porque pensamos que es así como ustedes querrán interactuar con los robots en el futuro.

160
0:09:33.33,000 --> 0:09:35,000
Sólo quiero mostrarles un diagrama técnico aquí.

161
0:09:35.33,000 --> 0:09:39,000
Lo más importante para construir un robot con el cual se pueda interactuar socialmente

162
0:09:39.33,000 --> 0:09:41,000
es su sistema de atención visual.

163
0:09:41.33,000 --> 0:09:44,000
Porque a lo que le presta atención es a lo que está viendo

164
0:09:44.33,000 --> 0:09:47,000
y con lo que está interactuando, y es así que comprendes lo que está haciendo.

165
0:09:47.33,000 --> 0:09:5,000
Así que, en los videos que voy a mostrarles,

166
0:09:50.33,000 --> 0:09:54,000
van a ver un sistema de atención visual de un robot

167
0:09:54.33,000 --> 0:09:58,000
el cual tiene -- busca tonos de piel en el espacio HSV,

168
0:09:58.33,000 --> 0:10:02,000
así que trabaja a través de todos los, bueno, los colores humanos.

169
0:10:02.33,000 --> 0:10:04,000
Busca colores muy saturados, en los juguetes.

170
0:10:04.33,000 --> 0:10:06,000
Y busca cosas que se mueven.

171
0:10:06.33,000 --> 0:10:09,000
Y compara todo eso en una ventana de atención,

172
0:10:09.33,000 --> 0:10:11,000
y busca el lugar de mayor puntuación --

173
0:10:11.33,000 --> 0:10:13,000
aquello en donde lo más interesante está ocurriendo.

174
0:10:13.33,000 --> 0:10:17,000
Y es a eso a lo que sus ojos siguen.

175
0:10:17.33,000 --> 0:10:19,000
Y mira directo hacia eso.

176
0:10:19.33,000 --> 0:10:22,000
Al mismo tiempo, toma decisiones comunes: :

177
0:10:22.33,000 --> 0:10:25,000
puede decidir que se siente solo y buscar tono de piel,

178
0:10:25.33,000 --> 0:10:28,000
o podría decidir que está aburrido y buscar un juguete para jugar.

179
0:10:28.33,000 --> 0:10:3,000
Así que estos pesos cambian.

180
0:10:30.33,000 --> 0:10:32,000
Y aquí arriba, a la derecha,

181
0:10:32.33,000 --> 0:10:35,000
esto es lo que llamamos el módulo en memoria de Steven Spielberg.

182
0:10:35.33,000 --> 0:10:37,000
¿Vieron la película IA? Audiencia: Sí.

183
0:10:37.33,000 --> 0:10:39,000
RB: Sí, era bastante mala, pero --

184
0:10:39.33,000 --> 0:10:43,000
¿recuerdan, especialmente cuando Haley Joel Osment, el pequeño robot,

185
0:10:43.33,000 --> 0:10:47,000
miró al hada azul por 2,000 años sin quitar sus ojos de ella?

186
0:10:47.33,000 --> 0:10:49,000
Bien, esto elimina ese asunto,

187
0:10:49.33,000 --> 0:10:53,000
porque esto es una habituación Gaussiana que se vuelve negativa,

188
0:10:53.33,000 --> 0:10:56,000
y más y más intensa cuando mira algo.

189
0:10:56.33,000 --> 0:10:59,000
Y se aburre, así que entonces mira hacia otro lado.

190
0:10:59.33,000 --> 0:11:03,000
Así que, una vez que tienes eso -- y aquí está un robot, aquí está Kismet,

191
0:11:03.33,000 --> 0:11:07,000
mirando alrededor buscando un juguete. Puedes darte cuenta de lo que está mirando.

192
0:11:07.33,000 --> 0:11:12,000
Puedes estimar la dirección de su mirada por esos globos oculares que cubren la cámara,

193
0:11:12.33,000 --> 0:11:15,000
y puedes darte cuenta de cuándo está viendo directamente al juguete.

194
0:11:15.33,000 --> 0:11:17,000
Y tiene algo de respuesta emocional aquí.

195
0:11:17.33,000 --> 0:11:18,000
(Risas)

196
0:11:18.33,000 --> 0:11:2,000
Pero, aún así va a poner atención

197
0:11:20.33,000 --> 0:11:24,000
si algo más significativo penetra en su campo visual --

198
0:11:24.33,000 --> 0:11:28,000
como, por ejemplo Cynthia Breazeal, quien construyó este robot -- desde la derecha.

199
0:11:28.33,000 --> 0:11:33,000
La mira, le presta atención a ella.

200
0:11:33.33,000 --> 0:11:37,000
Kismet tiene, subyacente, un espacio emocional tridimensional,

201
0:11:37.33,000 --> 0:11:4,000
un espacio vectorial, de dónde se encuentra emocionalmente.

202
0:11:40.33,000 --> 0:11:45,000
Y en diferentes lugares de ese espacio él expresa --

203
0:11:46.33,000 --> 0:11:48,000
¿podemos tener volumen aquí?

204
0:11:48.33,000 --> 0:11:5,000
¿Pueden escuchar eso ahora, ahí? Audiencia: Sí.

205
0:11:50.33,000 --> 0:11:55,000
Kismet: ¿Realmente piensas eso? ¿Realmente piensas eso?

206
0:11:57.33,000 --> 0:11:59,000
¿Realmente lo piensas?

207
0:12:00.33,000 --> 0:12:03,000
RB: Así que está expresando su emoción mediante su cara

208
0:12:03.33,000 --> 0:12:05,000
y la prosodia en su voz.

209
0:12:05.33,000 --> 0:12:09,000
Y, cuando yo estaba trabajando con mi robot aquí,

210
0:12:09.33,000 --> 0:12:12,000
Chris, el robot, estaba midiendo la prosodia en mi voz,

211
0:12:12.33,000 --> 0:12:17,000
así que hicimos que el robot midiera la prosodia a partir de cuatro mensajes básicos

212
0:12:17.33,000 --> 0:12:21,000
que las madres dan a sus hijos de forma pre-lingüística.

213
0:12:21.33,000 --> 0:12:24,000
Aquí tenemos sujetos ingenuos elogiando al robot,

214
0:12:26.33,000 --> 0:12:28,000
Voz: Lindo robot.

215
0:12:29.33,000 --> 0:12:31,000
Eres un robot tan lindo.

216
0:12:31.33,000 --> 0:12:33,000
(Risas)

217
0:12:33.33,000 --> 0:12:35,000
Y el robot está reaccionando adecuadamente.

218
0:12:35.33,000 --> 0:12:39,000
Voz: ... muy bien, Kismet.

219
0:12:40.33,000 --> 0:12:42,000
(Risas)

220
0:12:42.33,000 --> 0:12:44,000
Voz: Mira mi sonrisa.

221
0:12:46.33,000 --> 0:12:49,000
RB: Se sonríe. Imita la sonrisa. Esto pasa muchas veces.

222
0:12:49.33,000 --> 0:12:51,000
Esos son sujetos ingenuos.

223
0:12:51.33,000 --> 0:12:54,000
Aquí les pedimos que captaran la atención del robot

224
0:12:54.33,000 --> 0:12:57,000
y que indicaran cuándo tenían la atención del robot.

225
0:12:57.33,000 --> 0:13:01,000
Voz: Hey, Kismet, ah, ahí está.

226
0:13:01.33,000 --> 0:13:05,000
RB: Así que se da cuenta de que tiene la atención del robot.

227
0:13:08.33,000 --> 0:13:12,000
Voz: Kismet, ¿te gusta el juguete? Oh.

228
0:13:13.33,000 --> 0:13:15,000
RB: Ahora aquí les pedimos que prohibieran algo al robot,

229
0:13:15.33,000 --> 0:13:19,000
y la primera mujer realmente empuja al robot hacia un rincón emocional.

230
0:13:19.33,000 --> 0:13:24,000
Voz: No. No. No debes hacer eso. ¡No!

231
0:13:24.33,000 --> 0:13:27,000
(Risas)

232
0:13:27.33,000 --> 0:13:33,000
Voz: No es apropiado. No. No.

233
0:13:33.33,000 --> 0:13:36,000
(Risas)

234
0:13:36.33,000 --> 0:13:38,000
RB: Lo voy a dejar ahí.

235
0:13:38.33,000 --> 0:13:4,000
Integramos todo eso. Luego agregamos el tomar turnos.

236
0:13:40.33,000 --> 0:13:43,000
Cuando conversamos con alguien, hablamos.

237
0:13:43.33,000 --> 0:13:47,000
Luego levantamos nuestras cejas, movemos nuestros ojos,

238
0:13:47.33,000 --> 0:13:5,000
para darle a la otra persona la idea de que es su turno de hablar.

239
0:13:50.33,000 --> 0:13:54,000
Y luego ellos hablan, y nos pasamos la batuta el uno al otro.

240
0:13:54.33,000 --> 0:13:56,000
Así que ponemos esto en el robot.

241
0:13:56.33,000 --> 0:13:58,000
Buscamos un grupo de sujetos ingenuos,

242
0:13:58.33,000 --> 0:14:,000
no les dijimos nada acerca del robot,

243
0:14:00.33,000 --> 0:14:02,000
los sentamos frente al robot y les dijimos "habla con el robot".

244
0:14:02.33,000 --> 0:14:04,000
Ahora, lo que no sabían era que,

245
0:14:04.33,000 --> 0:14:06,000
el robot no comprendía ni una palabra de lo que ellos le decían,

246
0:14:06.33,000 --> 0:14:09,000
y que el robot no estaba hablando Inglés.

247
0:14:09.33,000 --> 0:14:11,000
Sólo pronunciaba fonemas de Inglés que eran aleatorios.

248
0:14:11.33,000 --> 0:14:13,000
Y quiero que vean con atención, al inicio de esto,

249
0:14:13.33,000 --> 0:14:17,000
cuando esta persona, Ritchie, quien habló con el robot por 25 minutos --

250
0:14:17.33,000 --> 0:14:19,000
(Risas)

251
0:14:19.33,000 --> 0:14:21,000
-- dice "Quiero mostrarte algo.

252
0:14:21.33,000 --> 0:14:23,000
Quiero mostrarte mi reloj."

253
0:14:23.33,000 --> 0:14:28,000
Y coloca el centro del reloj dentro del campo visual del robot,

254
0:14:28.33,000 --> 0:14:3,000
lo señala, le da una pista emocional,

255
0:14:30.33,000 --> 0:14:32,000
y el robot mira al reloj exitosamente.

256
0:14:32.33,000 --> 0:14:35,000
No sabemos si él entendió o no que el robot --

257
0:14:36.33,000 --> 0:14:38,000
Fíjense en la toma de turnos.

258
0:14:38.33,000 --> 0:14:41,000
Ritchie: Bien, quiero mostrarte algo. Bueno, esto es un reloj

259
0:14:41.33,000 --> 0:14:44,000
que mi novia me regaló.

260
0:14:44.33,000 --> 0:14:46,000
Robot: Oh, bien.

261
0:14:46.33,000 --> 0:14:5,000
Ritchie: Sí, mira, tiene incluso una pequeña luz azul aquí. casi lo pierdo esta semana.

262
0:14:51.33,000 --> 0:14:55,000
(Risas)

263
0:14:55.33,000 --> 0:14:58,000
RB: Así que hace contacto visual con él, siguiendo sus ojos.

264
0:14:58.33,000 --> 0:15:,000
Ritchie: ¿Puedes hacer lo mismo? Robot: Sí, claro.

265
0:15:00.33,000 --> 0:15:02,000
RB: Y ellos tienen este tipo de comunicación con éxito.

266
0:15:02.33,000 --> 0:15:06,000
Y aquí hay otro aspecto del tipo de cosas que Chris y yo estuvimos haciendo.

267
0:15:06.33,000 --> 0:15:08,000
Este es otro robot, Cog.

268
0:15:08.33,000 --> 0:15:14,000
Ellos primero hicieron contacto visual, y luego, cuando Christie mira hacia este juguete,

269
0:15:14.33,000 --> 0:15:16,000
el robot estima la dirección de la mirada de ella

270
0:15:16.33,000 --> 0:15:18,000
y mira el mismo objeto que ella está mirando.

271
0:15:18.33,000 --> 0:15:19,000
(Risas)

272
0:15:19.33,000 --> 0:15:22,000
Vamos a ver más y más de este tipo de robots

273
0:15:22.33,000 --> 0:15:24,000
en los próximos años en los laboratorios.

274
0:15:24.33,000 --> 0:15:29,000
Pero, entonces las grandes preguntas, dos grandes preguntas que la gente me hace son:

275
0:15:29.33,000 --> 0:15:31,000
si hacemos a estos robots cada vez más parecidos a los humanos,

276
0:15:31.33,000 --> 0:15:36,000
¿los aceptaremos? -- ¿tendrán derechos eventualmente?

277
0:15:36.33,000 --> 0:15:39,000
Y la otra pregunta que la gente me hace es, ¿querrán ellos tomar el control?

278
0:15:39.33,000 --> 0:15:4,000
(Risas)

279
0:15:40.33,000 --> 0:15:43,000
En cuanto a la primera -- saben, esto ha sido mucho un tema de Hollywood

280
0:15:43.33,000 --> 0:15:46,000
en muchas películas. Probablemente reconozcan a estos personajes aquí --

281
0:15:46.33,000 --> 0:15:5,000
en cada uno de estos casos, los robots querían más respeto.

282
0:15:50.33,000 --> 0:15:53,000
Bueno pero, realmente necesitamos respetar a los robots?

283
0:15:54.33,000 --> 0:15:56,000
Son sólo máquinas, después de todo.

284
0:15:56.33,000 --> 0:16:,000
Pero creo que, bueno, tenemos que aceptar que nosotros somos sólo máquinas.

285
0:16:00.33,000 --> 0:16:05,000
Después de todo, eso es en verdad lo que la biología molecular moderna dice sobre nosotros.

286
0:16:05.33,000 --> 0:16:08,000
Tú no ves una descripción de cómo, bueno,

287
0:16:08.33,000 --> 0:16:12,000
la Molécula A, bueno, llega y se une a esta otra molecula.

288
0:16:12.33,000 --> 0:16:15,000
Y entonces se mueve hacia adelante, bueno, impulsada por varias cargas,

289
0:16:15.33,000 --> 0:16:19,000
y luego el alma entra y pellizca esas moleculas para que se conecten.

290
0:16:19.33,000 --> 0:16:22,000
Es todo mecánico, somos mecanismos.

291
0:16:22.33,000 --> 0:16:25,000
Si somos máquinas, entonces en principio al menos,

292
0:16:25.33,000 --> 0:16:29,000
deberíamos ser capaces de contruir máquinas con otros materiales,

293
0:16:29.33,000 --> 0:16:33,000
que estén tan vivas como estamos nosotros.

294
0:16:33.33,000 --> 0:16:35,000
Pero creo que para que podamos admitir esto,

295
0:16:35.33,000 --> 0:16:38,000
debemos renunciar a ser especiales, de alguna manera.

296
0:16:38.33,000 --> 0:16:4,000
Y hemos tenido una retirada de esto de ser especiales

297
0:16:40.33,000 --> 0:16:43,000
bajo la descarga de artillería de la ciencia y de la tecnología muchas veces

298
0:16:43.33,000 --> 0:16:45,000
en los últimos cientos de años, al menos.

299
0:16:45.33,000 --> 0:16:47,000
Hace 500 años tuvimos que abandonar la idea

300
0:16:47.33,000 --> 0:16:5,000
de que éramos el centro del universo

301
0:16:50.33,000 --> 0:16:52,000
cuando la Tierra comenzó a girar alrededor del Sol;

302
0:16:52.33,000 --> 0:16:57,000
hace 150 años, con Darwin, tuvimos que abandonar la idea de que éramos diferentes de los animales.

303
0:16:57.33,000 --> 0:17:,000
Y que, bueno, imaginar -- saben, siempre es difícil para nosotros.

304
0:17:00.33,000 --> 0:17:03,000
Incluso, recientemente nos han maltratado con la idea de que tal vez

305
0:17:03.33,000 --> 0:17:05,000
ni siquiera tuvimos nuestro evento de creación, aquí en la Tierra,

306
0:17:05.33,000 --> 0:17:08,000
lo cual a la gente no le gustó mucho. Y luego el genoma humano dijo que

307
0:17:08.33,000 --> 0:17:11,000
tal vez sólo tenemos unos 35,000 genes. Y eso fue realmente --

308
0:17:11.33,000 --> 0:17:14,000
a la gente no le gustó, tenemos más genes que eso.

309
0:17:14.33,000 --> 0:17:17,000
No nos gusta abandonar nuestra especialidad, así que, saben,

310
0:17:17.33,000 --> 0:17:19,000
la idea de que los robots puedan en verdad tener emociones,

311
0:17:19.33,000 --> 0:17:21,000
o que los robots puedan ser criaturas vivientes --

312
0:17:21.33,000 --> 0:17:23,000
creo que eso va a ser difícil de aceptar para nosotros.

313
0:17:23.33,000 --> 0:17:27,000
Pero, lo aceptaremos de aquí a los próximos 50 años, más o menos.

314
0:17:27.33,000 --> 0:17:3,000
Y la segunda pregunta es, ¿querrán las máquinas tomar el control?

315
0:17:30.33,000 --> 0:17:35,000
Y aquí el escenario es que nosotros creamos estas cosas,

316
0:17:35.33,000 --> 0:17:38,000
ellas crecen, las alimentamos, ellas aprenden mucho de nosotros,

317
0:17:38.33,000 --> 0:17:42,000
y ellas comienzar a pensar que nosotros somos muy aburridos, lentos.

318
0:17:42.33,000 --> 0:17:44,000
Ellas quieren controlarnos.

319
0:17:44.33,000 --> 0:17:47,000
Y para aquellos de ustedes que tienen adolescentes, saben lo que se siente.

320
0:17:47.33,000 --> 0:17:48,000
(Risas)

321
0:17:48.33,000 --> 0:17:51,000
Pero, Hollywood lo extiende hasta los robots.

322
0:17:51.33,000 --> 0:17:54,000
Y la pregunta es, bueno,

323
0:17:54.33,000 --> 0:17:58,000
¿construirá alguien, accidentalmente, un robot que quiera tomar el control?

324
0:17:58.33,000 --> 0:18:01,000
Y es un poco este tipo solitario, en el patio trasero de su casa,

325
0:18:01.33,000 --> 0:18:04,000
y, bueno, "accidentalmente construí un 747."

326
0:18:04.33,000 --> 0:18:06,000
No creo que eso vaya a pasar.

327
0:18:06.33,000 --> 0:18:08,000
Y no creo que --

328
0:18:08.33,000 --> 0:18:09,000
(Risas)

329
0:18:09.33,000 --> 0:18:12,000
-- no creo que vamos deliberadamente a construir robots

330
0:18:12.33,000 --> 0:18:14,000
con los cuales nos sintamos incómodos.

331
0:18:14.33,000 --> 0:18:16,000
Bueno, pues, no van a construir un robot súper malvado.

332
0:18:16.33,000 --> 0:18:19,000
Antes que eso tendrá que haber, bueno, un robot medianamente malvado,

333
0:18:19.33,000 --> 0:18:21,000
y antes que ese uno que casi no sea malvado.

334
0:18:21.33,000 --> 0:18:22,000
(Risas)

335
0:18:22.33,000 --> 0:18:24,000
Y no vamos a dejar que eso ocurra.

336
0:18:24.33,000 --> 0:18:25,000
(Risas)

337
0:18:25.33,000 --> 0:18:31,000
Así que lo voy a dejar ahí: los robots ya vienen,

338
0:18:31.33,000 --> 0:18:34,000
no tenemos mucho de qué preocuparnos, será muy divertido,

339
0:18:34.33,000 --> 0:18:38,000
y espero que todos ustedes disfruten el viaje de los próximos 50 años.

340
0:18:38.33,000 --> 0:18:4,000
(Aplauso)

