1
0:00:12.988,000 --> 0:00:14,000
Let me tell you a story.

2
0:00:15.304,000 --> 0:00:16,000
It goes back 200 million years.

3
0:00:17.103,000 --> 0:00:18,000
It's a story of the neocortex,

4
0:00:19.087,000 --> 0:00:2,000
which means "new rind."

5
0:00:21.061,000 --> 0:00:23,000
So in these early mammals,

6
0:00:23.492,000 --> 0:00:25,000
because only mammals have a neocortex,

7
0:00:25.547,000 --> 0:00:26,000
rodent-like creatures.

8
0:00:27.211,000 --> 0:00:3,000
It was the size of a postage stamp and just as thin,

9
0:00:30.79,000 --> 0:00:31,000
and was a thin covering around

10
0:00:32.229,000 --> 0:00:34,000
their walnut-sized brain,

11
0:00:34.493,000 --> 0:00:37,000
but it was capable of a new type of thinking.

12
0:00:38.194,000 --> 0:00:39,000
Rather than the fixed behaviors

13
0:00:39.761,000 --> 0:00:4,000
that non-mammalian animals have,

14
0:00:41.753,000 --> 0:00:43,000
it could invent new behaviors.

15
0:00:44.445,000 --> 0:00:46,000
So a mouse is escaping a predator,

16
0:00:46.998,000 --> 0:00:47,000
its path is blocked,

17
0:00:48.538,000 --> 0:00:5,000
it'll try to invent a new solution.

18
0:00:50.667,000 --> 0:00:51,000
That may work, it may not,

19
0:00:51.933,000 --> 0:00:52,000
but if it does, it will remember that

20
0:00:53.843,000 --> 0:00:54,000
and have a new behavior,

21
0:00:55.135,000 --> 0:00:56,000
and that can actually spread virally

22
0:00:56.592,000 --> 0:00:58,000
through the rest of the community.

23
0:00:58.787,000 --> 0:00:59,000
Another mouse watching this could say,

24
0:01:00.396,000 --> 0:01:02,000
"Hey, that was pretty clever, going around that rock,"

25
0:01:03.1,000 --> 0:01:06,000
and it could adopt a new behavior as well.

26
0:01:06.825,000 --> 0:01:07,000
Non-mammalian animals

27
0:01:08.542,000 --> 0:01:09,000
couldn't do any of those things.

28
0:01:10.255,000 --> 0:01:11,000
They had fixed behaviors.

29
0:01:11.47,000 --> 0:01:12,000
Now they could learn a new behavior

30
0:01:12.801,000 --> 0:01:14,000
but not in the course of one lifetime.

31
0:01:15.377,000 --> 0:01:16,000
In the course of maybe a thousand lifetimes,

32
0:01:17.144,000 --> 0:01:2,000
it could evolve a new fixed behavior.

33
0:01:20.474,000 --> 0:01:23,000
That was perfectly okay 200 million years ago.

34
0:01:23.851,000 --> 0:01:24,000
The environment changed very slowly.

35
0:01:25.832,000 --> 0:01:26,000
It could take 10,000 years for there to be

36
0:01:27.386,000 --> 0:01:29,000
a significant environmental change,

37
0:01:29.478,000 --> 0:01:3,000
and during that period of time

38
0:01:30.86,000 --> 0:01:32,000
it would evolve a new behavior.

39
0:01:33.789,000 --> 0:01:34,000
Now that went along fine,

40
0:01:35.31,000 --> 0:01:36,000
but then something happened.

41
0:01:37.014,000 --> 0:01:39,000
Sixty-five million years ago,

42
0:01:39.26,000 --> 0:01:41,000
there was a sudden, violent change to the environment.

43
0:01:41.875,000 --> 0:01:44,000
We call it the Cretaceous extinction event.

44
0:01:45.38,000 --> 0:01:47,000
That's when the dinosaurs went extinct,

45
0:01:47.673,000 --> 0:01:5,000
that's when 75 percent of the

46
0:01:51.122,000 --> 0:01:53,000
animal and plant species went extinct,

47
0:01:53.868,000 --> 0:01:54,000
and that's when mammals

48
0:01:55.613,000 --> 0:01:57,000
overtook their ecological niche,

49
0:01:57.765,000 --> 0:02:,000
and to anthropomorphize, biological evolution said,

50
0:02:01.419,000 --> 0:02:03,000
"Hmm, this neocortex is pretty good stuff,"

51
0:02:03.444,000 --> 0:02:04,000
and it began to grow it.

52
0:02:05.237,000 --> 0:02:06,000
And mammals got bigger,

53
0:02:06.579,000 --> 0:02:08,000
their brains got bigger at an even faster pace,

54
0:02:09.494,000 --> 0:02:12,000
and the neocortex got bigger even faster than that

55
0:02:13.301,000 --> 0:02:15,000
and developed these distinctive ridges and folds

56
0:02:16.23,000 --> 0:02:18,000
basically to increase its surface area.

57
0:02:19.111,000 --> 0:02:2,000
If you took the human neocortex

58
0:02:20.93,000 --> 0:02:21,000
and stretched it out,

59
0:02:22.231,000 --> 0:02:23,000
it's about the size of a table napkin,

60
0:02:23.944,000 --> 0:02:24,000
and it's still a thin structure.

61
0:02:25.25,000 --> 0:02:26,000
It's about the thickness of a table napkin.

62
0:02:27.23,000 --> 0:02:29,000
But it has so many convolutions and ridges

63
0:02:29.727,000 --> 0:02:32,000
it's now 80 percent of our brain,

64
0:02:32.802,000 --> 0:02:34,000
and that's where we do our thinking,

65
0:02:35.263,000 --> 0:02:36,000
and it's the great sublimator.

66
0:02:37.024,000 --> 0:02:38,000
We still have that old brain

67
0:02:38.138,000 --> 0:02:4,000
that provides our basic drives and motivations,

68
0:02:40.902,000 --> 0:02:42,000
but I may have a drive for conquest,

69
0:02:43.618,000 --> 0:02:45,000
and that'll be sublimated by the neocortex

70
0:02:46.333,000 --> 0:02:48,000
into writing a poem or inventing an app

71
0:02:49.242,000 --> 0:02:5,000
or giving a TED Talk,

72
0:02:50.751,000 --> 0:02:53,000
and it's really the neocortex that's where

73
0:02:54.373,000 --> 0:02:55,000
the action is.

74
0:02:56.341,000 --> 0:02:57,000
Fifty years ago, I wrote a paper

75
0:02:58.058,000 --> 0:02:59,000
describing how I thought the brain worked,

76
0:02:59.976,000 --> 0:03:02,000
and I described it as a series of modules.

77
0:03:03.175,000 --> 0:03:05,000
Each module could do things with a pattern.

78
0:03:05.303,000 --> 0:03:07,000
It could learn a pattern. It could remember a pattern.

79
0:03:08.049,000 --> 0:03:09,000
It could implement a pattern.

80
0:03:09.456,000 --> 0:03:11,000
And these modules were organized in hierarchies,

81
0:03:12.135,000 --> 0:03:14,000
and we created that hierarchy with our own thinking.

82
0:03:15.089,000 --> 0:03:18,000
And there was actually very little to go on

83
0:03:18.422,000 --> 0:03:19,000
50 years ago.

84
0:03:19.984,000 --> 0:03:21,000
It led me to meet President Johnson.

85
0:03:22.099,000 --> 0:03:24,000
I've been thinking about this for 50 years,

86
0:03:24.272,000 --> 0:03:26,000
and a year and a half ago I came out with the book

87
0:03:27.1,000 --> 0:03:28,000
"How To Create A Mind,"

88
0:03:28.365,000 --> 0:03:29,000
which has the same thesis,

89
0:03:29.978,000 --> 0:03:31,000
but now there's a plethora of evidence.

90
0:03:32.79,000 --> 0:03:33,000
The amount of data we're getting about the brain

91
0:03:34.604,000 --> 0:03:36,000
from neuroscience is doubling every year.

92
0:03:36.807,000 --> 0:03:38,000
Spatial resolution of brainscanning of all types

93
0:03:39.461,000 --> 0:03:41,000
is doubling every year.

94
0:03:41.746,000 --> 0:03:42,000
We can now see inside a living brain

95
0:03:43.463,000 --> 0:03:45,000
and see individual interneural connections

96
0:03:46.333,000 --> 0:03:48,000
connecting in real time, firing in real time.

97
0:03:49.036,000 --> 0:03:51,000
We can see your brain create your thoughts.

98
0:03:51.455,000 --> 0:03:52,000
We can see your thoughts create your brain,

99
0:03:53.03,000 --> 0:03:54,000
which is really key to how it works.

100
0:03:55.029,000 --> 0:03:57,000
So let me describe briefly how it works.

101
0:03:57.248,000 --> 0:03:59,000
I've actually counted these modules.

102
0:03:59.523,000 --> 0:04:01,000
We have about 300 million of them,

103
0:04:01.569,000 --> 0:04:03,000
and we create them in these hierarchies.

104
0:04:03.798,000 --> 0:04:05,000
I'll give you a simple example.

105
0:04:05.88,000 --> 0:04:07,000
I've got a bunch of modules

106
0:04:08.685,000 --> 0:04:11,000
that can recognize the crossbar to a capital A,

107
0:04:12.088,000 --> 0:04:13,000
and that's all they care about.

108
0:04:14.002,000 --> 0:04:15,000
A beautiful song can play,

109
0:04:15.58,000 --> 0:04:16,000
a pretty girl could walk by,

110
0:04:17.014,000 --> 0:04:19,000
they don't care, but they see a crossbar to a capital A,

111
0:04:19.86,000 --> 0:04:22,000
they get very excited and they say "crossbar,"

112
0:04:22.881,000 --> 0:04:24,000
and they put out a high probability

113
0:04:24.993,000 --> 0:04:25,000
on their output axon.

114
0:04:26.627,000 --> 0:04:27,000
That goes to the next level,

115
0:04:27.96,000 --> 0:04:29,000
and these layers are organized in conceptual levels.

116
0:04:30.71,000 --> 0:04:31,000
Each is more abstract than the next one,

117
0:04:32.566,000 --> 0:04:34,000
so the next one might say "capital A."

118
0:04:34.984,000 --> 0:04:36,000
That goes up to a higher level that might say "Apple."

119
0:04:37.875,000 --> 0:04:39,000
Information flows down also.

120
0:04:40.042,000 --> 0:04:42,000
If the apple recognizer has seen A-P-P-L,

121
0:04:42.978,000 --> 0:04:45,000
it'll think to itself, "Hmm, I think an E is probably likely,"

122
0:04:46.197,000 --> 0:04:48,000
and it'll send a signal down to all the E recognizers

123
0:04:48.761,000 --> 0:04:49,000
saying, "Be on the lookout for an E,

124
0:04:50.38,000 --> 0:04:51,000
I think one might be coming."

125
0:04:51.936,000 --> 0:04:53,000
The E recognizers will lower their threshold

126
0:04:54.779,000 --> 0:04:55,000
and they see some sloppy thing, could be an E.

127
0:04:56.724,000 --> 0:04:57,000
Ordinarily you wouldn't think so,

128
0:04:58.214,000 --> 0:05:,000
but we're expecting an E, it's good enough,

129
0:05:00.223,000 --> 0:05:01,000
and yeah, I've seen an E, and then apple says,

130
0:05:02.04,000 --> 0:05:03,000
"Yeah, I've seen an Apple."

131
0:05:03.768,000 --> 0:05:04,000
Go up another five levels,

132
0:05:05.514,000 --> 0:05:06,000
and you're now at a pretty high level

133
0:05:06.867,000 --> 0:05:07,000
of this hierarchy,

134
0:05:08.436,000 --> 0:05:1,000
and stretch down into the different senses,

135
0:05:10.789,000 --> 0:05:12,000
and you may have a module that sees a certain fabric,

136
0:05:13.444,000 --> 0:05:15,000
hears a certain voice quality, smells a certain perfume,

137
0:05:16.288,000 --> 0:05:18,000
and will say, "My wife has entered the room."

138
0:05:18.801,000 --> 0:05:19,000
Go up another 10 levels, and now you're at

139
0:05:20.696,000 --> 0:05:21,000
a very high level.

140
0:05:21.856,000 --> 0:05:22,000
You're probably in the frontal cortex,

141
0:05:23.793,000 --> 0:05:26,000
and you'll have modules that say, "That was ironic.

142
0:05:27.56,000 --> 0:05:29,000
That's funny. She's pretty."

143
0:05:29.93,000 --> 0:05:31,000
You might think that those are more sophisticated,

144
0:05:32.035,000 --> 0:05:33,000
but actually what's more complicated

145
0:05:33.541,000 --> 0:05:35,000
is the hierarchy beneath them.

146
0:05:36.21,000 --> 0:05:38,000
There was a 16-year-old girl, she had brain surgery,

147
0:05:38.83,000 --> 0:05:4,000
and she was conscious because the surgeons

148
0:05:40.881,000 --> 0:05:41,000
wanted to talk to her.

149
0:05:42.418,000 --> 0:05:43,000
You can do that because there's no pain receptors

150
0:05:44.24,000 --> 0:05:45,000
in the brain.

151
0:05:45.278,000 --> 0:05:46,000
And whenever they stimulated particular,

152
0:05:47.078,000 --> 0:05:49,000
very small points on her neocortex,

153
0:05:49.541,000 --> 0:05:51,000
shown here in red, she would laugh.

154
0:05:52.206,000 --> 0:05:53,000
So at first they thought they were triggering

155
0:05:53.646,000 --> 0:05:54,000
some kind of laugh reflex,

156
0:05:55.366,000 --> 0:05:57,000
but no, they quickly realized they had found

157
0:05:57.885,000 --> 0:06:,000
the points in her neocortex that detect humor,

158
0:06:00.929,000 --> 0:06:01,000
and she just found everything hilarious

159
0:06:02.898,000 --> 0:06:04,000
whenever they stimulated these points.

160
0:06:05.335,000 --> 0:06:06,000
"You guys are so funny just standing around,"

161
0:06:07.26,000 --> 0:06:08,000
was the typical comment,

162
0:06:08.998,000 --> 0:06:1,000
and they weren't funny,

163
0:06:11.3,000 --> 0:06:14,000
not while doing surgery.

164
0:06:14.547,000 --> 0:06:18,000
So how are we doing today?

165
0:06:19.377,000 --> 0:06:22,000
Well, computers are actually beginning to master

166
0:06:22.431,000 --> 0:06:24,000
human language with techniques

167
0:06:24.432,000 --> 0:06:26,000
that are similar to the neocortex.

168
0:06:27.299,000 --> 0:06:28,000
I actually described the algorithm,

169
0:06:28.813,000 --> 0:06:3,000
which is similar to something called

170
0:06:30.867,000 --> 0:06:32,000
a hierarchical hidden Markov model,

171
0:06:33.1,000 --> 0:06:36,000
something I've worked on since the '90s.

172
0:06:36.341,000 --> 0:06:39,000
"Jeopardy" is a very broad natural language game,

173
0:06:39.579,000 --> 0:06:4,000
and Watson got a higher score

174
0:06:41.471,000 --> 0:06:43,000
than the best two players combined.

175
0:06:43.471,000 --> 0:06:45,000
It got this query correct:

176
0:06:45.97,000 --> 0:06:47,000
"A long, tiresome speech

177
0:06:48.055,000 --> 0:06:5,000
delivered by a frothy pie topping,"

178
0:06:50.207,000 --> 0:06:52,000
and it quickly responded, "What is a meringue harangue?"

179
0:06:53.003,000 --> 0:06:55,000
And Jennings and the other guy didn't get that.

180
0:06:55.638,000 --> 0:06:56,000
It's a pretty sophisticated example of

181
0:06:57.564,000 --> 0:06:58,000
computers actually understanding human language,

182
0:06:59.478,000 --> 0:07:,000
and it actually got its knowledge by reading

183
0:07:01.13,000 --> 0:07:04,000
Wikipedia and several other encyclopedias.

184
0:07:04.915,000 --> 0:07:06,000
Five to 10 years from now,

185
0:07:07.048,000 --> 0:07:09,000
search engines will actually be based on

186
0:07:09.232,000 --> 0:07:11,000
not just looking for combinations of words and links

187
0:07:12.026,000 --> 0:07:13,000
but actually understanding,

188
0:07:13.94,000 --> 0:07:15,000
reading for understanding the billions of pages

189
0:07:16.351,000 --> 0:07:18,000
on the web and in books.

190
0:07:19.084,000 --> 0:07:21,000
So you'll be walking along, and Google will pop up

191
0:07:21.7,000 --> 0:07:24,000
and say, "You know, Mary, you expressed concern

192
0:07:24.781,000 --> 0:07:27,000
to me a month ago that your glutathione supplement

193
0:07:27.8,000 --> 0:07:29,000
wasn't getting past the blood-brain barrier.

194
0:07:30.031,000 --> 0:07:32,000
Well, new research just came out 13 seconds ago

195
0:07:32.624,000 --> 0:07:33,000
that shows a whole new approach to that

196
0:07:34.335,000 --> 0:07:35,000
and a new way to take glutathione.

197
0:07:36.328,000 --> 0:07:38,000
Let me summarize it for you."

198
0:07:38.89,000 --> 0:07:41,000
Twenty years from now, we'll have nanobots,

199
0:07:42.574,000 --> 0:07:43,000
because another exponential trend

200
0:07:44.201,000 --> 0:07:45,000
is the shrinking of technology.

201
0:07:45.816,000 --> 0:07:47,000
They'll go into our brain

202
0:07:48.186,000 --> 0:07:49,000
through the capillaries

203
0:07:49.889,000 --> 0:07:51,000
and basically connect our neocortex

204
0:07:52.366,000 --> 0:07:55,000
to a synthetic neocortex in the cloud

205
0:07:55.551,000 --> 0:07:58,000
providing an extension of our neocortex.

206
0:07:59.142,000 --> 0:08:,000
Now today, I mean,

207
0:08:00.72,000 --> 0:08:01,000
you have a computer in your phone,

208
0:08:02.25,000 --> 0:08:04,000
but if you need 10,000 computers for a few seconds

209
0:08:05.004,000 --> 0:08:06,000
to do a complex search,

210
0:08:06.499,000 --> 0:08:09,000
you can access that for a second or two in the cloud.

211
0:08:09.895,000 --> 0:08:12,000
In the 2030s, if you need some extra neocortex,

212
0:08:12.99,000 --> 0:08:14,000
you'll be able to connect to that in the cloud

213
0:08:15.263,000 --> 0:08:16,000
directly from your brain.

214
0:08:16.911,000 --> 0:08:17,000
So I'm walking along and I say,

215
0:08:18.454,000 --> 0:08:19,000
"Oh, there's Chris Anderson.

216
0:08:19.817,000 --> 0:08:2,000
He's coming my way.

217
0:08:21.342,000 --> 0:08:23,000
I'd better think of something clever to say.

218
0:08:23.677,000 --> 0:08:24,000
I've got three seconds.

219
0:08:25.201,000 --> 0:08:28,000
My 300 million modules in my neocortex

220
0:08:28.298,000 --> 0:08:29,000
isn't going to cut it.

221
0:08:29.538,000 --> 0:08:3,000
I need a billion more."

222
0:08:30.784,000 --> 0:08:33,000
I'll be able to access that in the cloud.

223
0:08:34.107,000 --> 0:08:36,000
And our thinking, then, will be a hybrid

224
0:08:36.919,000 --> 0:08:39,000
of biological and non-biological thinking,

225
0:08:40.441,000 --> 0:08:41,000
but the non-biological portion

226
0:08:42.339,000 --> 0:08:44,000
is subject to my law of accelerating returns.

227
0:08:45.021,000 --> 0:08:47,000
It will grow exponentially.

228
0:08:47.26,000 --> 0:08:49,000
And remember what happens

229
0:08:49.276,000 --> 0:08:51,000
the last time we expanded our neocortex?

230
0:08:51.921,000 --> 0:08:52,000
That was two million years ago

231
0:08:53.347,000 --> 0:08:54,000
when we became humanoids

232
0:08:54.583,000 --> 0:08:55,000
and developed these large foreheads.

233
0:08:56.177,000 --> 0:08:58,000
Other primates have a slanted brow.

234
0:08:58.76,000 --> 0:08:59,000
They don't have the frontal cortex.

235
0:09:00.505,000 --> 0:09:03,000
But the frontal cortex is not really qualitatively different.

236
0:09:04.19,000 --> 0:09:06,000
It's a quantitative expansion of neocortex,

237
0:09:06.933,000 --> 0:09:08,000
but that additional quantity of thinking

238
0:09:09.636,000 --> 0:09:1,000
was the enabling factor for us to take

239
0:09:11.415,000 --> 0:09:14,000
a qualitative leap and invent language

240
0:09:14.761,000 --> 0:09:15,000
and art and science and technology

241
0:09:16.728,000 --> 0:09:17,000
and TED conferences.

242
0:09:18.182,000 --> 0:09:2,000
No other species has done that.

243
0:09:20.313,000 --> 0:09:22,000
And so, over the next few decades,

244
0:09:22.388,000 --> 0:09:23,000
we're going to do it again.

245
0:09:24.148,000 --> 0:09:26,000
We're going to again expand our neocortex,

246
0:09:26.422,000 --> 0:09:27,000
only this time we won't be limited

247
0:09:28.178,000 --> 0:09:32,000
by a fixed architecture of enclosure.

248
0:09:32.458,000 --> 0:09:35,000
It'll be expanded without limit.

249
0:09:35.762,000 --> 0:09:37,000
That additional quantity will again

250
0:09:38.005,000 --> 0:09:41,000
be the enabling factor for another qualitative leap

251
0:09:41.01,000 --> 0:09:42,000
in culture and technology.

252
0:09:42.645,000 --> 0:09:44,000
Thank you very much.

253
0:09:44.699,000 --> 0:09:47,000
(Applause)

