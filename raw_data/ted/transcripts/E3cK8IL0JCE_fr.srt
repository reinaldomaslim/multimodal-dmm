1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Juliet Vdt

2
0:00:13.143,000 --> 0:00:17,000
Ma relation avec internet me rappelle le début stéréotypé

3
0:00:17.566,000 --> 0:00:18,000
d'un film d'horreur.

4
0:00:19.867,000 --> 0:00:23,000
La famille parfaitement heureuse emménage dans sa parfaite nouvelle maison,

5
0:00:24.277,000 --> 0:00:26,000
enthousiaste au sujet de son parfait avenir,

6
0:00:26.582,000 --> 0:00:29,000
il fait beau dehors et les oiseaux pépient...

7
0:00:30.857,000 --> 0:00:31,000
Puis la nuit tombe.

8
0:00:32.72,000 --> 0:00:34,000
Il y a des bruits dans le grenier.

9
0:00:35.092,000 --> 0:00:39,000
Nous réalisons que cette parfaite nouvelle maison n'est pas si parfaite.

10
0:00:40.485,000 --> 0:00:43,000
Quand j'ai commencé à travailler chez Google en 2006,

11
0:00:43.64,000 --> 0:00:44,000
Facebook n'avait que deux ans

12
0:00:45.431,000 --> 0:00:47,000
et Twitter n'était pas encore né.

13
0:00:47.848,000 --> 0:00:51,000
J'étais totalement émerveillée par internet et sa promesse

14
0:00:52.282,000 --> 0:00:53,000
de nous rapprocher,

15
0:00:53.743,000 --> 0:00:55,000
de nous rendre plus intelligents et plus libres.

16
0:00:57.265,000 --> 0:01:,000
Alors que nous créions des moteurs de recherche,

17
0:01:01.003,000 --> 0:01:03,000
des sites de partage de vidéos et des réseaux sociaux,

18
0:01:04.907,000 --> 0:01:08,000
les criminels, les dictateurs et les terroristes comprenaient

19
0:01:09.235,000 --> 0:01:12,000
comment utiliser ces mêmes plateformes contre nous.

20
0:01:13.417,000 --> 0:01:15,000
Nous n'avons pas eu la clairvoyance de les en empêcher.

21
0:01:16.746,000 --> 0:01:18,000
Ces dernières années, des forces géopolitiques

22
0:01:19.209,000 --> 0:01:21,000
ont causé des ravages en ligne.

23
0:01:21.869,000 --> 0:01:22,000
En réponse à cela,

24
0:01:23.062,000 --> 0:01:27,000
Google a soutenu quelques collègues et moi dans la création d'un groupe, Jigsaw,

25
0:01:27.864,000 --> 0:01:31,000
ayant pour mandat de protéger les gens des menaces comme l'extrémisme violent,

26
0:01:32.484,000 --> 0:01:34,000
la censure, la persécution...

27
0:01:35.186,000 --> 0:01:39,000
Des menaces qui me semblent très personnelles car je suis née en Iran

28
0:01:39.327,000 --> 0:01:41,000
et je suis partie au lendemain d'une révolution violente.

29
0:01:43.525,000 --> 0:01:47,000
Mais j'ai réalisé que, même si nous avions toutes les ressources

30
0:01:47.895,000 --> 0:01:49,000
de toutes les entreprises de technologie du monde,

31
0:01:51.595,000 --> 0:01:52,000
nous échouerions

32
0:01:53.586,000 --> 0:01:55,000
si nous négligions un ingrédient crucial :

33
0:01:57.653,000 --> 0:02:02,000
les expériences humaines des victimes et des auteurs de ces menaces.

34
0:02:04.935,000 --> 0:02:06,000
Je pourrais vous parler de nombreux défis aujourd'hui.

35
0:02:07.695,000 --> 0:02:08,000
Je vais me concentrer sur deux.

36
0:02:09.623,000 --> 0:02:11,000
Le premier est le terrorisme.

37
0:02:13.563,000 --> 0:02:15,000
Afin de comprendre le processus de radicalisation,

38
0:02:16.144,000 --> 0:02:16,000
nous avons rencontré

39
0:02:17.12,000 --> 0:02:2,000
des dizaines d'anciens membres de groupes extrémistes violents.

40
0:02:21.59,000 --> 0:02:23,000
Parmi eux, il y avait une écolière britannique

41
0:02:25.049,000 --> 0:02:28,000
qui avait été sortie d'un avion à l'aéroport de Londres

42
0:02:28.772,000 --> 0:02:32,000
alors qu'elle essayait d'aller en Syrie pour rejoindre Daech.

43
0:02:34.281,000 --> 0:02:35,000
Elle avait 13 ans.

44
0:02:37.792,000 --> 0:02:41,000
Nous avons discuté avec elle et son père et j'ai demandé : « Pourquoi ? »

45
0:02:42.441,000 --> 0:02:43,000
Elle a dit :

46
0:02:44.182,000 --> 0:02:47,000
« Je regardais des photos de ce à quoi la vie ressemblait en Syrie

47
0:02:47.845,000 --> 0:02:5,000
et je pensais que j'allais vivre dans un Disneyland islamique. »

48
0:02:52.527,000 --> 0:02:54,000
Voilà ce qu'elle a vu en Daech.

49
0:02:54.635,000 --> 0:02:57,000
Elle pensait qu'elle allait rencontrer et épouser un Brad Pitt djihadiste,

50
0:02:58.151,000 --> 0:03:01,000
aller faire du shopping toute la journée et vivre heureuse.

51
0:03:02.977,000 --> 0:03:04,000
Daech comprend ce qui motive les gens

52
0:03:05.825,000 --> 0:03:08,000
et ils élaborent soigneusement un message pour chaque public.

53
0:03:11.122,000 --> 0:03:12,000
Regardez en combien de langues

54
0:03:12.657,000 --> 0:03:14,000
leur matériel promotionnel est traduit.

55
0:03:15.677,000 --> 0:03:17,000
Ils créent des brochures, des émissions de radio, des vidéos,

56
0:03:18.528,000 --> 0:03:19,000
pas seulement en anglais et en arabe

57
0:03:20.365,000 --> 0:03:24,000
mais en allemand, en russe, en français, en turc, en kurde,

58
0:03:25.15,000 --> 0:03:26,000
en hébreu,

59
0:03:26.846,000 --> 0:03:27,000
en mandarin.

60
0:03:29.309,000 --> 0:03:33,000
J'ai même vu une vidéo produite par Daech en langue des signes.

61
0:03:34.605,000 --> 0:03:35,000
Réfléchissez-y un instant :

62
0:03:36.513,000 --> 0:03:38,000
Daech a pris le temps et a fait l'effot

63
0:03:38.845,000 --> 0:03:41,000
de s'assurer que son message touche les sourds et malentendants.

64
0:03:45.143,000 --> 0:03:47,000
Ce n'est pas leur savoir-faire technologique

65
0:03:47.311,000 --> 0:03:49,000
qui permet de conquérir les cœurs et les esprits.

66
0:03:49.93,000 --> 0:03:53,000
C'est leur connaissance des préjugés, des vulnérabilités, des désirs

67
0:03:54.117,000 --> 0:03:55,000
des gens qu'ils tentent d'atteindre

68
0:03:55.915,000 --> 0:03:56,000
qui le leur permet.

69
0:03:57.718,000 --> 0:03:58,000
Il ne suffit donc pas

70
0:03:59.171,000 --> 0:04:,000
que les plateformes en ligne

71
0:04:00.524,000 --> 0:04:03,000
se concentrent sur le retrait des contenus de recrutement.

72
0:04:04.518,000 --> 0:04:07,000
Si nous voulons avoir une chance de développer une technologie utile

73
0:04:08.123,000 --> 0:04:09,000
pour lutter contre la radicalisation,

74
0:04:10.021,000 --> 0:04:12,000
nous devons commencer par le voyage humain qui est en son cœur.

75
0:04:13.884,000 --> 0:04:15,000
Nous sommes allés en Irak

76
0:04:16.045,000 --> 0:04:19,000
pour parler à des jeunes hommes convaincus par la promesse de Daech

77
0:04:19.186,000 --> 0:04:21,000
d'héroïsme et de vertu,

78
0:04:22.165,000 --> 0:04:23,000
qui avaient pris les armes pour eux

79
0:04:24.036,000 --> 0:04:25,000
et qui s'étaient enfuis

80
0:04:25.398,000 --> 0:04:28,000
après avoir été témoins de la brutalité du règne de Daech.

81
0:04:28.88,000 --> 0:04:31,000
Je me suis retrouvée dans cette prison de fortune dans le nord de l'Irak

82
0:04:32.279,000 --> 0:04:36,000
avec ce jeune de 23 ans qui avait été formé pour être un kamikaze

83
0:04:36.67,000 --> 0:04:37,000
avant de s'enfuir.

84
0:04:39.08,000 --> 0:04:4,000
Il disait :

85
0:04:41.119,000 --> 0:04:44,000
« Je suis arrivé en Syrie plein d'espoir

86
0:04:44.363,000 --> 0:04:48,000
et immédiatement, deux de mes biens les plus précieux ont été confisqués :

87
0:04:48.752,000 --> 0:04:5,000
mon passeport et mon téléphone portable. »

88
0:04:52.14,000 --> 0:04:54,000
Les symboles de sa liberté physique et numérique

89
0:04:54.57,000 --> 0:04:55,000
lui ont été ôtés à son arrivée.

90
0:04:57.248,000 --> 0:05:,000
Voici comment il m'a décrit ce moment de perte.

91
0:05:01.356,000 --> 0:05:02,000
Il a dit :

92
0:05:02.966,000 --> 0:05:04,000
« Vous savez, dans « Tom et Jerry »,

93
0:05:06.192,000 --> 0:05:09,000
quand Jerry veut s'échapper et que Tom ferme la porte à clé

94
0:05:09.319,000 --> 0:05:1,000
et avale la clé

95
0:05:10.499,000 --> 0:05:13,000
et vous la voyez passer dans sa gorge ? »

96
0:05:14.446,000 --> 0:05:17,000
Bien sûr, je pouvais voir l'image qu'il décrivait

97
0:05:17.623,000 --> 0:05:2,000
et l'associer au sentiment qu'il essayait de communiquer,

98
0:05:21.308,000 --> 0:05:23,000
le sentiment d'être condamné,

99
0:05:23.353,000 --> 0:05:25,000
quand vous savez qu'il n'y a pas d'issue.

100
0:05:26.551,000 --> 0:05:27,000
Je me suis demandé :

101
0:05:28.644,000 --> 0:05:3,000
qu'est-ce qui aurait pu lui faire changer d'avis

102
0:05:31.35,000 --> 0:05:32,000
le jour où il est parti ?

103
0:05:32.614,000 --> 0:05:33,000
J'ai demandé :

104
0:05:33.888,000 --> 0:05:36,000
« Si vous saviez tout ce que vous savez maintenant

105
0:05:37.09,000 --> 0:05:4,000
sur la souffrance, la corruption, la brutalité,

106
0:05:40.165,000 --> 0:05:43,000
le jour où vous êtes parti, seriez-vous parti ? »

107
0:05:43.786,000 --> 0:05:44,000
Il a dit : « Oui. »

108
0:05:45.846,000 --> 0:05:47,000
Et j'ai pensé : « Merde, il a dit « oui ». »

109
0:05:48.694,000 --> 0:05:49,000
Puis il a dit :

110
0:05:49.937,000 --> 0:05:52,000
« À ce moment-là, j'avais subi un tel lavage de cerveau,

111
0:05:52.962,000 --> 0:05:55,000
je n'assimilais plus aucune information contradictoire.

112
0:05:56.744,000 --> 0:05:57,000
Je n'aurais pas pu être influencé.

113
0:05:59.235,000 --> 0:06:01,000
- Et si vous aviez su tout ce que vous savez maintenant

114
0:06:01.812,000 --> 0:06:03,000
six mois avant le jour où vous êtes parti ?

115
0:06:05.345,000 --> 0:06:08,000
- A ce moment-là, je pense que j'aurais probablement changé d'avis. »

116
0:06:10.138,000 --> 0:06:13,000
La radicalisation n'est pas un choix simple entre oui et non.

117
0:06:14.007,000 --> 0:06:16,000
C'est un processus, durant lequel les gens ont des questions

118
0:06:17.008,000 --> 0:06:2,000
au sujet de l'idéologie, la religion, les conditions de vie.

119
0:06:20.808,000 --> 0:06:22,000
Et ils cherchent les réponses en ligne,

120
0:06:23.598,000 --> 0:06:25,000
ce qui offre une opportunité de les atteindre.

121
0:06:25.905,000 --> 0:06:27,000
Il y a des vidéos en ligne venant de gens ayant des réponses :

122
0:06:28.893,000 --> 0:06:3,000
des transfuges, par exemple, racontant l'histoire de leur voyage

123
0:06:31.889,000 --> 0:06:32,000
vers la violence et leur retour ;

124
0:06:33.466,000 --> 0:06:36,000
des histoires comme celle d'un homme rencontré dans une prison irakienne.

125
0:06:37.914,000 --> 0:06:39,000
Ce sont des locaux qui ont publié des vidéos

126
0:06:40.528,000 --> 0:06:43,000
de ce qu'est vraiment la vie dans le califat sous le règne de Daech.

127
0:06:44.055,000 --> 0:06:47,000
Il y a des religieux qui partagent des interprétations pacifiques de l'islam.

128
0:06:48.83,000 --> 0:06:49,000
Mais vous savez quoi ?

129
0:06:50.004,000 --> 0:06:53,000
Ces gens n'ont en général pas le talent que Daech a pour le marketing.

130
0:06:54.049,000 --> 0:06:58,000
Ils risquent leur vie pour s'exprimer et s'opposer à la propagande terroriste

131
0:06:58.605,000 --> 0:07:,000
et, tragiquement, ils ne touchent pas les gens

132
0:07:00.84,000 --> 0:07:01,000
qui ont le plus besoin de les entendre.

133
0:07:03.173,000 --> 0:07:05,000
Nous voulions voir si la technologie pouvait changer cela.

134
0:07:06.205,000 --> 0:07:1,000
En 2016, nous avons établi un partenariat avec Moonshot CVE

135
0:07:10.412,000 --> 0:07:13,000
afin d'expérimenter une approche pour lutter contre la radicalisation

136
0:07:13.652,000 --> 0:07:14,000
appelée « Méthode de Redirection ».

137
0:07:16.453,000 --> 0:07:19,000
Elle utilise le pouvoir de la publicité en ligne

138
0:07:19.489,000 --> 0:07:23,000
pour rapprocher ceux qui sont réceptifs au message de Daech

139
0:07:24.027,000 --> 0:07:27,000
et ces voix crédibles qui réfutent ce message.

140
0:07:28.633,000 --> 0:07:29,000
Cela fonctionne ainsi :

141
0:07:29.807,000 --> 0:07:31,000
quelqu'un cherchant du contenu extrémiste,

142
0:07:31.858,000 --> 0:07:33,000
cherchant, disons, « Comment rejoindre Daech ? »,

143
0:07:34.806,000 --> 0:07:36,000
verra apparaître une publicité

144
0:07:37.306,000 --> 0:07:41,000
qui l'invite à regarder une vidéo YouTube d'un religieux, d'un transfuge ;

145
0:07:42.212,000 --> 0:07:44,000
quelqu'un qui a une réponse authentique.

146
0:07:44.546,000 --> 0:07:47,000
Ce ciblage est basé, non pas sur un profil de leur identité,

147
0:07:48.193,000 --> 0:07:51,000
mais sur la détermination d'une chose pertinente

148
0:07:51.27,000 --> 0:07:52,000
pour leur interrogation ou question.

149
0:07:54.122,000 --> 0:07:56,000
Durant notre pilote de huit semaines en anglais et en arabe,

150
0:07:56.988,000 --> 0:07:59,000
nous avons touché plus de 300 000 personnes

151
0:08:00.291,000 --> 0:08:03,000
ayant exprimé de l'intérêt ou de la sympathie

152
0:08:03.556,000 --> 0:08:05,000
pour un groupe djihadiste.

153
0:08:06.626,000 --> 0:08:08,000
Ces gens regardent maintenant des vidéos

154
0:08:08.914,000 --> 0:08:11,000
qui pourraient les empêcher de prendre des décisions dévastatrices.

155
0:08:13.405,000 --> 0:08:16,000
Puisque l'extrémisme violent n'est pas limité à une langue en particulier,

156
0:08:17.156,000 --> 0:08:18,000
une religion ou une idéologie,

157
0:08:18.984,000 --> 0:08:21,000
la méthode de redirection est en cours de déploiement à l'échelle mondiale

158
0:08:22.509,000 --> 0:08:25,000
afin de protéger les gens courtisés en ligne par des idéologues violents,

159
0:08:26.337,000 --> 0:08:28,000
que ce soient des islamistes, des suprémacistes blancs

160
0:08:28.957,000 --> 0:08:3,000
ou d'autres extrémistes violents,

161
0:08:31.084,000 --> 0:08:33,000
avec l'objectif de leur donner une chance d'entendre quelqu'un

162
0:08:33.997,000 --> 0:08:35,000
ayant suivi ce chemin ;

163
0:08:36.096,000 --> 0:08:38,000
leur donner une chance de choisir un autre chemin.

164
0:08:40.749,000 --> 0:08:45,000
Il s'avère que souvent les méchants savent bien exploiter internet,

165
0:08:46.753,000 --> 0:08:49,000
pas parce qu'ils sont des génies de la technologie,

166
0:08:50.521,000 --> 0:08:52,000
mais parce qu'ils comprennent ce qui fait réagir les gens.

167
0:08:54.855,000 --> 0:08:56,000
Je veux vous donner un second exemple :

168
0:08:58.019,000 --> 0:08:59,000
le cyber-harcèlement.

169
0:09:00.629,000 --> 0:09:03,000
Les cyber-harceleurs cherchent aussi ce qui va faire écho

170
0:09:04.016,000 --> 0:09:05,000
pour un autre être humain.

171
0:09:05.655,000 --> 0:09:08,000
Pas pour les recruter comme le fait Daech,

172
0:09:08.789,000 --> 0:09:09,000
mais pour les blesser.

173
0:09:11.259,000 --> 0:09:12,000
Imaginez :

174
0:09:13.347,000 --> 0:09:14,000
vous êtes une femme,

175
0:09:15.03,000 --> 0:09:16,000
vous êtes mariée,

176
0:09:16.467,000 --> 0:09:17,000
vous avez un enfant.

177
0:09:18.834,000 --> 0:09:19,000
Vous postez sur les réseaux sociaux

178
0:09:20.778,000 --> 0:09:22,000
et dans une réponse, on vous dit que vous serez violée,

179
0:09:24.577,000 --> 0:09:25,000
que votre fils regardera,

180
0:09:26.825,000 --> 0:09:27,000
avec des détails sur quand et où.

181
0:09:29.148,000 --> 0:09:32,000
En fait, votre adresse est mise en ligne publiquement.

182
0:09:33.58,000 --> 0:09:35,000
Cela semble être une menace plutôt réelle.

183
0:09:37.113,000 --> 0:09:39,000
Pensez-vous que vous rentreriez chez vous ?

184
0:09:39.999,000 --> 0:09:42,000
Pensez-vous que vous continueriez à faire ce que vous faisiez ?

185
0:09:43.071,000 --> 0:09:46,000
Continueriez-vous à faire cette chose qui irrite votre agresseur ?

186
0:09:48.016,000 --> 0:09:51,000
Le cyber-harcèlement est l'art pervers

187
0:09:51.136,000 --> 0:09:54,000
de découvrir ce qui met les gens en colère,

188
0:09:54.628,000 --> 0:09:56,000
ce qui les effraie,

189
0:09:56.784,000 --> 0:09:57,000
ce qui les dérange,

190
0:09:58.449,000 --> 0:10:01,000
et puis d'appuyer sur ces points de pression jusqu'à les faire taire.

191
0:10:02.333,000 --> 0:10:04,000
Quand les cyber-harceleurs restent impunis,

192
0:10:04.661,000 --> 0:10:05,000
la liberté de parole est étouffée.

193
0:10:07.196,000 --> 0:10:09,000
Même les gens qui animent la conversation

194
0:10:09.347,000 --> 0:10:11,000
lèvent les bras au ciel et jettent l'éponge,

195
0:10:11.391,000 --> 0:10:13,000
désactivant complètement les commentaires et les forums.

196
0:10:14.186,000 --> 0:10:16,000
Cela signifie que nous perdons des espaces en ligne

197
0:10:17.059,000 --> 0:10:18,000
où se rencontrer et échanger des idées.

198
0:10:19.939,000 --> 0:10:21,000
Là où les espaces en ligne demeurent,

199
0:10:22.126,000 --> 0:10:26,000
nous descendons dans des chambres d'écho avec des gens qui pensent comme nous.

200
0:10:27.688,000 --> 0:10:29,000
Mais cela permet la prolifération de la désinformation ;

201
0:10:30.327,000 --> 0:10:32,000
cela facilite la polarisation.

202
0:10:34.508,000 --> 0:10:39,000
Et si, au lieu de cela, la technologie favorisait l'empathie à grande échelle ?

203
0:10:40.451,000 --> 0:10:42,000
C'était la question qui a motivé notre partenariat

204
0:10:42.891,000 --> 0:10:45,000
avec l'équipe de lutte contre les abus de Google, Wikipédia

205
0:10:46.006,000 --> 0:10:48,000
et des journaux tels que le New York Times.

206
0:10:48.01,000 --> 0:10:5,000
Nous voulions élaborer des modèles d'apprentissage automatique

207
0:10:50.92,000 --> 0:10:53,000
qui pouvaient comprendre l'incidence émotionnelle du langage.

208
0:10:55.062,000 --> 0:10:58,000
Pouvions-nous prévoir quels commentaires allaient pousser quelqu'un à quitter

209
0:10:58.702,000 --> 0:10:59,000
la conversation en ligne ?

210
0:11:00.515,000 --> 0:11:03,000
Ce n'est pas une mince affaire.

211
0:11:04.426,000 --> 0:11:05,000
Ce n'est pas un exploit trivial

212
0:11:06.016,000 --> 0:11:08,000
d'avoir une IA capable de faire une telle chose.

213
0:11:08.603,000 --> 0:11:11,000
Considérez ces deux exemples de messages

214
0:11:12.356,000 --> 0:11:14,000
qu'on aurait pu m'envoyer la semaine dernière.

215
0:11:15.517,000 --> 0:11:16,000
« Merde pour TED ! »

216
0:11:17.42,000 --> 0:11:18,000
et

217
0:11:18.608,000 --> 0:11:2,000
« Tu seras une merde à TED ».

218
0:11:20.758,000 --> 0:11:21,000
(Rires)

219
0:11:22.028,000 --> 0:11:23,000
Vous êtes humains,

220
0:11:23.565,000 --> 0:11:25,000
c'est pourquoi la différence vous paraît évidente

221
0:11:25.855,000 --> 0:11:27,000
même si les mots sont plus ou moins les mêmes.

222
0:11:28.047,000 --> 0:11:31,000
Pour une IA, il faut un entraînement pour apprendre aux modèles

223
0:11:31.15,000 --> 0:11:32,000
à reconnaître cette différence.

224
0:11:32.745,000 --> 0:11:35,000
La beauté de la conception d'une IA qui peut faire la différence

225
0:11:36.014,000 --> 0:11:38,000
est que cette IA peut s'étendre à l'échelle

226
0:11:38.068,000 --> 0:11:41,000
du phénomène de toxicité en ligne

227
0:11:41.088,000 --> 0:11:42,000
et c'était notre objectif

228
0:11:42.306,000 --> 0:11:44,000
lors de l'élaboration de notre technologie, Perspective.

229
0:11:45.086,000 --> 0:11:46,000
Avec l'aide de Perspective,

230
0:11:46.507,000 --> 0:11:47,000
le New York Times, par exemple,

231
0:11:48.114,000 --> 0:11:5,000
a accru l'espace de conversation en ligne.

232
0:11:51.005,000 --> 0:11:52,000
Avant notre collaboration,

233
0:11:52.339,000 --> 0:11:56,000
les commentaires n'étaient activés que sur 10% de leurs articles.

234
0:11:57.495,000 --> 0:11:58,000
Avec notre apprentissage automatique,

235
0:11:59.269,000 --> 0:12:,000
ils ont augmenté ce nombre à 30%.

236
0:12:01.084,000 --> 0:12:02,000
Ils l'ont triplé

237
0:12:02.264,000 --> 0:12:03,000
et ce n'est que le début.

238
0:12:04.872,000 --> 0:12:07,000
Il est question de bien plus que de rendre les modérateurs plus efficaces.

239
0:12:10.076,000 --> 0:12:11,000
Actuellement, je peux vous voir

240
0:12:11.95,000 --> 0:12:14,000
et je peux évaluer comment vous recevez ce que je dis.

241
0:12:16.37,000 --> 0:12:18,000
Vous n'avez pas cette opportunité en ligne.

242
0:12:18.558,000 --> 0:12:21,000
Imaginez si l'apprentissage automatique pouvait offrir aux commentateurs,

243
0:12:22.217,000 --> 0:12:23,000
pendant qu'ils tapent,

244
0:12:23.403,000 --> 0:12:26,000
un retour en temps réel quant à comment leurs mots pourraient être pris,

245
0:12:27.609,000 --> 0:12:3,000
comme le font les expressions faciales lors d'une conversation en face à face.

246
0:12:32.926,000 --> 0:12:34,000
L'apprentissage automatique n'est pas parfait

247
0:12:35.018,000 --> 0:12:37,000
et fait encore plein d'erreurs.

248
0:12:37.21,000 --> 0:12:38,000
Mais en créant une technologie

249
0:12:38.791,000 --> 0:12:41,000
qui comprend l'incidence émotionnelle du langage,

250
0:12:42.108,000 --> 0:12:43,000
nous développons l'empathie.

251
0:12:43.592,000 --> 0:12:45,000
Nous pouvons avoir un dialogue entre des personnes

252
0:12:46.041,000 --> 0:12:47,000
ayant différentes opinions politiques,

253
0:12:47.881,000 --> 0:12:5,000
conceptions du monde et valeurs.

254
0:12:51.359,000 --> 0:12:55,000
Nous pouvons relancer les espaces en ligne que beaucoup d'entre nous ont délaissés.

255
0:12:57.857,000 --> 0:13:,000
Quand les gens utilisent la technologie pour exploiter et blesser les autres,

256
0:13:01.666,000 --> 0:13:04,000
ils s'attaquent à nos peurs et vulnérabilités humaines.

257
0:13:06.461,000 --> 0:13:09,000
Si nous pensions que nous pouvions développer un internet

258
0:13:09.993,000 --> 0:13:11,000
isolé du côté obscur de l'humanité,

259
0:13:12.595,000 --> 0:13:13,000
nous avions tort.

260
0:13:14.361,000 --> 0:13:16,000
Si nous voulons aujourd'hui créer des technologies

261
0:13:16.721,000 --> 0:13:19,000
pouvant surmonter les défis auxquels nous faisons face,

262
0:13:19.806,000 --> 0:13:23,000
nous devons nous lancer complètement dans la compréhension des problèmes

263
0:13:23.873,000 --> 0:13:24,000
et l'élaboration de solutions

264
0:13:25.79,000 --> 0:13:28,000
qui sont aussi humaines que les problèmes qu'elles ont l'ambition de résoudre.

265
0:13:30.071,000 --> 0:13:31,000
Mettons-nous au travail !

266
0:13:31.924,000 --> 0:13:32,000
Merci.

267
0:13:33.098,000 --> 0:13:36,000
(Applaudissements)

