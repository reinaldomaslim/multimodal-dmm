1
0:00:,000 --> 0:00:07,000
Translator: Ivana Korom Reviewer: Krystian Aparta

2
0:00:13.468,000 --> 0:00:18,000
So, on April 23 of 2013,

3
0:00:18.714,000 --> 0:00:23,000
the Associated Press put out the following tweet on Twitter.

4
0:00:24.252,000 --> 0:00:26,000
It said, "Breaking news:

5
0:00:26.673,000 --> 0:00:28,000
Two explosions at the White House

6
0:00:29.268,000 --> 0:00:31,000
and Barack Obama has been injured."

7
0:00:32.212,000 --> 0:00:37,000
This tweet was retweeted 4,000 times in less than five minutes,

8
0:00:37.661,000 --> 0:00:39,000
and it went viral thereafter.

9
0:00:40.76,000 --> 0:00:44,000
Now, this tweet wasn't real news put out by the Associated Press.

10
0:00:45.134,000 --> 0:00:48,000
In fact it was false news, or fake news,

11
0:00:48.491,000 --> 0:00:5,000
that was propagated by Syrian hackers

12
0:00:51.34,000 --> 0:00:55,000
that had infiltrated the Associated Press Twitter handle.

13
0:00:56.407,000 --> 0:00:59,000
Their purpose was to disrupt society, but they disrupted much more.

14
0:01:00.32,000 --> 0:01:02,000
Because automated trading algorithms

15
0:01:02.82,000 --> 0:01:05,000
immediately seized on the sentiment on this tweet,

16
0:01:06.204,000 --> 0:01:08,000
and began trading based on the potential

17
0:01:09.196,000 --> 0:01:12,000
that the president of the United States had been injured or killed

18
0:01:12.601,000 --> 0:01:13,000
in this explosion.

19
0:01:14.188,000 --> 0:01:15,000
And as they started tweeting,

20
0:01:16.204,000 --> 0:01:19,000
they immediately sent the stock market crashing,

21
0:01:19.577,000 --> 0:01:24,000
wiping out 140 billion dollars in equity value in a single day.

22
0:01:25.062,000 --> 0:01:29,000
Robert Mueller, special counsel prosecutor in the United States,

23
0:01:29.562,000 --> 0:01:32,000
issued indictments against three Russian companies

24
0:01:33.478,000 --> 0:01:35,000
and 13 Russian individuals

25
0:01:36.121,000 --> 0:01:39,000
on a conspiracy to defraud the United States

26
0:01:39.312,000 --> 0:01:42,000
by meddling in the 2016 presidential election.

27
0:01:43.855,000 --> 0:01:46,000
And what this indictment tells as a story

28
0:01:47.443,000 --> 0:01:5,000
is the story of the Internet Research Agency,

29
0:01:50.609,000 --> 0:01:53,000
the shadowy arm of the Kremlin on social media.

30
0:01:54.815,000 --> 0:01:56,000
During the presidential election alone,

31
0:01:57.616,000 --> 0:01:58,000
the Internet Agency's efforts

32
0:01:59.529,000 --> 0:02:04,000
reached 126 million people on Facebook in the United States,

33
0:02:04.72,000 --> 0:02:07,000
issued three million individual tweets

34
0:02:08.021,000 --> 0:02:11,000
and 43 hours' worth of YouTube content.

35
0:02:11.887,000 --> 0:02:12,000
All of which was fake --

36
0:02:13.563,000 --> 0:02:19,000
misinformation designed to sow discord in the US presidential election.

37
0:02:20.996,000 --> 0:02:22,000
A recent study by Oxford University

38
0:02:23.67,000 --> 0:02:26,000
showed that in the recent Swedish elections,

39
0:02:26.964,000 --> 0:02:3,000
one third of all of the information spreading on social media

40
0:02:31.363,000 --> 0:02:32,000
about the election

41
0:02:32.585,000 --> 0:02:34,000
was fake or misinformation.

42
0:02:35.037,000 --> 0:02:4,000
In addition, these types of social-media misinformation campaigns

43
0:02:40.139,000 --> 0:02:44,000
can spread what has been called "genocidal propaganda,"

44
0:02:44.314,000 --> 0:02:47,000
for instance against the Rohingya in Burma,

45
0:02:47.449,000 --> 0:02:49,000
triggering mob killings in India.

46
0:02:49.776,000 --> 0:02:5,000
We studied fake news

47
0:02:51.294,000 --> 0:02:54,000
and began studying it before it was a popular term.

48
0:02:55.03,000 --> 0:03:,000
And we recently published the largest-ever longitudinal study

49
0:03:00.094,000 --> 0:03:02,000
of the spread of fake news online

50
0:03:02.404,000 --> 0:03:05,000
on the cover of "Science" in March of this year.

51
0:03:06.523,000 --> 0:03:1,000
We studied all of the verified true and false news stories

52
0:03:10.708,000 --> 0:03:11,000
that ever spread on Twitter,

53
0:03:12.485,000 --> 0:03:15,000
from its inception in 2006 to 2017.

54
0:03:16.612,000 --> 0:03:18,000
And when we studied this information,

55
0:03:18.95,000 --> 0:03:2,000
we studied verified news stories

56
0:03:21.85,000 --> 0:03:24,000
that were verified by six independent fact-checking organizations.

57
0:03:25.792,000 --> 0:03:27,000
So we knew which stories were true

58
0:03:28.578,000 --> 0:03:3,000
and which stories were false.

59
0:03:30.728,000 --> 0:03:31,000
We can measure their diffusion,

60
0:03:32.625,000 --> 0:03:33,000
the speed of their diffusion,

61
0:03:34.3,000 --> 0:03:36,000
the depth and breadth of their diffusion,

62
0:03:36.419,000 --> 0:03:4,000
how many people become entangled in this information cascade and so on.

63
0:03:40.942,000 --> 0:03:41,000
And what we did in this paper

64
0:03:42.45,000 --> 0:03:45,000
was we compared the spread of true news to the spread of false news.

65
0:03:46.339,000 --> 0:03:47,000
And here's what we found.

66
0:03:48.046,000 --> 0:03:51,000
We found that false news diffused further, faster, deeper

67
0:03:52.049,000 --> 0:03:53,000
and more broadly than the truth

68
0:03:53.879,000 --> 0:03:56,000
in every category of information that we studied,

69
0:03:56.906,000 --> 0:03:58,000
sometimes by an order of magnitude.

70
0:03:59.842,000 --> 0:04:02,000
And in fact, false political news was the most viral.

71
0:04:03.39,000 --> 0:04:06,000
It diffused further, faster, deeper and more broadly

72
0:04:06.561,000 --> 0:04:08,000
than any other type of false news.

73
0:04:09.387,000 --> 0:04:1,000
When we saw this,

74
0:04:10.704,000 --> 0:04:12,000
we were at once worried but also curious.

75
0:04:13.569,000 --> 0:04:14,000
Why?

76
0:04:14.744,000 --> 0:04:17,000
Why does false news travel so much further, faster, deeper

77
0:04:18.141,000 --> 0:04:19,000
and more broadly than the truth?

78
0:04:20.339,000 --> 0:04:22,000
The first hypothesis that we came up with was,

79
0:04:23.324,000 --> 0:04:27,000
"Well, maybe people who spread false news have more followers or follow more people,

80
0:04:28.14,000 --> 0:04:29,000
or tweet more often,

81
0:04:29.721,000 --> 0:04:33,000
or maybe they're more often 'verified' users of Twitter, with more credibility,

82
0:04:33.871,000 --> 0:04:35,000
or maybe they've been on Twitter longer."

83
0:04:36.077,000 --> 0:04:38,000
So we checked each one of these in turn.

84
0:04:38.691,000 --> 0:04:4,000
And what we found was exactly the opposite.

85
0:04:41.635,000 --> 0:04:43,000
False-news spreaders had fewer followers,

86
0:04:44.095,000 --> 0:04:46,000
followed fewer people, were less active,

87
0:04:46.373,000 --> 0:04:47,000
less often "verified"

88
0:04:47.857,000 --> 0:04:49,000
and had been on Twitter for a shorter period of time.

89
0:04:50.841,000 --> 0:04:51,000
And yet,

90
0:04:52.054,000 --> 0:04:57,000
false news was 70 percent more likely to be retweeted than the truth,

91
0:04:57.111,000 --> 0:05:,000
controlling for all of these and many other factors.

92
0:05:00.498,000 --> 0:05:02,000
So we had to come up with other explanations.

93
0:05:03.212,000 --> 0:05:06,000
And we devised what we called a "novelty hypothesis."

94
0:05:07.038,000 --> 0:05:08,000
So if you read the literature,

95
0:05:09.022,000 --> 0:05:12,000
it is well known that human attention is drawn to novelty,

96
0:05:12.8,000 --> 0:05:14,000
things that are new in the environment.

97
0:05:15.343,000 --> 0:05:16,000
And if you read the sociology literature,

98
0:05:17.352,000 --> 0:05:21,000
you know that we like to share novel information.

99
0:05:21.676,000 --> 0:05:24,000
It makes us seem like we have access to inside information,

100
0:05:25.538,000 --> 0:05:28,000
and we gain in status by spreading this kind of information.

101
0:05:29.792,000 --> 0:05:35,000
So what we did was we measured the novelty of an incoming true or false tweet,

102
0:05:36.268,000 --> 0:05:4,000
compared to the corpus of what that individual had seen

103
0:05:40.347,000 --> 0:05:42,000
in the 60 days prior on Twitter.

104
0:05:43.323,000 --> 0:05:45,000
But that wasn't enough, because we thought to ourselves,

105
0:05:46.006,000 --> 0:05:5,000
"Well, maybe false news is more novel in an information-theoretic sense,

106
0:05:50.238,000 --> 0:05:53,000
but maybe people don't perceive it as more novel."

107
0:05:53.849,000 --> 0:05:56,000
So to understand people's perceptions of false news,

108
0:05:57.8,000 --> 0:06:,000
we looked at the information and the sentiment

109
0:06:01.514,000 --> 0:06:05,000
contained in the replies to true and false tweets.

110
0:06:06.022,000 --> 0:06:07,000
And what we found

111
0:06:07.252,000 --> 0:06:11,000
was that across a bunch of different measures of sentiment --

112
0:06:11.49,000 --> 0:06:14,000
surprise, disgust, fear, sadness,

113
0:06:14.815,000 --> 0:06:16,000
anticipation, joy and trust --

114
0:06:17.323,000 --> 0:06:22,000
false news exhibited significantly more surprise and disgust

115
0:06:23.204,000 --> 0:06:25,000
in the replies to false tweets.

116
0:06:26.392,000 --> 0:06:29,000
And true news exhibited significantly more anticipation,

117
0:06:30.205,000 --> 0:06:31,000
joy and trust

118
0:06:31.776,000 --> 0:06:33,000
in reply to true tweets.

119
0:06:34.347,000 --> 0:06:37,000
The surprise corroborates our novelty hypothesis.

120
0:06:38.157,000 --> 0:06:42,000
This is new and surprising, and so we're more likely to share it.

121
0:06:43.092,000 --> 0:06:45,000
At the same time, there was congressional testimony

122
0:06:46.041,000 --> 0:06:49,000
in front of both houses of Congress in the United States,

123
0:06:49.101,000 --> 0:06:52,000
looking at the role of bots in the spread of misinformation.

124
0:06:52.863,000 --> 0:06:53,000
So we looked at this too --

125
0:06:54.241,000 --> 0:06:57,000
we used multiple sophisticated bot-detection algorithms

126
0:06:57.863,000 --> 0:07:,000
to find the bots in our data and to pull them out.

127
0:07:01.347,000 --> 0:07:03,000
So we pulled them out, we put them back in

128
0:07:04.03,000 --> 0:07:07,000
and we compared what happens to our measurement.

129
0:07:07.173,000 --> 0:07:09,000
And what we found was that, yes indeed,

130
0:07:09.49,000 --> 0:07:12,000
bots were accelerating the spread of false news online,

131
0:07:13.196,000 --> 0:07:15,000
but they were accelerating the spread of true news

132
0:07:15.871,000 --> 0:07:17,000
at approximately the same rate.

133
0:07:18.3,000 --> 0:07:2,000
Which means bots are not responsible

134
0:07:21.182,000 --> 0:07:25,000
for the differential diffusion of truth and falsity online.

135
0:07:25.919,000 --> 0:07:27,000
We can't abdicate that responsibility,

136
0:07:28.792,000 --> 0:07:32,000
because we, humans, are responsible for that spread.

137
0:07:34.472,000 --> 0:07:37,000
Now, everything that I have told you so far,

138
0:07:37.83,000 --> 0:07:38,000
unfortunately for all of us,

139
0:07:39.608,000 --> 0:07:4,000
is the good news.

140
0:07:42.67,000 --> 0:07:46,000
The reason is because it's about to get a whole lot worse.

141
0:07:47.85,000 --> 0:07:5,000
And two specific technologies are going to make it worse.

142
0:07:52.207,000 --> 0:07:57,000
We are going to see the rise of a tremendous wave of synthetic media.

143
0:07:57.403,000 --> 0:08:03,000
Fake video, fake audio that is very convincing to the human eye.

144
0:08:03.458,000 --> 0:08:05,000
And this will powered by two technologies.

145
0:08:06.236,000 --> 0:08:09,000
The first of these is known as "generative adversarial networks."

146
0:08:10.093,000 --> 0:08:12,000
This is a machine-learning model with two networks:

147
0:08:12.68,000 --> 0:08:13,000
a discriminator,

148
0:08:14.251,000 --> 0:08:18,000
whose job it is to determine whether something is true or false,

149
0:08:18.475,000 --> 0:08:19,000
and a generator,

150
0:08:19.666,000 --> 0:08:22,000
whose job it is to generate synthetic media.

151
0:08:22.84,000 --> 0:08:27,000
So the synthetic generator generates synthetic video or audio,

152
0:08:27.966,000 --> 0:08:31,000
and the discriminator tries to tell, "Is this real or is this fake?"

153
0:08:32.665,000 --> 0:08:34,000
And in fact, it is the job of the generator

154
0:08:35.563,000 --> 0:08:39,000
to maximize the likelihood that it will fool the discriminator

155
0:08:40.022,000 --> 0:08:43,000
into thinking the synthetic video and audio that it is creating

156
0:08:43.633,000 --> 0:08:44,000
is actually true.

157
0:08:45.387,000 --> 0:08:47,000
Imagine a machine in a hyperloop,

158
0:08:47.784,000 --> 0:08:49,000
trying to get better and better at fooling us.

159
0:08:51.114,000 --> 0:08:53,000
This, combined with the second technology,

160
0:08:53.638,000 --> 0:08:58,000
which is essentially the democratization of artificial intelligence to the people,

161
0:08:59.384,000 --> 0:09:01,000
the ability for anyone,

162
0:09:01.597,000 --> 0:09:03,000
without any background in artificial intelligence

163
0:09:04.451,000 --> 0:09:05,000
or machine learning,

164
0:09:05.657,000 --> 0:09:09,000
to deploy these kinds of algorithms to generate synthetic media

165
0:09:09.784,000 --> 0:09:13,000
makes it ultimately so much easier to create videos.

166
0:09:14.355,000 --> 0:09:18,000
The White House issued a false, doctored video

167
0:09:18.8,000 --> 0:09:22,000
of a journalist interacting with an intern who was trying to take his microphone.

168
0:09:23.427,000 --> 0:09:24,000
They removed frames from this video

169
0:09:25.45,000 --> 0:09:28,000
in order to make his actions seem more punchy.

170
0:09:29.157,000 --> 0:09:32,000
And when videographers and stuntmen and women

171
0:09:32.566,000 --> 0:09:34,000
were interviewed about this type of technique,

172
0:09:35.017,000 --> 0:09:38,000
they said, "Yes, we use this in the movies all the time

173
0:09:38.869,000 --> 0:09:42,000
to make our punches and kicks look more choppy and more aggressive."

174
0:09:44.268,000 --> 0:09:45,000
They then put out this video

175
0:09:46.159,000 --> 0:09:48,000
and partly used it as justification

176
0:09:48.683,000 --> 0:09:51,000
to revoke Jim Acosta, the reporter's, press pass

177
0:09:52.706,000 --> 0:09:53,000
from the White House.

178
0:09:54.069,000 --> 0:09:58,000
And CNN had to sue to have that press pass reinstated.

179
0:10:00.538,000 --> 0:10:05,000
There are about five different paths that I can think of that we can follow

180
0:10:06.165,000 --> 0:10:09,000
to try and address some of these very difficult problems today.

181
0:10:10.379,000 --> 0:10:11,000
Each one of them has promise,

182
0:10:12.213,000 --> 0:10:14,000
but each one of them has its own challenges.

183
0:10:15.236,000 --> 0:10:17,000
The first one is labeling.

184
0:10:17.268,000 --> 0:10:18,000
Think about it this way:

185
0:10:18.649,000 --> 0:10:21,000
when you go to the grocery store to buy food to consume,

186
0:10:22.284,000 --> 0:10:23,000
it's extensively labeled.

187
0:10:24.212,000 --> 0:10:25,000
You know how many calories it has,

188
0:10:26.228,000 --> 0:10:27,000
how much fat it contains --

189
0:10:28.053,000 --> 0:10:32,000
and yet when we consume information, we have no labels whatsoever.

190
0:10:32.355,000 --> 0:10:33,000
What is contained in this information?

191
0:10:34.307,000 --> 0:10:35,000
Is the source credible?

192
0:10:35.784,000 --> 0:10:37,000
Where is this information gathered from?

193
0:10:38.125,000 --> 0:10:39,000
We have none of that information

194
0:10:39.974,000 --> 0:10:41,000
when we are consuming information.

195
0:10:42.101,000 --> 0:10:45,000
That is a potential avenue, but it comes with its challenges.

196
0:10:45.363,000 --> 0:10:51,000
For instance, who gets to decide, in society, what's true and what's false?

197
0:10:52.387,000 --> 0:10:53,000
Is it the governments?

198
0:10:54.053,000 --> 0:10:55,000
Is it Facebook?

199
0:10:55.601,000 --> 0:10:58,000
Is it an independent consortium of fact-checkers?

200
0:10:59.387,000 --> 0:11:01,000
And who's checking the fact-checkers?

201
0:11:02.427,000 --> 0:11:05,000
Another potential avenue is incentives.

202
0:11:05.535,000 --> 0:11:07,000
We know that during the US presidential election

203
0:11:08.193,000 --> 0:11:11,000
there was a wave of misinformation that came from Macedonia

204
0:11:11.907,000 --> 0:11:13,000
that didn't have any political motive

205
0:11:14.268,000 --> 0:11:16,000
but instead had an economic motive.

206
0:11:16.752,000 --> 0:11:18,000
And this economic motive existed,

207
0:11:18.924,000 --> 0:11:21,000
because false news travels so much farther, faster

208
0:11:22.472,000 --> 0:11:24,000
and more deeply than the truth,

209
0:11:24.506,000 --> 0:11:28,000
and you can earn advertising dollars as you garner eyeballs and attention

210
0:11:29.49,000 --> 0:11:3,000
with this type of information.

211
0:11:31.474,000 --> 0:11:34,000
But if we can depress the spread of this information,

212
0:11:35.331,000 --> 0:11:37,000
perhaps it would reduce the economic incentive

213
0:11:38.252,000 --> 0:11:4,000
to produce it at all in the first place.

214
0:11:40.966,000 --> 0:11:42,000
Third, we can think about regulation,

215
0:11:43.49,000 --> 0:11:45,000
and certainly, we should think about this option.

216
0:11:45.839,000 --> 0:11:46,000
In the United States, currently,

217
0:11:47.474,000 --> 0:11:51,000
we are exploring what might happen if Facebook and others are regulated.

218
0:11:52.346,000 --> 0:11:55,000
While we should consider things like regulating political speech,

219
0:11:56.171,000 --> 0:11:58,000
labeling the fact that it's political speech,

220
0:11:58.703,000 --> 0:12:01,000
making sure foreign actors can't fund political speech,

221
0:12:02.546,000 --> 0:12:04,000
it also has its own dangers.

222
0:12:05.522,000 --> 0:12:09,000
For instance, Malaysia just instituted a six-year prison sentence

223
0:12:10.424,000 --> 0:12:12,000
for anyone found spreading misinformation.

224
0:12:13.696,000 --> 0:12:15,000
And in authoritarian regimes,

225
0:12:15.799,000 --> 0:12:19,000
these kinds of policies can be used to suppress minority opinions

226
0:12:20.489,000 --> 0:12:23,000
and to continue to extend repression.

227
0:12:24.68,000 --> 0:12:27,000
The fourth possible option is transparency.

228
0:12:28.843,000 --> 0:12:31,000
We want to know how do Facebook's algorithms work.

229
0:12:32.581,000 --> 0:12:34,000
How does the data combine with the algorithms

230
0:12:35.485,000 --> 0:12:37,000
to produce the outcomes that we see?

231
0:12:38.347,000 --> 0:12:4,000
We want them to open the kimono

232
0:12:40.72,000 --> 0:12:44,000
and show us exactly the inner workings of how Facebook is working.

233
0:12:44.958,000 --> 0:12:46,000
And if we want to know social media's effect on society,

234
0:12:47.761,000 --> 0:12:49,000
we need scientists, researchers

235
0:12:49.871,000 --> 0:12:52,000
and others to have access to this kind of information.

236
0:12:53.038,000 --> 0:12:54,000
But at the same time,

237
0:12:54.609,000 --> 0:12:57,000
we are asking Facebook to lock everything down,

238
0:12:58.434,000 --> 0:13:,000
to keep all of the data secure.

239
0:13:00.631,000 --> 0:13:03,000
So, Facebook and the other social media platforms

240
0:13:03.814,000 --> 0:13:06,000
are facing what I call a transparency paradox.

241
0:13:07.266,000 --> 0:13:09,000
We are asking them, at the same time,

242
0:13:09.964,000 --> 0:13:13,000
to be open and transparent and, simultaneously secure.

243
0:13:14.797,000 --> 0:13:16,000
This is a very difficult needle to thread,

244
0:13:17.512,000 --> 0:13:18,000
but they will need to thread this needle

245
0:13:19.449,000 --> 0:13:22,000
if we are to achieve the promise of social technologies

246
0:13:23.26,000 --> 0:13:24,000
while avoiding their peril.

247
0:13:24.926,000 --> 0:13:28,000
The final thing that we could think about is algorithms and machine learning.

248
0:13:29.641,000 --> 0:13:34,000
Technology devised to root out and understand fake news, how it spreads,

249
0:13:34.942,000 --> 0:13:36,000
and to try and dampen its flow.

250
0:13:37.824,000 --> 0:13:39,000
Humans have to be in the loop of this technology,

251
0:13:40.745,000 --> 0:13:42,000
because we can never escape

252
0:13:43.047,000 --> 0:13:47,000
that underlying any technological solution or approach

253
0:13:47.109,000 --> 0:13:51,000
is a fundamental ethical and philosophical question

254
0:13:51.18,000 --> 0:13:54,000
about how do we define truth and falsity,

255
0:13:54.474,000 --> 0:13:57,000
to whom do we give the power to define truth and falsity

256
0:13:57.678,000 --> 0:13:59,000
and which opinions are legitimate,

257
0:14:00.162,000 --> 0:14:03,000
which type of speech should be allowed and so on.

258
0:14:03.892,000 --> 0:14:05,000
Technology is not a solution for that.

259
0:14:06.244,000 --> 0:14:09,000
Ethics and philosophy is a solution for that.

260
0:14:10.95,000 --> 0:14:13,000
Nearly every theory of human decision making,

261
0:14:14.292,000 --> 0:14:16,000
human cooperation and human coordination

262
0:14:17.077,000 --> 0:14:2,000
has some sense of the truth at its core.

263
0:14:21.347,000 --> 0:14:23,000
But with the rise of fake news,

264
0:14:23.427,000 --> 0:14:24,000
the rise of fake video,

265
0:14:24.894,000 --> 0:14:25,000
the rise of fake audio,

266
0:14:26.8,000 --> 0:14:29,000
we are teetering on the brink of the end of reality,

267
0:14:30.748,000 --> 0:14:33,000
where we cannot tell what is real from what is fake.

268
0:14:34.661,000 --> 0:14:37,000
And that's potentially incredibly dangerous.

269
0:14:38.931,000 --> 0:14:41,000
We have to be vigilant in defending the truth

270
0:14:42.903,000 --> 0:14:43,000
against misinformation.

271
0:14:44.919,000 --> 0:14:47,000
With our technologies, with our policies

272
0:14:48.379,000 --> 0:14:49,000
and, perhaps most importantly,

273
0:14:50.323,000 --> 0:14:53,000
with our own individual responsibilities,

274
0:14:53.561,000 --> 0:14:56,000
decisions, behaviors and actions.

275
0:14:57.553,000 --> 0:14:58,000
Thank you very much.

276
0:14:59.014,000 --> 0:15:02,000
(Applause)

