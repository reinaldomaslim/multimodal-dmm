1
0:00:18.33,000 --> 0:00:24,000
If you ask people about what part of psychology do they think is hard,

2
0:00:24.33,000 --> 0:00:27,000
and you say, "Well, what about thinking and emotions?"

3
0:00:27.33,000 --> 0:00:3,000
Most people will say, "Emotions are terribly hard.

4
0:00:30.33,000 --> 0:00:36,000
They're incredibly complex. They can't -- I have no idea of how they work.

5
0:00:36.33,000 --> 0:00:38,000
But thinking is really very straightforward:

6
0:00:38.33,000 --> 0:00:42,000
it's just sort of some kind of logical reasoning, or something.

7
0:00:42.33,000 --> 0:00:45,000
But that's not the hard part."

8
0:00:45.33,000 --> 0:00:47,000
So here's a list of problems that come up.

9
0:00:47.33,000 --> 0:00:5,000
One nice problem is, what do we do about health?

10
0:00:50.33,000 --> 0:00:54,000
The other day, I was reading something, and the person said

11
0:00:54.33,000 --> 0:01:,000
probably the largest single cause of disease is handshaking in the West.

12
0:01:00.33,000 --> 0:01:04,000
And there was a little study about people who don't handshake,

13
0:01:04.33,000 --> 0:01:07,000
and comparing them with ones who do handshake.

14
0:01:07.33,000 --> 0:01:12,000
And I haven't the foggiest idea of where you find the ones that don't handshake,

15
0:01:12.33,000 --> 0:01:15,000
because they must be hiding.

16
0:01:15.33,000 --> 0:01:19,000
And the people who avoid that

17
0:01:19.33,000 --> 0:01:23,000
have 30 percent less infectious disease or something.

18
0:01:23.33,000 --> 0:01:26,000
Or maybe it was 31 and a quarter percent.

19
0:01:26.33,000 --> 0:01:3,000
So if you really want to solve the problem of epidemics and so forth,

20
0:01:30.33,000 --> 0:01:34,000
let's start with that. And since I got that idea,

21
0:01:34.33,000 --> 0:01:38,000
I've had to shake hundreds of hands.

22
0:01:38.33,000 --> 0:01:43,000
And I think the only way to avoid it

23
0:01:43.33,000 --> 0:01:45,000
is to have some horrible visible disease,

24
0:01:45.33,000 --> 0:01:48,000
and then you don't have to explain.

25
0:01:48.33,000 --> 0:01:52,000
Education: how do we improve education?

26
0:01:52.33,000 --> 0:01:56,000
Well, the single best way is to get them to understand

27
0:01:56.33,000 --> 0:01:59,000
that what they're being told is a whole lot of nonsense.

28
0:01:59.33,000 --> 0:02:01,000
And then, of course, you have to do something

29
0:02:01.33,000 --> 0:02:06,000
about how to moderate that, so that anybody can -- so they'll listen to you.

30
0:02:06.33,000 --> 0:02:1,000
Pollution, energy shortage, environmental diversity, poverty.

31
0:02:10.33,000 --> 0:02:14,000
How do we make stable societies? Longevity.

32
0:02:14.33,000 --> 0:02:17,000
Okay, there're lots of problems to worry about.

33
0:02:17.33,000 --> 0:02:19,000
Anyway, the question I think people should talk about --

34
0:02:19.33,000 --> 0:02:24,000
and it's absolutely taboo -- is, how many people should there be?

35
0:02:24.33,000 --> 0:02:31,000
And I think it should be about 100 million or maybe 500 million.

36
0:02:31.33,000 --> 0:02:36,000
And then notice that a great many of these problems disappear.

37
0:02:36.33,000 --> 0:02:38,000
If you had 100 million people

38
0:02:38.33,000 --> 0:02:44,000
properly spread out, then if there's some garbage,

39
0:02:44.33,000 --> 0:02:51,000
you throw it away, preferably where you can't see it, and it will rot.

40
0:02:51.33,000 --> 0:02:56,000
Or you throw it into the ocean and some fish will benefit from it.

41
0:02:56.33,000 --> 0:02:58,000
The problem is, how many people should there be?

42
0:02:58.33,000 --> 0:03:01,000
And it's a sort of choice we have to make.

43
0:03:01.33,000 --> 0:03:04,000
Most people are about 60 inches high or more,

44
0:03:04.33,000 --> 0:03:08,000
and there's these cube laws. So if you make them this big,

45
0:03:08.33,000 --> 0:03:11,000
by using nanotechnology, I suppose --

46
0:03:11.33,000 --> 0:03:12,000
(Laughter)

47
0:03:12.33,000 --> 0:03:14,000
-- then you could have a thousand times as many.

48
0:03:14.33,000 --> 0:03:16,000
That would solve the problem, but I don't see anybody

49
0:03:16.33,000 --> 0:03:19,000
doing any research on making people smaller.

50
0:03:19.33,000 --> 0:03:24,000
Now, it's nice to reduce the population, but a lot of people want to have children.

51
0:03:24.33,000 --> 0:03:27,000
And there's one solution that's probably only a few years off.

52
0:03:27.33,000 --> 0:03:32,000
You know you have 46 chromosomes. If you're lucky, you've got 23

53
0:03:32.33,000 --> 0:03:38,000
from each parent. Sometimes you get an extra one or drop one out,

54
0:03:38.33,000 --> 0:03:42,000
but -- so you can skip the grandparent and great-grandparent stage

55
0:03:42.33,000 --> 0:03:47,000
and go right to the great-great-grandparent. And you have 46 people

56
0:03:47.33,000 --> 0:03:5,000
and you give them a scanner, or whatever you need,

57
0:03:50.33,000 --> 0:03:54,000
and they look at their chromosomes and each of them says

58
0:03:54.33,000 --> 0:03:59,000
which one he likes best, or she -- no reason to have just two sexes

59
0:03:59.33,000 --> 0:04:04,000
any more, even. So each child has 46 parents,

60
0:04:04.33,000 --> 0:04:1,000
and I suppose you could let each group of 46 parents have 15 children.

61
0:04:10.33,000 --> 0:04:12,000
Wouldn't that be enough? And then the children

62
0:04:12.33,000 --> 0:04:16,000
would get plenty of support, and nurturing, and mentoring,

63
0:04:16.33,000 --> 0:04:18,000
and the world population would decline very rapidly

64
0:04:18.33,000 --> 0:04:21,000
and everybody would be totally happy.

65
0:04:21.33,000 --> 0:04:24,000
Timesharing is a little further off in the future.

66
0:04:24.33,000 --> 0:04:27,000
And there's this great novel that Arthur Clarke wrote twice,

67
0:04:27.33,000 --> 0:04:31,000
called "Against the Fall of Night" and "The City and the Stars."

68
0:04:31.33,000 --> 0:04:34,000
They're both wonderful and largely the same,

69
0:04:34.33,000 --> 0:04:36,000
except that computers happened in between.

70
0:04:36.33,000 --> 0:04:41,000
And Arthur was looking at this old book, and he said, "Well, that was wrong.

71
0:04:41.33,000 --> 0:04:43,000
The future must have some computers."

72
0:04:43.33,000 --> 0:04:48,000
So in the second version of it, there are 100 billion

73
0:04:48.33,000 --> 0:04:56,000
or 1,000 billion people on Earth, but they're all stored on hard disks or floppies,

74
0:04:56.33,000 --> 0:04:58,000
or whatever they have in the future.

75
0:04:58.33,000 --> 0:05:02,000
And you let a few million of them out at a time.

76
0:05:02.33,000 --> 0:05:06,000
A person comes out, they live for a thousand years

77
0:05:06.33,000 --> 0:05:12,000
doing whatever they do, and then, when it's time to go back

78
0:05:12.33,000 --> 0:05:16,000
for a billion years -- or a million, I forget, the numbers don't matter --

79
0:05:16.33,000 --> 0:05:2,000
but there really aren't very many people on Earth at a time.

80
0:05:20.33,000 --> 0:05:22,000
And you get to think about yourself and your memories,

81
0:05:22.33,000 --> 0:05:27,000
and before you go back into suspension, you edit your memories

82
0:05:27.33,000 --> 0:05:3,000
and you change your personality and so forth.

83
0:05:30.33,000 --> 0:05:36,000
The plot of the book is that there's not enough diversity,

84
0:05:36.33,000 --> 0:05:39,000
so that the people who designed the city

85
0:05:39.33,000 --> 0:05:43,000
make sure that every now and then an entirely new person is created.

86
0:05:43.33,000 --> 0:05:49,000
And in the novel, a particular one named Alvin is created. And he says,

87
0:05:49.33,000 --> 0:05:53,000
maybe this isn't the best way, and wrecks the whole system.

88
0:05:53.33,000 --> 0:05:55,000
I don't think the solutions that I proposed

89
0:05:55.33,000 --> 0:05:58,000
are good enough or smart enough.

90
0:05:58.33,000 --> 0:06:02,000
I think the big problem is that we're not smart enough

91
0:06:02.33,000 --> 0:06:06,000
to understand which of the problems we're facing are good enough.

92
0:06:06.33,000 --> 0:06:1,000
Therefore, we have to build super intelligent machines like HAL.

93
0:06:10.33,000 --> 0:06:15,000
As you remember, at some point in the book for "2001,"

94
0:06:15.33,000 --> 0:06:2,000
HAL realizes that the universe is too big, and grand, and profound

95
0:06:20.33,000 --> 0:06:24,000
for those really stupid astronauts. If you contrast HAL's behavior

96
0:06:24.33,000 --> 0:06:28,000
with the triviality of the people on the spaceship,

97
0:06:28.33,000 --> 0:06:31,000
you can see what's written between the lines.

98
0:06:31.33,000 --> 0:06:34,000
Well, what are we going to do about that? We could get smarter.

99
0:06:34.33,000 --> 0:06:39,000
I think that we're pretty smart, as compared to chimpanzees,

100
0:06:39.33,000 --> 0:06:45,000
but we're not smart enough to deal with the colossal problems that we face,

101
0:06:45.33,000 --> 0:06:47,000
either in abstract mathematics

102
0:06:47.33,000 --> 0:06:52,000
or in figuring out economies, or balancing the world around.

103
0:06:52.33,000 --> 0:06:55,000
So one thing we can do is live longer.

104
0:06:55.33,000 --> 0:06:57,000
And nobody knows how hard that is,

105
0:06:57.33,000 --> 0:07:,000
but we'll probably find out in a few years.

106
0:07:00.33,000 --> 0:07:03,000
You see, there's two forks in the road. We know that people live

107
0:07:03.33,000 --> 0:07:07,000
twice as long as chimpanzees almost,

108
0:07:07.33,000 --> 0:07:11,000
and nobody lives more than 120 years,

109
0:07:11.33,000 --> 0:07:14,000
for reasons that aren't very well understood.

110
0:07:14.33,000 --> 0:07:17,000
But lots of people now live to 90 or 100,

111
0:07:17.33,000 --> 0:07:21,000
unless they shake hands too much or something like that.

112
0:07:21.33,000 --> 0:07:26,000
And so maybe if we lived 200 years, we could accumulate enough skills

113
0:07:26.33,000 --> 0:07:31,000
and knowledge to solve some problems.

114
0:07:31.33,000 --> 0:07:33,000
So that's one way of going about it.

115
0:07:33.33,000 --> 0:07:36,000
And as I said, we don't know how hard that is. It might be --

116
0:07:36.33,000 --> 0:07:42,000
after all, most other mammals live half as long as the chimpanzee,

117
0:07:42.33,000 --> 0:07:45,000
so we're sort of three and a half or four times, have four times

118
0:07:45.33,000 --> 0:07:51,000
the longevity of most mammals. And in the case of the primates,

119
0:07:51.33,000 --> 0:07:55,000
we have almost the same genes. We only differ from chimpanzees,

120
0:07:55.33,000 --> 0:08:01,000
in the present state of knowledge, which is absolute hogwash,

121
0:08:01.33,000 --> 0:08:03,000
maybe by just a few hundred genes.

122
0:08:03.33,000 --> 0:08:06,000
What I think is that the gene counters don't know what they're doing yet.

123
0:08:06.33,000 --> 0:08:09,000
And whatever you do, don't read anything about genetics

124
0:08:09.33,000 --> 0:08:12,000
that's published within your lifetime, or something.

125
0:08:12.33,000 --> 0:08:15,000
(Laughter)

126
0:08:15.33,000 --> 0:08:19,000
The stuff has a very short half-life, same with brain science.

127
0:08:19.33,000 --> 0:08:25,000
And so it might be that if we just fix four or five genes,

128
0:08:25.33,000 --> 0:08:27,000
we can live 200 years.

129
0:08:27.33,000 --> 0:08:3,000
Or it might be that it's just 30 or 40,

130
0:08:30.33,000 --> 0:08:32,000
and I doubt that it's several hundred.

131
0:08:32.33,000 --> 0:08:36,000
So this is something that people will be discussing

132
0:08:36.33,000 --> 0:08:39,000
and lots of ethicists -- you know, an ethicist is somebody

133
0:08:39.33,000 --> 0:08:42,000
who sees something wrong with whatever you have in mind.

134
0:08:42.33,000 --> 0:08:45,000
(Laughter)

135
0:08:45.33,000 --> 0:08:49,000
And it's very hard to find an ethicist who considers any change

136
0:08:49.33,000 --> 0:08:53,000
worth making, because he says, what about the consequences?

137
0:08:53.33,000 --> 0:08:56,000
And, of course, we're not responsible for the consequences

138
0:08:56.33,000 --> 0:09:02,000
of what we're doing now, are we? Like all this complaint about clones.

139
0:09:02.33,000 --> 0:09:05,000
And yet two random people will mate and have this child,

140
0:09:05.33,000 --> 0:09:09,000
and both of them have some pretty rotten genes,

141
0:09:09.33,000 --> 0:09:13,000
and the child is likely to come out to be average.

142
0:09:13.33,000 --> 0:09:19,000
Which, by chimpanzee standards, is very good indeed.

143
0:09:19.33,000 --> 0:09:22,000
If we do have longevity, then we'll have to face the population growth

144
0:09:22.33,000 --> 0:09:26,000
problem anyway. Because if people live 200 or 1,000 years,

145
0:09:26.33,000 --> 0:09:32,000
then we can't let them have a child more than about once every 200 or 1,000 years.

146
0:09:32.33,000 --> 0:09:35,000
And so there won't be any workforce.

147
0:09:35.33,000 --> 0:09:39,000
And one of the things Laurie Garrett pointed out, and others have,

148
0:09:39.33,000 --> 0:09:44,000
is that a society that doesn't have people

149
0:09:44.33,000 --> 0:09:47,000
of working age is in real trouble. And things are going to get worse,

150
0:09:47.33,000 --> 0:09:53,000
because there's nobody to educate the children or to feed the old.

151
0:09:53.33,000 --> 0:09:55,000
And when I'm talking about a long lifetime, of course,

152
0:09:55.33,000 --> 0:10:01,000
I don't want somebody who's 200 years old to be like our image

153
0:10:01.33,000 --> 0:10:05,000
of what a 200-year-old is -- which is dead, actually.

154
0:10:05.33,000 --> 0:10:07,000
You know, there's about 400 different parts of the brain

155
0:10:07.33,000 --> 0:10:09,000
which seem to have different functions.

156
0:10:09.33,000 --> 0:10:12,000
Nobody knows how most of them work in detail,

157
0:10:12.33,000 --> 0:10:16,000
but we do know that there're lots of different things in there.

158
0:10:16.33,000 --> 0:10:18,000
And they don't always work together. I like Freud's theory

159
0:10:18.33,000 --> 0:10:22,000
that most of them are cancelling each other out.

160
0:10:22.33,000 --> 0:10:26,000
And so if you think of yourself as a sort of city

161
0:10:26.33,000 --> 0:10:32,000
with a hundred resources, then, when you're afraid, for example,

162
0:10:32.33,000 --> 0:10:36,000
you may discard your long-range goals, but you may think deeply

163
0:10:36.33,000 --> 0:10:4,000
and focus on exactly how to achieve that particular goal.

164
0:10:40.33,000 --> 0:10:43,000
You throw everything else away. You become a monomaniac --

165
0:10:43.33,000 --> 0:10:47,000
all you care about is not stepping out on that platform.

166
0:10:47.33,000 --> 0:10:51,000
And when you're hungry, food becomes more attractive, and so forth.

167
0:10:51.33,000 --> 0:10:57,000
So I see emotions as highly evolved subsets of your capability.

168
0:10:57.33,000 --> 0:11:01,000
Emotion is not something added to thought. An emotional state

169
0:11:01.33,000 --> 0:11:05,000
is what you get when you remove 100 or 200

170
0:11:05.33,000 --> 0:11:08,000
of your normally available resources.

171
0:11:08.33,000 --> 0:11:11,000
So thinking of emotions as the opposite of -- as something

172
0:11:11.33,000 --> 0:11:15,000
less than thinking is immensely productive. And I hope,

173
0:11:15.33,000 --> 0:11:19,000
in the next few years, to show that this will lead to smart machines.

174
0:11:19.33,000 --> 0:11:22,000
And I guess I better skip all the rest of this, which are some details

175
0:11:22.33,000 --> 0:11:27,000
on how we might make those smart machines and --

176
0:11:27.33,000 --> 0:11:32,000
(Laughter)

177
0:11:32.33,000 --> 0:11:37,000
-- and the main idea is in fact that the core of a really smart machine

178
0:11:37.33,000 --> 0:11:42,000
is one that recognizes that a certain kind of problem is facing you.

179
0:11:42.33,000 --> 0:11:45,000
This is a problem of such and such a type,

180
0:11:45.33,000 --> 0:11:5,000
and therefore there's a certain way or ways of thinking

181
0:11:50.33,000 --> 0:11:52,000
that are good for that problem.

182
0:11:52.33,000 --> 0:11:56,000
So I think the future, main problem of psychology is to classify

183
0:11:56.33,000 --> 0:12:,000
types of predicaments, types of situations, types of obstacles

184
0:12:00.33,000 --> 0:12:06,000
and also to classify available and possible ways to think and pair them up.

185
0:12:06.33,000 --> 0:12:09,000
So you see, it's almost like a Pavlovian --

186
0:12:09.33,000 --> 0:12:11,000
we lost the first hundred years of psychology

187
0:12:11.33,000 --> 0:12:14,000
by really trivial theories, where you say,

188
0:12:14.33,000 --> 0:12:2,000
how do people learn how to react to a situation? What I'm saying is,

189
0:12:20.33,000 --> 0:12:25,000
after we go through a lot of levels, including designing

190
0:12:25.33,000 --> 0:12:28,000
a huge, messy system with thousands of ports,

191
0:12:28.33,000 --> 0:12:32,000
we'll end up again with the central problem of psychology.

192
0:12:32.33,000 --> 0:12:35,000
Saying, not what are the situations,

193
0:12:35.33,000 --> 0:12:37,000
but what are the kinds of problems

194
0:12:37.33,000 --> 0:12:4,000
and what are the kinds of strategies, how do you learn them,

195
0:12:40.33,000 --> 0:12:43,000
how do you connect them up, how does a really creative person

196
0:12:43.33,000 --> 0:12:48,000
invent a new way of thinking out of the available resources and so forth.

197
0:12:48.33,000 --> 0:12:5,000
So, I think in the next 20 years,

198
0:12:50.33,000 --> 0:12:55,000
if we can get rid of all of the traditional approaches to artificial intelligence,

199
0:12:55.33,000 --> 0:12:57,000
like neural nets and genetic algorithms

200
0:12:57.33,000 --> 0:13:03,000
and rule-based systems, and just turn our sights a little bit higher to say,

201
0:13:03.33,000 --> 0:13:05,000
can we make a system that can use all those things

202
0:13:05.33,000 --> 0:13:09,000
for the right kind of problem? Some problems are good for neural nets;

203
0:13:09.33,000 --> 0:13:12,000
we know that others, neural nets are hopeless on them.

204
0:13:12.33,000 --> 0:13:15,000
Genetic algorithms are great for certain things;

205
0:13:15.33,000 --> 0:13:19,000
I suspect I know what they're bad at, and I won't tell you.

206
0:13:19.33,000 --> 0:13:2,000
(Laughter)

207
0:13:20.33,000 --> 0:13:22,000
Thank you.

208
0:13:22.33,000 --> 0:13:28,000
(Applause)

