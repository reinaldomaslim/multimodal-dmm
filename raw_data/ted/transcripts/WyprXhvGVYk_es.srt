1
0:00:,000 --> 0:00:07,000
Traductor: Ciro Gomez Revisor: Lidia Cámara de la Fuente

2
0:00:12.857,000 --> 0:00:13,000
Este es un gráfico

3
0:00:14.36,000 --> 0:00:17,000
que representa la historia económica de la civilización humana.

4
0:00:18.043,000 --> 0:00:2,000
[PBI mundial per cápita en los últimos 200 000 años]

5
0:00:23.757,000 --> 0:00:25,000
No pasa mucho, está ahí.

6
0:00:26.751,000 --> 0:00:28,000
Durante la mayor parte de la historia humana,

7
0:00:29.835,000 --> 0:00:33,000
casi todos vivían con el equivalente a un dólar por día,

8
0:00:33.907,000 --> 0:00:34,000
y no mucho cambiaba.

9
0:00:36.757,000 --> 0:00:38,000
Pero luego, sucedió algo extraordinario:

10
0:00:40.677,000 --> 0:00:42,000
las Revoluciones Científica e Industrial.

11
0:00:43.512,000 --> 0:00:45,000
Y el gráfico básicamente plano que acaban de ver

12
0:00:46.321,000 --> 0:00:48,000
se transforma en esto.

13
0:00:50.612,000 --> 0:00:54,000
Lo que este gráfico significa es que, en términos de poder para cambiar el mundo,

14
0:00:55.271,000 --> 0:00:58,000
vivimos en un tiempo sin precedentes en la historia humana,

15
0:00:58.733,000 --> 0:01:01,000
y creo que nuestra comprensión ética aún no ha alcanzado este hecho.

16
0:01:03.716,000 --> 0:01:04,000
Las revoluciones científica e industrial

17
0:01:05.724,000 --> 0:01:07,000
transformaron tanto nuestra comprensión del mundo

18
0:01:08.657,000 --> 0:01:09,000
como nuestra habilidad para alterarlo.

19
0:01:11.505,000 --> 0:01:14,000
Lo que necesitamos es una revolución ética

20
0:01:15.196,000 --> 0:01:16,000
para que podamos entrenar

21
0:01:16.768,000 --> 0:01:19,000
cómo usar esta tremenda abundancia de recursos

22
0:01:19.944,000 --> 0:01:2,000
para mejorar el mundo.

23
0:01:22.249,000 --> 0:01:23,000
Durante los últimos 10 años,

24
0:01:23.864,000 --> 0:01:26,000
mis colegas y yo hemos desarrollado una filosofía y un programa de investigación

25
0:01:27.721,000 --> 0:01:28,000
que llamamos altruismo efectivo.

26
0:01:30.366,000 --> 0:01:33,000
Intenta responder a estos cambios radicales en nuestro mundo,

27
0:01:33.985,000 --> 0:01:37,000
usando evidencia y razonamiento cuidadoso para intentar responder a esta pregunta:

28
0:01:40.173,000 --> 0:01:42,000
¿Cómo podemos hacer el mayor bien?

29
0:01:44.265,000 --> 0:01:47,000
Existen muchos problemas que hay que abordar

30
0:01:47.51,000 --> 0:01:49,000
si se quiere afrontar este problema:

31
0:01:49.797,000 --> 0:01:51,000
haciendo el bien mediante caridad

32
0:01:51.852,000 --> 0:01:53,000
o mediante carrera, compromiso político,

33
0:01:54.028,000 --> 0:01:56,000
en qué programas enfocarse, o con quién trabajar.

34
0:01:57.624,000 --> 0:01:58,000
Pero de lo que quiero hablar

35
0:01:59.124,000 --> 0:02:01,000
es de lo que creo que es el problema más fundamental.

36
0:02:02.02,000 --> 0:02:04,000
De todos los muchos problemas que enfrenta el mundo,

37
0:02:05.962,000 --> 0:02:07,000
¿cuál deberíamos tratar de resolver primero?

38
0:02:10.668,000 --> 0:02:13,000
Ahora, voy a darles un marco para pensar sobre esta pregunta,

39
0:02:14.16,000 --> 0:02:15,000
y el marco es muy simple.

40
0:02:16.842,000 --> 0:02:17,000
La mayor prioridad de un problema,

41
0:02:19.416,000 --> 0:02:23,000
cuanto más grande, más fácil de resolver y más descuidado es.

42
0:02:24.694,000 --> 0:02:25,000
Mas grande es mejor,

43
0:02:26.36,000 --> 0:02:28,000
porque tenemos más que ganar si resolvemos el problema.

44
0:02:30.221,000 --> 0:02:31,000
Más fácil de resolver es mejor

45
0:02:31.814,000 --> 0:02:34,000
porque puedo resolver el problema con menos tiempo o dinero.

46
0:02:35.737,000 --> 0:02:37,000
Y lo más sutil,

47
0:02:38.681,000 --> 0:02:41,000
más descuidado es mejor, debido a los rendimientos decrecientes.

48
0:02:42.285,000 --> 0:02:45,000
Cuantos más recursos ya se hayan invertido para resolver un problema,

49
0:02:46.023,000 --> 0:02:48,000
más difícil será hacer un progreso adicional.

50
0:02:50.56,000 --> 0:02:54,000
Ahora, la clave que quiero dejarles es este marco,

51
0:02:54.643,000 --> 0:02:55,000
para que puedan pensar solos

52
0:02:56.651,000 --> 0:02:58,000
es cuáles son las prioridades globales más altas.

53
0:02:59.954,000 --> 0:03:01,000
Pero yo y otros en la comunidad efectiva de altruismo

54
0:03:02.67,000 --> 0:03:07,000
convergimos en tres cuestiones morales que son inusualmente importantes,

55
0:03:08.573,000 --> 0:03:1,000
puntúan inusualmente bien en este marco.

56
0:03:11.151,000 --> 0:03:13,000
Primero es la salud global.

57
0:03:13.988,000 --> 0:03:15,000
Esto es supersolucionable.

58
0:03:16.423,000 --> 0:03:19,000
Tenemos una increíble trayectoria en salud global.

59
0:03:19.844,000 --> 0:03:24,000
Tasas de mortalidad por sarampión, malaria y enfermedad diarreica

60
0:03:25.288,000 --> 0:03:27,000
han bajado en más del 70 %.

61
0:03:29.534,000 --> 0:03:31,000
Y en 1980 erradicamos la viruela.

62
0:03:33.815,000 --> 0:03:36,000
Estimo que salvamos más de 60 millones de vidas.

63
0:03:37.506,000 --> 0:03:4,000
Son más vidas salvadas que si hubiéramos logrado la paz mundial

64
0:03:40.594,000 --> 0:03:41,000
en ese mismo período de tiempo.

65
0:03:43.893,000 --> 0:03:46,000
En nuestras mejores estimaciones actuales, podemos salvar una vida distribuyendo

66
0:03:47.728,000 --> 0:03:49,000
mosquiteros tratados con insecticida de larga duración

67
0:03:50.394,000 --> 0:03:51,000
por solo unos pocos miles de dólares.

68
0:03:52.911,000 --> 0:03:53,000
Esta es una oportunidad increíble.

69
0:03:55.594,000 --> 0:03:57,000
La segunda gran prioridad es la agricultura industrial.

70
0:03:58.681,000 --> 0:03:59,000
Esto está supernegado.

71
0:04:00.768,000 --> 0:04:04,000
Hay 50 mil millones de animales terrestres que se consumen anualmente para comida,

72
0:04:05.625,000 --> 0:04:07,000
y la gran mayoría de esa producción industrial,

73
0:04:08.197,000 --> 0:04:1,000
vive en condiciones de sufrimiento horrible.

74
0:04:10.601,000 --> 0:04:13,000
Quizá estén entre las criaturas más desfavorecidas de este planeta,

75
0:04:13.776,000 --> 0:04:16,000
y en muchos casos, podríamos mejorar significativamente sus vidas

76
0:04:16.914,000 --> 0:04:17,000
por solo centavos por animal.

77
0:04:19.123,000 --> 0:04:21,000
Sin embargo, esto está muy descuidado.

78
0:04:21.229,000 --> 0:04:24,000
Hay 3000 veces más animales en granjas industriales

79
0:04:25.063,000 --> 0:04:26,000
que mascotas callejeras

80
0:04:28.6,000 --> 0:04:3,000
pero, aun así, la agricultura industrial logra

81
0:04:30.786,000 --> 0:04:32,000
el 15 % de los fondos filantrópicos.

82
0:04:34.211,000 --> 0:04:36,000
Eso significa que recursos adicionales aquí

83
0:04:36.363,000 --> 0:04:38,000
tendrían un impacto verdaderamente transformador.

84
0:04:39.458,000 --> 0:04:41,000
Ahora, la tercera área es en la que me quiero enfocar más,

85
0:04:42.467,000 --> 0:04:44,000
y esa es la categoría de riesgos existenciales:

86
0:04:45.475,000 --> 0:04:48,000
eventos como una guerra nuclear o una pandemia global

87
0:04:50.824,000 --> 0:04:52,000
que podría descarrilar definitivamente la civilización

88
0:04:54.156,000 --> 0:04:56,000
o incluso llevar a la extinción de la raza humana.

89
0:04:57.882,000 --> 0:04:59,000
Permítanme explicar por qué creo que es una gran prioridad

90
0:05:00.742,000 --> 0:05:01,000
en términos de este marco.

91
0:05:02.992,000 --> 0:05:03,000
Primero, tamaño.

92
0:05:05.341,000 --> 0:05:08,000
¿Qué tan malo sería si hubiera una verdadera catástrofe existencial?

93
0:05:10.92,000 --> 0:05:16,000
Eso implicaría la muerte de los 7 mil millones de personas de este planeta

94
0:05:17.286,000 --> 0:05:2,000
y eso incluye a Ud. y a todos los que conoce y ama.

95
0:05:21.214,000 --> 0:05:23,000
Es sencillamente una tragedia de tamaño inimaginable.

96
0:05:25.684,000 --> 0:05:26,000
Pero aún no queda ahí,

97
0:05:27.684,000 --> 0:05:3,000
también implicaría la reducción del potencial futuro de la humanidad,

98
0:05:31.313,000 --> 0:05:33,000
y creo que el potencial de la humanidad es enorme.

99
0:05:35.551,000 --> 0:05:38,000
La raza humana ha existido por alrededor de 200 000 años,

100
0:05:39.026,000 --> 0:05:41,000
y si vive tanto como una típica especie de mamífero,

101
0:05:41.933,000 --> 0:05:43,000
duraría alrededor de 2 millones de años.

102
0:05:46.884,000 --> 0:05:48,000
Si la raza humana fuera un solo individuo,

103
0:05:49.599,000 --> 0:05:51,000
tendría solo 10 años hoy.

104
0:05:53.526,000 --> 0:05:57,000
Y lo que es más, la raza humana no es una especie típica de mamíferos.

105
0:05:58.95,000 --> 0:06:,000
No hay razón por la cual, si tenemos cuidado,

106
0:06:01.126,000 --> 0:06:03,000
debamos morir después de solo 2 millones de años.

107
0:06:03.839,000 --> 0:06:07,000
La Tierra seguirá siendo habitable durante 500 millones de años por venir.

108
0:06:08.696,000 --> 0:06:1,000
Y si algún día, llegamos a las estrellas,

109
0:06:11.64,000 --> 0:06:14,000
la civilización podría continuar miles de millones más.

110
0:06:16.193,000 --> 0:06:18,000
Y, creo que el futuro será muy grande

111
0:06:19.669,000 --> 0:06:2,000
pero ¿va a ser bueno?

112
0:06:21.495,000 --> 0:06:23,000
¿Es realmente digna de preservación la raza humana?

113
0:06:26.54,000 --> 0:06:29,000
Escuchamos todo el tiempo cómo las cosas han empeorado,

114
0:06:31.459,000 --> 0:06:33,000
pero creo que cuando tomamos el largo plazo,

115
0:06:34.176,000 --> 0:06:36,000
las cosas han ido mejorando radicalmente.

116
0:06:37.453,000 --> 0:06:39,000
Aquí, por ejemplo, la esperanza de vida en el tiempo.

117
0:06:40.892,000 --> 0:06:43,000
Aquí la proporción de personas que no viven en la pobreza extrema.

118
0:06:45.106,000 --> 0:06:49,000
Aquí el número de países que con el tiempo han despenalizado la homosexualidad.

119
0:06:50.848,000 --> 0:06:53,000
Aquí la cantidad de países que se han vuelto democracias a lo largo del tiempo.

120
0:06:55.015,000 --> 0:06:59,000
Y, al mirar al futuro, podría haber mucho más para ganar nuevamente.

121
0:06:59.658,000 --> 0:07:,000
Seremos mucho más ricos,

122
0:07:00.91,000 --> 0:07:03,000
podremos resolver muchos problemas que son intratables hoy.

123
0:07:05.389,000 --> 0:07:09,000
Así, si esto es una especie de gráfico de cómo la humanidad ha progresado

124
0:07:09.858,000 --> 0:07:11,000
en términos de florecimiento humano total en el tiempo,

125
0:07:12.772,000 --> 0:07:15,000
bueno, esto es lo que esperaríamos que fuera el progreso futuro.

126
0:07:16.881,000 --> 0:07:17,000
Es vasto

127
0:07:18.953,000 --> 0:07:19,000
Aquí, por ejemplo,

128
0:07:20.175,000 --> 0:07:23,000
es donde no esperaríamos que nadie viviera en la pobreza extrema.

129
0:07:25.93,000 --> 0:07:28,000
Aquí es donde esperamos que todos estén mejor

130
0:07:29.156,000 --> 0:07:3,000
que la persona más rica viva hoy.

131
0:07:32.081,000 --> 0:07:35,000
Quizás aquí es donde descubriríamos las leyes naturales fundamentales

132
0:07:35.453,000 --> 0:07:36,000
que gobiernan nuestro mundo.

133
0:07:37.516,000 --> 0:07:4,000
Quizás aquí es donde descubrimos una forma de arte completamente nueva,

134
0:07:41.245,000 --> 0:07:44,000
una forma de música que actualmente no tenemos oídos para escuchar.

135
0:07:45.072,000 --> 0:07:47,000
Y esto solo son los próximos miles de años.

136
0:07:47.827,000 --> 0:07:49,000
Una vez que pensamos más allá de eso, bueno,

137
0:07:50.056,000 --> 0:07:54,000
ni siquiera podemos imaginar las alturas que el logro humano podría alcanzar.

138
0:07:54.247,000 --> 0:07:57,000
El futuro podría ser muy grande y podría ser muy bueno,

139
0:07:57.311,000 --> 0:07:59,000
pero ¿hay formas en que podamos perder este valor?

140
0:08:00.366,000 --> 0:08:01,000
Y lamentablemente, creo que las hay.

141
0:08:02.216,000 --> 0:08:05,000
Los últimos dos siglos trajeron un tremendo progreso tecnológico,

142
0:08:05.753,000 --> 0:08:08,000
pero también trajeron los riesgos globales de la guerra nuclear

143
0:08:08.939,000 --> 0:08:1,000
y la posibilidad de un cambio climático extremo.

144
0:08:11.725,000 --> 0:08:12,000
Cuando miramos a los siglos venideros,

145
0:08:13.592,000 --> 0:08:15,000
deberíamos esperar ver el mismo patrón de nuevo.

146
0:08:16.187,000 --> 0:08:19,000
Y podemos ver algunas tecnologías radicalmente poderosas en el horizonte.

147
0:08:20.132,000 --> 0:08:22,000
La biología sintética podría darnos el poder de crear virus

148
0:08:23.005,000 --> 0:08:26,000
de contagio y letalidad sin precedentes.

149
0:08:27.131,000 --> 0:08:31,000
La geoingeniería podría darnos el poder de alterar drásticamente el clima terrestre.

150
0:08:31.798,000 --> 0:08:35,000
La inteligencia artificial podría darnos el poder de crear agentes inteligentes

151
0:08:36.021,000 --> 0:08:38,000
con habilidades mayores que la nuestra.

152
0:08:40.222,000 --> 0:08:43,000
No estoy diciendo que ninguno de estos riesgos sea particularmente probable,

153
0:08:43.93,000 --> 0:08:44,000
pero cuando hay mucho en juego,

154
0:08:45.802,000 --> 0:08:47,000
incluso las pequeñas probabilidades importan mucho.

155
0:08:49.568,000 --> 0:08:52,000
Imaginen si se suben a un avión y están nerviosos,

156
0:08:52.592,000 --> 0:08:55,000
y el piloto los tranquiliza diciendo,

157
0:08:56.061,000 --> 0:09:,000
"Solo hay una posibilidad de chocar entre mil. No se preocupen".

158
0:09:02.157,000 --> 0:09:03,000
¿Se sentirían seguros?

159
0:09:04.509,000 --> 0:09:08,000
Por estas razones, creo que preservar el futuro de la humanidad

160
0:09:08.621,000 --> 0:09:11,000
es uno de los problemas más importantes que enfrentamos actualmente.

161
0:09:12.546,000 --> 0:09:14,000
Pero sigamos usando este marco.

162
0:09:14.72,000 --> 0:09:15,000
¿Este problema se descuida?

163
0:09:18.085,000 --> 0:09:2,000
Creo que la respuesta es sí,

164
0:09:20.391,000 --> 0:09:23,000
y eso es porque los problemas que afectan a las generaciones futuras

165
0:09:23.74,000 --> 0:09:24,000
a menudo son enormemente descuidados.

166
0:09:26.93,000 --> 0:09:27,000
¿Por qué?

167
0:09:28.36,000 --> 0:09:31,000
Porque la gente del futuro no participa en los mercados de hoy.

168
0:09:31.862,000 --> 0:09:32,000
No tienen voto.

169
0:09:33.931,000 --> 0:09:36,000
No es que haya algo como un lobby que represente los intereses

170
0:09:36.964,000 --> 0:09:37,000
de los nacidos en 2300 AD.

171
0:09:40.313,000 --> 0:09:43,000
No influyen en las decisiones que tomamos hoy.

172
0:09:43.995,000 --> 0:09:44,000
No tienen voz.

173
0:09:46.49,000 --> 0:09:49,000
Y eso significa que aún gastamos una cantidad miserable en estos temas:

174
0:09:49.959,000 --> 0:09:5,000
no proliferación nuclear,

175
0:09:51.782,000 --> 0:09:53,000
geoingeniería, riesgo biológico,

176
0:09:55.414,000 --> 0:09:56,000
inteligencia artificial segura.

177
0:09:57.923,000 --> 0:10:,000
Todos estos reciben solo unas pocas decenas de millones de dólares

178
0:10:01.147,000 --> 0:10:02,000
de financiación filantrópica cada año.

179
0:10:04.044,000 --> 0:10:07,000
Eso es muy pequeño en comparación con los USD 390 mil millones

180
0:10:08.79,000 --> 0:10:1,000
que se gasta en filantropía estadounidense en total.

181
0:10:13.885,000 --> 0:10:15,000
El aspecto final de nuestro marco

182
0:10:17.083,000 --> 0:10:18,000
¿es solucionable?

183
0:10:19.289,000 --> 0:10:2,000
Yo creo que lo es.

184
0:10:21.014,000 --> 0:10:24,000
Pueden contribuir con su dinero,

185
0:10:24.085,000 --> 0:10:26,000
su carrera o su compromiso político.

186
0:10:28.225,000 --> 0:10:3,000
Con su dinero, pueden apoyar organizaciones

187
0:10:30.314,000 --> 0:10:31,000
que se enfocan en estos riesgos,

188
0:10:31.926,000 --> 0:10:33,000
como la Iniciativa de Amenaza Nuclear,

189
0:10:34.329,000 --> 0:10:37,000
que hace campaña para sacar las armas nucleares de la alerta de disparo,

190
0:10:38.013,000 --> 0:10:41,000
o el Blue Ribbon Panel, que desarrolla políticas para minimizar el daño

191
0:10:41.608,000 --> 0:10:43,000
de pandemias naturales y artificiales,

192
0:10:45.158,000 --> 0:10:48,000
o el Centro para la IA humana-compatible, que hace investigación técnica

193
0:10:48.648,000 --> 0:10:51,000
para garantizar que los sistemas de IA sean seguros y confiables.

194
0:10:52.652,000 --> 0:10:53,000
Con su compromiso político,

195
0:10:54.192,000 --> 0:10:57,000
pueden votar por candidatos que se preocupan por estos riesgos,

196
0:10:57.312,000 --> 0:10:59,000
y pueden apoyar una mayor cooperación internacional.

197
0:11:01.767,000 --> 0:11:04,000
Y luego, con su carrera, hay mucho que pueden hacer.

198
0:11:05.333,000 --> 0:11:08,000
Por supuesto, necesitamos científicos, políticos y líderes de organizaciones,

199
0:11:09.865,000 --> 0:11:1,000
pero igual de importante,

200
0:11:11.207,000 --> 0:11:14,000
también necesitamos contadores, gerentes y asistentes

201
0:11:16.691,000 --> 0:11:19,000
para trabajar en estas organizaciones que están abordando estos problemas.

202
0:11:20.469,000 --> 0:11:23,000
Ahora, el programa de investigación de altruismo efectivo

203
0:11:25.191,000 --> 0:11:26,000
todavía está en su infancia,

204
0:11:27.262,000 --> 0:11:29,000
y todavía hay una gran cantidad que no sabemos.

205
0:11:31.173,000 --> 0:11:33,000
Pero incluso con lo que hemos aprendido hasta ahora,

206
0:11:34.748,000 --> 0:11:36,000
podemos ver eso pensando con cuidado

207
0:11:37.494,000 --> 0:11:41,000
y al enfocarnos en aquellos problemas que son grandes, solucionables y descuidados,

208
0:11:43.152,000 --> 0:11:45,000
podemos marcar una gran diferencia en el mundo

209
0:11:45.884,000 --> 0:11:46,000
para los miles de años por venir.

210
0:11:47.963,000 --> 0:11:48,000
Gracias.

211
0:11:49.138,000 --> 0:11:53,000
(Aplausos)

