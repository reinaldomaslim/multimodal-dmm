1
0:00:,000 --> 0:00:07,000
Traductor: Lidia Cámara de la Fuente Revisor: Sebastian Betti

2
0:00:12.739,000 --> 0:00:16,000
Empecé mi primer trabajo como programadora informática

3
0:00:16.885,000 --> 0:00:17,000
en mi primer año de universidad,

4
0:00:18.705,000 --> 0:00:19,000
básicamente, siendo aún adolescente.

5
0:00:20.889,000 --> 0:00:21,000
Poco después de empezar a trabajar,

6
0:00:22.575,000 --> 0:00:23,000
programando software en una empresa,

7
0:00:24.799,000 --> 0:00:27,000
un gerente que trabajaba en la compañía vino allí donde estaba yo,

8
0:00:28.458,000 --> 0:00:29,000
y me dijo al oído:

9
0:00:30.229,000 --> 0:00:32,000
"¿Puede decir ella si estoy mintiendo?"

10
0:00:33.806,000 --> 0:00:35,000
No había nadie más en la habitación.

11
0:00:37.032,000 --> 0:00:41,000
"¿Puede "quién" decir si está mintiendo? ¿Y por qué estamos susurrando?"

12
0:00:42.266,000 --> 0:00:45,000
El gerente señaló la computadora de la habitación.

13
0:00:45.397,000 --> 0:00:48,000
"¿Puede ella decir si estoy mintiendo?"

14
0:00:49.613,000 --> 0:00:53,000
Bueno, el gerente tenía una aventura con la recepcionista.

15
0:00:53.999,000 --> 0:00:54,000
(Risas)

16
0:00:55.135,000 --> 0:00:56,000
Y yo todavía era adolescente.

17
0:00:57.447,000 --> 0:00:59,000
Por lo tanto, le susurro yo a él:

18
0:00:59.49,000 --> 0:01:02,000
"Sí, la computadora puede determinar si Ud. está mintiendo".

19
0:01:03.138,000 --> 0:01:04,000
(Risas)

20
0:01:04.968,000 --> 0:01:06,000
Bueno, me reí, pero, en realidad, me reía de mí.

21
0:01:07.915,000 --> 0:01:1,000
Hoy en día, existen sistemas informáticos

22
0:01:11.207,000 --> 0:01:14,000
que pueden detectar estados emocionales e incluso mentir

23
0:01:14.669,000 --> 0:01:16,000
a partir del procesamiento de rostros humanos.

24
0:01:17.248,000 --> 0:01:21,000
Los anunciantes, e incluso hay gobiernos muy interesados.

25
0:01:21.859,000 --> 0:01:23,000
Me había convertido en programadora informática

26
0:01:24.231,000 --> 0:01:27,000
porque yo era una de esas chicas locas por las matemáticas y la ciencia.

27
0:01:27.812,000 --> 0:01:3,000
Pero también me había interesado por las armas nucleares,

28
0:01:31.22,000 --> 0:01:34,000
y había empezado a realmente a preocuparme por la ética de la ciencia.

29
0:01:34.646,000 --> 0:01:35,000
Yo estaba preocupada.

30
0:01:36.234,000 --> 0:01:38,000
Sin embargo, por circunstancias familiares,

31
0:01:38.479,000 --> 0:01:4,000
también debía empezar a trabajar lo antes posible.

32
0:01:41.265,000 --> 0:01:44,000
Así que me dije, bueno, vamos a elegir un campo técnico

33
0:01:44.478,000 --> 0:01:45,000
donde poder conseguir un trabajo fácil

34
0:01:46.408,000 --> 0:01:5,000
y donde no tenga que lidiar con preguntas molestas sobre ética.

35
0:01:51.022,000 --> 0:01:52,000
Así que elegí las computadoras.

36
0:01:52.575,000 --> 0:01:53,000
(Risas)

37
0:01:53.703,000 --> 0:01:56,000
Bueno, ¡ja, ja, ja! Todas las risas a mi costa.

38
0:01:57.137,000 --> 0:01:59,000
Hoy en día, los informáticos construyen plataformas

39
0:01:59.915,000 --> 0:02:03,000
que controlan lo que millones de personas ven todos los días.

40
0:02:05.052,000 --> 0:02:08,000
Están desarrollando automóviles que podrían decidir a quién atropellar.

41
0:02:09.707,000 --> 0:02:12,000
Es más, están construyendo máquinas, armas,

42
0:02:12.944,000 --> 0:02:14,000
que podrían matar a seres humanos en la guerra.

43
0:02:15.253,000 --> 0:02:17,000
Esto es ética a fondo.

44
0:02:19.183,000 --> 0:02:21,000
La inteligencia artificial está aquí.

45
0:02:21.823,000 --> 0:02:24,000
Estamos usando la computación para tomar todo tipo de decisiones,

46
0:02:24.951,000 --> 0:02:26,000
además de nuevos tipos de decisiones.

47
0:02:27.101,000 --> 0:02:31,000
Planteamos preguntas a las computadoras que no tienen respuestas

48
0:02:31.563,000 --> 0:02:33,000
correctas individuales, por ser subjetivas

49
0:02:33.653,000 --> 0:02:35,000
e indefinidas y cargadas de valores.

50
0:02:36.002,000 --> 0:02:37,000
Planteamos preguntas como:

51
0:02:37.784,000 --> 0:02:39,000
"¿A quién debe contratar la empresa?"

52
0:02:40.096,000 --> 0:02:42,000
"¿Qué actualización de qué amigo debe mostrarse?"

53
0:02:42.879,000 --> 0:02:44,000
"¿Qué convicto tiene más probabilidades de reincidir?"

54
0:02:45.535,000 --> 0:02:48,000
"¿Qué artículo de noticias o película se deben recomendar a la gente?"

55
0:02:48.968,000 --> 0:02:5,000
Miren, sí, hemos venido usando computadoras hace tiempo,

56
0:02:51.988,000 --> 0:02:52,000
pero esto es diferente.

57
0:02:53.529,000 --> 0:02:55,000
Se trata de un giro histórico,

58
0:02:55.62,000 --> 0:03:,000
porque no podemos anclar el cálculo para este tipo de decisiones subjetivas

59
0:03:00.981,000 --> 0:03:05,000
como anclamos el cálculo para pilotar aviones, construir puentes

60
0:03:06.425,000 --> 0:03:07,000
o ir a la luna.

61
0:03:08.449,000 --> 0:03:11,000
¿Son los aviones más seguros? ¿Se balanceó el puente y cayó?

62
0:03:11.732,000 --> 0:03:15,000
Ahí, hemos acordado puntos de referencia bastante claros,

63
0:03:16.254,000 --> 0:03:18,000
y tenemos leyes de la naturaleza que nos guían.

64
0:03:18.517,000 --> 0:03:21,000
Nosotros no tenemos tales anclas y puntos de referencia

65
0:03:21.935,000 --> 0:03:24,000
para las decisiones sobre cuestiones humanas desordenadas.

66
0:03:25.922,000 --> 0:03:29,000
Para complicar más las cosas, nuestro software es cada vez más potente,

67
0:03:30.183,000 --> 0:03:33,000
pero también es cada vez menos transparente y más complejo.

68
0:03:34.542,000 --> 0:03:36,000
Recientemente, en la última década,

69
0:03:36.606,000 --> 0:03:38,000
algunos algoritmos complejos han hecho grandes progresos.

70
0:03:39.359,000 --> 0:03:4,000
Pueden reconocer rostros humanos.

71
0:03:41.985,000 --> 0:03:43,000
Pueden descifrar la letra.

72
0:03:44.116,000 --> 0:03:46,000
Pueden detectar el fraude de tarjetas de crédito

73
0:03:46.526,000 --> 0:03:47,000
y bloquear el spam

74
0:03:47.739,000 --> 0:03:49,000
y pueden traducir a otros idiomas.

75
0:03:49.8,000 --> 0:03:51,000
Pueden detectar tumores en imágenes médicas.

76
0:03:52.374,000 --> 0:03:54,000
Puede vencer a los humanos en el ajedrez y en el Go.

77
0:03:55.264,000 --> 0:03:59,000
Gran parte de este progreso viene de un método llamado "aprendizaje automático".

78
0:03:59.905,000 --> 0:04:02,000
El aprendizaje automático es diferente a la programación tradicional,

79
0:04:03.386,000 --> 0:04:06,000
donde se da al equipo instrucciones exactas, detalladas y meticulosas.

80
0:04:07.378,000 --> 0:04:1,000
Es como si uno alimentara el sistema con una gran cantidad de datos,

81
0:04:11.344,000 --> 0:04:12,000
incluyendo los datos no estructurados,

82
0:04:13.264,000 --> 0:04:15,000
como los que generamos en nuestras vidas digitales.

83
0:04:15.782,000 --> 0:04:17,000
Y el sistema aprende de esos datos.

84
0:04:18.669,000 --> 0:04:19,000
Y también, de manera crucial,

85
0:04:20.219,000 --> 0:04:24,000
estos sistemas no funcionan bajo una lógica de una sola respuesta.

86
0:04:24.623,000 --> 0:04:26,000
No producen una respuesta sencilla; es más probabilístico:

87
0:04:27.606,000 --> 0:04:3,000
"Esto es probablemente parecido a lo que estás buscando".

88
0:04:31.833,000 --> 0:04:33,000
La ventaja es que este método es muy potente.

89
0:04:34.437,000 --> 0:04:37,000
El jefe de sistemas de inteligencia artificial de Google lo llama:

90
0:04:37.663,000 --> 0:04:38,000
"la eficacia irrazonable de los datos".

91
0:04:39.791,000 --> 0:04:4,000
La desventaja es que

92
0:04:41.738,000 --> 0:04:44,000
realmente no entendemos lo que aprendió el sistema.

93
0:04:44.833,000 --> 0:04:45,000
De hecho, ese es su poder.

94
0:04:46.946,000 --> 0:04:49,000
Esto no se parece a dar instrucciones a una computadora;

95
0:04:51.2,000 --> 0:04:55,000
se parece más a la formación de una criatura cachorro máquina

96
0:04:55.288,000 --> 0:04:57,000
que realmente no entendemos o controlamos.

97
0:04:58.052,000 --> 0:05:01,000
Así que este es nuestro problema; un problema cuando el sistema

98
0:05:01.746,000 --> 0:05:03,000
de inteligencia artificial hace cosas erróneas.

99
0:05:04.713,000 --> 0:05:07,000
Es también un problema cuando hace bien las cosas,

100
0:05:08.037,000 --> 0:05:11,000
porque ni siquiera sabemos qué es qué cuando se trata de un problema subjetivo.

101
0:05:11.929,000 --> 0:05:13,000
No sabemos qué está pensando esta cosa.

102
0:05:15.493,000 --> 0:05:18,000
Por lo tanto, piensen en un algoritmo de contratación,

103
0:05:20.123,000 --> 0:05:24,000
un sistema usado para contratar, usa sistemas de aprendizaje automático.

104
0:05:25.052,000 --> 0:05:28,000
un sistema así habría sido entrenado con anteriores datos de empleados

105
0:05:28.655,000 --> 0:05:3,000
y tiene la instrucción de encontrar y contratar

106
0:05:31.27,000 --> 0:05:34,000
personas como las de alto rendimiento existentes en la empresa.

107
0:05:34.814,000 --> 0:05:35,000
Suena bien.

108
0:05:35.991,000 --> 0:05:36,000
Una vez asistí a una conferencia

109
0:05:37.884,000 --> 0:05:4,000
que reunió a los responsables de recursos humanos y ejecutivos,

110
0:05:40.923,000 --> 0:05:41,000
las personas de alto nivel,

111
0:05:42.303,000 --> 0:05:44,000
que usaban estos sistemas en la contratación.

112
0:05:44.452,000 --> 0:05:45,000
Estaban muy emocionados.

113
0:05:45.646,000 --> 0:05:49,000
Pensaban que esto haría la contratación más objetiva, menos tendenciosa,

114
0:05:50.323,000 --> 0:05:53,000
para dar a las mujeres y a las minorías mejores oportunidades

115
0:05:53.347,000 --> 0:05:55,000
contra los administradores humanos tendenciosos.

116
0:05:55.715,000 --> 0:05:57,000
La contratación humana es tendenciosa.

117
0:05:59.099,000 --> 0:06:,000
Lo sé.

118
0:06:00.308,000 --> 0:06:03,000
Es decir, en uno de mis primeros trabajos como programadora,

119
0:06:03.337,000 --> 0:06:06,000
mi jefa a veces venía allí donde yo estaba

120
0:06:07.229,000 --> 0:06:1,000
muy temprano en la mañana o muy tarde por la tarde,

121
0:06:11.006,000 --> 0:06:14,000
y decía: "Zeynep, ¡vayamos a comer!"

122
0:06:14.174,000 --> 0:06:16,000
Me dejaba perpleja por el momento extraño de preguntar.

123
0:06:17.151,000 --> 0:06:18,000
Son las 16. ¿Almuerzo?

124
0:06:19.068,000 --> 0:06:22,000
Estaba en la ruina, así que, ante un almuerzo gratis, siempre fui.

125
0:06:22.428,000 --> 0:06:24,000
Más tarde me di cuenta de lo que estaba ocurriendo.

126
0:06:24.985,000 --> 0:06:27,000
Mis jefes inmediatos no habían confesado a sus altos mandos

127
0:06:28.969,000 --> 0:06:31,000
que el programador contratado para un trabajo serio era una adolescente

128
0:06:32.416,000 --> 0:06:35,000
que llevaba pantalones vaqueros y zapatillas de deporte en el trabajo.

129
0:06:36.624,000 --> 0:06:38,000
Yo hacía un buen trabajo, solo que no encajaba

130
0:06:39.23,000 --> 0:06:4,000
por la edad y por el sexo equivocado.

131
0:06:41.123,000 --> 0:06:44,000
Así que contratar a ciegas independiente del género y de la raza

132
0:06:44.493,000 --> 0:06:45,000
ciertamente me parece bien.

133
0:06:47.031,000 --> 0:06:5,000
Sin embargo, con estos sistemas, es más complicado, y he aquí por qué:

134
0:06:50.968,000 --> 0:06:55,000
Hoy los sistemas informáticos pueden deducir todo tipo de cosas sobre Uds.

135
0:06:56.783,000 --> 0:06:57,000
a partir de sus pistas digitales,

136
0:06:58.679,000 --> 0:07:,000
incluso si no las han dado a conocer.

137
0:07:01.506,000 --> 0:07:03,000
Pueden inferir su orientación sexual,

138
0:07:04.994,000 --> 0:07:05,000
sus rasgos de personalidad,

139
0:07:06.859,000 --> 0:07:07,000
sus inclinaciones políticas.

140
0:07:08.83,000 --> 0:07:11,000
Tienen poder predictivo con altos niveles de precisión.

141
0:07:13.082,000 --> 0:07:15,000
Recuerden, por cosas que ni siquiera han dado a conocer.

142
0:07:15.964,000 --> 0:07:16,000
Esta es la inferencia.

143
0:07:17.579,000 --> 0:07:2,000
Tengo una amiga que desarrolló este tipo de sistemas informáticos

144
0:07:20.864,000 --> 0:07:23,000
para predecir la probabilidad de depresión clínica o posparto

145
0:07:24.529,000 --> 0:07:25,000
a partir de datos de medios sociales.

146
0:07:26.676,000 --> 0:07:27,000
Los resultados son impresionantes.

147
0:07:28.492,000 --> 0:07:31,000
Su sistema puede predecir la probabilidad de depresión

148
0:07:31.873,000 --> 0:07:34,000
meses antes de la aparición de cualquier síntoma,

149
0:07:35.8,000 --> 0:07:36,000
meses antes.

150
0:07:37.197,000 --> 0:07:39,000
No hay síntomas, sí hay predicción.

151
0:07:39.467,000 --> 0:07:43,000
Ella espera que se use para la intervención temprana. ¡Estupendo!

152
0:07:44.721,000 --> 0:07:46,000
Pero ahora pongan esto en el contexto de la contratación.

153
0:07:48.027,000 --> 0:07:51,000
Así que en esa conferencia de recursos humanos,

154
0:07:51.097,000 --> 0:07:55,000
me acerqué a una gerenta de alto nivel de una empresa muy grande,

155
0:07:55.83,000 --> 0:07:59,000
y le dije: "Mira, ¿qué pasaría si, sin su conocimiento,

156
0:08:00.432,000 --> 0:08:06,000
el sistema elimina a las personas con alta probabilidad futura de la depresión?

157
0:08:07.761,000 --> 0:08:1,000
No están deprimidos ahora, solo quizá en el futuro, sea probable.

158
0:08:11.853,000 --> 0:08:14,000
¿Y si elimina a las mujeres con más probabilidades de estar embarazadas

159
0:08:15.293,000 --> 0:08:17,000
en el próximo año o dos, pero no está embarazada ahora?

160
0:08:18.844,000 --> 0:08:23,000
¿Y si contratamos a personas agresivas, porque esa es su cultura de trabajo?"

161
0:08:25.173,000 --> 0:08:27,000
No se puede saber esto mirando un desglose por sexos.

162
0:08:27.798,000 --> 0:08:28,000
Estos pueden ser equilibrados.

163
0:08:29.284,000 --> 0:08:33,000
Y como esto es aprendizaje automático, no la programación tradicional,

164
0:08:33.521,000 --> 0:08:37,000
no hay una variable etiquetada como "mayor riesgo de depresión",

165
0:08:37.926,000 --> 0:08:38,000
"mayor riesgo de embarazo",

166
0:08:39.783,000 --> 0:08:4,000
"escala de chico agresivo".

167
0:08:41.995,000 --> 0:08:44,000
Ud. no solo no sabe lo que su sistema selecciona,

168
0:08:45.698,000 --> 0:08:47,000
sino que ni siquiera sabe por dónde empezar a buscar.

169
0:08:48.044,000 --> 0:08:49,000
Es una caja negra.

170
0:08:49.315,000 --> 0:08:51,000
Tiene capacidad de predicción, pero uno no lo entiende.

171
0:08:52.486,000 --> 0:08:54,000
"¿Qué salvaguardia", pregunté,

172
0:08:54.879,000 --> 0:08:57,000
"puede asegurar que la caja negra no hace algo perjudicial?"

173
0:09:00.863,000 --> 0:09:03,000
Ella me miró como si acabara de romper algo valioso.

174
0:09:04.765,000 --> 0:09:05,000
(Risas)

175
0:09:06.037,000 --> 0:09:08,000
Me miró y dijo:

176
0:09:08.556,000 --> 0:09:12,000
"No quiero oír ni una palabra de esto".

177
0:09:13.458,000 --> 0:09:15,000
Dio la vuelta y se alejó.

178
0:09:16.064,000 --> 0:09:17,000
Eso sí, ella no fue grosera.

179
0:09:17.574,000 --> 0:09:23,000
Era claramente: lo que no sé, no es mi problema, vete, encara la muerte.

180
0:09:23.906,000 --> 0:09:24,000
(Risas)

181
0:09:25.652,000 --> 0:09:28,000
Un sistema de este tipo puede ser incluso menos sesgado

182
0:09:29.325,000 --> 0:09:31,000
que los administradores humanos en algunos aspectos.

183
0:09:31.888,000 --> 0:09:33,000
Y podría tener sentido monetario.

184
0:09:34.573,000 --> 0:09:35,000
Pero también podría llevar

185
0:09:36.247,000 --> 0:09:4,000
a un cierre constante pero sigiloso del mercado de trabajo

186
0:09:41.019,000 --> 0:09:43,000
a las personas con mayor riesgo de depresión.

187
0:09:43.553,000 --> 0:09:45,000
¿Es este el tipo de sociedad la que queremos construir,

188
0:09:46.373,000 --> 0:09:48,000
sin siquiera saber que lo hemos hecho,

189
0:09:48.682,000 --> 0:09:51,000
porque nos movemos en torno a decisiones de máquinas que no entendemos totalmente?

190
0:09:53.265,000 --> 0:09:54,000
Otro problema es el siguiente:

191
0:09:55.314,000 --> 0:09:59,000
estos sistemas son a menudo entrenados con datos generados

192
0:09:59.56,000 --> 0:10:01,000
por nuestras acciones, por huellas humanas.

193
0:10:02.188,000 --> 0:10:05,000
Podrían pues estar reflejando nuestros prejuicios,

194
0:10:06.02,000 --> 0:10:09,000
y estos sistemas podrían dar cuenta de nuestros prejuicios

195
0:10:09.637,000 --> 0:10:1,000
y la amplificación de ellos

196
0:10:10.974,000 --> 0:10:11,000
volviendo a nosotros,

197
0:10:12.416,000 --> 0:10:13,000
mientras que decimos:

198
0:10:13.902,000 --> 0:10:16,000
"Somos objetivos, es el cómputo neutral".

199
0:10:18.314,000 --> 0:10:21,000
Los investigadores encontraron que en Google las mujeres tienen

200
0:10:22.134,000 --> 0:10:25,000
menos probabilidades que los hombres

201
0:10:25.463,000 --> 0:10:28,000
de que les aparezcan anuncios de trabajo bien remunerados.

202
0:10:28.463,000 --> 0:10:3,000
Y buscando nombres afroestadounidenses

203
0:10:31.017,000 --> 0:10:35,000
es más probable que aparezcan anuncios que sugieren antecedentes penales,

204
0:10:35.747,000 --> 0:10:36,000
incluso cuando no existan.

205
0:10:38.693,000 --> 0:10:41,000
Estos sesgos ocultos y algoritmos de la caja negra

206
0:10:42.266,000 --> 0:10:45,000
que descubren los investigadores a veces, pero a veces no,

207
0:10:46.263,000 --> 0:10:48,000
pueden tener consecuencias que cambian la vida.

208
0:10:49.958,000 --> 0:10:53,000
En Wisconsin, un acusado fue condenado a seis años de prisión

209
0:10:54.141,000 --> 0:10:55,000
por escaparse de la policía.

210
0:10:55.904,000 --> 0:10:58,000
Quizá no lo sepan, pero los algoritmos se usan cada vez más

211
0:10:58.97,000 --> 0:11:01,000
en las decisiones de libertad condicional y de sentencia.

212
0:11:02.056,000 --> 0:11:04,000
El acusado quiso saber: ¿Cómo se calcula la puntuación?

213
0:11:05.795,000 --> 0:11:06,000
Es una caja negra comercial.

214
0:11:07.484,000 --> 0:11:11,000
La empresa se negó a que se cuestionara su algoritmo en audiencia pública.

215
0:11:12.206,000 --> 0:11:16,000
Pero ProPublica, organización no lucrativa de investigación,

216
0:11:16.992,000 --> 0:11:19,000
auditó precisamente ese algoritmo con los datos públicos que encontró,

217
0:11:20.178,000 --> 0:11:22,000
y descubrió que sus resultados estaban sesgados

218
0:11:22.598,000 --> 0:11:25,000
y su capacidad de predicción era pésima, apenas mejor que el azar,

219
0:11:25.985,000 --> 0:11:29,000
