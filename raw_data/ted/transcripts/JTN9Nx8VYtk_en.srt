1
0:00:13.267,000 --> 0:00:15,000
My name is Dan Cohen and I am an academic, as he said.

2
0:00:16.5,000 --> 0:00:19,000
And what that means is that I argue.

3
0:00:19.6,000 --> 0:00:2,000
It's an important part of my life.

4
0:00:21.267,000 --> 0:00:22,000
And I like to argue.

5
0:00:22.733,000 --> 0:00:25,000
And I'm not just an academic, I'm a philosopher,

6
0:00:26.133,000 --> 0:00:28,000
so I like to think that I'm actually pretty good at arguing.

7
0:00:29.1,000 --> 0:00:31,000
But I also like to think a lot about arguing.

8
0:00:32.367,000 --> 0:00:35,000
And in thinking about arguing, I've come across some puzzles.

9
0:00:35.867,000 --> 0:00:36,000
And one of the puzzles is that,

10
0:00:37.8,000 --> 0:00:39,000
as I've been thinking about arguing over the years --

11
0:00:40.367,000 --> 0:00:41,000
and it's been decades now --

12
0:00:41.8,000 --> 0:00:42,000
I've gotten better at arguing.

13
0:00:43.5,000 --> 0:00:46,000
But the more that I argue and the better I get at arguing,

14
0:00:47.033,000 --> 0:00:48,000
the more that I lose.

15
0:00:48.967,000 --> 0:00:49,000
And that's a puzzle.

16
0:00:50.233,000 --> 0:00:52,000
And the other puzzle is that I'm actually okay with that.

17
0:00:53.5,000 --> 0:00:54,000
Why is it that I'm okay with losing

18
0:00:55.367,000 --> 0:00:58,000
and why is it that I think good arguers are actually better at losing?

19
0:00:58.833,000 --> 0:01:,000
Well, there are some other puzzles.

20
0:01:00.867,000 --> 0:01:01,000
One is: why do we argue?

21
0:01:02.767,000 --> 0:01:03,000
Who benefits from arguments?

22
0:01:04.433,000 --> 0:01:06,000
When I think about arguments, I'm talking about --

23
0:01:06.8,000 --> 0:01:08,000
let's call them academic arguments or cognitive arguments --

24
0:01:09.667,000 --> 0:01:1,000
where something cognitive is at stake:

25
0:01:11.533,000 --> 0:01:13,000
Is this proposition true? Is this theory a good theory?

26
0:01:14.3,000 --> 0:01:18,000
Is this a viable interpretation of the data or the text? And so on.

27
0:01:18.8,000 --> 0:01:21,000
I'm not interested really in arguments about whose turn it is to do the dishes

28
0:01:22.8,000 --> 0:01:23,000
or who has to take out the garbage.

29
0:01:24.5,000 --> 0:01:26,000
Yeah, we have those arguments, too.

30
0:01:26.833,000 --> 0:01:28,000
I tend to win those arguments, because I know the tricks.

31
0:01:29.6,000 --> 0:01:3,000
But those aren't the important arguments.

32
0:01:31.6,000 --> 0:01:32,000
I'm interested in academic arguments,

33
0:01:33.4,000 --> 0:01:34,000
and here are the things that puzzle me.

34
0:01:36.667,000 --> 0:01:39,000
First, what do good arguers win when they win an argument?

35
0:01:39.767,000 --> 0:01:41,000
What do I win if I convince you

36
0:01:42.267,000 --> 0:01:44,000
that utilitarianism isn't really the right framework

37
0:01:44.72,000 --> 0:01:45,000
for thinking about ethical theories?

38
0:01:46.5,000 --> 0:01:47,000
What do we win when we win an argument?

39
0:01:48.4,000 --> 0:01:49,000
Even before that,

40
0:01:49.767,000 --> 0:01:5,000
what does it matter to me

41
0:01:51.033,000 --> 0:01:53,000
whether you have this idea that Kant's theory works

42
0:01:54,000 --> 0:01:57,000
or Mill is the right ethicist to follow?

43
0:01:57.2,000 --> 0:01:58,000
It's no skin off my back

44
0:01:58.6,000 --> 0:02:01,000
whether you think functionalism is a viable theory of mind.

45
0:02:02.3,000 --> 0:02:04,000
So why do we even try to argue?

46
0:02:04.367,000 --> 0:02:05,000
Why do we try to convince other people

47
0:02:06.233,000 --> 0:02:08,000
to believe things they don't want to believe,

48
0:02:08.433,000 --> 0:02:09,000
and is that even a nice thing to do?

49
0:02:10.233,000 --> 0:02:12,000
Is that a nice way to treat another human being,

50
0:02:12.5,000 --> 0:02:14,000
try and make them think something they don't want to think?

51
0:02:15.5,000 --> 0:02:19,000
Well, my answer is going to make reference to three models for arguments.

52
0:02:20.199,000 --> 0:02:22,000
The first model -- let's call it the dialectical model --

53
0:02:22.967,000 --> 0:02:24,000
is we think of arguments as war; you know what that's like --

54
0:02:25.9,000 --> 0:02:27,000
a lot of screaming and shouting and winning and losing.

55
0:02:28.633,000 --> 0:02:3,000
That's not a very helpful model for arguing,

56
0:02:30.8,000 --> 0:02:32,000
but it's a pretty common and entrenched model for arguing.

57
0:02:33.567,000 --> 0:02:36,000
But there's a second model for arguing: arguments as proofs.

58
0:02:36.833,000 --> 0:02:38,000
Think of a mathematician's argument.

59
0:02:38.933,000 --> 0:02:4,000
Here's my argument. Does it work? Is it any good?

60
0:02:41.733,000 --> 0:02:45,000
Are the premises warranted? Are the inferences valid?

61
0:02:46.233,000 --> 0:02:48,000
Does the conclusion follow from the premises?

62
0:02:48.8,000 --> 0:02:5,000
No opposition, no adversariality --

63
0:02:51.233,000 --> 0:02:56,000
not necessarily any arguing in the adversarial sense.

64
0:02:56.933,000 --> 0:02:57,000
But there's a third model to keep in mind

65
0:02:58.933,000 --> 0:02:59,000
that I think is going to be very helpful,

66
0:03:00.933,000 --> 0:03:05,000
and that is arguments as performances, arguments in front of an audience.

67
0:03:05.967,000 --> 0:03:07,000
We can think of a politician trying to present a position,

68
0:03:08.933,000 --> 0:03:1,000
trying to convince the audience of something.

69
0:03:11.1,000 --> 0:03:14,000
But there's another twist on this model that I really think is important;

70
0:03:14.6,000 --> 0:03:18,000
namely, that when we argue before an audience,

71
0:03:18.633,000 --> 0:03:22,000
sometimes the audience has a more participatory role in the argument;

72
0:03:22.733,000 --> 0:03:26,000
that is, arguments are also [performances] in front of juries,

73
0:03:27.267,000 --> 0:03:29,000
who make a judgment and decide the case.

74
0:03:30.067,000 --> 0:03:31,000
Let's call this the rhetorical model,

75
0:03:31.9,000 --> 0:03:34,000
where you have to tailor your argument to the audience at hand.

76
0:03:35.633,000 --> 0:03:37,000
You know, presenting a sound, well-argued,

77
0:03:38.3,000 --> 0:03:41,000
tight argument in English before a francophone audience

78
0:03:41.7,000 --> 0:03:42,000
just isn't going to work.

79
0:03:43.8,000 --> 0:03:46,000
So we have these models -- argument as war, argument as proof

80
0:03:47.467,000 --> 0:03:49,000
and argument as performance.

81
0:03:50.167,000 --> 0:03:53,000
Of those three, the argument as war is the dominant one.

82
0:03:54.467,000 --> 0:03:56,000
It dominates how we talk about arguments,

83
0:03:57.1,000 --> 0:03:59,000
it dominates how we think about arguments,

84
0:03:59.167,000 --> 0:04:01,000
and because of that, it shapes how we argue,

85
0:04:02.167,000 --> 0:04:03,000
our actual conduct in arguments.

86
0:04:03.967,000 --> 0:04:04,000
Now, when we talk about arguments,

87
0:04:05.667,000 --> 0:04:06,000
we talk in a very militaristic language.

88
0:04:07.667,000 --> 0:04:1,000
We want strong arguments, arguments that have a lot of punch,

89
0:04:10.933,000 --> 0:04:11,000
arguments that are right on target.

90
0:04:12.633,000 --> 0:04:15,000
We want to have our defenses up and our strategies all in order.

91
0:04:15.833,000 --> 0:04:17,000
We want killer arguments.

92
0:04:18.433,000 --> 0:04:19,000
That's the kind of argument we want.

93
0:04:21.2,000 --> 0:04:23,000
It is the dominant way of thinking about arguments.

94
0:04:23.633,000 --> 0:04:24,000
When I'm talking about arguments,

95
0:04:25.267,000 --> 0:04:27,000
that's probably what you thought of, the adversarial model.

96
0:04:28.445,000 --> 0:04:3,000
But the war metaphor,

97
0:04:30.966,000 --> 0:04:32,000
the war paradigm or model for thinking about arguments,

98
0:04:33.733,000 --> 0:04:35,000
has, I think, deforming effects on how we argue.

99
0:04:37.1,000 --> 0:04:39,000
First, it elevates tactics over substance.

100
0:04:40.967,000 --> 0:04:42,000
You can take a class in logic, argumentation.

101
0:04:43.133,000 --> 0:04:44,000
You learn all about the subterfuges

102
0:04:44.867,000 --> 0:04:46,000
that people use to try and win arguments -- the false steps.

103
0:04:47.767,000 --> 0:04:5,000
It magnifies the us-versus them aspect of it.

104
0:04:50.967,000 --> 0:04:53,000
It makes it adversarial; it's polarizing.

105
0:04:54.4,000 --> 0:04:59,000
And the only foreseeable outcomes are triumph -- glorious triumph --

106
0:05:00.233,000 --> 0:05:03,000
or abject, ignominious defeat.

107
0:05:03.333,000 --> 0:05:04,000
I think those are deforming effects,

108
0:05:05.1,000 --> 0:05:08,000
and worst of all, it seems to prevent things like negotiation

109
0:05:08.967,000 --> 0:05:12,000
or deliberation or compromise or collaboration.

110
0:05:14.233,000 --> 0:05:17,000
Think about that one -- have you ever entered an argument thinking,

111
0:05:17.424,000 --> 0:05:2,000
"Let's see if we can hash something out, rather than fight it out.

112
0:05:20.833,000 --> 0:05:21,000
What can we work out together?"

113
0:05:22.767,000 --> 0:05:24,000
I think the argument-as-war metaphor

114
0:05:25.163,000 --> 0:05:29,000
inhibits those other kinds of resolutions to argumentation.

115
0:05:29.6,000 --> 0:05:31,000
And finally -- this is really the worst thing --

116
0:05:32.067,000 --> 0:05:34,000
arguments don't seem to get us anywhere; they're dead ends.

117
0:05:34.9,000 --> 0:05:39,000
They are like roundabouts or traffic jams or gridlock in conversation.

118
0:05:40.6,000 --> 0:05:41,000
We don't get anywhere.

119
0:05:42.433,000 --> 0:05:43,000
And one more thing.

120
0:05:43.733,000 --> 0:05:45,000
And as an educator, this is the one that really bothers me:

121
0:05:46.667,000 --> 0:05:48,000
If argument is war,

122
0:05:48.867,000 --> 0:05:53,000
then there's an implicit equation of learning with losing.

123
0:05:53.9,000 --> 0:05:54,000
And let me explain what I mean.

124
0:05:56.067,000 --> 0:05:58,000
Suppose you and I have an argument.

125
0:05:58.633,000 --> 0:06:01,000
You believe a proposition, P, and I don't.

126
0:06:02.5,000 --> 0:06:03,000
And I say, "Well, why do you believe P?"

127
0:06:04.438,000 --> 0:06:05,000
And you give me your reasons.

128
0:06:05.867,000 --> 0:06:07,000
And I object and say, "Well, what about ...?"

129
0:06:08.267,000 --> 0:06:09,000
And you answer my objection.

130
0:06:09.8,000 --> 0:06:11,000
And I have a question: "Well, what do you mean?

131
0:06:12.233,000 --> 0:06:13,000
How does it apply over here?"

132
0:06:14.133,000 --> 0:06:15,000
And you answer my question.

133
0:06:15.8,000 --> 0:06:16,000
Now, suppose at the end of the day,

134
0:06:17.533,000 --> 0:06:19,000
I've objected, I've questioned,

135
0:06:19.667,000 --> 0:06:21,000
I've raised all sorts of counter counter-considerations

136
0:06:22.287,000 --> 0:06:25,000
and in every case you've responded to my satisfaction.

137
0:06:25.9,000 --> 0:06:27,000
And so at the end of the day, I say,

138
0:06:28.567,000 --> 0:06:31,000
"You know what? I guess you're right: P."

139
0:06:32.5,000 --> 0:06:34,000
So, I have a new belief.

140
0:06:34.867,000 --> 0:06:35,000
And it's not just any belief;

141
0:06:36.304,000 --> 0:06:42,000
it's well-articulated, examined -- it's a battle-tested belief.

142
0:06:43.8,000 --> 0:06:44,000
Great cognitive gain.

143
0:06:44.984,000 --> 0:06:45,000
OK, who won that argument?

144
0:06:47.6,000 --> 0:06:5,000
Well, the war metaphor seems to force us into saying you won,

145
0:06:51.567,000 --> 0:06:53,000
even though I'm the only one who made any cognitive gain.

146
0:06:54.3,000 --> 0:06:57,000
What did you gain, cognitively, from convincing me?

147
0:06:57.967,000 --> 0:06:59,000
Sure, you got some pleasure out of it, maybe your ego stroked,

148
0:07:00.967,000 --> 0:07:01,000
maybe you get some professional status

149
0:07:02.967,000 --> 0:07:04,000
in the field -- "This guy's a good arguer."

150
0:07:05.567,000 --> 0:07:07,000
But just from a cognitive point of view,

151
0:07:08.567,000 --> 0:07:09,000
who was the winner?

152
0:07:09.867,000 --> 0:07:13,000
The war metaphor forces us into thinking that you're the winner and I lost,

153
0:07:14.7,000 --> 0:07:16,000
even though I gained.

154
0:07:16.767,000 --> 0:07:18,000
And there's something wrong with that picture.

155
0:07:19.067,000 --> 0:07:21,000
And that's the picture I really want to change if we can.

156
0:07:21.8,000 --> 0:07:24,000
So, how can we find ways

157
0:07:25.173,000 --> 0:07:28,000
to make arguments yield something positive?

158
0:07:29.633,000 --> 0:07:32,000
What we need is new exit strategies for arguments.

159
0:07:33.367,000 --> 0:07:35,000
But we're not going to have new exit strategies for arguments

160
0:07:36.267,000 --> 0:07:39,000
until we have new entry approaches to arguments.

161
0:07:39.6,000 --> 0:07:42,000
We need to think of new kinds of arguments.

162
0:07:43.267,000 --> 0:07:45,000
In order to do that, well --

163
0:07:45.867,000 --> 0:07:46,000
I don't know how to do that.

164
0:07:48.1,000 --> 0:07:49,000
That's the bad news.

165
0:07:49.467,000 --> 0:07:52,000
The argument-as-war metaphor is just ... it's a monster.

166
0:07:52.5,000 --> 0:07:54,000
It's just taken up habitation in our mind,

167
0:07:54.919,000 --> 0:07:56,000
and there's no magic bullet that's going to kill it.

168
0:07:57.39,000 --> 0:07:59,000
There's no magic wand that's going to make it disappear.

169
0:08:00.049,000 --> 0:08:01,000
I don't have an answer.

170
0:08:01.333,000 --> 0:08:02,000
But I have some suggestions.

171
0:08:02.7,000 --> 0:08:03,000
Here's my suggestion:

172
0:08:05.7,000 --> 0:08:07,000
If we want to think of new kinds of arguments,

173
0:08:07.908,000 --> 0:08:1,000
what we need to do is think of new kinds of arguers.

174
0:08:11.9,000 --> 0:08:12,000
So try this:

175
0:08:14.767,000 --> 0:08:18,000
Think of all the roles that people play in arguments.

176
0:08:19.233,000 --> 0:08:21,000
There's the proponent and the opponent

177
0:08:22.233,000 --> 0:08:24,000
in an adversarial, dialectical argument.

178
0:08:24.433,000 --> 0:08:26,000
There's the audience in rhetorical arguments.

179
0:08:26.6,000 --> 0:08:28,000
There's the reasoner in arguments as proofs.

180
0:08:30.767,000 --> 0:08:31,000
All these different roles.

181
0:08:32.1,000 --> 0:08:35,000
Now, can you imagine an argument in which you are the arguer,

182
0:08:35.967,000 --> 0:08:38,000
but you're also in the audience, watching yourself argue?

183
0:08:39.967,000 --> 0:08:42,000
Can you imagine yourself watching yourself argue,

184
0:08:43.033,000 --> 0:08:47,000
losing the argument, and yet still, at the end of the argument, saying,

185
0:08:47.533,000 --> 0:08:49,000
"Wow, that was a good argument!"

186
0:08:51.133,000 --> 0:08:52,000
Can you do that?

187
0:08:52.5,000 --> 0:08:55,000
I think you can, and I think if you can imagine that kind of argument,

188
0:08:55.832,000 --> 0:08:58,000
where the loser says to the winner and the audience and the jury can say,

189
0:08:59.633,000 --> 0:09:,000
"Yeah, that was a good argument,"

190
0:09:01.6,000 --> 0:09:02,000
then you have imagined a good argument.

191
0:09:03.482,000 --> 0:09:04,000
And more than that,

192
0:09:04.667,000 --> 0:09:05,000
I think you've imagined a good arguer,

193
0:09:06.667,000 --> 0:09:1,000
an arguer that's worthy of the kind of arguer you should try to be.

194
0:09:11.567,000 --> 0:09:13,000
Now, I lose a lot of arguments.

195
0:09:14.367,000 --> 0:09:16,000
It takes practice to become a good arguer,

196
0:09:16.8,000 --> 0:09:19,000
in the sense of being able to benefit from losing, but fortunately,

197
0:09:19.98,000 --> 0:09:21,000
I've had many, many colleagues who have been willing to step up

198
0:09:22.967,000 --> 0:09:23,000
and provide that practice for me.

199
0:09:24.7,000 --> 0:09:25,000
Thank you.

200
0:09:25.899,000 --> 0:09:29,000
(Applause)

