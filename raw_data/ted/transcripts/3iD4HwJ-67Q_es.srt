1
0:00:,000 --> 0:00:07,000
Traductor: Ciro Gomez Revisor: Lidia Cámara de la Fuente

2
0:00:13.548,000 --> 0:00:15,000
No importa quiénes son o dónde viven,

3
0:00:16.088,000 --> 0:00:18,000
supongo que tienen al menos un pariente

4
0:00:18.468,000 --> 0:00:2,000
al que le gusta reenviar esos correos electrónicos.

5
0:00:21.206,000 --> 0:00:22,000
Ya saben de los que estoy hablando...

6
0:00:23.461,000 --> 0:00:25,000
los que tienen reclamos dudosos o videos de conspiración.

7
0:00:26.255,000 --> 0:00:28,000
Y probablemente ya los han silenciado en Facebook

8
0:00:29.007,000 --> 0:00:31,000
por compartir publicaciones sociales como esta.

9
0:00:31.389,000 --> 0:00:32,000
Es una imagen de un plátano

10
0:00:32.807,000 --> 0:00:34,000
con una extraña cruz roja corriendo por el centro.

11
0:00:35.498,000 --> 0:00:37,000
Y el texto a su alrededor advierte a las personas

12
0:00:37.861,000 --> 0:00:38,000
de no comer frutas que se vean así,

13
0:00:39.843,000 --> 0:00:41,000
sugiriendo que han sido inyectadas con sangre

14
0:00:42.25,000 --> 0:00:43,000
contaminada con el virus del VIH.

15
0:00:44.049,000 --> 0:00:46,000
Y el mensaje social compartido arriba simplemente dice:

16
0:00:46.676,000 --> 0:00:48,000
"Por favor, reenvíe para salvar vidas".

17
0:00:49.672,000 --> 0:00:52,000
Los verificadores de hechos han desacreditado esto durante años,

18
0:00:52.99,000 --> 0:00:54,000
pero es uno de esos rumores que simplemente no morirán.

19
0:00:55.823,000 --> 0:00:56,000
Un rumor zombie.

20
0:00:57.513,000 --> 0:00:59,000
Y, por supuesto, es completamente falso.

21
0:01:00.18,000 --> 0:01:02,000
Puede ser tentador reírse de un ejemplo como este, decir:

22
0:01:03.163,000 --> 0:01:05,000
"Bueno, ¿quién creería esto, de todos modos?".

23
0:01:05.419,000 --> 0:01:06,000
Pero es un rumor zombie

24
0:01:06.709,000 --> 0:01:1,000
porque aprovecha los temores más profundos de las personas sobre su propia seguridad

25
0:01:10.982,000 --> 0:01:12,000
y la de las personas que aman.

26
0:01:13.783,000 --> 0:01:16,000
Y si pasan tanto tiempo como yo mirando información engañosa,

27
0:01:16.906,000 --> 0:01:18,000
saben que este es solo un ejemplo de muchos que se aprovechan

28
0:01:19.874,000 --> 0:01:22,000
de los temores y vulnerabilidades más profundos de las personas.

29
0:01:23.214,000 --> 0:01:27,000
Todos los días, en todo el mundo, vemos decenas de nuevos memes en Instagram

30
0:01:27.607,000 --> 0:01:3,000
alentando a los padres a no vacunar a sus hijos.

31
0:01:30.67,000 --> 0:01:34,000
Vemos nuevos videos en YouTube que dicen que el cambio climático es un engaño.

32
0:01:35.226,000 --> 0:01:36,000
Y en todas las plataformas,

33
0:01:36.661,000 --> 0:01:39,000
vemos un sinfín de publicaciones diseñadas para demonizar a los demás.

34
0:01:39.992,000 --> 0:01:42,000
sobre la base de su raza, religión o sexualidad.

35
0:01:44.314,000 --> 0:01:47,000
Bienvenidos a uno de los desafíos centrales de nuestro tiempo.

36
0:01:47.647,000 --> 0:01:51,000
¿Cómo mantener un Internet con libertad de expresión en el núcleo,

37
0:01:51.696,000 --> 0:01:54,000
a la vez de asegurar de que el contenido que se difunde

38
0:01:55.023,000 --> 0:01:58,000
no causa daños irreparables a nuestras democracias, nuestras comunidades

39
0:01:58.933,000 --> 0:02:,000
y a nuestro bienestar físico y mental?

40
0:02:01.998,000 --> 0:02:03,000
Porque vivimos en la era de la información,

41
0:02:04.109,000 --> 0:02:07,000
todavía la moneda central de la que todos dependemos, información,

42
0:02:07.68,000 --> 0:02:09,000
ya no se considera totalmente confiable

43
0:02:10.061,000 --> 0:02:12,000
y, a veces, puede parecer francamente peligrosa.

44
0:02:12.811,000 --> 0:02:13,000
Esto se debe en parte

45
0:02:13.889,000 --> 0:02:16,000
al crecimiento desbocado de las plataformas de intercambio social.

46
0:02:17.052,000 --> 0:02:18,000
que nos permiten ir allá

47
0:02:18.438,000 --> 0:02:2,000
donde mentiras y hechos se sientan juntos,

48
0:02:20.684,000 --> 0:02:23,000
pero sin ninguna de las señales tradicionales de confiabilidad.

49
0:02:24.268,000 --> 0:02:27,000
Y Dios, nuestro lenguaje en torno a esto está terriblemente confuso.

50
0:02:27.911,000 --> 0:02:3,000
La gente todavía está obsesionada con la frase "noticias falsas"

51
0:02:31.028,000 --> 0:02:33,000
a pesar del hecho de que es extraordinariamente inútil

52
0:02:33.713,000 --> 0:02:36,000
y solía describir una serie de cosas que en realidad son muy diferentes:

53
0:02:37.207,000 --> 0:02:4,000
mentiras, rumores, engaños, conspiraciones, propaganda.

54
0:02:40.911,000 --> 0:02:42,000
Realmente quiero que podamos dejar de usar una frase

55
0:02:43.847,000 --> 0:02:45,000
que ha sido cooptada por políticos de todo el mundo,

56
0:02:46.733,000 --> 0:02:47,000
de izquierda a derecha

57
0:02:48.228,000 --> 0:02:51,000
utilizada como arma para atacar a una prensa libre e independiente.

58
0:02:52.307,000 --> 0:02:56,000
(Aplausos)

59
0:02:57.033,000 --> 0:03:,000
Porque necesitamos nuestros medios profesionales ahora más que nunca.

60
0:03:00.792,000 --> 0:03:03,000
Además, la mayor parte de este contenido ni siquiera se disfraza de noticia.

61
0:03:04.369,000 --> 0:03:06,000
Son memes, videos, publicaciones sociales.

62
0:03:06.945,000 --> 0:03:09,000
Y la mayor parte no es falsa; es engañosa.

63
0:03:10.422,000 --> 0:03:13,000
Tendemos a fijarnos en lo que es verdadero o falso.

64
0:03:13.461,000 --> 0:03:17,000
Pero la mayor preocupación es en realidad la armamentización del contexto.

65
0:03:18.855,000 --> 0:03:19,000
Porque la desinformación más efectiva

66
0:03:20.847,000 --> 0:03:23,000
siempre ha sido la que tiene un núcleo de verdad.

67
0:03:23.919,000 --> 0:03:26,000
Tomemos este ejemplo de Londres, de marzo de 2017,

68
0:03:27.419,000 --> 0:03:28,000
un tuit que circuló ampliamente

69
0:03:28.983,000 --> 0:03:31,000
tras un incidente terrorista en el puente de Westminster.

70
0:03:32.594,000 --> 0:03:34,000
Esta es una imagen genuina, no falsa.

71
0:03:35.046,000 --> 0:03:38,000
La mujer que aparece en la fotografía fue entrevistada después,

72
0:03:38.239,000 --> 0:03:4,000
y explicó que estaba completamente traumatizada.

73
0:03:40.672,000 --> 0:03:41,000
Hablaba por teléfono con un ser querido,

74
0:03:42.724,000 --> 0:03:44,000
y no miraba a la víctima por respeto.

75
0:03:45.076,000 --> 0:03:48,000
Pero aún circulaba ampliamente con este marco islamofóbico,

76
0:03:49.06,000 --> 0:03:52,000
con múltiples etiquetas, que incluyen: #BanIslam

77
0:03:52.425,000 --> 0:03:54,000
Ahora, si trabajaran en Twitter, ¿qué harían?

78
0:03:54.847,000 --> 0:03:56,000
¿Lo quitarían o lo dejarían?

79
0:03:58.553,000 --> 0:04:01,000
Mi reacción intestinal, mi reacción emocional, es bajar esto.

80
0:04:02.006,000 --> 0:04:04,000
Odio el encuadre de esta imagen.

81
0:04:04.585,000 --> 0:04:06,000
Pero la libertad de expresión es un derecho humano,

82
0:04:06.997,000 --> 0:04:09,000
y si comenzamos a hablar mal de lo que nos hace sentir incómodos,

83
0:04:10.246,000 --> 0:04:11,000
tenemos un problema.

84
0:04:11.5,000 --> 0:04:13,000
Y esto podría parecer un caso claro,

85
0:04:13.578,000 --> 0:04:14,000
pero, en realidad, la mayoría no lo son.

86
0:04:15.53,000 --> 0:04:17,000
Estas líneas son increíblemente difíciles de trazar.

87
0:04:18,000 --> 0:04:2,000
Lo qué es una decisión bien intencionada para uno

88
0:04:20.305,000 --> 0:04:22,000
es una censura absoluta para la siguiente.

89
0:04:22.759,000 --> 0:04:24,000
Lo que ahora sabemos es que esta cuenta, Texas Lone Star,

90
0:04:25.712,000 --> 0:04:28,000
fue parte de una campaña de desinformación rusa más amplia,

91
0:04:28.966,000 --> 0:04:3,000
una que desde entonces ha sido eliminada.

92
0:04:31.141,000 --> 0:04:32,000
¿Eso cambiaría su punto de vista?

93
0:04:33.322,000 --> 0:04:34,000
Cambiaría el mío,

94
0:04:34.505,000 --> 0:04:36,000
porque ahora se trata de una campaña coordinada

95
0:04:36.83,000 --> 0:04:37,000
para sembrar discordia.

96
0:04:38.069,000 --> 0:04:39,000
Y para quienes les gustaría pensar

97
0:04:40.054,000 --> 0:04:42,000
que la IA resolverá todos nuestros problemas,

98
0:04:42.679,000 --> 0:04:44,000
creo que estaremos de acuerdo que estamos muy lejos

99
0:04:45.158,000 --> 0:04:48,000
de que la IA pueda dar sentido a publicaciones como esta.

100
0:04:48.426,000 --> 0:04:5,000
Así que me gustaría explicar tres problemas entrelazados

101
0:04:51.387,000 --> 0:04:53,000
que hacen que esto sea tan complejo

102
0:04:53.784,000 --> 0:04:56,000
y que luego piensen en formas para abordar estos desafíos.

103
0:04:57.348,000 --> 0:05:,000
Primero, simplemente no tenemos una relación racional con la información,

104
0:05:01.262,000 --> 0:05:02,000
Tenemos una emocional.

105
0:05:02.754,000 --> 0:05:05,000
Simplemente, no es cierto que más hechos harán que todo esté bien,

106
0:05:06.572,000 --> 0:05:09,000
porque los algoritmos que determinan qué contenido vemos

107
0:05:09.696,000 --> 0:05:12,000
están diseñados para recompensar nuestras respuestas emocionales.

108
0:05:12.847,000 --> 0:05:13,000
Y cuando tenemos miedo,

109
0:05:14.252,000 --> 0:05:17,000
narraciones demasiado simplificadas, explicaciones conspirativas

110
0:05:17.45,000 --> 0:05:2,000
y el lenguaje que demoniza a los demás es mucho más efectivo.

111
0:05:21.538,000 --> 0:05:22,000
Y además, muchas de estas empresas,

112
0:05:23.436,000 --> 0:05:25,000
su modelo de negocio está unido a la atención,

113
0:05:26.006,000 --> 0:05:29,000
lo que significa que estos algoritmos estarán sesgados hacia la emoción.

114
0:05:30.371,000 --> 0:05:34,000
Segundo, la mayor parte del discurso del que estoy hablando aquí es legal.

115
0:05:35.081,000 --> 0:05:36,000
Seria un asunto diferente

116
0:05:36.551,000 --> 0:05:38,000
si hablara de imágenes de abuso sexual infantil

117
0:05:38.916,000 --> 0:05:39,000
o contenido que incita a la violencia.

118
0:05:40.867,000 --> 0:05:43,000
Puede ser perfectamente legal publicar una mentira absoluta.

119
0:05:45.13,000 --> 0:05:49,000
Pero la gente sigue hablando de eliminar contenido "problemático" o "dañino",

120
0:05:49.188,000 --> 0:05:51,000
pero sin una definición clara de lo que quieren decir,

121
0:05:51.821,000 --> 0:05:52,000
incluyendo a Mark Zuckerberg,

122
0:05:53.259,000 --> 0:05:56,000
quien recientemente pidió una regulación global para moderar el habla.

123
0:05:56.87,000 --> 0:05:58,000
Y mi preocupación es que vemos gobiernos

124
0:05:59.109,000 --> 0:06:,000
en todo el mundo

125
0:06:00.425,000 --> 0:06:02,000
implementar decisiones políticas apresuradas

126
0:06:03.125,000 --> 0:06:05,000
que podrían desencadenar consecuencias mucho más graves

127
0:06:05.895,000 --> 0:06:06,000
cuando se trata de nuestra charla.

128
0:06:08.006,000 --> 0:06:11,000
E incluso si pudiéramos decidir qué charla dejar o eliminar.

129
0:06:11.736,000 --> 0:06:13,000
Nunca hemos tenido tantas charlas.

130
0:06:13.934,000 --> 0:06:15,000
Cada segundo, se suben millones de piezas de contenido

131
0:06:16.769,000 --> 0:06:17,000
de personas de todo el mundo

132
0:06:18.22,000 --> 0:06:19,000
en diferentes idiomas

133
0:06:19.412,000 --> 0:06:21,000
aprovechando miles de diferentes contextos culturales.

134
0:06:22.204,000 --> 0:06:24,000
Simplemente nunca hemos tenido mecanismos efectivos

135
0:06:24.636,000 --> 0:06:26,000
para moderar la libre expresión a esta escala,

136
0:06:26.852,000 --> 0:06:28,000
ya sea impulsada por humanos o por tecnología.

137
0:06:30.284,000 --> 0:06:33,000
Y en tercer lugar, estas empresas (Google, Twitter, Facebook, WhatsApp)

138
0:06:34.252,000 --> 0:06:36,000
son parte de un ecosistema de información más amplio.

139
0:06:37.117,000 --> 0:06:4,000
Nos gusta echarles toda la culpa a ellos, pero la verdad es que los medios

140
0:06:40.713,000 --> 0:06:43,000
y los funcionarios electos también pueden desempeñar un papel igual

141
0:06:44.347,000 --> 0:06:46,000
en amplificar rumores y conspiraciones cuando quieren.

142
0:06:47.8,000 --> 0:06:51,000
Como podemos nosotros, al reenviar sin pensar contenido divisivorio o engañoso

143
0:06:52.768,000 --> 0:06:53,000
sin estar seguros.

144
0:06:54.077,000 --> 0:06:56,000
Estamos agregando más a esa contaminación.

145
0:06:57.236,000 --> 0:06:59,000
Sé que todos estamos buscando una solución fácil.

146
0:06:59.878,000 --> 0:07:,000
Pero simplemente no la hay.

147
0:07:01.95,000 --> 0:07:03,000
Cualquier solución tendrá que implementarse

148
0:07:04.172,000 --> 0:07:06,000
a gran escala, escala de Internet,

149
0:07:06.419,000 --> 0:07:09,000
y sí, las plataformas, están acostumbradas a operar a ese nivel.

150
0:07:09.704,000 --> 0:07:12,000
pero ¿podemos y debemos permitirles que solucionen estos problemas?

151
0:07:13.448,000 --> 0:07:14,000
Ciertamente lo están intentando.

152
0:07:15.004,000 --> 0:07:17,000
Estamos de acuerdo en que no queremos que sean

153
0:07:17.514,000 --> 0:07:2,000
las corporaciones globales las guardianas de la verdad y equidad en línea.

154
0:07:21.18,000 --> 0:07:24,000
Y también creo que las plataformas estarían de acuerdo con eso.

155
0:07:24.257,000 --> 0:07:26,000
Y en este momento, están marcando su propia tarea.

156
0:07:27.162,000 --> 0:07:27,000
Les gusta decirnos

157
0:07:28.104,000 --> 0:07:3,000
que las intervenciones que están implementando funcionan,

158
0:07:30.877,000 --> 0:07:32,000
pero como hacen sus propios informes de transparencia,

159
0:07:33.551,000 --> 0:07:36,000
no se puede verificar independientemente lo que realmente está sucediendo.

160
0:07:38.431,000 --> 0:07:41,000
(Aplausos)

161
0:07:41.797,000 --> 0:07:43,000
Y seamos claros que la mayoría de los cambios que vemos

162
0:07:44.423,000 --> 0:07:47,000
solo suceden después de que los periodistas inicien una investigación

163
0:07:47.881,000 --> 0:07:48,000
y encuentren evidencia de sesgo

164
0:07:49.426,000 --> 0:07:51,000
o contenido que rompe las pautas de su comunidad.

165
0:07:52.815,000 --> 0:07:56,000
Sí, estas compañías tienen que jugar de verdad un papel importante en el proceso,

166
0:07:57.434,000 --> 0:07:58,000
pero no pueden controlarlo.

167
0:07:59.855,000 --> 0:08:,000
¿Y qué hay de los gobiernos?

168
0:08:01.863,000 --> 0:08:04,000
Muchos creen que la regulación global es nuestra última esperanza.

169
0:08:04.983,000 --> 0:08:06,000
en términos de limpieza de nuestro ecosistema de información.

170
0:08:07.887,000 --> 0:08:1,000
Pero lo que veo son legisladores que luchan por mantenerse al día

171
0:08:11.077,000 --> 0:08:13,000
con los rápidos cambios en la tecnología.

172
0:08:13.442,000 --> 0:08:14,000
Y peor, trabajan en la oscuridad,

173
0:08:15.37,000 --> 0:08:16,000
porque no tienen acceso a los datos

174
0:08:17.215,000 --> 0:08:19,000
para entender lo que está sucediendo en estas plataformas.

175
0:08:20.26,000 --> 0:08:23,000
Y de todos modos, ¿en qué gobiernos confiaríamos para hacer esto?

176
0:08:23.355,000 --> 0:08:25,000
Necesitamos una respuesta global, no nacional.

177
0:08:27.419,000 --> 0:08:29,000
Entonces el eslabón perdido somos nosotros.

178
0:08:29.72,000 --> 0:08:32,000
Son esas personas que usan estas tecnologías todos los días.

179
0:08:33.26,000 --> 0:08:37,000
¿Podemos diseñar una nueva infraestructura para soportar información de calidad?

180
0:08:38.371,000 --> 0:08:39,000
Bueno, creo que podemos

181
0:08:39.625,000 --> 0:08:42,000
y tengo algunas ideas sobre lo que realmente podríamos hacer.

182
0:08:43.006,000 --> 0:08:46,000
En primer lugar, si nos tomamos en serio atraer al público a esto,

183
0:08:46.133,000 --> 0:08:48,000
¿podemos inspirarnos en Wikipedia?

184
0:08:48.538,000 --> 0:08:49,000
Nos han mostrado lo que es posible.

185
0:08:50.386,000 --> 0:08:51,000
Si, no es perfecto

186
0:08:51.561,000 --> 0:08:53,000
pero han demostrado que con las estructuras correctas,

187
0:08:54.219,000 --> 0:08:56,000
con una perspectiva global y mucha, mucha transparencia,

188
0:08:56.878,000 --> 0:08:59,000
puedes construir algo que se ganará la confianza de las personas.

189
0:08:59.998,000 --> 0:09:02,000
Tenemos que encontrar una manera de aprovechar la sabiduría colectiva

190
0:09:03.354,000 --> 0:09:05,000
y la experiencia de todos los usuarios.

191
0:09:05.517,000 --> 0:09:07,000
En particular las mujeres, las personas de color

192
0:09:08.057,000 --> 0:09:09,000
y los grupos subrepresentados.

193
0:09:09.557,000 --> 0:09:1,000
¿Porque adivinen qué?

194
0:09:10.747,000 --> 0:09:12,000
Son expertos en odio y desinformación,

195
0:09:13.506,000 --> 0:09:16,000
porque han sido los objetivos de estas campañas durante mucho tiempo.

196
0:09:17.046,000 --> 0:09:19,000
Y a lo largo de los años, han levantando banderas,

197
0:09:19.42,000 --> 0:09:2,000
y no han sido escuchados

198
0:09:21.109,000 --> 0:09:22,000
Esto tiene que cambiar.

199
0:09:22.807,000 --> 0:09:26,000
¿Podríamos construir una Wikipedia para la confianza?

200
0:09:27.157,000 --> 0:09:31,000
¿Podríamos encontrar una forma en que los usuarios realmente proporcionen ideas?

201
0:09:31.37,000 --> 0:09:34,000
Podrían ofrecer ideas sobre decisiones difíciles de moderación de contenido.

202
0:09:35.091,000 --> 0:09:36,000
Podrían comentar

203
0:09:36.578,000 --> 0:09:39,000
cuando las plataformas decidan implementar nuevos cambios.

204
0:09:40.241,000 --> 0:09:44,000
Segundo, las experiencias de las personas con la información son personalizadas.

205
0:09:44.427,000 --> 0:09:46,000
Mi fuente de noticias de Facebook es diferente de la de Uds.

206
0:09:47.384,000 --> 0:09:49,000
Igual sus recomendaciones de YouTube.

207
0:09:49.863,000 --> 0:09:51,000
Eso hace que sea imposible para nosotros examinar

208
0:09:52.379,000 --> 0:09:54,000
qué información está viendo la gente.

209
0:09:54.815,000 --> 0:09:55,000
Entonces ¿podríamos imaginar desarrollar

210
0:09:56.838,000 --> 0:10:,000
algún tipo de repositorio abierto centralizado para datos anonimos,

211
0:10:01.03,000 --> 0:10:03,000
con privacidad y preocupaciones éticas incorporadas?

212
0:10:04.22,000 --> 0:10:05,000
Porque imaginen lo que aprenderíamos

213
0:10:06.022,000 --> 0:10:09,000
si construyeramos una red global de ciudadanos preocupados

214
0:10:09.307,000 --> 0:10:12,000
que quisieran donar sus datos sociales a la ciencia.

215
0:10:13.101,000 --> 0:10:14,000
Porque sabemos muy poco

216
0:10:14.443,000 --> 0:10:17,000
sobre las consecuencias a largo plazo del odio y la desinformación

217
0:10:17.599,000 --> 0:10:19,000
sobre las actitudes y comportamientos de las personas.

218
0:10:20.236,000 --> 0:10:21,000
Y lo que sabemos,

219
0:10:21.427,000 --> 0:10:23,000
la mayoría se ha llevado a cabo en EE. UU.

220
0:10:23.754,000 --> 0:10:25,000
a pesar del hecho de que es un problema global.

221
0:10:26.049,000 --> 0:10:27,000
Necesitamos trabajar en eso también.

222
0:10:28.192,000 --> 0:10:28,000
Y tercero,

223
0:10:29.196,000 --> 0:10:31,000
¿podemos encontrar una manera de conectar los puntos?

224
0:10:31.7,000 --> 0:10:34,000
Ningún sector, y mucho menos sin fines de lucro, start-up o gobierno,

225
0:10:35.162,000 --> 0:10:36,000
va a resolver esto

226
0:10:36.608,000 --> 0:10:38,000
Pero hay personas muy inteligentes en todo el mundo.

227
0:10:39.196,000 --> 0:10:4,000
trabajando en estos desafíos,

228
0:10:40.601,000 --> 0:10:43,000
de redacciones, sociedad civil, academia, grupos activistas.

229
0:10:44.201,000 --> 0:10:45,000
Pueden ver algunos aquí.

230
0:10:46.123,000 --> 0:10:48,000
Unos construyen indicadores de credibilidad de contenido.

231
0:10:49.024,000 --> 0:10:51,000
Otros verifican hechos, para que las plataformas

232
0:10:51.324,000 --> 0:10:53,000
puedan clasificar noticias falsas, videos e imágenes

233
0:10:53.919,000 --> 0:10:55,000
Una organización sin fines de lucro, First Draft,

234
0:10:56.456,000 --> 0:10:58,000
trabaja con redacciones normalmente competitivas del mundo

235
0:10:59.188,000 --> 0:11:02,000
para ayudarlos a desarrollar programas de investigación y colaboración.

236
0:11:03.231,000 --> 0:11:04,000
Danny Hillis, un arquitecto de software,

237
0:11:05.194,000 --> 0:11:07,000
está diseñando un nuevo sistema llamado The Underlay,

238
0:11:07.859,000 --> 0:11:09,000
un registro de todas las declaraciones públicas de hechos

239
0:11:10.668,000 --> 0:11:11,000
conectado a sus fuentes,

240
0:11:12.051,000 --> 0:11:15,000
para que las personas y los algoritmos puedan juzgar mejor lo que es creíble.

241
0:11:16.8,000 --> 0:11:19,000
Y los educadores de todo el mundo están probando diferentes técnicas.

242
0:11:20.18,000 --> 0:11:23,000
para encontrar formas de hacer que la gente critique el contenido que consume.

243
0:11:24.633,000 --> 0:11:27,000
Todos estos esfuerzos son maravillosos, pero están trabajando en silos,

244
0:11:28.078,000 --> 0:11:3,000
y muchos de ellos no tienen fondos suficientes.

245
0:11:30.502,000 --> 0:11:32,000
También hay cientos de personas muy inteligentes.

246
0:11:32.859,000 --> 0:11:33,000
trabajando en esas empresas,

247
0:11:34.255,000 --> 0:11:36,000
pero de nuevo, esos esfuerzos pueden sentirse desarticulados,

248
0:11:37.224,000 --> 0:11:4,000
porque están desarrollando diferentes soluciones a los mismos problemas.

249
0:11:41.205,000 --> 0:11:43,000
¿Cómo encontrar una manera de unir a las personas

250
0:11:43.678,000 --> 0:11:45,000
en una ubicación física por días o semanas,

251
0:11:46.69,000 --> 0:11:48,000
para que puedan abordar estos problemas juntos,

252
0:11:49.13,000 --> 0:11:5,000
pero desde sus diferentes perspectivas?

253
0:11:51.062,000 --> 0:11:52,000
¿Podemos hacer eso?

254
0:11:52.426,000 --> 0:11:55,000
¿Podemos construir una respuesta coordinada y ambiciosa,

255
0:11:55.689,000 --> 0:11:58,000
una que coincida con la escala y la complejidad del problema?

256
0:11:59.819,000 --> 0:12:,000
Realmente creo que podemos.

257
0:12:01.216,000 --> 0:12:03,000
Juntos, reconstruyamos nuestros bienes comunes de información.

258
0:12:04.819,000 --> 0:12:05,000
Gracias.

259
0:12:06.033,000 --> 0:12:09,000
(Aplausos)

