1
0:00:,000 --> 0:00:07,000
Traducteur: Juliette Bruneel Relecteur: Claire Ghyselen

2
0:00:12.86,000 --> 0:00:14,000
Permettez-moi de vous présenter un paradoxe.

3
0:00:16.429,000 --> 0:00:17,000
Ces 10 dernières années,

4
0:00:17.896,000 --> 0:00:2,000
de nombreuses entreprises ont essayé d'être moins bureaucratiques,

5
0:00:21.792,000 --> 0:00:23,000
d'avoir moins de règles et de procédures centralisées

6
0:00:24.649,000 --> 0:00:27,000
de rendre les équipes locales plus autonomes et plus agiles.

7
0:00:28.204,000 --> 0:00:32,000
Elles veulent maintenant utiliser l'intelligence artificielle, IA,

8
0:00:32.814,000 --> 0:00:34,000
sans se rendre compte que cette super technologie

9
0:00:35.283,000 --> 0:00:38,000
peut les rendre plus bureaucratiques que jamais.

10
0:00:39.378,000 --> 0:00:4,000
Pourquoi dis-je cela ?

11
0:00:40.553,000 --> 0:00:43,000
Parce qu'il y a une forte similarité entre l' IA et la bureaucratie.

12
0:00:44.403,000 --> 0:00:46,000
Oui, la bureaucratie ne se base que

13
0:00:46.839,000 --> 0:00:5,000
sur des règles et procédures pré-établies et fait fi du jugement humain.

14
0:00:51.887,000 --> 0:00:54,000
L' IA se réfère uniquement aux règles,

15
0:00:56.062,000 --> 0:00:58,000
dont beaucoup sont déduites de données antérieures,

16
0:00:58.914,000 --> 0:01:,000
et rien d'autre.

17
0:01:01.198,000 --> 0:01:04,000
Et si l'esprit humain n'intervient pas,

18
0:01:04.958,000 --> 0:01:08,000
l'IA apportera une nouvelle forme de bureaucratie terrifiante

19
0:01:09.483,000 --> 0:01:11,000
que j'appelle « Algocratie ».

20
0:01:12.563,000 --> 0:01:16,000
L'AI y prendra toutes les décisions essentielles

21
0:01:17.02,000 --> 0:01:2,000
en dehors de tout contrôle humain.

22
0:01:20.426,000 --> 0:01:22,000
Ce risque est-il réel ?

23
0:01:22.507,000 --> 0:01:22,000
Oui.

24
0:01:23.284,000 --> 0:01:26,000
Je supervise une équipe de 800 spécialistes en IA.

25
0:01:26.714,000 --> 0:01:29,000
Nous avons déployé plus de 100 solutions d'IA personnalisées

26
0:01:30.596,000 --> 0:01:32,000
dans de grandes entreprises partout dans le monde.

27
0:01:33.422,000 --> 0:01:37,000
Et je vois trop de dirigeants qui se comportent

28
0:01:38.129,000 --> 0:01:39,000
comme les bureaucrates du passé.

29
0:01:39.779,000 --> 0:01:43,000
Ils veulent se passer des hommes trop chers et démodés

30
0:01:44.716,000 --> 0:01:48,000
et ne s'appuyer que sur l'IA dans la prise de décision.

31
0:01:49.235,000 --> 0:01:54,000
C'est ce que j'appelle « système sans conscience ».

32
0:01:54.257,000 --> 0:01:56,000
Et pourquoi est-ce si tentant ?

33
0:01:56.862,000 --> 0:02:01,000
Parce que l'approche qui combine homme+IA prend plus de temps,

34
0:02:02.295,000 --> 0:02:04,000
elle est plus chère et plus difficile.

35
0:02:04.923,000 --> 0:02:07,000
Les équipes commerciales, techniques, et les analystes

36
0:02:08.237,000 --> 0:02:1,000
doivent travailler pendant des mois

37
0:02:10.336,000 --> 0:02:15,000
pour avoir un système collaboratif performant entre humain et l'IA.

38
0:02:16.159,000 --> 0:02:19,000
C'est plus long, plus coûteux et plus difficile.

39
0:02:19.848,000 --> 0:02:21,000
Mais ça vaut vraiment le coup.

40
0:02:22.326,000 --> 0:02:25,000
Selon une enquête récente du BCG et du MIT,

41
0:02:25.654,000 --> 0:02:29,000
18 % des entreprises dans le monde,

42
0:02:30.194,000 --> 0:02:32,000
développent l'IA

43
0:02:32.42,000 --> 0:02:34,000
et gagnent de l'argent grâce à ça.

44
0:02:35.177,000 --> 0:02:4,000
Leurs initiatives en IA se focalisent à 80%

45
0:02:40.774,000 --> 0:02:41,000
sur l'efficacité et la croissance,

46
0:02:42.745,000 --> 0:02:44,000
pour prendre de meilleures décisions

47
0:02:44.915,000 --> 0:02:49,000
au lieu de remplacer l'humain par l'IA pour réduire les coûts.

48
0:02:50.158,000 --> 0:02:54,000
Pourquoi l'intervention humaine est-elle si importante ?

49
0:02:54.22,000 --> 0:02:59,000
Tout simplement car laissée à elle-même, l'IA peut faire des choses stupides.

50
0:02:59.362,000 --> 0:03:02,000
Parfois sans conséquence, comme ce Tweet :

51
0:03:03.21,000 --> 0:03:04,000
« Cher Amazon ,

52
0:03:04.8,000 --> 0:03:05,000
j'ai acheté une lunette de toilette.

53
0:03:06.527,000 --> 0:03:07,000
Par besoin, pas par envie.

54
0:03:07.89,000 --> 0:03:08,000
Je ne les collectionne pas,

55
0:03:09.374,000 --> 0:03:11,000
Je ne suis pas accro aux toilettes.

56
0:03:11.52,000 --> 0:03:13,000
Vous aurez beau m'envoyer des courriels tentants,

57
0:03:13.871,000 --> 0:03:15,000
Vous ne me ferez pas dire : 'Oh, tiens,

58
0:03:16.243,000 --> 0:03:18,000
Je vais en acheter une de plus pour me faire plaisir' »

59
0:03:18.821,000 --> 0:03:18,000
(Rires)

60
0:03:19.767,000 --> 0:03:24,000
Parfois c'est plus gênant, comme ce Tweet :

61
0:03:24.866,000 --> 0:03:25,000
« J'étais dans la même situation

62
0:03:26.696,000 --> 0:03:28,000
avec l'urne funéraire de ma mère.

63
0:03:29.18,000 --> 0:03:29,000
(Rires)

64
0:03:30.108,000 --> 0:03:31,000
Plusieurs mois après son décès,

65
0:03:31.615,000 --> 0:03:33,000
j'ai reçu des messages d'Amazon me disant :

66
0:03:33.724,000 --> 0:03:34,000
« Si vous l'avez apprécié... »

67
0:03:35.184,000 --> 0:03:37,000
(Rires)

68
0:03:37.222,000 --> 0:03:39,000
Dans d'autres cas, les conséquences sont graves.

69
0:03:39.766,000 --> 0:03:43,000
Prenez le cas d'un calcul IA d'université qui rejette la candidature d'un étudiant.

70
0:03:44.544,000 --> 0:03:45,000
Pourquoi ?

71
0:03:45.709,000 --> 0:03:47,000
Parce qu'il ne se base que sur des données passées,

72
0:03:48.407,000 --> 0:03:51,000
les caractéristiques des étudiants qui ont réussi ou échoué.

73
0:03:51.607,000 --> 0:03:54,000
Certaines sont évidentes, comme les notes.

74
0:03:54.67,000 --> 0:03:58,000
Mais si, dans le passé, tous les étudiants venant d'une même ville ont échoué,

75
0:03:59.207,000 --> 0:04:02,000
Il est très probable que l'IA en fasse une règle

76
0:04:02.749,000 --> 0:04:05,000
et qu'elle rejette tous les étudiants venant de cette ville.

77
0:04:06.562,000 --> 0:04:11,000
Ne laissant à personne la chance de démontrer que la règle est fausse.

78
0:04:11.861,000 --> 0:04:13,000
Et personne n'est en mesure de vérifier toutes les règles,

79
0:04:14.583,000 --> 0:04:17,000
car l'IA avancée est en apprentissage constant.

80
0:04:18.273,000 --> 0:04:2,000
Et si l'humain est gardé à l'écart,

81
0:04:20.646,000 --> 0:04:23,000
on plonge dans le cauchemar algocratique.

82
0:04:24.462,000 --> 0:04:26,000
Qui est responsable d'avoir évincé l'étudiant ?

83
0:04:27.354,000 --> 0:04:28,000
Personne, c'est l'IA qui a décidé.

84
0:04:29.1,000 --> 0:04:3,000
Est-ce que c'est juste ? Oui.

85
0:04:30.707,000 --> 0:04:33,000
Le même ensemble de règles objectives a été appliqué à tout le monde.

86
0:04:34.374,000 --> 0:04:37,000
Pouvons-nous revoir le cas de ce gars brillant

87
0:04:37.377,000 --> 0:04:38,000
qui vient du mauvais endroit ?

88
0:04:38.887,000 --> 0:04:42,000
Non, les calculs ne changent pas d'avis.

89
0:04:42.972,000 --> 0:04:44,000
C'est maintenant qu'il nous faut choisir.

90
0:04:45.754,000 --> 0:04:5,000
Continuer dans cette algocratie ou choisir un système « humain + IA ».

91
0:04:51.177,000 --> 0:04:52,000
Et pour y arriver,

92
0:04:52.553,000 --> 0:04:55,000
nous devons arrêter de penser à « la technologie d'abord »

93
0:04:56.007,000 --> 0:05:,000
et commencer à appliquer la formule secrète

94
0:05:00.602,000 --> 0:05:02,000
pour déployer « homme + IA »,

95
0:05:02.715,000 --> 0:05:04,000
10% de l'effort porte sur le codage des algorithmes.

96
0:05:05.655,000 --> 0:05:08,000
20% sur la technologie autour de ceux-ci :

97
0:05:09.215,000 --> 0:05:13,000
récupérer les données, créer l'interface, les intégrer aux systèmes d'information.

98
0:05:13.358,000 --> 0:05:15,000
Mais 70%, le plus gros de l'effort,

99
0:05:16.279,000 --> 0:05:2,000
consiste à articuler l'IA avec les personnes et les processus

100
0:05:20.78,000 --> 0:05:23,000
pour en retirer le maximum.

101
0:05:24.136,000 --> 0:05:28,000
L'IA échoue quand on rogne sur ces 70%.

102
0:05:28.79,000 --> 0:05:31,000
Le coût peut être réduit,

103
0:05:31.976,000 --> 0:05:35,000
mais des millions de dollars sont gaspillés en technologie inutile.

104
0:05:35.98,000 --> 0:05:37,000
Qui s'en soucie ?

105
0:05:38.154,000 --> 0:05:4,000
De réelles tragédies peuvent survenir :

106
0:05:41.143,000 --> 0:05:48,000
346 morts dans les crashes récents de deux avions B-737

107
0:05:48.768,000 --> 0:05:51,000
dont les pilotes ne pouvaient pas interagir correctement

108
0:05:52.058,000 --> 0:05:55,000
à cause du système de commande informatisé.

109
0:05:55.973,000 --> 0:05:57,000
Pour que les 70% soient couronnés de succès,

110
0:05:58.131,000 --> 0:06:02,000
il faut d'abord s'assurer que les algorithmes sont codés par des analystes

111
0:06:02.903,000 --> 0:06:04,000
en équipe avec des experts du secteur.

112
0:06:05.44,000 --> 0:06:07,000
Prenez par exemple le secteur de la santé.

113
0:06:07.654,000 --> 0:06:09,000
Une de nos équipes a travaillé sur un nouveau médicament

114
0:06:10.289,000 --> 0:06:12,000
avec un léger problème.

115
0:06:12.699,000 --> 0:06:14,000
Lorsqu'ils le prenaient pour la première fois,

116
0:06:14.997,000 --> 0:06:17,000
certains patients, très peu, avaient une crise cardiaque.

117
0:06:18.1,000 --> 0:06:21,000
Et donc, tous les patients, pour leur première dose,

118
0:06:21.267,000 --> 0:06:23,000
devaient passer une journée à l'hôpital,

119
0:06:23.966,000 --> 0:06:25,000
sous surveillance, au cas où.

120
0:06:26.61,000 --> 0:06:29,000
Notre objectif a été d'identifier les patients

121
0:06:29.765,000 --> 0:06:31,000
à risque zéro de crise cardiaque,

122
0:06:32.185,000 --> 0:06:34,000
qui pouvaient éviter la journée à l'hôpital.

123
0:06:34.937,000 --> 0:06:39,000
Nous avons utilisé l'IA pour analyser les données d'essais cliniques,

124
0:06:40.138,000 --> 0:06:44,000
pour corréler les ECG, la composition sanguine, et les marqueurs biologiques

125
0:06:44.554,000 --> 0:06:46,000
avec le risque de crise cardiaque.

126
0:06:47.227,000 --> 0:06:48,000
En un mois,

127
0:06:48.516,000 --> 0:06:54,000
notre modèle pouvait détecter 62% de patients à risque zéro.

128
0:06:54.884,000 --> 0:06:56,000
Ils pouvaient éviter la journée à l'hôpital.

129
0:06:57.864,000 --> 0:07:,000
Vous seriez à l'aise pour prendre votre première dose à la maison

130
0:07:01.368,000 --> 0:07:02,000
avec ces résultats de calcul ?

131
0:07:02.929,000 --> 0:07:03,000
(Rires)

132
0:07:03.951,000 --> 0:07:05,000
Les médecins ne l'étaient pas.

133
0:07:05.953,000 --> 0:07:07,000
Et dans le cas de faux négatifs,

134
0:07:08.287,000 --> 0:07:13,000
les gens autorisés par l'IA à rester à la maison peuvent mourir ?

135
0:07:13.552,000 --> 0:07:14,000
(Rires)

136
0:07:14.932,000 --> 0:07:16,000
C'est là qu'ont commencé nos 70%.

137
0:07:17.414,000 --> 0:07:19,000
Nous avons travaillé avec une équipe de médecins

138
0:07:19.693,000 --> 0:07:22,000
à vérifier la logique médicale de chaque variable dans notre modèle.

139
0:07:23.528,000 --> 0:07:27,000
Par exemple, nous utilisions le taux d'une certaine enzyme du foie

140
0:07:28.13,000 --> 0:07:29,000
comme indice,

141
0:07:29.432,000 --> 0:07:32,000
ce qui n'était pas une logique médicale évidente.

142
0:07:33.151,000 --> 0:07:36,000
Le signal statistique était plutôt fort.

143
0:07:36.3,000 --> 0:07:38,000
Et s'il y avait un biais dans notre échantillon ?

144
0:07:39.154,000 --> 0:07:42,000
On a enlevé l'indice concerné du modèle.

145
0:07:42.302,000 --> 0:07:45,000
Nous avons aussi retiré les indices pour lesquels les experts affirmaient

146
0:07:45.928,000 --> 0:07:49,000
qu'ils ne pouvaient pas être mesurés avec précision dans la vie réelle.

147
0:07:50.371,000 --> 0:07:55,000
Après quatre mois, nous avions un modèle et un protocole médical.

148
0:07:55.506,000 --> 0:07:56,000
Tous deux ont été approuvés

149
0:07:57.201,000 --> 0:08:,000
par les autorités médicales américaines au printemps

150
0:08:00.461,000 --> 0:08:03,000
et cela a réduit le stress de la moitié des patients

151
0:08:04.182,000 --> 0:08:06,000
et a amélioré leur qualité de vie.

152
0:08:06.358,000 --> 0:08:08,000
Les ventes de ce médicament ont augmenté

153
0:08:09.28,000 --> 0:08:11,000
de plus de 100 millions de façon inattendue.

154
0:08:11.67,000 --> 0:08:15,000
70% pour créer le lien entre l'IA, les équipes et les processus,

155
0:08:15.886,000 --> 0:08:18,000
c'est aussi construire des interfaces puissantes

156
0:08:19.476,000 --> 0:08:24,000
pour résoudre des problèmes complexes, IA et humains ensemble.

157
0:08:25.285,000 --> 0:08:3,000
Un jour, un distributeur de mode nous a lancé un défi :

158
0:08:31.133,000 --> 0:08:33,000
« Nous avons les meilleurs acheteurs au monde.

159
0:08:33.664,000 --> 0:08:38,000
Pouvez-vous créer un moteur IA qui donne de meilleures prévisions de ventes qu'eux,

160
0:08:38.803,000 --> 0:08:42,000
qui dirait combien de chemises homme, haut de gamme, vert clair, taille XL

161
0:08:42.988,000 --> 0:08:44,000
nous devons acheter pour l'année prochaine ?

162
0:08:45.046,000 --> 0:08:49,000
Qui saura mieux prédire que nos stylistes sur ce qui se vendra ou pas. »

163
0:08:50.433,000 --> 0:08:53,000
En quelques semaines, notre équipe a créé un modèle sur des données passées,

164
0:08:54.42,000 --> 0:08:57,000
et la compétition s'est engagée face aux acheteurs humains.

165
0:08:58.35,000 --> 0:08:59,000
Résultat ?

166
0:09:00.06,000 --> 0:09:05,000
L'IA a gagné, en réduisant l'erreur de prévision de 25%.

167
0:09:05.906,000 --> 0:09:09,000
Ils auraient pu essayer de lancer ce modèle des « champions non humain »

168
0:09:10.759,000 --> 0:09:12,000
et le mettre en compétition avec tous les acheteurs humains.

169
0:09:13.599,000 --> 0:09:14,000
Amusez-vous bien !

170
0:09:15.201,000 --> 0:09:2,000
Mais nous savions que les acheteurs connaissaient les tendances

171
0:09:20.374,000 --> 0:09:23,000
qu'on ne pouvait pas déceler dans les données du passé.

172
0:09:23.698,000 --> 0:09:25,000
Ici ont commencé nos 70%.

173
0:09:26.565,000 --> 0:09:27,000
Nous avons commencé un second test,

174
0:09:28.545,000 --> 0:09:32,000
en faisant vérifier par les acheteurs les quantités suggérées par l'IA

175
0:09:33.362,000 --> 0:09:35,000
et en les faisant corriger si nécessaire.

176
0:09:36.18,000 --> 0:09:37,000
Résultat ?

177
0:09:37.707,000 --> 0:09:39,000
Les hommes qui utilisent l'IA...

178
0:09:39.853,000 --> 0:09:4,000
perdent.

179
0:09:41.8,000 --> 0:09:45,000
75% des corrections faites par un humain

180
0:09:45.974,000 --> 0:09:48,000
réduisaient la fiabilité.

181
0:09:48.995,000 --> 0:09:51,000
Fallait-il alors se débarrasser des acheteurs humains ?

182
0:09:52.205,000 --> 0:09:53,000
Non.

183
0:09:53.38,000 --> 0:09:55,000
Il était temps de recréer un modèle

184
0:09:56.012,000 --> 0:10:01,000
dans lequel les hommes n'essaient pas de deviner quand l'IA se trompe

185
0:10:01.11,000 --> 0:10:06,000
mais où les acheteurs humains contribueraient réellement à l'IA.

186
0:10:06.952,000 --> 0:10:08,000
Nous avons entièrement reconstruit le modèle

187
0:10:09.003,000 --> 0:10:14,000
en nous éloignant de notre interface initiale qui disait, plus ou moins :

188
0:10:14.578,000 --> 0:10:16,000
« Hé, humain ! Voici ce que je prévois,

189
0:10:17.052,000 --> 0:10:18,000
corrige ce que tu veux, »

190
0:10:18.827,000 --> 0:10:21,000
et nous l'avons orientée vers un modèle beaucoup plus riche, du genre :

191
0:10:22.485,000 --> 0:10:23,000
« Hé, humains !

192
0:10:24.416,000 --> 0:10:26,000
Je ne connais pas les tendances de l'année prochaine.

193
0:10:26.891,000 --> 0:10:29,000
Voulez-vous partager avec moi vos meilleurs paris créatifs ?

194
0:10:30.061,000 --> 0:10:31,000
Hé, humains !

195
0:10:31.191,000 --> 0:10:34,000
Voulez-vous m'aider à quantifier ces quelques articles importants ?

196
0:10:34.34,000 --> 0:10:38,000
Je ne trouve pas d'articles comparables pertinents dans le passé. »

197
0:10:38.403,000 --> 0:10:39,000
Résultat ?

198
0:10:40.205,000 --> 0:10:42,000
« Homme + IA » est gagnant,

199
0:10:42.334,000 --> 0:10:47,000
en réduisant l'erreur de prévision de 50%.

200
0:10:47.761,000 --> 0:10:5,000
Il a fallu un an pour finaliser l'outil.

201
0:10:51.066,000 --> 0:10:54,000
Long, coûteux et difficile.

202
0:10:55.026,000 --> 0:10:57,000
Mais les marges et les bénéfices

203
0:10:57.275,000 --> 0:11:03,000
ont dépassé les 100 millions d'économies par an pour ce distributeur.

204
0:11:03.473,000 --> 0:11:05,000
70% sur des sujets très sensibles

205
0:11:06.42,000 --> 0:11:09,000
veut aussi dire que les humains décident de ce qui est juste ou faux

206
0:11:10.216,000 --> 0:11:14,000
et définissent les règles de ce que l'IA peut faire ou pas,

207
0:11:14.332,000 --> 0:11:17,000
comme fixer des plafonds de prix pour empêcher les moteurs de calcul

208
0:11:17.858,000 --> 0:11:21,000
[de fixer] des prix exorbitants pour des consommateurs non avertis

209
0:11:22.386,000 --> 0:11:24,000
qui les accepteraient.

210
0:11:24.542,000 --> 0:11:26,000
Seuls les humains peuvent fixer ces limites -

211
0:11:27.126,000 --> 0:11:31,000
en aucune façon, l'IA ne les trouve dans des données passées.

212
0:11:31.24,000 --> 0:11:33,000
Certaines situations sont floues.

213
0:11:34.136,000 --> 0:11:36,000
Nous avons collaboré avec une société d'assurance maladie.

214
0:11:36.899,000 --> 0:11:4,000
Ils ont développé un moteur IA pour détecter parmi leurs clients

215
0:11:41.64,000 --> 0:11:43,000
ceux qui étaient sur le point de rentrer à l'hôpital

216
0:11:44.217,000 --> 0:11:46,000
pour leur vendre des services premium.

217
0:11:46.502,000 --> 0:11:47,000
Et le problème,

218
0:11:48.045,000 --> 0:11:5,000
c'est que l'équipe commerciale a appelé des prospects

219
0:11:51.043,000 --> 0:11:53,000
qui ne savaient même pas encore

220
0:11:53.762,000 --> 0:11:56,000
qu'ils devraient entrer à l'hôpital bientôt.

221
0:11:57.717,000 --> 0:11:59,000
Vous dirigez cette société.

222
0:12:00.056,000 --> 0:12:02,000
Arrêtez-vous ce programme ?

223
0:12:02.583,000 --> 0:12:03,000
Ce n'est pas une question facile.

224
0:12:04.501,000 --> 0:12:07,000
Et pour y répondre, certaines sociétés forment des équipes

225
0:12:08.058,000 --> 0:12:1,000
qui définissent des règles et des standards éthiques

226
0:12:10.561,000 --> 0:12:13,000
pour aider les équipes commerciales et techniques à poser des limites

227
0:12:13.915,000 --> 0:12:16,000
entre la personnalisation et la manipulation,

228
0:12:17.538,000 --> 0:12:19,000
la customisation des offres et la discrimination,

229
0:12:20.531,000 --> 0:12:24,000
le ciblage commercial et l'intrusion.

230
0:12:24.564,000 --> 0:12:27,000
Je suis convaincu que dans chaque entreprise,

231
0:12:28.26,000 --> 0:12:33,000
utiliser l'IA pour des sujets importants engendre des bénéfices considérables.

232
0:12:33.429,000 --> 0:12:35,000
Les dirigeants d'entreprise doivent oser

233
0:12:35.632,000 --> 0:12:37,000
et sélectionner un petit nombre de sujets,

234
0:12:37.646,000 --> 0:12:41,000
et pour chacun d'eux, mobiliser 10, 20, 30 personnes de leurs meilleures équipes -

235
0:12:42.606,000 --> 0:12:45,000
techniques, IA, analystes, éthique --

236
0:12:45.967,000 --> 0:12:51,000
et passer par le cycle complet 10-, 20-, 70% de « Homme + IA »

237
0:12:52.159,000 --> 0:12:54,000
s'ils veulent faire adopter l'IA

238
0:12:54.162,000 --> 0:12:56,000
efficacement dans leurs équipes et processus.

239
0:12:57.012,000 --> 0:12:58,000
Il n'y a pas d'autre moyen.

240
0:12:58.923,000 --> 0:13:03,000
Les citoyens des pays développés ont déjà peur de l'algocratie.

241
0:13:04.195,000 --> 0:13:07,000
7 000 ont été interrogées dans un sondage récent.

242
0:13:08.155,000 --> 0:13:11,000
Plus de 75% ont exprimé une réelle inquiétude

243
0:13:11.744,000 --> 0:13:14,000
quant à l'impact de l'IA sur leur travail, leur vie privée,

244
0:13:15.698,000 --> 0:13:18,000
quant au risque d'une société déshumanisée.

245
0:13:19.149,000 --> 0:13:24,000
Imposer l'algocratie crée un risque réel d'un retour de bâton contre l'IA

246
0:13:24.574,000 --> 0:13:28,000
dans les entreprises ou dans la société dans son ensemble.

247
0:13:29.011,000 --> 0:13:32,000
« Homme + IA » est notre seule option

248
0:13:32.317,000 --> 0:13:35,000
pour apporter les bénéfices de l'IA au monde réel.

249
0:13:36.037,000 --> 0:13:37,000
Et au final,

250
0:13:37.22,000 --> 0:13:41,000
les organisations fructueuses investiront dans la connaissance humaine

251
0:13:41.375,000 --> 0:13:44,000
pas seulement dans l'IA et les données.

252
0:13:44.789,000 --> 0:13:48,000
Recruter, former, récompenser les experts humains.

253
0:13:48.797,000 --> 0:13:51,000
On dit que les données sont le nouveau pétrole,

254
0:13:51.971,000 --> 0:13:55,000
mais croyez-moi, l'esprit humain fera la différence,

255
0:13:56.056,000 --> 0:13:59,000
car c'est la seule plateforme disponible

256
0:13:59.664,000 --> 0:14:03,000
pour extraire le pétrole des données.

257
0:14:04.626,000 --> 0:14:05,000
Merci.

258
0:14:05.821,000 --> 0:14:09,000
(Applaudissements)

