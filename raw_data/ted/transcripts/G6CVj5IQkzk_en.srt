1
0:00:25.476,000 --> 0:00:26,000
I do two things:

2
0:00:26.651,000 --> 0:00:28,000
I design mobile computers and I study brains.

3
0:00:28.793,000 --> 0:00:3,000
Today's talk is about brains and -- (Audience member cheers)

4
0:00:31.747,000 --> 0:00:32,000
Yay! I have a brain fan out there.

5
0:00:33.588,000 --> 0:00:36,000
(Laughter)

6
0:00:36.759,000 --> 0:00:37,000
If I could have my first slide,

7
0:00:38.338,000 --> 0:00:4,000
you'll see the title of my talk and my two affiliations.

8
0:00:41.211,000 --> 0:00:44,000
So what I'm going to talk about is why we don't have a good brain theory,

9
0:00:44.703,000 --> 0:00:46,000
why it is important that we should develop one

10
0:00:47.004,000 --> 0:00:48,000
and what we can do about it.

11
0:00:48.511,000 --> 0:00:49,000
I'll try to do all that in 20 minutes.

12
0:00:50.359,000 --> 0:00:51,000
I have two affiliations.

13
0:00:51.534,000 --> 0:00:53,000
Most of you know me from my Palm and Handspring days,

14
0:00:54.09,000 --> 0:00:56,000
but I also run a nonprofit scientific research institute

15
0:00:56.797,000 --> 0:00:58,000
called the Redwood Neuroscience Institute in Menlo Park.

16
0:00:59.453,000 --> 0:01:02,000
We study theoretical neuroscience and how the neocortex works.

17
0:01:02.865,000 --> 0:01:03,000
I'm going to talk all about that.

18
0:01:04.487,000 --> 0:01:06,000
I have one slide on my other life, the computer life,

19
0:01:07.256,000 --> 0:01:08,000
and that's this slide here.

20
0:01:08.581,000 --> 0:01:11,000
These are some of the products I've worked on over the last 20 years,

21
0:01:11.873,000 --> 0:01:12,000
starting from the very original laptop

22
0:01:13.739,000 --> 0:01:14,000
to some of the first tablet computers

23
0:01:15.55,000 --> 0:01:17,000
and so on, ending up most recently with the Treo,

24
0:01:17.872,000 --> 0:01:18,000
and we're continuing to do this.

25
0:01:19.428,000 --> 0:01:21,000
I've done this because I believe mobile computing

26
0:01:21.753,000 --> 0:01:22,000
is the future of personal computing,

27
0:01:23.501,000 --> 0:01:25,000
and I'm trying to make the world a little bit better

28
0:01:25.979,000 --> 0:01:26,000
by working on these things.

29
0:01:27.299,000 --> 0:01:28,000
But this was, I admit, all an accident.

30
0:01:29.197,000 --> 0:01:31,000
I really didn't want to do any of these products.

31
0:01:31.529,000 --> 0:01:32,000
Very early in my career

32
0:01:32.935,000 --> 0:01:34,000
I decided I was not going to be in the computer industry.

33
0:01:35.649,000 --> 0:01:36,000
Before that, I just have to tell you

34
0:01:37.394,000 --> 0:01:4,000
about this picture of Graffiti I picked off the web the other day.

35
0:01:40.526,000 --> 0:01:43,000
I was looking for a picture for Graffiti that'll text input language.

36
0:01:43.803,000 --> 0:01:46,000
I found a website dedicated to teachers who want to make script-writing things

37
0:01:47.516,000 --> 0:01:48,000
across the top of their blackboard,

38
0:01:49.214,000 --> 0:01:51,000
and they had added Graffiti to it, and I'm sorry about that.

39
0:01:52.071,000 --> 0:01:54,000
(Laughter)

40
0:01:54.342,000 --> 0:01:55,000
So what happened was,

41
0:01:55.666,000 --> 0:01:59,000
when I was young and got out of engineering school at Cornell in '79,

42
0:02:00.589,000 --> 0:02:03,000
I went to work for Intel and was in the computer industry,

43
0:02:03.8,000 --> 0:02:06,000
and three months into that, I fell in love with something else.

44
0:02:07.226,000 --> 0:02:1,000
I said, "I made the wrong career choice here,"

45
0:02:10.294,000 --> 0:02:12,000
and I fell in love with brains.

46
0:02:12.557,000 --> 0:02:13,000
This is not a real brain.

47
0:02:14.114,000 --> 0:02:16,000
This is a picture of one, a line drawing.

48
0:02:16.857,000 --> 0:02:18,000
And I don't remember exactly how it happened,

49
0:02:19,000 --> 0:02:22,000
but I have one recollection, which was pretty strong in my mind.

50
0:02:22.539,000 --> 0:02:23,000
In September of 1979,

51
0:02:24.173,000 --> 0:02:27,000
Scientific American came out with a single-topic issue about the brain.

52
0:02:27.561,000 --> 0:02:28,000
It was one of their best issues ever.

53
0:02:29.523,000 --> 0:02:31,000
They talked about the neuron, development, disease, vision

54
0:02:32.494,000 --> 0:02:34,000
and all the things you might want to know about brains.

55
0:02:35.114,000 --> 0:02:36,000
It was really quite impressive.

56
0:02:36.64,000 --> 0:02:38,000
One might've had the impression we knew a lot about brains.

57
0:02:39.436,000 --> 0:02:43,000
But the last article in that issue was written by Francis Crick of DNA fame.

58
0:02:43.655,000 --> 0:02:46,000
Today is, I think, the 50th anniversary of the discovery of DNA.

59
0:02:46.703,000 --> 0:02:49,000
And he wrote a story basically saying, this is all well and good,

60
0:02:49.802,000 --> 0:02:51,000
but you know, we don't know diddly squat about brains,

61
0:02:52.569,000 --> 0:02:53,000
and no one has a clue how they work,

62
0:02:54.332,000 --> 0:02:55,000
so don't believe what anyone tells you.

63
0:02:56.222,000 --> 0:02:58,000
This is a quote from that article, he says:

64
0:02:58.411,000 --> 0:03:02,000
"What is conspicuously lacking" -- he's a very proper British gentleman --

65
0:03:02.728,000 --> 0:03:04,000
"What is conspicuously lacking is a broad framework of ideas

66
0:03:05.582,000 --> 0:03:07,000
in which to interpret these different approaches."

67
0:03:07.958,000 --> 0:03:08,000
I thought the word "framework" was great.

68
0:03:09.95,000 --> 0:03:1,000
He didn't say we didn't have a theory.

69
0:03:11.791,000 --> 0:03:13,000
He says we don't even know how to begin to think about it.

70
0:03:14.54,000 --> 0:03:15,000
We don't even have a framework.

71
0:03:16.056,000 --> 0:03:19,000
We are in the pre-paradigm days, if you want to use Thomas Kuhn.

72
0:03:19.13,000 --> 0:03:2,000
So I fell in love with this.

73
0:03:20.493,000 --> 0:03:23,000
I said, look: We have all this knowledge about brains -- how hard can it be?

74
0:03:24.092,000 --> 0:03:27,000
It's something we can work on in my lifetime; I could make a difference.

75
0:03:27.554,000 --> 0:03:3,000
So I tried to get out of the computer business, into the brain business.

76
0:03:31.197,000 --> 0:03:33,000
First, I went to MIT, the AI lab was there.

77
0:03:33.225,000 --> 0:03:35,000
I said, I want to build intelligent machines too,

78
0:03:35.644,000 --> 0:03:37,000
but I want to study how brains work first.

79
0:03:38.185,000 --> 0:03:4,000
And they said, "Oh, you don't need to do that.

80
0:03:40.515,000 --> 0:03:42,000
You're just going to program computers, that's all.

81
0:03:42.929,000 --> 0:03:43,000
I said, you really ought to study brains.

82
0:03:44.916,000 --> 0:03:45,000
They said, "No, you're wrong."

83
0:03:46.372,000 --> 0:03:48,000
I said, "No, you're wrong," and I didn't get in.

84
0:03:48.642,000 --> 0:03:49,000
(Laughter)

85
0:03:49.744,000 --> 0:03:51,000
I was a little disappointed -- pretty young --

86
0:03:51.923,000 --> 0:03:52,000
but I went back again a few years later,

87
0:03:53.883,000 --> 0:03:55,000
this time in California, and I went to Berkeley.

88
0:03:56.266,000 --> 0:03:58,000
And I said, I'll go in from the biological side.

89
0:03:58.72,000 --> 0:04:01,000
So I got in the PhD program in biophysics.

90
0:04:01.833,000 --> 0:04:04,000
I was like, I'm studying brains now. Well, I want to study theory.

91
0:04:05.267,000 --> 0:04:07,000
They said, "You can't study theory about brains.

92
0:04:07.56,000 --> 0:04:08,000
You can't get funded for that.

93
0:04:09.579,000 --> 0:04:11,000
And as a graduate student, you can't do that."

94
0:04:11.758,000 --> 0:04:12,000
So I said, oh my gosh.

95
0:04:13,000 --> 0:04:16,000
I was depressed; I said, but I can make a difference in this field.

96
0:04:16.179,000 --> 0:04:18,000
I went back in the computer industry

97
0:04:18.211,000 --> 0:04:2,000
and said, I'll have to work here for a while.

98
0:04:20.34,000 --> 0:04:22,000
That's when I designed all those computer products.

99
0:04:22.757,000 --> 0:04:23,000
(Laughter)

100
0:04:24.082,000 --> 0:04:26,000
I said, I want to do this for four years, make some money,

101
0:04:27,000 --> 0:04:3,000
I was having a family, and I would mature a bit,

102
0:04:31,000 --> 0:04:33,000
and maybe the business of neuroscience would mature a bit.

103
0:04:33.84,000 --> 0:04:36,000
Well, it took longer than four years. It's been about 16 years.

104
0:04:36.865,000 --> 0:04:38,000
But I'm doing it now, and I'm going to tell you about it.

105
0:04:39.605,000 --> 0:04:41,000
So why should we have a good brain theory?

106
0:04:41.915,000 --> 0:04:44,000
Well, there's lots of reasons people do science.

107
0:04:45.041,000 --> 0:04:47,000
The most basic one is, people like to know things.

108
0:04:47.982,000 --> 0:04:49,000
We're curious, and we go out and get knowledge.

109
0:04:50.201,000 --> 0:04:51,000
Why do we study ants? It's interesting.

110
0:04:52.091,000 --> 0:04:55,000
Maybe we'll learn something useful, but it's interesting and fascinating.

111
0:04:55.581,000 --> 0:04:57,000
But sometimes a science has other attributes

112
0:04:57.662,000 --> 0:04:58,000
which makes it really interesting.

113
0:04:59.515,000 --> 0:05:01,000
Sometimes a science will tell something about ourselves;

114
0:05:02.166,000 --> 0:05:03,000
it'll tell us who we are.

115
0:05:03.414,000 --> 0:05:05,000
Evolution did this and Copernicus did this,

116
0:05:06.19,000 --> 0:05:08,000
where we have a new understanding of who we are.

117
0:05:08.548,000 --> 0:05:11,000
And after all, we are our brains. My brain is talking to your brain.

118
0:05:12,000 --> 0:05:14,000
Our bodies are hanging along for the ride,

119
0:05:14.054,000 --> 0:05:15,000
but my brain is talking to your brain.

120
0:05:15.903,000 --> 0:05:18,000
And if we want to understand who we are and how we feel and perceive,

121
0:05:19.175,000 --> 0:05:2,000
we need to understand brains.

122
0:05:20.59,000 --> 0:05:23,000
Another thing is sometimes science leads to big societal benefits, technologies,

123
0:05:24.398,000 --> 0:05:25,000
or businesses or whatever.

124
0:05:25.713,000 --> 0:05:27,000
This is one, too, because when we understand how brains work,

125
0:05:28.615,000 --> 0:05:3,000
we'll be able to build intelligent machines.

126
0:05:30.703,000 --> 0:05:31,000
That's a good thing on the whole,

127
0:05:32.425,000 --> 0:05:33,000
with tremendous benefits to society,

128
0:05:34.307,000 --> 0:05:35,000
just like a fundamental technology.

129
0:05:36,000 --> 0:05:38,000
So why don't we have a good theory of brains?

130
0:05:38.874,000 --> 0:05:4,000
People have been working on it for 100 years.

131
0:05:41.066,000 --> 0:05:43,000
Let's first take a look at what normal science looks like.

132
0:05:43.809,000 --> 0:05:44,000
This is normal science.

133
0:05:45.02,000 --> 0:05:49,000
Normal science is a nice balance between theory and experimentalists.

134
0:05:49.118,000 --> 0:05:51,000
The theorist guy says, "I think this is what's going on,"

135
0:05:51.833,000 --> 0:05:52,000
the experimentalist says, "You're wrong."

136
0:05:53.818,000 --> 0:05:56,000
It goes back and forth, this works in physics, this in geology.

137
0:05:56.846,000 --> 0:05:59,000
But if this is normal science, what does neuroscience look like?

138
0:05:59.879,000 --> 0:06:,000
This is what neuroscience looks like.

139
0:06:01.698,000 --> 0:06:02,000
We have this mountain of data,

140
0:06:03.164,000 --> 0:06:05,000
which is anatomy, physiology and behavior.

141
0:06:05.258,000 --> 0:06:08,000
You can't imagine how much detail we know about brains.

142
0:06:08.476,000 --> 0:06:11,000
There were 28,000 people who went to the neuroscience conference this year,

143
0:06:12.092,000 --> 0:06:14,000
and every one of them is doing research in brains.

144
0:06:14.479,000 --> 0:06:15,000
A lot of data, but no theory.

145
0:06:16.197,000 --> 0:06:18,000
There's a little wimpy box on top there.

146
0:06:18.221,000 --> 0:06:21,000
And theory has not played a role in any sort of grand way

147
0:06:21.627,000 --> 0:06:22,000
in the neurosciences.

148
0:06:23.08,000 --> 0:06:24,000
And it's a real shame.

149
0:06:24.344,000 --> 0:06:25,000
Now, why has this come about?

150
0:06:25.759,000 --> 0:06:27,000
If you ask neuroscientists why is this the state of affairs,

151
0:06:28.771,000 --> 0:06:29,000
first, they'll admit it.

152
0:06:30.041,000 --> 0:06:31,000
But if you ask them, they say,

153
0:06:31.55,000 --> 0:06:33,000
there's various reasons we don't have a good brain theory.

154
0:06:34.306,000 --> 0:06:35,000
Some say we still don't have enough data,

155
0:06:36.299,000 --> 0:06:39,000
we need more information, there's all these things we don't know.

156
0:06:39.382,000 --> 0:06:41,000
Well, I just told you there's data coming out of your ears.

157
0:06:42.247,000 --> 0:06:45,000
We have so much information, we don't even know how to organize it.

158
0:06:45.435,000 --> 0:06:46,000
What good is more going to do?

159
0:06:46.897,000 --> 0:06:49,000
Maybe we'll be lucky and discover some magic thing, but I don't think so.

160
0:06:50.369,000 --> 0:06:52,000
This is a symptom of the fact that we just don't have a theory.

161
0:06:53.366,000 --> 0:06:55,000
We don't need more data, we need a good theory.

162
0:06:56,000 --> 0:06:57,000
Another one is sometimes people say,

163
0:06:57.822,000 --> 0:07:,000
"Brains are so complex, it'll take another 50 years."

164
0:07:01,000 --> 0:07:04,000
I even think Chris said something like this yesterday, something like,

165
0:07:04.378,000 --> 0:07:06,000
it's one of the most complicated things in the universe.

166
0:07:07.029,000 --> 0:07:09,000
That's not true -- you're more complicated than your brain.

167
0:07:09.843,000 --> 0:07:1,000
You've got a brain.

168
0:07:11.018,000 --> 0:07:13,000
And although the brain looks very complicated,

169
0:07:13.192,000 --> 0:07:15,000
things look complicated until you understand them.

170
0:07:15.552,000 --> 0:07:16,000
That's always been the case.

171
0:07:16.911,000 --> 0:07:19,000
So we can say, my neocortex, the part of the brain I'm interested in,

172
0:07:20.178,000 --> 0:07:21,000
has 30 billion cells.

173
0:07:21.354,000 --> 0:07:23,000
But, you know what? It's very, very regular.

174
0:07:23.81,000 --> 0:07:26,000
In fact, it looks like it's the same thing repeated over and over again.

175
0:07:27.228,000 --> 0:07:29,000
It's not as complex as it looks. That's not the issue.

176
0:07:29.788,000 --> 0:07:31,000
Some people say, brains can't understand brains.

177
0:07:32.099,000 --> 0:07:33,000
Very Zen-like. Woo.

178
0:07:34.111,000 --> 0:07:36,000
(Laughter)

179
0:07:36.323,000 --> 0:07:38,000
You know, it sounds good, but why? I mean, what's the point?

180
0:07:39.206,000 --> 0:07:41,000
It's just a bunch of cells. You understand your liver.

181
0:07:41.799,000 --> 0:07:42,000
It's got a lot of cells in it too, right?

182
0:07:43.8,000 --> 0:07:45,000
So, you know, I don't think there's anything to that.

183
0:07:46.318,000 --> 0:07:48,000
And finally, some people say,

184
0:07:48.454,000 --> 0:07:5,000
"I don't feel like a bunch of cells -- I'm conscious.

185
0:07:51.461,000 --> 0:07:53,000
I've got this experience, I'm in the world.

186
0:07:53.554,000 --> 0:07:54,000
I can't be just a bunch of cells."

187
0:07:55.488,000 --> 0:07:58,000
Well, people used to believe there was a life force to be living,

188
0:07:58.735,000 --> 0:08:,000
and we now know that's really not true at all.

189
0:08:01.168,000 --> 0:08:02,000
And there's really no evidence,

190
0:08:03.09,000 --> 0:08:06,000
other than that people just disbelieve that cells can do what they do.

191
0:08:06.488,000 --> 0:08:09,000
So some people have fallen into the pit of metaphysical dualism,

192
0:08:09.553,000 --> 0:08:11,000
some really smart people, too, but we can reject all that.

193
0:08:12.307,000 --> 0:08:14,000
(Laughter)

194
0:08:15.226,000 --> 0:08:16,000
No, there's something else,

195
0:08:16.991,000 --> 0:08:17,000
something really fundamental, and it is:

196
0:08:19,000 --> 0:08:21,000
another reason why we don't have a good brain theory

197
0:08:21.475,000 --> 0:08:26,000
is because we have an intuitive, strongly held but incorrect assumption

198
0:08:27.034,000 --> 0:08:29,000
that has prevented us from seeing the answer.

199
0:08:29.17,000 --> 0:08:32,000
There's something we believe that just, it's obvious, but it's wrong.

200
0:08:32.982,000 --> 0:08:35,000
Now, there's a history of this in science and before I tell you what it is,

201
0:08:36.572,000 --> 0:08:38,000
I'll tell you about the history of it in science.

202
0:08:38.895,000 --> 0:08:39,000
Look at other scientific revolutions --

203
0:08:40.829,000 --> 0:08:41,000
the solar system, that's Copernicus,

204
0:08:42.732,000 --> 0:08:44,000
Darwin's evolution, and tectonic plates, that's Wegener.

205
0:08:46.059,000 --> 0:08:48,000
They all have a lot in common with brain science.

206
0:08:48.378,000 --> 0:08:5,000
First, they had a lot of unexplained data. A lot of it.

207
0:08:51.068,000 --> 0:08:53,000
But it got more manageable once they had a theory.

208
0:08:53.886,000 --> 0:08:55,000
The best minds were stumped -- really smart people.

209
0:08:56.717,000 --> 0:08:58,000
We're not smarter now than they were then;

210
0:08:58.745,000 --> 0:09:,000
it just turns out it's really hard to think of things,

211
0:09:01.296,000 --> 0:09:03,000
but once you've thought of them, it's easy to understand.

212
0:09:03.996,000 --> 0:09:05,000
My daughters understood these three theories,

213
0:09:06.126,000 --> 0:09:08,000
in their basic framework, in kindergarten.

214
0:09:08.668,000 --> 0:09:11,000
It's not that hard -- here's the apple, here's the orange,

215
0:09:11.958,000 --> 0:09:13,000
the Earth goes around, that kind of stuff.

216
0:09:14,000 --> 0:09:16,000
Another thing is the answer was there all along,

217
0:09:16.61,000 --> 0:09:18,000
but we kind of ignored it because of this obvious thing.

218
0:09:19.413,000 --> 0:09:21,000
It was an intuitive, strongly held belief that was wrong.

219
0:09:22.287,000 --> 0:09:23,000
In the case of the solar system,

220
0:09:24.001,000 --> 0:09:25,000
the idea that the Earth is spinning,

221
0:09:25.785,000 --> 0:09:27,000
the surface is going a thousand miles an hour,

222
0:09:28,000 --> 0:09:31,000
and it's going through the solar system at a million miles an hour --

223
0:09:31.273,000 --> 0:09:33,000
this is lunacy; we all know the Earth isn't moving.

224
0:09:33.773,000 --> 0:09:35,000
Do you feel like you're moving a thousand miles an hour?

225
0:09:36.674,000 --> 0:09:38,000
If you said Earth was spinning around in space and was huge --

226
0:09:39.617,000 --> 0:09:41,000
they would lock you up, that's what they did back then.

227
0:09:42.232,000 --> 0:09:45,000
So it was intuitive and obvious. Now, what about evolution?

228
0:09:45.531,000 --> 0:09:46,000
Evolution, same thing.

229
0:09:46.709,000 --> 0:09:49,000
We taught our kids the Bible says God created all these species,

230
0:09:49.813,000 --> 0:09:52,000
cats are cats; dogs are dogs; people are people; plants are plants;

231
0:09:52.98,000 --> 0:09:53,000
they don't change.

232
0:09:54.245,000 --> 0:09:56,000
Noah put them on the ark in that order, blah, blah.

233
0:09:56.918,000 --> 0:09:59,000
The fact is, if you believe in evolution, we all have a common ancestor.

234
0:10:00.337,000 --> 0:10:03,000
We all have a common ancestor with the plant in the lobby!

235
0:10:03.643,000 --> 0:10:06,000
This is what evolution tells us. And it's true. It's kind of unbelievable.

236
0:10:07.353,000 --> 0:10:09,000
And the same thing about tectonic plates.

237
0:10:09.934,000 --> 0:10:1,000
All the mountains and the continents

238
0:10:11.68,000 --> 0:10:13,000
are kind of floating around on top of the Earth.

239
0:10:14.048,000 --> 0:10:15,000
It doesn't make any sense.

240
0:10:15.318,000 --> 0:10:19,000
So what is the intuitive, but incorrect assumption,

241
0:10:19.943,000 --> 0:10:2,000
that's kept us from understanding brains?

242
0:10:21.934,000 --> 0:10:24,000
I'll tell you. It'll seem obvious that it's correct. That's the point.

243
0:10:25.251,000 --> 0:10:28,000
Then I'll make an argument why you're incorrect on the other assumption.

244
0:10:28.709,000 --> 0:10:29,000
The intuitive but obvious thing is:

245
0:10:30.415,000 --> 0:10:32,000
somehow, intelligence is defined by behavior;

246
0:10:32.753,000 --> 0:10:34,000
we're intelligent because of how we do things

247
0:10:35.127,000 --> 0:10:36,000
and how we behave intelligently.

248
0:10:36.723,000 --> 0:10:37,000
And I'm going to tell you that's wrong.

249
0:10:38.626,000 --> 0:10:4,000
Intelligence is defined by prediction.

250
0:10:40.781,000 --> 0:10:42,000
I'm going to work you through this in a few slides,

251
0:10:43.22,000 --> 0:10:45,000
and give you an example of what this means.

252
0:10:45.338,000 --> 0:10:46,000
Here's a system.

253
0:10:46.663,000 --> 0:10:48,000
Engineers and scientists like to look at systems like this.

254
0:10:49.595,000 --> 0:10:52,000
They say, we have a thing in a box. We have its inputs and outputs.

255
0:10:52.782,000 --> 0:10:55,000
The AI people said, the thing in the box is a programmable computer,

256
0:10:56.046,000 --> 0:10:57,000
because it's equivalent to a brain.

257
0:10:57.749,000 --> 0:11:,000
We'll feed it some inputs and get it to do something, have some behavior.

258
0:11:01.279,000 --> 0:11:03,000
Alan Turing defined the Turing test, which essentially says,

259
0:11:04.125,000 --> 0:11:07,000
we'll know if something's intelligent if it behaves identical to a human --

260
0:11:07.702,000 --> 0:11:09,000
a behavioral metric of what intelligence is

261
0:11:09.832,000 --> 0:11:11,000
that has stuck in our minds for a long time.

262
0:11:12,000 --> 0:11:14,000
Reality, though -- I call it real intelligence.

263
0:11:14.416,000 --> 0:11:16,000
Real intelligence is built on something else.

264
0:11:16.615,000 --> 0:11:19,000
We experience the world through a sequence of patterns,

265
0:11:19.853,000 --> 0:11:21,000
and we store them, and we recall them.

266
0:11:22.026,000 --> 0:11:24,000
When we recall them, we match them up against reality,

267
0:11:24.595,000 --> 0:11:26,000
and we're making predictions all the time.

268
0:11:26.87,000 --> 0:11:28,000
It's an internal metric; there's an internal metric about us,

269
0:11:29.852,000 --> 0:11:32,000
saying, do we understand the world, am I making predictions, and so on.

270
0:11:33.218,000 --> 0:11:36,000
You're all being intelligent now, but you're not doing anything.

271
0:11:36.244,000 --> 0:11:39,000
Maybe you're scratching yourself, but you're not doing anything.

272
0:11:39.27,000 --> 0:11:42,000
But you're being intelligent; you're understanding what I'm saying.

273
0:11:42.45,000 --> 0:11:44,000
Because you're intelligent and you speak English,

274
0:11:44.769,000 --> 0:11:45,000
you know the word at the end of this

275
0:11:46.544,000 --> 0:11:47,000
sentence.

276
0:11:47.727,000 --> 0:11:5,000
The word came to you; you make these predictions all the time.

277
0:11:50.903,000 --> 0:11:51,000
What I'm saying is,

278
0:11:52.626,000 --> 0:11:54,000
the internal prediction is the output in the neocortex,

279
0:11:55.281,000 --> 0:11:57,000
and somehow, prediction leads to intelligent behavior.

280
0:11:57.846,000 --> 0:11:58,000
Here's how that happens:

281
0:11:59.021,000 --> 0:12:,000
Let's start with a non-intelligent brain.

282
0:12:01,000 --> 0:12:04,000
I'll argue a non-intelligent brain, we'll call it an old brain.

283
0:12:04.033,000 --> 0:12:06,000
And we'll say it's a non-mammal, like a reptile,

284
0:12:06.928,000 --> 0:12:07,000
say, an alligator; we have an alligator.

285
0:12:08.937,000 --> 0:12:11,000
And the alligator has some very sophisticated senses.

286
0:12:12.332,000 --> 0:12:15,000
It's got good eyes and ears and touch senses and so on,

287
0:12:15.562,000 --> 0:12:16,000
a mouth and a nose.

288
0:12:17.055,000 --> 0:12:18,000
It has very complex behavior.

289
0:12:19.07,000 --> 0:12:22,000
It can run and hide. It has fears and emotions. It can eat you.

290
0:12:23,000 --> 0:12:26,000
It can attack. It can do all kinds of stuff.

291
0:12:27.193,000 --> 0:12:29,000
But we don't consider the alligator very intelligent,

292
0:12:30.073,000 --> 0:12:31,000
not in a human sort of way.

293
0:12:31.773,000 --> 0:12:33,000
But it has all this complex behavior already.

294
0:12:34.51,000 --> 0:12:35,000
Now in evolution, what happened?

295
0:12:36.335,000 --> 0:12:38,000
First thing that happened in evolution with mammals

296
0:12:38.744,000 --> 0:12:4,000
is we started to develop a thing called the neocortex.

297
0:12:41.299,000 --> 0:12:44,000
I'm going to represent the neocortex by this box on top of the old brain.

298
0:12:45.116,000 --> 0:12:48,000
Neocortex means "new layer." It's a new layer on top of your brain.

299
0:12:48.493,000 --> 0:12:5,000
It's the wrinkly thing on the top of your head

300
0:12:50.86,000 --> 0:12:53,000
that got wrinkly because it got shoved in there and doesn't fit.

301
0:12:53.968,000 --> 0:12:54,000
(Laughter)

302
0:12:55,000 --> 0:12:57,000
Literally, it's about the size of a table napkin

303
0:12:57.266,000 --> 0:12:58,000
and doesn't fit, so it's wrinkly.

304
0:12:58.864,000 --> 0:12:59,000
Now, look at how I've drawn this.

305
0:13:00.633,000 --> 0:13:01,000
The old brain is still there.

306
0:13:02.043,000 --> 0:13:05,000
You still have that alligator brain. You do. It's your emotional brain.

307
0:13:05.722,000 --> 0:13:07,000
It's all those gut reactions you have.

308
0:13:08.476,000 --> 0:13:11,000
On top of it, we have this memory system called the neocortex.

309
0:13:11.77,000 --> 0:13:15,000
And the memory system is sitting over the sensory part of the brain.

310
0:13:16.088,000 --> 0:13:19,000
So as the sensory input comes in and feeds from the old brain,

311
0:13:19.167,000 --> 0:13:21,000
it also goes up into the neocortex.

312
0:13:21.345,000 --> 0:13:22,000
And the neocortex is just memorizing.

313
0:13:23.282,000 --> 0:13:26,000
It's sitting there saying, I'm going to memorize all the things going on:

314
0:13:26.867,000 --> 0:13:29,000
where I've been, people I've seen, things I've heard, and so on.

315
0:13:29.91,000 --> 0:13:32,000
And in the future, when it sees something similar to that again,

316
0:13:33.296,000 --> 0:13:35,000
in a similar environment, or the exact same environment,

317
0:13:35.955,000 --> 0:13:38,000
it'll start playing it back: "Oh, I've been here before,"

318
0:13:39.534,000 --> 0:13:41,000
and when you were here before, this happened next.

319
0:13:41.922,000 --> 0:13:42,000
It allows you to predict the future.

320
0:13:43.672,000 --> 0:13:46,000
It literally feeds back the signals into your brain;

321
0:13:47.092,000 --> 0:13:49,000
they'll let you see what's going to happen next,

322
0:13:49.381,000 --> 0:13:51,000
will let you hear the word "sentence" before I said it.

323
0:13:52,000 --> 0:13:55,000
And it's this feeding back into the old brain

324
0:13:55.209,000 --> 0:13:57,000
that will allow you to make more intelligent decisions.

325
0:13:57.81,000 --> 0:14:,000
This is the most important slide of my talk, so I'll dwell on it a little.

326
0:14:01.323,000 --> 0:14:04,000
And all the time you say, "Oh, I can predict things,"

327
0:14:04.922,000 --> 0:14:07,000
so if you're a rat and you go through a maze, and you learn the maze,

328
0:14:08.306,000 --> 0:14:1,000
next time you're in one, you have the same behavior.

329
0:14:10.769,000 --> 0:14:12,000
But suddenly, you're smarter; you say, "I recognize this maze,

330
0:14:13.784,000 --> 0:14:16,000
I know which way to go; I've been here before; I can envision the future."

331
0:14:17.35,000 --> 0:14:18,000
That's what it's doing.

332
0:14:18.542,000 --> 0:14:2,000
This is true for all mammals --

333
0:14:21.406,000 --> 0:14:23,000
in humans, it got a lot worse.

334
0:14:23.461,000 --> 0:14:25,000
Humans actually developed the front of the neocortex,

335
0:14:26.072,000 --> 0:14:28,000
called the anterior part of the neocortex.

336
0:14:28.317,000 --> 0:14:29,000
And nature did a little trick.

337
0:14:29.779,000 --> 0:14:31,000
It copied the posterior, the back part, which is sensory,

338
0:14:32.49,000 --> 0:14:33,000
and put it in the front.

339
0:14:33.665,000 --> 0:14:35,000
Humans uniquely have the same mechanism on the front,

340
0:14:36.169,000 --> 0:14:37,000
but we use it for motor control.

341
0:14:37.747,000 --> 0:14:4,000
So we're now able to do very sophisticated motor planning, things like that.

342
0:14:41.352,000 --> 0:14:44,000
I don't have time to explain, but to understand how a brain works,

343
0:14:44.502,000 --> 0:14:47,000
you have to understand how the first part of the mammalian neocortex works,

344
0:14:48.063,000 --> 0:14:5,000
how it is we store patterns and make predictions.

345
0:14:50.38,000 --> 0:14:52,000
Let me give you a few examples of predictions.

346
0:14:52.592,000 --> 0:14:53,000
I already said the word "sentence."

347
0:14:54.292,000 --> 0:14:57,000
In music, if you've heard a song before,

348
0:14:57.522,000 --> 0:14:59,000
when you hear it, the next note pops into your head already --

349
0:15:00.455,000 --> 0:15:01,000
you anticipate it.

350
0:15:01.63,000 --> 0:15:04,000
With an album, at the end of a song, the next song pops into your head.

351
0:15:05.008,000 --> 0:15:07,000
It happens all the time, you make predictions.

352
0:15:07.337,000 --> 0:15:1,000
I have this thing called the "altered door" thought experiment.

353
0:15:10.4,000 --> 0:15:12,000
It says, you have a door at home;

354
0:15:13.253,000 --> 0:15:14,000
when you're here, I'm changing it --

355
0:15:15.032,000 --> 0:15:18,000
I've got a guy back at your house right now, moving the door around,

356
0:15:18.252,000 --> 0:15:19,000
moving your doorknob over two inches.

357
0:15:20.045,000 --> 0:15:23,000
When you go home tonight, you'll put your hand out, reach for the doorknob,

358
0:15:23.653,000 --> 0:15:24,000
notice it's in the wrong spot

359
0:15:25.191,000 --> 0:15:26,000
and go, "Whoa, something happened."

360
0:15:26.902,000 --> 0:15:28,000
It may take a second, but something happened.

361
0:15:29.027,000 --> 0:15:31,000
I can change your doorknob in other ways --

362
0:15:31.054,000 --> 0:15:34,000
make it larger, smaller, change its brass to silver, make it a lever,

363
0:15:34.319,000 --> 0:15:36,000
I can change the door; put colors on, put windows in.

364
0:15:36.919,000 --> 0:15:38,000
I can change a thousand things about your door

365
0:15:39.094,000 --> 0:15:41,000
and in the two seconds you take to open it,

366
0:15:41.126,000 --> 0:15:42,000
you'll notice something has changed.

367
0:15:42.872,000 --> 0:15:44,000
Now, the engineering approach, the AI approach to this,

368
0:15:45.48,000 --> 0:15:47,000
is to build a door database with all the door attributes.

369
0:15:48.179,000 --> 0:15:5,000
And as you go up to the door, we check them off one at time:

370
0:15:51.022,000 --> 0:15:52,000
door, door, color ...

371
0:15:52.392,000 --> 0:15:54,000
We don't do that. Your brain doesn't do that.

372
0:15:54.516,000 --> 0:15:56,000
Your brain is making constant predictions all the time

373
0:15:57.08,000 --> 0:15:59,000
about what will happen in your environment.

374
0:15:59.138,000 --> 0:16:01,000
As I put my hand on this table, I expect to feel it stop.

375
0:16:01.908,000 --> 0:16:04,000
When I walk, every step, if I missed it by an eighth of an inch,

376
0:16:04.951,000 --> 0:16:05,000
I'll know something has changed.

377
0:16:06.508,000 --> 0:16:08,000
You're constantly making predictions about your environment.

378
0:16:09.352,000 --> 0:16:1,000
I'll talk about vision, briefly.

379
0:16:10.969,000 --> 0:16:11,000
This is a picture of a woman.

380
0:16:12.376,000 --> 0:16:15,000
When we look at people, our eyes saccade over two to three times a second.

381
0:16:15.89,000 --> 0:16:17,000
We're not aware of it, but our eyes are always moving.

382
0:16:18.443,000 --> 0:16:21,000
When we look at a face, we typically go from eye to eye to nose to mouth.

383
0:16:21.902,000 --> 0:16:22,000
When your eye moves from eye to eye,

384
0:16:23.795,000 --> 0:16:25,000
if there was something else there like a nose,

385
0:16:25.977,000 --> 0:16:28,000
you'd see a nose where an eye is supposed to be and go, "Oh, shit!"

386
0:16:29.547,000 --> 0:16:3,000
(Laughter)

387
0:16:30.967,000 --> 0:16:32,000
"There's something wrong about this person."

388
0:16:33.1,000 --> 0:16:35,000
That's because you're making a prediction.

389
0:16:35.129,000 --> 0:16:38,000
It's not like you just look over and say, "What am I seeing? A nose? OK."

390
0:16:38.592,000 --> 0:16:4,000
No, you have an expectation of what you're going to see.

391
0:16:41.25,000 --> 0:16:42,000
Every single moment.

392
0:16:42.425,000 --> 0:16:44,000
And finally, let's think about how we test intelligence.

393
0:16:45.078,000 --> 0:16:48,000
We test it by prediction: What is the next word in this ...?

394
0:16:48.183,000 --> 0:16:51,000
This is to this as this is to this. What is the next number in this sentence?

395
0:16:51.834,000 --> 0:16:53,000
Here's three visions of an object. What's the fourth one?

396
0:16:54.548,000 --> 0:16:56,000
That's how we test it. It's all about prediction.

397
0:16:57.573,000 --> 0:16:59,000
So what is the recipe for brain theory?

398
0:17:00.219,000 --> 0:17:02,000
First of all, we have to have the right framework.

399
0:17:02.609,000 --> 0:17:03,000
And the framework is a memory framework,

400
0:17:04.546,000 --> 0:17:06,000
not a computational or behavior framework,

401
0:17:06.594,000 --> 0:17:07,000
it's a memory framework.

402
0:17:07.781,000 --> 0:17:09,000
How do you store and recall these sequences of patterns?

403
0:17:10.428,000 --> 0:17:11,000
It's spatiotemporal patterns.

404
0:17:11.894,000 --> 0:17:14,000
Then, if in that framework, you take a bunch of theoreticians --

405
0:17:14.927,000 --> 0:17:16,000
biologists generally are not good theoreticians.

406
0:17:17.197,000 --> 0:17:2,000
Not always, but generally, there's not a good history of theory in biology.

407
0:17:20.75,000 --> 0:17:22,000
I've found the best people to work with are physicists,

408
0:17:23.348,000 --> 0:17:24,000
engineers and mathematicians,

409
0:17:24.755,000 --> 0:17:25,000
who tend to think algorithmically.

410
0:17:26.475,000 --> 0:17:29,000
Then they have to learn the anatomy and the physiology.

411
0:17:29.763,000 --> 0:17:33,000
You have to make these theories very realistic in anatomical terms.

412
0:17:34.283,000 --> 0:17:36,000
Anyone who tells you their theory about how the brain works

413
0:17:37.072,000 --> 0:17:39,000
and doesn't tell you exactly how it's working

414
0:17:39.193,000 --> 0:17:4,000
and how the wiring works --

415
0:17:40.52,000 --> 0:17:41,000
it's not a theory.

416
0:17:41.811,000 --> 0:17:43,000
And that's what we do at the Redwood Neuroscience Institute.

417
0:17:44.668,000 --> 0:17:47,000
I'd love to tell you we're making fantastic progress in this thing,

418
0:17:48,000 --> 0:17:51,000
and I expect to be back on this stage sometime in the not too distant future,

419
0:17:51.686,000 --> 0:17:52,000
to tell you about it.

420
0:17:52.874,000 --> 0:17:54,000
I'm really excited; this is not going to take 50 years.

421
0:17:55.492,000 --> 0:17:56,000
What will brain theory look like?

422
0:17:57.094,000 --> 0:17:59,000
First of all, it's going to be about memory.

423
0:17:59.173,000 --> 0:18:01,000
Not like computer memory -- not at all like computer memory.

424
0:18:02.019,000 --> 0:18:03,000
It's very different.

425
0:18:03.194,000 --> 0:18:05,000
It's a memory of very high-dimensional patterns,

426
0:18:05.475,000 --> 0:18:06,000
like the things that come from your eyes.

427
0:18:07.461,000 --> 0:18:08,000
It's also memory of sequences:

428
0:18:08.922,000 --> 0:18:1,000
you cannot learn or recall anything outside of a sequence.

429
0:18:11.676,000 --> 0:18:13,000
A song must be heard in sequence over time,

430
0:18:14.537,000 --> 0:18:16,000
and you must play it back in sequence over time.

431
0:18:16.912,000 --> 0:18:18,000
And these sequences are auto-associatively recalled,

432
0:18:19.385,000 --> 0:18:21,000
so if I see something, I hear something, it reminds me of it,

433
0:18:22.282,000 --> 0:18:23,000
and it plays back automatically.

434
0:18:23.839,000 --> 0:18:24,000
It's an automatic playback.

435
0:18:25.157,000 --> 0:18:27,000
And prediction of future inputs is the desired output.

436
0:18:27.729,000 --> 0:18:29,000
And as I said, the theory must be biologically accurate,

437
0:18:30.373,000 --> 0:18:32,000
it must be testable and you must be able to build it.

438
0:18:32.881,000 --> 0:18:34,000
If you don't build it, you don't understand it.

439
0:18:35.116,000 --> 0:18:36,000
One more slide.

440
0:18:36.672,000 --> 0:18:38,000
What is this going to result in?

441
0:18:39.005,000 --> 0:18:41,000
Are we going to really build intelligent machines?

442
0:18:41.377,000 --> 0:18:44,000
Absolutely. And it's going to be different than people think.

443
0:18:45.508,000 --> 0:18:47,000
No doubt that it's going to happen, in my mind.

444
0:18:47.924,000 --> 0:18:5,000
First of all, we're going to build this stuff out of silicon.

445
0:18:51.064,000 --> 0:18:53,000
The same techniques we use to build silicon computer memories,

446
0:18:54,000 --> 0:18:55,000
we can use here.

447
0:18:55.175,000 --> 0:18:57,000
But they're very different types of memories.

448
0:18:57.308,000 --> 0:18:59,000
And we'll attach these memories to sensors,

449
0:18:59.355,000 --> 0:19:01,000
and the sensors will experience real-live, real-world data,

450
0:19:02.156,000 --> 0:19:03,000
and learn about their environment.

451
0:19:03.932,000 --> 0:19:06,000
Now, it's very unlikely the first things you'll see are like robots.

452
0:19:07.401,000 --> 0:19:09,000
Not that robots aren't useful; people can build robots.

453
0:19:1,000 --> 0:19:13,000
But the robotics part is the hardest part. That's old brain. That's really hard.

454
0:19:13.791,000 --> 0:19:15,000
The new brain is easier than the old brain.

455
0:19:15.822,000 --> 0:19:18,000
So first we'll do things that don't require a lot of robotics.

456
0:19:18.928,000 --> 0:19:2,000
So you're not going to see C-3PO.

457
0:19:21.131,000 --> 0:19:23,000
You're going to see things more like intelligent cars

458
0:19:23.64,000 --> 0:19:25,000
that really understand what traffic is, what driving is

459
0:19:26.472,000 --> 0:19:29,000
and have learned that cars with the blinkers on for half a minute

460
0:19:29.774,000 --> 0:19:3,000
probably aren't going to turn.

461
0:19:31.372,000 --> 0:19:32,000
(Laughter)

462
0:19:32.687,000 --> 0:19:34,000
We can also do intelligent security systems.

463
0:19:34.775,000 --> 0:19:37,000
Anytime we're basically using our brain but not doing a lot of mechanics --

464
0:19:38.372,000 --> 0:19:4,000
those are the things that will happen first.

465
0:19:40.455,000 --> 0:19:41,000
But ultimately, the world's the limit.

466
0:19:42.299,000 --> 0:19:43,000
I don't know how this will turn out.

467
0:19:44.055,000 --> 0:19:46,000
I know a lot of people who invented the microprocessor.

468
0:19:46.67,000 --> 0:19:48,000
And if you talk to them,

469
0:19:48.858,000 --> 0:19:5,000
they knew what they were doing was really significant,

470
0:19:51.457,000 --> 0:19:53,000
but they didn't really know what was going to happen.

471
0:19:53.981,000 --> 0:19:55,000
They couldn't anticipate cell phones and the Internet

472
0:19:56.773,000 --> 0:19:57,000
and all this kind of stuff.

473
0:19:58.532,000 --> 0:20:,000
They just knew like, "We're going to build calculators

474
0:20:01.177,000 --> 0:20:02,000
and traffic-light controllers.

475
0:20:02.641,000 --> 0:20:03,000
But it's going to be big!"

476
0:20:03.964,000 --> 0:20:05,000
In the same way, brain science and these memories

477
0:20:06.329,000 --> 0:20:08,000
are going to be a very fundamental technology,

478
0:20:08.578,000 --> 0:20:11,000
and it will lead to unbelievable changes in the next 100 years.

479
0:20:12.044,000 --> 0:20:15,000
And I'm most excited about how we're going to use them in science.

480
0:20:15.473,000 --> 0:20:17,000
So I think that's all my time -- I'm over,

481
0:20:18.334,000 --> 0:20:2,000
and I'm going to end my talk right there.

