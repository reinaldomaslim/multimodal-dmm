1
0:00:,000 --> 0:00:07,000
Traductor: Lidia Cámara de la Fuente Revisor: Paula Motter

2
0:00:12.36,000 --> 0:00:14,000
[Esta charla incluye contenido para adultos]

3
0:00:16.913,000 --> 0:00:19,000
Moritz Riesewieck: El 23 de marzo de 2013,

4
0:00:21.389,000 --> 0:00:25,000
usuarios en todo el mundo descubrieron en su servicio de noticias

5
0:00:25.47,000 --> 0:00:3,000
el video de una joven que era violada por un hombre mayor.

6
0:00:31.478,000 --> 0:00:34,000
Antes de que este video fuera retirado de Facebook,

7
0:00:35.358,000 --> 0:00:39,000
ya se había compartido 16 000 veces,

8
0:00:39.998,000 --> 0:00:42,000
e incluso había recibido 4000 "Me gusta".

9
0:00:45.268,000 --> 0:00:48,000
Este video se hizo viral e invadió la red.

10
0:00:49.873,000 --> 0:00:51,000
Hans Block: Y ese fue el momento en que nos preguntamos:

11
0:00:52.992,000 --> 0:00:54,000
¿cómo podría algo así llegar a Facebook?

12
0:00:55.794,000 --> 0:00:59,000
Y, al mismo tiempo, ¿por qué no vemos contenido semejante más a menudo?

13
0:01:00.22,000 --> 0:01:03,000
Después de todo, hay una gran cantidad de material repugnante en línea,

14
0:01:03.927,000 --> 0:01:07,000
pero ¿por qué tan pocas veces vemos basura en Facebook, Twitter o Google?

15
0:01:08.846,000 --> 0:01:1,000
MR: Aunque el software de reconocimiento de imágenes

16
0:01:11.363,000 --> 0:01:14,000
puede identificar el contorno de órganos sexuales,

17
0:01:15.53,000 --> 0:01:19,000
sangre o piel desnuda en imágenes y videos,

18
0:01:20.482,000 --> 0:01:25,000
tiene inmensas dificultades para distinguir el contenido pornográfico

19
0:01:26.046,000 --> 0:01:3,000
de fotos de vacaciones, estatuas de Adonis

20
0:01:30.418,000 --> 0:01:32,000
o campañas de prevención del cáncer de mama.

21
0:01:33.14,000 --> 0:01:36,000
No puede distinguir la escena de muerte entre Romeo y Julieta

22
0:01:37.419,000 --> 0:01:39,000
de un ataque real con arma blanca.

23
0:01:39.998,000 --> 0:01:43,000
No puede distinguir la sátira de la propaganda,

24
0:01:45.244,000 --> 0:01:48,000
ni la ironía del odio, y así sucesivamente.

25
0:01:50.077,000 --> 0:01:52,000
Por lo tanto, se necesita la intervención humana

26
0:01:52.829,000 --> 0:01:56,000
para decidir cuál de esos contenidos sospechosos debe eliminarse

27
0:01:58.204,000 --> 0:01:59,000
y cuál deben permanecer.

28
0:02:00.909,000 --> 0:02:02,000
Son personas a las que apenas conocemos,

29
0:02:03.744,000 --> 0:02:04,000
porque trabajan en secreto,

30
0:02:06.053,000 --> 0:02:07,000
firman un acuerdo de confidencialidad,

31
0:02:07.982,000 --> 0:02:09,000
que les prohíbe hablar y compartir

32
0:02:10.959,000 --> 0:02:13,000
lo que ven en sus pantallas y lo que supone este trabajo para ellos.

33
0:02:14.887,000 --> 0:02:16,000
Están obligados a usar palabras codificadas

34
0:02:17.063,000 --> 0:02:19,000
para ocultar para quién trabajan.

35
0:02:19.585,000 --> 0:02:21,000
Son supervisados por empresas de seguridad privada

36
0:02:22.514,000 --> 0:02:25,000
para asegurarse de que no hablen con periodistas.

37
0:02:26.048,000 --> 0:02:29,000
Y están amenazados con multas en caso de que hablen.

38
0:02:30.421,000 --> 0:02:33,000
Suena como una extraña historia policial,

39
0:02:34.207,000 --> 0:02:35,000
pero es la verdad.

40
0:02:35.561,000 --> 0:02:36,000
Estas personas existen,

41
0:02:37.633,000 --> 0:02:41,000
y se llaman "moderadores de contenido".

42
0:02:42.942,000 --> 0:02:45,000
HB: Somos directores del documental "The Cleaners", o los limpiadores,

43
0:02:46.411,000 --> 0:02:47,000
y nos gustaría llevarlos

44
0:02:48.395,000 --> 0:02:5,000
a un mundo que muchos quizá desconocen todavía.

45
0:02:51.006,000 --> 0:02:53,000
He aquí un breve clip de nuestro documental.

46
0:02:58.639,000 --> 0:03:01,000
(Música)

47
0:03:04.4,000 --> 0:03:07,000
(Video) Moderador: Tengo que estar en el anonimato, por un contrato firmado.

48
0:03:09.784,000 --> 0:03:13,000
No se nos permite revelar con quién estamos trabajando.

49
0:03:14.807,000 --> 0:03:15,000
La razón por la que hablo

50
0:03:16.594,000 --> 0:03:2,000
es porque el mundo debe saber que estamos aquí.

51
0:03:22.544,000 --> 0:03:24,000
Hay alguien que controla las redes sociales.

52
0:03:26.317,000 --> 0:03:29,000
Hacemos todo lo posible para que esta plataforma

53
0:03:29.945,000 --> 0:03:3,000
sea segura para todos.

54
0:03:42.438,000 --> 0:03:42,000
Eliminar.

55
0:03:44.278,000 --> 0:03:44,000
Ignorar.

56
0:03:45.596,000 --> 0:03:45,000
Eliminar.

57
0:03:47.279,000 --> 0:03:47,000
Ignorar.

58
0:03:48.6,000 --> 0:03:48,000
Eliminar.

59
0:03:50.68,000 --> 0:03:5,000
Ignorar.

60
0:03:51.585,000 --> 0:03:51,000
Ignorar.

61
0:03:53.625,000 --> 0:03:53,000
Eliminar.

62
0:03:58.03,000 --> 0:03:59,000
HB: Los llamados moderadores de contenido

63
0:04:00.031,000 --> 0:04:03,000
no reciben su paga directamente de Facebook,Twitter o Google,

64
0:04:03.731,000 --> 0:04:05,000
sino de empresas subcontratadas en todo el mundo

65
0:04:06.396,000 --> 0:04:08,000
para mantener bajos los salarios.

66
0:04:08.833,000 --> 0:04:09,000
Hay decenas de miles de jóvenes

67
0:04:10.824,000 --> 0:04:13,000
que ven todo lo que se supone que no deben ver los demás.

68
0:04:14.061,000 --> 0:04:17,000
Y estamos hablando de decapitaciones, mutilaciones,

69
0:04:17.633,000 --> 0:04:2,000
ejecuciones, necrofilia, tortura, abuso infantil.

70
0:04:21.743,000 --> 0:04:23,000
Miles de imágenes por cada turno laboral:

71
0:04:24.041,000 --> 0:04:26,000
ignorar, eliminar, día y noche.

72
0:04:27.393,000 --> 0:04:29,000
Y gran parte de este trabajo se hace en Manila,

73
0:04:30.815,000 --> 0:04:33,000
donde los residuos tóxicos analógicos del mundo occidental

74
0:04:33.957,000 --> 0:04:35,000
desde hace años se llevan allí en buques portacontenedores.

75
0:04:36.993,000 --> 0:04:39,000
Ahora los residuos digitales se vierten allí mediante el cable de fibra óptica.

76
0:04:40.8,000 --> 0:04:42,000
Y así como los llamados "carroñeros"

77
0:04:43.871,000 --> 0:04:46,000
hurgan en montañas gigantescas en la periferia de la ciudad,

78
0:04:47.371,000 --> 0:04:51,000
los moderadores se abren camino a golpe de clic por un océano sin fin

79
0:04:52.228,000 --> 0:04:56,000
de imágenes, videos tóxicos y todo tipo de basura intelectual,

80
0:04:56.339,000 --> 0:04:57,000
para que no tengamos que verlo nosotros.

81
0:04:58.665,000 --> 0:05:01,000
MR: Pero a diferencia de las heridas de los carroñeros,

82
0:05:02.229,000 --> 0:05:05,000
las de los moderadores de contenido permanecen invisibles.

83
0:05:06.117,000 --> 0:05:08,000
Todo ese contenido impactante y perturbador,

84
0:05:09.221,000 --> 0:05:11,000
esas fotos y videos, penetran en sus memorias,

85
0:05:12.508,000 --> 0:05:15,000
donde, en cualquier momento, pueden tener efectos impredecibles:

86
0:05:15.977,000 --> 0:05:17,000
trastornos de la alimentación, pérdida de la libido,

87
0:05:19.358,000 --> 0:05:22,000
trastornos de ansiedad, alcoholismo,

88
0:05:22.641,000 --> 0:05:24,000
depresión, que puede incluso conducir al suicidio.

89
0:05:26.1,000 --> 0:05:28,000
Las fotos y videos los infectan,

90
0:05:28.569,000 --> 0:05:3,000
y con frecuencia no se van de su memoria.

91
0:05:30.982,000 --> 0:05:34,000
Si tienen mala suerte, desarrollan trastornos de estrés postraumático,

92
0:05:35.847,000 --> 0:05:37,000
como los soldados tras las misiones de guerra.

93
0:05:39.445,000 --> 0:05:42,000
En nuestro documental, contamos la historia de un joven

94
0:05:43.112,000 --> 0:05:48,000
que tuvo que supervisar videos de automutilaciones e intentos de suicidio,

95
0:05:48.194,000 --> 0:05:49,000
una y otra vez,

96
0:05:50.033,000 --> 0:05:53,000
y que al final él mismo se suicidó.

97
0:05:53.787,000 --> 0:05:55,000
No es un caso aislado, como se nos ha dicho.

98
0:05:57.184,000 --> 0:06:,000
Este es el precio que todos pagamos

99
0:06:01.188,000 --> 0:06:06,000
por tener un ambiente teóricamente limpio, seguro y "sano"

100
0:06:06.539,000 --> 0:06:08,000
en las redes sociales.

101
0:06:10.482,000 --> 0:06:12,000
Nunca antes en la historia de la humanidad

102
0:06:13.101,000 --> 0:06:16,000
ha sido más fácil llegar a millones de personas en todo el mundo

103
0:06:16.457,000 --> 0:06:17,000
en unos pocos segundos.

104
0:06:18.148,000 --> 0:06:21,000
Lo que se publica en las redes sociales, de manera rápida,

105
0:06:22.117,000 --> 0:06:26,000
se convierte en viral y excita la mente de las personas en todo el mundo.

106
0:06:26.45,000 --> 0:06:27,000
Antes de eliminarlo,

107
0:06:28.538,000 --> 0:06:29,000
a menudo es demasiado tarde.

108
0:06:30.966,000 --> 0:06:32,000
Millones de personas ya han sido infectadas

109
0:06:33.22,000 --> 0:06:34,000
con el odio y la ira,

110
0:06:35.101,000 --> 0:06:37,000
quienes, o bien son activas en línea

111
0:06:37.855,000 --> 0:06:39,000
mediante la difusión o la amplificación del odio,

112
0:06:41.022,000 --> 0:06:44,000
o salen a la calle y toman las armas.

113
0:06:45.236,000 --> 0:06:47,000
HB: Por eso, un ejército de moderadores de contenido

114
0:06:47.87,000 --> 0:06:5,000
se sienta ante una pantalla para evitar nuevos daños colaterales.

115
0:06:52.434,000 --> 0:06:54,000
Y ellos deciden, tan pronto como sea posible,

116
0:06:54.577,000 --> 0:06:57,000
si el contenido permanece en la plataforma, es decir, lo ignoran,

117
0:06:58.766,000 --> 0:07:,000
o si desaparece, es decir, lo eliminan.

118
0:07:01.823,000 --> 0:07:03,000
Pero no todas las decisiones son tan claras

119
0:07:04.474,000 --> 0:07:06,000
como la decisión acerca de un video de abuso infantil.

120
0:07:07.211,000 --> 0:07:1,000
¿Qué pasa con el contenido controvertido, el contenido ambivalente,

121
0:07:10.416,000 --> 0:07:13,000
subido por los defensores de derechos civiles o periodistas ciudadanos?

122
0:07:14.048,000 --> 0:07:17,000
Los moderadores de contenido a menudo deciden sobre estos casos

123
0:07:17.294,000 --> 0:07:19,000
a la misma velocidad que en los casos claros.

124
0:07:21.515,000 --> 0:07:23,000
MR: Les mostraremos un video ahora,

125
0:07:24.198,000 --> 0:07:26,000
y nos gustaría pedirles que decidieran:

126
0:07:27.531,000 --> 0:07:28,000
¿Lo eliminarían,

127
0:07:29.245,000 --> 0:07:3,000
o no lo eliminarían?

128
0:07:31.07,000 --> 0:07:32,000
(Video) (Sonido de ataque aéreo)

129
0:07:33.1,000 --> 0:07:35,000
(Explosión)

130
0:07:40.076,000 --> 0:07:43,000
(Personas que hablan en árabe)

131
0:07:46.053,000 --> 0:07:48,000
MR: Hicimos que algunas partes quedaran borrosas adrede.

132
0:07:49.196,000 --> 0:07:52,000
Un niño se vería perturbado peligrosamente

133
0:07:52.975,000 --> 0:07:54,000
y muy asustado con dicho contenido.

134
0:07:55.808,000 --> 0:07:57,000
Por lo tanto, ¿mejor eliminarlo?

135
0:07:59.61,000 --> 0:08:03,000
Pero ¿y si el video ayudara a investigar los crímenes de guerra en Siria?

136
0:08:04.717,000 --> 0:08:06,000
¿Qué pasa si nadie se hubiera enterado de este ataque aéreo,

137
0:08:07.908,000 --> 0:08:1,000
porque Facebook, YouTube y Twitter decidieron eliminarlo?

138
0:08:12.895,000 --> 0:08:15,000
Airwars, una organización no gubernamental con sede en Londres,

139
0:08:17.244,000 --> 0:08:19,000
trata de encontrar esos videos lo antes posible

140
0:08:20.165,000 --> 0:08:22,000
cada vez que se suben a las redes sociales,

141
0:08:22.749,000 --> 0:08:23,000
con el fin de archivarlos.

142
0:08:24.693,000 --> 0:08:26,000
Porque saben que, tarde o temprano,

143
0:08:27.55,000 --> 0:08:3,000
Facebook, YouTube o Twitter eliminarán dicho contenido de sus plataformas.

144
0:08:31.345,000 --> 0:08:33,000
Las personas armadas con sus teléfonos móviles

145
0:08:33.577,000 --> 0:08:36,000
pueden hacer visible contenido al que los periodistas no suelen tener acceso.

146
0:08:37.8,000 --> 0:08:4,000
Los grupos de derechos civiles no tienen ninguna opción mejor

147
0:08:40.887,000 --> 0:08:43,000
para hacer rápidamente accesibles sus grabaciones a una gran audiencia

148
0:08:44.712,000 --> 0:08:46,000
que subirlas a las redes sociales.

149
0:08:47.95,000 --> 0:08:51,000
¿No era ese el potencial de empoderamiento que la web debe tener?

150
0:08:52.966,000 --> 0:08:53,000
¿No era este, en sus etapas iniciales,

151
0:08:54.95,000 --> 0:08:58,000
el sueño de las personas en torno a la web?

152
0:08:59.608,000 --> 0:09:01,000
¿Pueden imágenes y videos como estos

153
0:09:02.427,000 --> 0:09:06,000
convencer a las personas que se han vuelto insensibles a los hechos

154
0:09:07.585,000 --> 0:09:08,000
a reflexionar?

155
0:09:09.917,000 --> 0:09:12,000
HB: Pero en vez de eso, todo lo que podría ser molesto se elimina.

156
0:09:13.543,000 --> 0:09:15,000
Y hay un cambio general en la sociedad.

157
0:09:15.625,000 --> 0:09:18,000
Los medios de comunicación, por ejemplo, ponen más menudo notas de advertencia

158
0:09:19.626,000 --> 0:09:2,000
en la parte superior de los artículos,

159
0:09:21.443,000 --> 0:09:24,000
que algunas personas pueden percibir como ofensivo o preocupante.

160
0:09:24.876,000 --> 0:09:27,000
O cada vez más estudiantes en las universidades de EE. UU.

161
0:09:28.554,000 --> 0:09:31,000
exigen que se eliminen del plan de estudios los clásicos antiguos

162
0:09:31.715,000 --> 0:09:33,000
donde se represente la violencia sexual o el abuso.

163
0:09:34.991,000 --> 0:09:36,000
Pero ¿hasta dónde deberíamos ir?

164
0:09:37.875,000 --> 0:09:4,000
La integridad física está garantizada como un derecho humano

165
0:09:41.279,000 --> 0:09:42,000
en las constituciones de todo el mundo.

166
0:09:43.422,000 --> 0:09:46,000
En la Carta de los Derechos Fundamentales de la Unión Europea,

167
0:09:47.2,000 --> 0:09:5,000
este derecho se aplica expresamente a la integridad psíquica.

168
0:09:51.347,000 --> 0:09:53,000
Pero incluso si el efecto potencialmente traumático

169
0:09:54.029,000 --> 0:09:56,000
de imágenes y videos es difícil de predecir,

170
0:09:56.879,000 --> 0:09:57,000
¿queremos llegar a ser tan cautelosos

171
0:09:58.86,000 --> 0:10:01,000
de correr el riesgo de perder la conciencia social de la injusticia?

172
0:10:03.203,000 --> 0:10:04,000
Entonces ¿qué hay que hacer?

173
0:10:04.942,000 --> 0:10:06,000
Mark Zuckerberg declaró recientemente que en el futuro,

174
0:10:07.958,000 --> 0:10:09,000
los usuarios, o casi todo el mundo,

175
0:10:11.784,000 --> 0:10:13,000
decidirá de forma individual

176
0:10:14.069,000 --> 0:10:16,000
lo que les gusta ver en la plataforma,

177
0:10:16.141,000 --> 0:10:17,000
mediante ajustes de filtros personales.

178
0:10:18.196,000 --> 0:10:21,000
De este modo, todos podrían fácilmente permanecer imperturbables

179
0:10:21.49,000 --> 0:10:24,000
frente a imágenes de guerra u otros conflictos violentos, como...

180
0:10:25.849,000 --> 0:10:29,000
MR: Yo soy el tipo de persona a quien no le importa ver pechos

181
0:10:30.319,000 --> 0:10:33,000
y estoy muy interesado en el calentamiento global,

182
0:10:34.109,000 --> 0:10:36,000
pero no me gusta la guerra.

183
0:10:36.673,000 --> 0:10:38,000
HB: Sí, yo soy más bien lo contrario.

184
0:10:38.855,000 --> 0:10:42,000
Tengo cero interés en senos o cuerpos desnudos.

185
0:10:43.209,000 --> 0:10:45,000
Pero ¿por qué no armas de fuego? Me gustan las armas, sí.

186
0:10:46.901,000 --> 0:10:49,000
MR: Como ven, si no compartimos una conciencia social similar,

187
0:10:50.67,000 --> 0:10:52,000
¿cómo vamos a debatir sobre los problemas sociales?

188
0:10:53.363,000 --> 0:10:55,000
¿Cómo vamos a llamar a la gente a la acción?

189
0:10:55.784,000 --> 0:10:58,000
Surgirían incluso más burbujas solitarias.

190
0:10:59.665,000 --> 0:11:02,000
Una de las cuestiones centrales es: ¿Cómo, en el futuro,

191
0:11:02.92,000 --> 0:11:06,000
la libertad de expresión irá contra la necesidad de protección del pueblo?

192
0:11:08.441,000 --> 0:11:09,000
Es una cuestión de principios.

193
0:11:10.602,000 --> 0:11:14,000
¿Queremos diseñar una sociedad abierta o cerrada

194
0:11:14.874,000 --> 0:11:15,000
en el espacio digital?

195
0:11:17.054,000 --> 0:11:22,000
En el meollo de la cuestión está en "la libertad frente a la seguridad".

196
0:11:24.412,000 --> 0:11:28,000
Facebook siempre ha querido ser una plataforma "sana".

197
0:11:28.886,000 --> 0:11:31,000
Por encima de todo, los usuarios deben sentirse seguros y protegidos.

198
0:11:32.618,000 --> 0:11:33,000
Es la misma elección de las palabras

199
0:11:34.762,000 --> 0:11:36,000
de los moderadores de contenido encuestados en las Filipinas

200
0:11:37.744,000 --> 0:11:38,000
en muchas de nuestras entrevistas.

201
0:11:40.188,000 --> 0:11:42,000
(Video) El mundo en el que vivimos ahora,

202
0:11:42.593,000 --> 0:11:44,000
creo que, en realidad, no es sano.

203
0:11:44.783,000 --> 0:11:45,000
(Música)

204
0:11:46.355,000 --> 0:11:48,000
En este mundo existe un mal real.

205
0:11:49.537,000 --> 0:11:52,000
(Música)

206
0:11:52.799,000 --> 0:11:54,000
Tenemos que vigilarlo.

207
0:11:54.886,000 --> 0:11:55,000
(Música)

208
0:11:56.792,000 --> 0:11:59,000
Tenemos que controlar lo bueno y lo malo.

209
0:12:00.646,000 --> 0:12:03,000
(Música)

210
0:12:10.193,000 --> 0:12:13,000
[Mira hacia arriba. Dios]

211
0:12:14.952,000 --> 0:12:18,000
MR: Para los moderadores de contenido jóvenes en la muy católica Filipinas,

212
0:12:19.254,000 --> 0:12:21,000
su trabajo está vinculado a una misión cristiana

213
0:12:22.833,000 --> 0:12:24,000
para contrarrestar los pecados del mundo

214
0:12:25.823,000 --> 0:12:27,000
que se extienden a través de la web.

215
0:12:28.641,000 --> 0:12:3,000
"La limpieza se aproxima a lo divino"

216
0:12:32.077,000 --> 0:12:35,000
es un dicho que en Filipinas todo el mundo sabe.

217
0:12:36.035,000 --> 0:12:37,000
HB: Y otros se motivan

218
0:12:37.718,000 --> 0:12:4,000
comparándose con su presidente Rodrigo Duterte.

219
0:12:41.837,000 --> 0:12:44,000
Gobierna Filipinas desde 2016,

220
0:12:45.352,000 --> 0:12:49,000
y ganó las elecciones con la promesa: "Voy a limpiar todo".

221
0:12:49.892,000 --> 0:12:52,000
Y lo que esto significa es eliminar todo tipo de problemas

222
0:12:53.233,000 --> 0:12:55,000
matando, literalmente, a la gente en las calles,

223
0:12:55.692,000 --> 0:12:57,000
personas supuestamente delincuentes, o lo que eso signifique.

224
0:12:58.641,000 --> 0:12:59,000
Y desde que fue elegido,

225
0:12:59.895,000 --> 0:13:02,000
unas 20 000 personas han muerto.

226
0:13:03.655,000 --> 0:13:05,000
Y un moderador de nuestro documental dice:

227
0:13:06.18,000 --> 0:13:08,000
"Lo que hace Duterte en las calles,

228
0:13:08.259,000 --> 0:13:09,000
yo lo hago en internet".

229
0:13:10.934,000 --> 0:13:12,000
Y aquí están, los autoproclamados superhéroes,

230
0:13:14.522,000 --> 0:13:16,000
que hacen cumplir la ley y el orden en nuestro mundo digital.

231
0:13:17.522,000 --> 0:13:19,000
Limpian, pulen y dejan todo impoluto,

232
0:13:19.927,000 --> 0:13:21,000
para liberarnos de todo lo malo.

233
0:13:22.284,000 --> 0:13:25,000
Tareas antes reservadas a las autoridades estatales

234
0:13:26.037,000 --> 0:13:29,000
ahora las realizan graduados universitarios de 20 a 25 años,

235
0:13:29.736,000 --> 0:13:31,000
con solo tres a cinco días de entrenamiento,

236
0:13:32.653,000 --> 0:13:33,000
que es el requisito

237
0:13:34.613,000 --> 0:13:37,000
para trabajar en nada menos que el rescate del mundo.

238
0:13:38.756,000 --> 0:13:41,000
MR: Las soberanías nacionales están en manos de empresas privadas,

239
0:13:42.999,000 --> 0:13:45,000
y derivan sus responsabilidades a terceras partes.

240
0:13:46.847,000 --> 0:13:49,000
Es una subcontratación de la subcontratación de la subcontratación

241
0:13:50.118,000 --> 0:13:51,000
lo que está ocurriendo.

242
0:13:51.618,000 --> 0:13:52,000
Con las redes sociales,

243
0:13:53.038,000 --> 0:13:56,000
se trata de una infraestructura completamente nueva,

244
0:13:56.077,000 --> 0:13:57,000
con sus propios mecanismos,

245
0:13:57.617,000 --> 0:13:58,000
su propia lógica de acción

246
0:13:59.22,000 --> 0:14:04,000
y por lo tanto, también, sus propios nuevos peligros,

247
0:14:04.589,000 --> 0:14:07,000
que no existían en la esfera pública predigitalizada.

248
0:14:08.276,000 --> 0:14:1,000
HB: Cuando Mark Zuckerberg estuvo en el Congreso de EE. UU.

249
0:14:11.103,000 --> 0:14:12,000
o en el Parlamento Europeo,

250
0:14:12.565,000 --> 0:14:14,000
debió enfrentarse a todo tipo de críticos.

251
0:14:15.224,000 --> 0:14:17,000
Y su reacción era siempre la misma:

252
0:14:18.501,000 --> 0:14:19,000
"Vamos a solucionarlo,

253
0:14:19.993,000 --> 0:14:21,000
y voy a hacer un seguimiento con mi equipo".

254
0:14:23.167,000 --> 0:14:26,000
Pero tal debate no debería celebrarse en la trastienda de Facebook,

255
0:14:26.969,000 --> 0:14:27,000
Twitter o Google.

256
0:14:28.278,000 --> 0:14:32,000
Ese debate debe discutirse abiertamente en nuevos parlamentos cosmopolitas,

257
0:14:33.118,000 --> 0:14:37,000
en nuevas instituciones que reflejen la diversidad de personas

258
0:14:38.002,000 --> 0:14:42,000
que contribuyan al proyecto utópico de una red global.

259
0:14:42.568,000 --> 0:14:45,000
Y si bien puede parecer imposible el tener en cuenta los valores

260
0:14:45.945,000 --> 0:14:46,000
de los usuarios en todo el mundo,

261
0:14:48.235,000 --> 0:14:49,000
vale la pena creer

262
0:14:49.941,000 --> 0:14:52,000
que es más lo que nos une que lo que nos separa.

263
0:14:53.624,000 --> 0:14:56,000
MR: Sí, en un momento en que el populismo está tomando fuerza,

264
0:14:57.355,000 --> 0:14:59,000
se hace popular el justificar los síntomas,

265
0:15:00.577,000 --> 0:15:01,000
erradicarlos,

266
0:15:01.879,000 --> 0:15:02,000
para hacerlos invisibles.

267
0:15:04.919,000 --> 0:15:07,000
Esta ideología se está extendiendo en todo el mundo,

268
0:15:08.292,000 --> 0:15:1,000
tanto analógico como digital,

269
0:15:11.903,000 --> 0:15:13,000
y es nuestro deber detenerla

270
0:15:15.419,000 --> 0:15:16,000
antes de que sea demasiado tarde.

271
0:15:17.665,000 --> 0:15:2,000
La cuestión de la libertad y la democracia

272
0:15:21.673,000 --> 0:15:23,000
no solo debe tener estas dos opciones:

273
0:15:25.053,000 --> 0:15:26,000
HB: Eliminar...

274
0:15:26.243,000 --> 0:15:27,000
MR: O ignorar.

275
0:15:29.3,000 --> 0:15:3,000
HB: Muchas gracias.

276
0:15:30.461,000 --> 0:15:32,000
(Aplausos)

