1
0:00:18.33,000 --> 0:00:24,000
What technology can we really apply to reducing global poverty?

2
0:00:24.33,000 --> 0:00:28,000
And what I found was quite surprising.

3
0:00:28.33,000 --> 0:00:31,000
We started looking at things like death rates in the 20th century,

4
0:00:31.33,000 --> 0:00:34,000
and how they'd been improved, and very simple things turned out.

5
0:00:34.33,000 --> 0:00:37,000
You'd think maybe antibiotics made more difference than clean water,

6
0:00:37.33,000 --> 0:00:4,000
but it's actually the opposite.

7
0:00:40.33,000 --> 0:00:43,000
And so very simple things -- off-the-shelf technologies

8
0:00:43.33,000 --> 0:00:48,000
that we could easily find on the then-early Web --

9
0:00:48.33,000 --> 0:00:53,000
would clearly make a huge difference to that problem.

10
0:00:53.33,000 --> 0:00:57,000
But I also, in looking at more powerful technologies

11
0:00:57.33,000 --> 0:01:02,000
and nanotechnology and genetic engineering and other new emerging

12
0:01:02.33,000 --> 0:01:06,000
kind of digital technologies, became very concerned

13
0:01:06.33,000 --> 0:01:1,000
about the potential for abuse.

14
0:01:10.33,000 --> 0:01:15,000
If you think about it, in history, a long, long time ago

15
0:01:15.33,000 --> 0:01:18,000
we dealt with the problem of an individual abusing another individual.

16
0:01:18.33,000 --> 0:01:21,000
We came up with something -- the Ten Commandments: Thou shalt not kill.

17
0:01:21.33,000 --> 0:01:23,000
That's a, kind of a one-on-one thing.

18
0:01:23.33,000 --> 0:01:27,000
We organized into cities. We had many people.

19
0:01:27.33,000 --> 0:01:31,000
And to keep the many from tyrannizing the one,

20
0:01:31.33,000 --> 0:01:35,000
we came up with concepts like individual liberty.

21
0:01:35.33,000 --> 0:01:36,000
And then, to have to deal with large groups,

22
0:01:36.33,000 --> 0:01:39,000
say, at the nation-state level,

23
0:01:39.33,000 --> 0:01:41,000
and we had to have mutual non-aggression,

24
0:01:41.33,000 --> 0:01:45,000
or through a series of conflicts, we eventually came to

25
0:01:45.33,000 --> 0:01:51,000
a rough international bargain to largely keep the peace.

26
0:01:51.33,000 --> 0:01:56,000
But now we have a new situation, really what people call

27
0:01:56.33,000 --> 0:01:59,000
an asymmetric situation, where technology is so powerful

28
0:01:59.33,000 --> 0:02:03,000
that it extends beyond a nation-state.

29
0:02:03.33,000 --> 0:02:06,000
It's not the nation-states that have potential access

30
0:02:06.33,000 --> 0:02:11,000
to mass destruction, but individuals.

31
0:02:11.33,000 --> 0:02:16,000
And this is a consequence of the fact that these new technologies tend to be digital.

32
0:02:16.33,000 --> 0:02:2,000
We saw genome sequences.

33
0:02:20.33,000 --> 0:02:21,000
You can download the gene sequences

34
0:02:21.33,000 --> 0:02:25,000
of pathogens off the Internet if you want to,

35
0:02:25.33,000 --> 0:02:3,000
and clearly someone recently -- I saw in a science magazine --

36
0:02:30.33,000 --> 0:02:35,000
they said, well, the 1918 flu is too dangerous to FedEx around.

37
0:02:35.33,000 --> 0:02:38,000
If people want to use it in their labs for working on research,

38
0:02:38.33,000 --> 0:02:41,000
just reconstruct it yourself,

39
0:02:41.33,000 --> 0:02:45,000
because, you know, it might break in FedEx.

40
0:02:45.33,000 --> 0:02:5,000
So that this is possible to do this is not deniable.

41
0:02:50.33,000 --> 0:02:55,000
So individuals in small groups super-empowered by access to these

42
0:02:55.33,000 --> 0:03:,000
kinds of self-replicating technologies, whether it be biological

43
0:03:00.33,000 --> 0:03:03,000
or other, are clearly a danger in our world.

44
0:03:03.33,000 --> 0:03:07,000
And the danger is that they can cause roughly what's a pandemic.

45
0:03:07.33,000 --> 0:03:1,000
And we really don't have experience with pandemics,

46
0:03:10.33,000 --> 0:03:13,000
and we're also not very good as a society at acting

47
0:03:13.33,000 --> 0:03:17,000
to things we don't have direct and sort of gut-level experience with.

48
0:03:17.33,000 --> 0:03:21,000
So it's not in our nature to pre-act.

49
0:03:21.33,000 --> 0:03:26,000
And in this case, piling on more technology doesn't solve the problem,

50
0:03:26.33,000 --> 0:03:29,000
because it only super-empowers people more.

51
0:03:29.33,000 --> 0:03:33,000
So the solution has to be, as people like Russell and Einstein

52
0:03:33.33,000 --> 0:03:35,000
and others imagine in a conversation that existed

53
0:03:35.33,000 --> 0:03:39,000
in a much stronger form, I think, early in the 20th century,

54
0:03:39.33,000 --> 0:03:42,000
that the solution had to be not just the head but the heart.

55
0:03:42.33,000 --> 0:03:47,000
You know, public policy and moral progress.

56
0:03:47.33,000 --> 0:03:53,000
The bargain that gives us civilization is a bargain to not use power.

57
0:03:53.33,000 --> 0:03:56,000
We get our individual rights by society protecting us from others

58
0:03:56.33,000 --> 0:04:01,000
not doing everything they can do but largely doing only what is legal.

59
0:04:01.33,000 --> 0:04:06,000
And so to limit the danger of these new things, we have to limit,

60
0:04:06.33,000 --> 0:04:08,000
ultimately, the ability of individuals

61
0:04:08.33,000 --> 0:04:11,000
to have access, essentially, to pandemic power.

62
0:04:11.33,000 --> 0:04:15,000
We also have to have sensible defense, because no limitation

63
0:04:15.33,000 --> 0:04:18,000
is going to prevent a crazy person from doing something.

64
0:04:18.33,000 --> 0:04:2,000
And you know, and the troubling thing is that

65
0:04:20.33,000 --> 0:04:22,000
it's much easier to do something bad than to defend

66
0:04:22.33,000 --> 0:04:24,000
against all possible bad things,

67
0:04:24.33,000 --> 0:04:28,000
so the offensive uses really have an asymmetric advantage.

68
0:04:28.33,000 --> 0:04:32,000
So these are the kind of thoughts I was thinking in 1999 and 2000,

69
0:04:32.33,000 --> 0:04:34,000
and my friends told me I was getting really depressed,

70
0:04:34.33,000 --> 0:04:36,000
and they were really worried about me.

71
0:04:36.33,000 --> 0:04:39,000
And then I signed a book contract to write more gloomy thoughts about this

72
0:04:39.33,000 --> 0:04:41,000
and moved into a hotel room in New York

73
0:04:41.33,000 --> 0:04:45,000
with one room full of books on the Plague,

74
0:04:45.33,000 --> 0:04:48,000
and you know, nuclear bombs exploding in New York

75
0:04:48.33,000 --> 0:04:51,000
where I would be within the circle, and so on.

76
0:04:51.33,000 --> 0:04:55,000
And then I was there on September 11th,

77
0:04:55.33,000 --> 0:04:56,000
and I stood in the streets with everyone.

78
0:04:56.33,000 --> 0:04:58,000
And it was quite an experience to be there.

79
0:04:58.33,000 --> 0:05:01,000
I got up the next morning and walked out of the city,

80
0:05:01.33,000 --> 0:05:04,000
and all the sanitation trucks were parked on Houston Street

81
0:05:04.33,000 --> 0:05:06,000
and ready to go down and start taking the rubble away.

82
0:05:06.33,000 --> 0:05:08,000
And I walked down the middle, up to the train station,

83
0:05:08.33,000 --> 0:05:11,000
and everything below 14th Street was closed.

84
0:05:11.33,000 --> 0:05:15,000
It was quite a compelling experience, but not really, I suppose,

85
0:05:15.33,000 --> 0:05:18,000
a surprise to someone who'd had his room full of the books.

86
0:05:18.33,000 --> 0:05:22,000
It was always a surprise that it happened then and there,

87
0:05:22.33,000 --> 0:05:26,000
but it wasn't a surprise that it happened at all.

88
0:05:26.33,000 --> 0:05:28,000
And everyone then started writing about this.

89
0:05:28.33,000 --> 0:05:29,000
Thousands of people started writing about this.

90
0:05:29.33,000 --> 0:05:31,000
And I eventually abandoned the book, and then Chris called me

91
0:05:31.33,000 --> 0:05:34,000
to talk at the conference. I really don't talk about this anymore

92
0:05:34.33,000 --> 0:05:39,000
because, you know, there's enough frustrating and depressing things going on.

93
0:05:39.33,000 --> 0:05:42,000
But I agreed to come and say a few things about this.

94
0:05:42.33,000 --> 0:05:45,000
And I would say that we can't give up the rule of law

95
0:05:45.33,000 --> 0:05:49,000
to fight an asymmetric threat, which is what we seem to be doing

96
0:05:49.33,000 --> 0:05:54,000
because of the present, the people that are in power,

97
0:05:54.33,000 --> 0:05:59,000
because that's to give up the thing that makes civilization.

98
0:05:59.33,000 --> 0:06:02,000
And we can't fight the threat in the kind of stupid way we're doing,

99
0:06:02.33,000 --> 0:06:04,000
because a million-dollar act

100
0:06:04.33,000 --> 0:06:07,000
causes a billion dollars of damage, causes a trillion dollar response

101
0:06:07.33,000 --> 0:06:1,000
which is largely ineffective and arguably, probably almost certainly,

102
0:06:10.33,000 --> 0:06:12,000
has made the problem worse.

103
0:06:12.33,000 --> 0:06:17,000
So we can't fight the thing with a million-to-one cost,

104
0:06:17.33,000 --> 0:06:23,000
one-to-a-million cost-benefit ratio.

105
0:06:24.33,000 --> 0:06:29,000
So after giving up on the book -- and I had the great honor

106
0:06:29.33,000 --> 0:06:33,000
to be able to join Kleiner Perkins about a year ago,

107
0:06:33.33,000 --> 0:06:4,000
and to work through venture capital on the innovative side,

108
0:06:40.33,000 --> 0:06:44,000
and to try to find some innovations that could address what I saw as

109
0:06:44.33,000 --> 0:06:46,000
some of these big problems.

110
0:06:46.33,000 --> 0:06:49,000
Things where, you know, a factor of 10 difference

111
0:06:49.33,000 --> 0:06:53,000
can make a factor of 1,000 difference in the outcome.

112
0:06:53.33,000 --> 0:06:56,000
I've been amazed in the last year at the incredible quality

113
0:06:56.33,000 --> 0:07:01,000
and excitement of the innovations that have come across my desk.

114
0:07:01.33,000 --> 0:07:04,000
It's overwhelming at times. I'm very thankful for Google and Wikipedia

115
0:07:04.33,000 --> 0:07:08,000
so I can understand at least a little of what people are talking about

116
0:07:08.33,000 --> 0:07:1,000
who come through the doors.

117
0:07:10.33,000 --> 0:07:13,000
But I wanted to share with you three areas

118
0:07:13.33,000 --> 0:07:16,000
that I'm particularly excited about and that relate to the problems

119
0:07:16.33,000 --> 0:07:21,000
that I was talking about in the Wired article.

120
0:07:21.33,000 --> 0:07:23,000
The first is this whole area of education,

121
0:07:23.33,000 --> 0:07:27,000
and it really relates to what Nicholas was talking about with a $100 computer.

122
0:07:27.33,000 --> 0:07:31,000
And that is to say that there's a lot of legs left in Moore's Law.

123
0:07:31.33,000 --> 0:07:35,000
The most advanced transistors today are at 65 nanometers,

124
0:07:35.33,000 --> 0:07:38,000
and we've seen, and I've had the pleasure to invest

125
0:07:38.33,000 --> 0:07:44,000
in, companies that give me great confidence that we'll extend Moore's Law

126
0:07:44.33,000 --> 0:07:47,000
all the way down to roughly the 10 nanometer scale.

127
0:07:47.33,000 --> 0:07:53,000
Another factor of, say, six in dimensional reduction,

128
0:07:53.33,000 --> 0:07:58,000
which should give us about another factor of 100 in raw improvement

129
0:07:58.33,000 --> 0:08:03,000
in what the chips can do. And so, to put that in practical terms,

130
0:08:03.33,000 --> 0:08:07,000
if something costs about 1,000 dollars today,

131
0:08:07.33,000 --> 0:08:12,000
say, the best personal computer you can buy, that might be its cost,

132
0:08:12.33,000 --> 0:08:18,000
I think we can have that in 2020 for 10 dollars. Okay?

133
0:08:18.33,000 --> 0:08:23,000
Now, just imagine what that $100 computer will be in 2020

134
0:08:23.33,000 --> 0:08:25,000
as a tool for education.

135
0:08:25.33,000 --> 0:08:27,000
I think the challenge for us is --

136
0:08:27.33,000 --> 0:08:29,000
I'm very certain that that will happen, the challenge is,

137
0:08:29.33,000 --> 0:08:34,000
will we develop the kind of educational tools and things with the net

138
0:08:34.33,000 --> 0:08:37,000
to let us take advantage of that device?

139
0:08:37.33,000 --> 0:08:41,000
I'd argue today that we have incredibly powerful computers,

140
0:08:41.33,000 --> 0:08:43,000
but we don't have very good software for them.

141
0:08:43.33,000 --> 0:08:46,000
And it's only in retrospect, after the better software comes along,

142
0:08:46.33,000 --> 0:08:48,000
and you take it and you run it on a ten-year-old machine, you say,

143
0:08:48.33,000 --> 0:08:5,000
God, the machine was that fast?

144
0:08:50.33,000 --> 0:08:52,000
I remember when they took the Apple Mac interface

145
0:08:52.33,000 --> 0:08:55,000
and they put it back on the Apple II.

146
0:08:55.33,000 --> 0:08:58,000
The Apple II was perfectly capable of running that kind of interface,

147
0:08:58.33,000 --> 0:09:01,000
we just didn't know how to do it at the time.

148
0:09:01.33,000 --> 0:09:03,000
So given that we know and should believe --

149
0:09:03.33,000 --> 0:09:06,000
because Moore's Law's been, like, a constant,

150
0:09:06.33,000 --> 0:09:09,000
I mean, it's just been very predictable progress

151
0:09:09.33,000 --> 0:09:12,000
over the last 40 years or whatever.

152
0:09:12.33,000 --> 0:09:16,000
We can know what the computers are going to be like in 2020.

153
0:09:16.33,000 --> 0:09:18,000
It's great that we have initiatives to say,

154
0:09:18.33,000 --> 0:09:21,000
let's go create the education and educate people in the world,

155
0:09:21.33,000 --> 0:09:23,000
because that's a great force for peace.

156
0:09:23.33,000 --> 0:09:26,000
And we can give everyone in the world a $100 computer

157
0:09:26.33,000 --> 0:09:31,000
or a $10 computer in the next 15 years.

158
0:09:31.33,000 --> 0:09:36,000
The second area that I'm focusing on is the environmental problem,

159
0:09:36.33,000 --> 0:09:4,000
because that's clearly going to put a lot of pressure on this world.

160
0:09:40.33,000 --> 0:09:44,000
We'll hear a lot more about that from Al Gore very shortly.

161
0:09:44.33,000 --> 0:09:47,000
The thing that we see as the kind of Moore's Law trend

162
0:09:47.33,000 --> 0:09:5,000
that's driving improvement in our ability to address

163
0:09:50.33,000 --> 0:09:54,000
the environmental problem is new materials.

164
0:09:54.33,000 --> 0:09:58,000
We have a challenge, because the urban population is growing

165
0:09:58.33,000 --> 0:10:01,000
in this century from two billion to six billion

166
0:10:01.33,000 --> 0:10:03,000
in a very short amount of time. People are moving to the cities.

167
0:10:03.33,000 --> 0:10:06,000
They all need clean water, they need energy, they need transportation,

168
0:10:06.33,000 --> 0:10:1,000
and we want them to develop in a green way.

169
0:10:10.33,000 --> 0:10:12,000
We're reasonably efficient in the industrial sectors.

170
0:10:12.33,000 --> 0:10:15,000
We've made improvements in energy and resource efficiency,

171
0:10:15.33,000 --> 0:10:19,000
but the consumer sector, especially in America, is very inefficient.

172
0:10:19.33,000 --> 0:10:23,000
But these new materials bring such incredible innovations

173
0:10:23.33,000 --> 0:10:27,000
that there's a strong basis for hope that these things

174
0:10:27.33,000 --> 0:10:29,000
will be so profitable that they can be brought to the market.

175
0:10:29.33,000 --> 0:10:32,000
And I want to give you a specific example of a new material

176
0:10:32.33,000 --> 0:10:35,000
that was discovered 15 years ago.

177
0:10:35.33,000 --> 0:10:4,000
If we take carbon nanotubes, you know, Iijima discovered them in 1991,

178
0:10:40.33,000 --> 0:10:42,000
they just have incredible properties.

179
0:10:42.33,000 --> 0:10:43,000
And these are the kinds of things we're going to discover

180
0:10:43.33,000 --> 0:10:46,000
as we start to engineer at the nano scale.

181
0:10:46.33,000 --> 0:10:49,000
Their strength: they're almost the strongest material,

182
0:10:49.33,000 --> 0:10:51,000
tensile strength material known.

183
0:10:52.33,000 --> 0:10:57,000
They're very, very stiff. They stretch very, very little.

184
0:10:57.33,000 --> 0:11:,000
In two dimensions, if you make, like, a fabric out of them,

185
0:11:00.33,000 --> 0:11:03,000
they're 30 times stronger than Kevlar.

186
0:11:03.33,000 --> 0:11:06,000
And if you make a three-dimensional structure, like a buckyball,

187
0:11:06.33,000 --> 0:11:08,000
they have all sorts of incredible properties.

188
0:11:08.33,000 --> 0:11:11,000
If you shoot a particle at them and knock a hole in them,

189
0:11:11.33,000 --> 0:11:14,000
they repair themselves; they go zip and they repair the hole

190
0:11:14.33,000 --> 0:11:17,000
in femtoseconds, which is not -- is really quick.

191
0:11:17.33,000 --> 0:11:2,000
(Laughter)

192
0:11:20.33,000 --> 0:11:24,000
If you shine a light on them, they produce electricity.

193
0:11:24.33,000 --> 0:11:27,000
In fact, if you flash them with a camera they catch on fire.

194
0:11:27.33,000 --> 0:11:31,000
If you put electricity on them, they emit light.

195
0:11:31.33,000 --> 0:11:34,000
If you run current through them, you can run 1,000 times more current

196
0:11:34.33,000 --> 0:11:38,000
through one of these than through a piece of metal.

197
0:11:38.33,000 --> 0:11:41,000
You can make both p- and n-type semiconductors,

198
0:11:41.33,000 --> 0:11:43,000
which means you can make transistors out of them.

199
0:11:43.33,000 --> 0:11:46,000
They conduct heat along their length but not across --

200
0:11:46.33,000 --> 0:11:48,000
well, there is no width, but not in the other direction

201
0:11:48.33,000 --> 0:11:54,000
if you stack them up; that's a property of carbon fiber also.

202
0:11:54.33,000 --> 0:11:57,000
If you put particles in them, and they go shooting out the tip --

203
0:11:57.33,000 --> 0:12:,000
they're like miniature linear accelerators or electron guns.

204
0:12:00.33,000 --> 0:12:03,000
The inside of the nanotubes is so small --

205
0:12:03.33,000 --> 0:12:05,000
the smallest ones are 0.7 nanometers --

206
0:12:05.33,000 --> 0:12:07,000
that it's basically a quantum world.

207
0:12:07.33,000 --> 0:12:1,000
It's a strange place inside a nanotube.

208
0:12:10.33,000 --> 0:12:13,000
And so we begin to see, and we've seen business plans already,

209
0:12:13.33,000 --> 0:12:16,000
where the kind of things Lisa Randall's talking about are in there.

210
0:12:16.33,000 --> 0:12:18,000
I had one business plan where I was trying to learn more about

211
0:12:18.33,000 --> 0:12:21,000
Witten's cosmic dimension strings to try to understand

212
0:12:21.33,000 --> 0:12:24,000
what the phenomenon was going on in this proposed nanomaterial.

213
0:12:24.33,000 --> 0:12:3,000
So inside of a nanotube, we're really at the limit here.

214
0:12:30.33,000 --> 0:12:34,000
So what we see is with these and other new materials

215
0:12:34.33,000 --> 0:12:38,000
that we can do things with different properties -- lighter, stronger --

216
0:12:38.33,000 --> 0:12:44,000
and apply these new materials to the environmental problems.

217
0:12:44.33,000 --> 0:12:45,000
New materials that can make water,

218
0:12:45.33,000 --> 0:12:47,000
new materials that can make fuel cells work better,

219
0:12:47.33,000 --> 0:12:51,000
new materials that catalyze chemical reactions,

220
0:12:51.33,000 --> 0:12:54,000
that cut pollution and so on.

221
0:12:54.33,000 --> 0:12:57,000
Ethanol -- new ways of making ethanol.

222
0:12:57.33,000 --> 0:13:,000
New ways of making electric transportation.

223
0:13:00.33,000 --> 0:13:04,000
The whole green dream -- because it can be profitable.

224
0:13:04.33,000 --> 0:13:06,000
And we've dedicated -- we've just raised a new fund,

225
0:13:06.33,000 --> 0:13:09,000
we dedicated 100 million dollars to these kinds of investments.

226
0:13:09.33,000 --> 0:13:13,000
We believe that Genentech, the Compaq, the Lotus, the Sun,

227
0:13:13.33,000 --> 0:13:17,000
the Netscape, the Amazon, the Google in these fields

228
0:13:17.33,000 --> 0:13:2,000
are yet to be found, because this materials revolution

229
0:13:20.33,000 --> 0:13:23,000
will drive these things forward.

230
0:13:24.33,000 --> 0:13:26,000
The third area that we're working on,

231
0:13:26.33,000 --> 0:13:3,000
and we just announced last week -- we were all in New York.

232
0:13:30.33,000 --> 0:13:36,000
We raised 200 million dollars in a specialty fund

233
0:13:36.33,000 --> 0:13:4,000
to work on a pandemic in biodefense.

234
0:13:40.33,000 --> 0:13:43,000
And to give you an idea of the last fund that Kleiner raised

235
0:13:43.33,000 --> 0:13:48,000
was a $400 million fund, so this for us is a very substantial fund.

236
0:13:48.33,000 --> 0:13:52,000
And what we did, over the last few months -- well, a few months ago,

237
0:13:52.33,000 --> 0:13:55,000
Ray Kurzweil and I wrote an op-ed in the New York Times

238
0:13:55.33,000 --> 0:13:58,000
about how publishing the 1918 genome was very dangerous.

239
0:13:58.33,000 --> 0:14:02,000
And John Doerr and Brook and others got concerned, [unclear],

240
0:14:02.33,000 --> 0:14:06,000
and we started looking around at what the world was doing

241
0:14:06.33,000 --> 0:14:11,000
about being prepared for a pandemic. And we saw a lot of gaps.

242
0:14:11.33,000 --> 0:14:15,000
And so we asked ourselves, you know, can we find innovative things

243
0:14:15.33,000 --> 0:14:19,000
that will go fill these gaps? And Brooks told me in a break here,

244
0:14:19.33,000 --> 0:14:21,000
he said he's found so much stuff he can't sleep,

245
0:14:21.33,000 --> 0:14:24,000
because there's so many great technologies out there,

246
0:14:24.33,000 --> 0:14:27,000
we're essentially buried. And we need them, you know.

247
0:14:27.33,000 --> 0:14:3,000
We have one antiviral that people are talking about stockpiling

248
0:14:30.33,000 --> 0:14:33,000
that still works, roughly. That's Tamiflu.

249
0:14:33.33,000 --> 0:14:38,000
But Tamiflu -- the virus is resistant. It is resistant to Tamiflu.

250
0:14:38.33,000 --> 0:14:42,000
We've discovered with AIDS we need cocktails to work well

251
0:14:42.33,000 --> 0:14:45,000
so that the viral resistance -- we need several anti-virals.

252
0:14:45.33,000 --> 0:14:47,000
We need better surveillance.

253
0:14:47.33,000 --> 0:14:5,000
We need networks that can find out what's going on.

254
0:14:50.33,000 --> 0:14:54,000
We need rapid diagnostics so that we can tell if somebody has

255
0:14:54.33,000 --> 0:14:58,000
a strain of flu which we have only identified very recently.

256
0:14:58.33,000 --> 0:15:,000
We've got to be able to make the rapid diagnostics quickly.

257
0:15:00.33,000 --> 0:15:03,000
We need new anti-virals and cocktails. We need new kinds of vaccines.

258
0:15:03.33,000 --> 0:15:05,000
Vaccines that are broad spectrum.

259
0:15:05.33,000 --> 0:15:09,000
Vaccines that we can manufacture quickly.

260
0:15:09.33,000 --> 0:15:11,000
Cocktails, more polyvalent vaccines.

261
0:15:11.33,000 --> 0:15:14,000
You normally get a trivalent vaccine against three possible strains.

262
0:15:14.33,000 --> 0:15:17,000
We need -- we don't know where this thing is going.

263
0:15:17.33,000 --> 0:15:2,000
We believe that if we could fill these 10 gaps,

264
0:15:20.33,000 --> 0:15:26,000
we have a chance to help really reduce the risk of a pandemic.

265
0:15:26.33,000 --> 0:15:3,000
And the difference between a normal flu season and a pandemic

266
0:15:30.33,000 --> 0:15:33,000
is about a factor of 1,000 in deaths

267
0:15:33.33,000 --> 0:15:36,000
and certainly enormous economic impact.

268
0:15:36.33,000 --> 0:15:39,000
So we're very excited because we think we can fund 10,

269
0:15:39.33,000 --> 0:15:43,000
or speed up 10 projects and see them come to market

270
0:15:43.33,000 --> 0:15:46,000
in the next couple years that will address this.

271
0:15:46.33,000 --> 0:15:49,000
So if we can address, use technology, help address education,

272
0:15:49.33,000 --> 0:15:52,000
help address the environment, help address the pandemic,

273
0:15:52.33,000 --> 0:15:56,000
does that solve the larger problem that I was talking about

274
0:15:56.33,000 --> 0:16:01,000
in the Wired article? And I'm afraid the answer is really no,

275
0:16:01.33,000 --> 0:16:05,000
because you can't solve a problem with the management of technology

276
0:16:05.33,000 --> 0:16:08,000
with more technology.

277
0:16:08.33,000 --> 0:16:13,000
If we let an unlimited amount of power loose, then we will --

278
0:16:13.33,000 --> 0:16:15,000
a very small number of people will be able to abuse it.

279
0:16:15.33,000 --> 0:16:19,000
We can't fight at a million-to-one disadvantage.

280
0:16:19.33,000 --> 0:16:22,000
So what we need to do is, we need better policy.

281
0:16:22.33,000 --> 0:16:25,000
And for example, some things we could do

282
0:16:25.33,000 --> 0:16:29,000
that would be policy solutions which are not really in the political air right now

283
0:16:29.33,000 --> 0:16:33,000
but perhaps with the change of administration would be -- use markets.

284
0:16:33.33,000 --> 0:16:35,000
Markets are a very strong force.

285
0:16:35.33,000 --> 0:16:38,000
For example, rather than trying to regulate away problems,

286
0:16:38.33,000 --> 0:16:4,000
which probably won't work, if we could price

287
0:16:40.33,000 --> 0:16:45,000
into the cost of doing business, the cost of catastrophe,

288
0:16:45.33,000 --> 0:16:48,000
so that people who are doing things that had a higher cost of catastrophe

289
0:16:48.33,000 --> 0:16:51,000
would have to take insurance against that risk.

290
0:16:51.33,000 --> 0:16:53,000
So if you wanted to put a drug on the market you could put it on.

291
0:16:53.33,000 --> 0:16:55,000
But it wouldn't have to be approved by regulators;

292
0:16:55.33,000 --> 0:16:59,000
you'd have to convince an actuary that it would be safe.

293
0:16:59.33,000 --> 0:17:02,000
And if you apply the notion of insurance more broadly,

294
0:17:02.33,000 --> 0:17:05,000
you can use a more powerful force, a market force,

295
0:17:05.33,000 --> 0:17:07,000
to provide feedback.

296
0:17:07.33,000 --> 0:17:08,000
How could you keep the law?

297
0:17:08.33,000 --> 0:17:1,000
I think the law would be a really good thing to keep.

298
0:17:10.33,000 --> 0:17:12,000
Well, you have to hold people accountable.

299
0:17:12.33,000 --> 0:17:14,000
The law requires accountability.

300
0:17:14.33,000 --> 0:17:17,000
Today scientists, technologists, businessmen, engineers

301
0:17:17.33,000 --> 0:17:19,000
don't have any personal responsibility

302
0:17:19.33,000 --> 0:17:21,000
for the consequences of their actions.

303
0:17:21.33,000 --> 0:17:25,000
So if you tie that -- you have to tie that back with the law.

304
0:17:25.33,000 --> 0:17:29,000
And finally, I think we have to do something that's not really --

305
0:17:29.33,000 --> 0:17:3,000
it's almost unacceptable to say this -- which,

306
0:17:30.33,000 --> 0:17:33,000
we have to begin to design the future.

307
0:17:33.33,000 --> 0:17:37,000
We can't pick the future, but we can steer the future.

308
0:17:37.33,000 --> 0:17:39,000
Our investment in trying to prevent pandemic flu

309
0:17:39.33,000 --> 0:17:43,000
is affecting the distribution of possible outcomes.

310
0:17:43.33,000 --> 0:17:45,000
We may not be able to stop it, but the likelihood

311
0:17:45.33,000 --> 0:17:49,000
that it will get past us is lower if we focus on that problem.

312
0:17:49.33,000 --> 0:17:53,000
So we can design the future if we choose what kind of things

313
0:17:53.33,000 --> 0:17:56,000
we want to have happen and not have happen,

314
0:17:56.33,000 --> 0:17:59,000
and steer us to a lower-risk place.

315
0:17:59.33,000 --> 0:18:05,000
Vice President Gore will talk about how we could steer the climate trajectory

316
0:18:05.33,000 --> 0:18:08,000
into a lower probability of catastrophic risk.

317
0:18:08.33,000 --> 0:18:11,000
But above all, what we have to do is we have to help the good guys,

318
0:18:11.33,000 --> 0:18:13,000
the people on the defensive side,

319
0:18:13.33,000 --> 0:18:17,000
have an advantage over the people who want to abuse things.

320
0:18:17.33,000 --> 0:18:19,000
And what we have to do to do that

321
0:18:19.33,000 --> 0:18:22,000
is we have to limit access to certain information.

322
0:18:22.33,000 --> 0:18:25,000
And growing up as we have, and holding very high

323
0:18:25.33,000 --> 0:18:29,000
the value of free speech, this is a hard thing for us to accept --

324
0:18:29.33,000 --> 0:18:3,000
for all of us to accept.

325
0:18:30.33,000 --> 0:18:35,000
It's especially hard for the scientists to accept who still remember,

326
0:18:35.33,000 --> 0:18:37,000
you know, Galileo essentially locked up,

327
0:18:37.33,000 --> 0:18:41,000
and who are still fighting this battle against the church.

328
0:18:41.33,000 --> 0:18:46,000
But that's the price of having a civilization.

329
0:18:46.33,000 --> 0:18:48,000
The price of retaining the rule of law

330
0:18:48.33,000 --> 0:18:53,000
is to limit the access to the great and kind of unbridled power.

331
0:18:53.33,000 --> 0:18:54,000
Thank you.

332
0:18:54.33,000 --> 0:18:56,000
(Applause)

