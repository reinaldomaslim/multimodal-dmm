1
0:00:,000 --> 0:00:07,000
Traducteur: eric vautier Relecteur: Claire Ghyselen

2
0:00:12.535,000 --> 0:00:15,000
[Cette intervention comporte du contenu réservé aux adultes]

3
0:00:17.762,000 --> 0:00:19,000
Rana Ayyub est journaliste en Inde.

4
0:00:20.778,000 --> 0:00:22,000
Elle a exposé la corruption du gouvernement

5
0:00:24.411,000 --> 0:00:26,000
et des violations des droits de l'homme.

6
0:00:26.99,000 --> 0:00:27,000
Et au fil des années,

7
0:00:28.181,000 --> 0:00:31,000
elle s'est habituée à la critique et à la controverse sur son travail.

8
0:00:32.149,000 --> 0:00:37,000
Mais rien n'aurait pu la préparer à ce qu'il s'est passé en avril 2018.

9
0:00:38.125,000 --> 0:00:41,000
Elle était dans un café avec un ami quand elle a vu la vidéo :

10
0:00:41.8,000 --> 0:00:45,000
une vidéo de deux minutes vingt la montrant en plein ébat sexuel.

11
0:00:47.188,000 --> 0:00:49,000
Elle n'en croyait pas ses yeux.

12
0:00:49.561,000 --> 0:00:51,000
Elle n'avait jamais tourné une telle vidéo.

13
0:00:52.506,000 --> 0:00:55,000
Mais malheureusement, des milliers de gens

14
0:00:55.995,000 --> 0:00:56,000
ont cru que c'était elle.

15
0:00:58.673,000 --> 0:01:,000
J'ai interviewé Mme Ayyub il y a trois mois,

16
0:01:01.617,000 --> 0:01:04,000
dans le cadre de mon livre sur la protection de l'intimité sexuelle.

17
0:01:04.811,000 --> 0:01:07,000
Je suis professeur de droit, juriste et avocate en droit civil.

18
0:01:08.204,000 --> 0:01:12,000
Il est incroyablement frustrant, sachant tout cela,

19
0:01:12.839,000 --> 0:01:14,000
que la loi ne puisse pas beaucoup l'aider.

20
0:01:15.458,000 --> 0:01:16,000
Lors de notre conversation,

21
0:01:17.029,000 --> 0:01:21,000
elle m'a expliqué qu'elle aurait dû voir venir cette fausse vidéo porno.

22
0:01:22.038,000 --> 0:01:25,000
Elle m'a dit : « Après tout, le sexe est si souvent utilisé

23
0:01:25.336,000 --> 0:01:27,000
pour rabaisser les femmes ou leur faire honte,

24
0:01:27.658,000 --> 0:01:29,000
surtout les femmes issues des minorités,

25
0:01:30.11,000 --> 0:01:34,000
et surtout les femmes des minorités qui osent défier des hommes puissants » -

26
0:01:34.446,000 --> 0:01:35,000
ce qu'elle avait fait par son travail.

27
0:01:37.191,000 --> 0:01:4,000
En 48 heures, la vidéo truquée est devenue virale.

28
0:01:42.064,000 --> 0:01:47,000
Tous ses comptes ont été bombardés d'images de cette vidéo,

29
0:01:47.395,000 --> 0:01:49,000
accompagnées de menaces de viol, de menaces de mort,

30
0:01:50.046,000 --> 0:01:52,000
et d'insultes à sa foi de musulmane.

31
0:01:53.426,000 --> 0:01:57,000
Des posts en ligne suggéraient qu'elle était « disponible » sexuellement.

32
0:01:58.014,000 --> 0:01:59,000
Et elle a été « doxée » :

33
0:01:59.648,000 --> 0:02:01,000
son adresse personnelle et son numéro de portable

34
0:02:02.45,000 --> 0:02:03,000
ont été dévoilés sur Internet.

35
0:02:04.879,000 --> 0:02:08,000
La vidéo a été partagée plus de 40 000 fois.

36
0:02:09.76,000 --> 0:02:12,000
Quand quelqu'un est visé par ce genre de cyber-harcèlement,

37
0:02:13.72,000 --> 0:02:15,000
les conséquences sont immensément douloureuses.

38
0:02:16.482,000 --> 0:02:19,000
La vie de Rana Ayyub a été chamboulée.

39
0:02:20.211,000 --> 0:02:23,000
Pendant des semaines, elle arrivait à peine à manger ou à parler.

40
0:02:23.919,000 --> 0:02:26,000
Elle a arrêté d'écrire et a fermé tous ses comptes sur les réseaux sociaux,

41
0:02:27.632,000 --> 0:02:3,000
ce qui, vous savez, est un acte difficile quand vous êtes journaliste.

42
0:02:31.188,000 --> 0:02:34,000
Elle avait peur de sortir de chez elle.

43
0:02:34.672,000 --> 0:02:37,000
Et si les harceleurs voulaient mettre leurs menaces à exécution ?

44
0:02:38.395,000 --> 0:02:42,000
Le Conseil de l'ONU pour les droits de l'homme ont confirmé qu'elle avait raison.

45
0:02:42.784,000 --> 0:02:46,000
Il a publié une déclaration disant qu'il s'inquiétait pour sa sécurité.

46
0:02:48.776,000 --> 0:02:52,000
Rana Ayyub a été la cible de ce qu'on appelle un deepfake :

47
0:02:53.029,000 --> 0:02:55,000
une technologie basée sur l'apprentissage automatique

48
0:02:55.593,000 --> 0:02:59,000
qui manipule ou fabrique des enregistrements audio et vidéo

49
0:02:59.728,000 --> 0:03:01,000
qui montrent des gens faisant ou disant des choses

50
0:03:02.475,000 --> 0:03:03,000
qu'ils n'ont jamais faites ou dites.

51
0:03:04.807,000 --> 0:03:07,000
Les deepfakes semblent authentiques et réalistes, mais ne le sont pas ;

52
0:03:08.192,000 --> 0:03:09,000
ce sont des falsifications complètes.

53
0:03:11.228,000 --> 0:03:14,000
Bien que la technologie soit encore dans une phase d'amélioration,

54
0:03:15.022,000 --> 0:03:16,000
elle est déjà largement accessible.

55
0:03:17.371,000 --> 0:03:2,000
les deepfakes ont tout récemment attiré l'attention,

56
0:03:20.467,000 --> 0:03:22,000
comme tant de choses sur Internet,

57
0:03:22.652,000 --> 0:03:23,000
grâce à la pornographie.

58
0:03:24.498,000 --> 0:03:26,000
Début 2018,

59
0:03:26.633,000 --> 0:03:28,000
quelqu'un a posté un outil sur Reddit

60
0:03:29.125,000 --> 0:03:33,000
permettant à ses utilisateurs d'intégrer des visages dans des vidéos pornos.

61
0:03:33.561,000 --> 0:03:36,000
S'en est suivi un déluge de fausses vidéos pornos

62
0:03:37.025,000 --> 0:03:4,000
mettant en scène les célébrités féminines préférées du public.

63
0:03:40.712,000 --> 0:03:43,000
Aujourd'hui, vous pouvez faire défiler sur Youtube les nombreux tutoriels

64
0:03:44.213,000 --> 0:03:46,000
montrant pas à pas

65
0:03:46.523,000 --> 0:03:49,000
comment créer un deepfake sur votre ordinateur personnel.

66
0:03:50.26,000 --> 0:03:53,000
On dit qu'on pourrait même bientôt le faire sur nos téléphones.

67
0:03:55.072,000 --> 0:04:,000
On est là au croisement de certaines des plus basiques fragilités humaines

68
0:04:00.478,000 --> 0:04:01,000
et des outils technologiques

69
0:04:02.184,000 --> 0:04:04,000
qui peuvent transformer les deepfakes en armes.

70
0:04:04.874,000 --> 0:04:05,000
Laissez-moi préciser.

71
0:04:06.875,000 --> 0:04:1,000
Les êtres humains ont une réaction instinctive à l'audio et la vidéo.

72
0:04:11.86,000 --> 0:04:12,000
On croit que c'est vrai,

73
0:04:13.372,000 --> 0:04:15,000
car, par principe, nous croyons évidemment

74
0:04:15.474,000 --> 0:04:17,000
ce que nos yeux et nos oreilles nous disent.

75
0:04:18.476,000 --> 0:04:19,000
Et c'est ce mécanisme

76
0:04:20.199,000 --> 0:04:23,000
qui peut saper notre sens partagé de la réalité.

77
0:04:23.897,000 --> 0:04:26,000
Bien que nous pensions que les deepfakes soient vrais, ils ne le sont pas.

78
0:04:27.604,000 --> 0:04:31,000
Et nous sommes attirés par ce qui est salace, provoquant.

79
0:04:32.365,000 --> 0:04:35,000
On a tendance à croire et à diffuser des informations

80
0:04:35.436,000 --> 0:04:37,000
qui sont négatives et nouvelles.

81
0:04:37.809,000 --> 0:04:42,000
Des chercheurs ont trouvé que les canulars en ligne se répandent dix fois plus vite

82
0:04:42.852,000 --> 0:04:43,000
que les histoires vraies.

83
0:04:46.015,000 --> 0:04:5,000
De plus, nous sommes aussi attirés par l'information

84
0:04:50.419,000 --> 0:04:51,000
qui coïncide avec notre point de vue.

85
0:04:52.95,000 --> 0:04:55,000
Les psychologues appellent cela un biais de confirmation.

86
0:04:57.3,000 --> 0:05:01,000
Les réseaux sociaux amplifient cette tendance,

87
0:05:01.711,000 --> 0:05:04,000
en nous permettant de partager une information instantanément et largement,

88
0:05:05.616,000 --> 0:05:07,000
si elle correspond à notre point de vue.

89
0:05:08.735,000 --> 0:05:1,000
Mais les deepfakes ont le potentiel

90
0:05:10.969,000 --> 0:05:13,000
de causer de graves dommages individuels et sociétaux.

91
0:05:15.204,000 --> 0:05:17,000
Imaginez un deepfake

92
0:05:17.252,000 --> 0:05:21,000
qui montrerait des soldats américains brûlant un Coran en Afghanistan.

93
0:05:22.807,000 --> 0:05:25,000
On imagine bien qu'une telle vidéo déclencherait des violences

94
0:05:25.855,000 --> 0:05:26,000
contre ces soldats.

95
0:05:27.847,000 --> 0:05:29,000
Et imaginons que le lendemain

96
0:05:30.744,000 --> 0:05:32,000
tombe un nouveau deepfake

97
0:05:33.022,000 --> 0:05:36,000
montrant un imam de Londres bien connu

98
0:05:36.363,000 --> 0:05:38,000
appelant à attaquer ces soldats.

99
0:05:39.617,000 --> 0:05:42,000
Ça pourrait déclencher des violences et des troubles civils,

100
0:05:42.804,000 --> 0:05:45,000
pas seulement en Afghanistan ou au Royaume-Uni,

101
0:05:46.077,000 --> 0:05:47,000
mais partout dans le monde.

102
0:05:48.251,000 --> 0:05:51,000
Vous pourriez me répondre : « Allons, c'est tiré par les cheveux. »

103
0:05:51.704,000 --> 0:05:52,000
Mais ce n'est pas le cas.

104
0:05:53.293,000 --> 0:05:55,000
On a vu des mensonges se répandre

105
0:05:55.508,000 --> 0:05:57,000
sur WhatsApp et d'autres services de messagerie en ligne

106
0:05:58.254,000 --> 0:06:,000
et mener à des violences envers des minorités ethniques.

107
0:06:01.039,000 --> 0:06:02,000
Et ce n'était que du texte -

108
0:06:02.95,000 --> 0:06:04,000
imaginez si ça avait été des vidéos.

109
0:06:06.593,000 --> 0:06:11,000
Les deepfakes ont le potentiel d'entamer la confiance que nous avons

110
0:06:11.974,000 --> 0:06:12,000
dans les institutions démocratiques.

111
0:06:15.006,000 --> 0:06:17,000
Imaginez qu'à la veille d'une élection,

112
0:06:17.996,000 --> 0:06:2,000
on voit un deepfake montrant un des candidats du parti majoritaire

113
0:06:21.258,000 --> 0:06:22,000
gravement malade.

114
0:06:23.202,000 --> 0:06:25,000
Ce deepfake pourrait faire basculer l'élection

115
0:06:25.559,000 --> 0:06:28,000
et mettre en doute la légitimité qu'on attribue à l'élection.

116
0:06:30.515,000 --> 0:06:33,000
Imaginez qu'à la veille de l'entrée en bourse

117
0:06:33.865,000 --> 0:06:35,000
d'une banque importante,

118
0:06:36.222,000 --> 0:06:39,000
il y ait un deepfake montrant le PDG de la banque

119
0:06:39.395,000 --> 0:06:41,000
complètement saoûl, soutenant des théories complotistes.

120
0:06:42.887,000 --> 0:06:45,000
Ce deepfake pourrait couler cette introduction en bourse,

121
0:06:45.958,000 --> 0:06:49,000
et pire encore, nous faire douter de la stabilité des marchés financiers.

122
0:06:51.385,000 --> 0:06:54,000
Les deepfakes savent exploiter et amplifier

123
0:06:55.189,000 --> 0:06:58,000
la méfiance profonde que nous avons déjà

124
0:06:58.398,000 --> 0:07:02,000
dans les politiciens, les patrons et les autres leaders influents.

125
0:07:02.945,000 --> 0:07:05,000
Ils rencontrent un public tout prêt à les croire.

126
0:07:07.287,000 --> 0:07:09,000
La recherche de la vérité est en danger également.

127
0:07:11.077,000 --> 0:07:14,000
Les experts en technologie s'attendent à ce qu'avec les avancées de l'IA,

128
0:07:14.665,000 --> 0:07:17,000
il soit bientôt difficile, sinon impossible,

129
0:07:18.371,000 --> 0:07:21,000
de faire la différence entre une vraie et une fausse vidéo.

130
0:07:23.022,000 --> 0:07:25,000
Comment la vérité pourrait donc émerger

131
0:07:25.402,000 --> 0:07:27,000
sur une scène des idées contaminée par les deepfakes ?

132
0:07:28.752,000 --> 0:07:31,000
Suivrons-nous juste la voie de moindre résistance

133
0:07:32.196,000 --> 0:07:34,000
pour croire ce que nous voulons croire,

134
0:07:34.657,000 --> 0:07:35,000
et au diable la vérité ?

135
0:07:36.831,000 --> 0:07:39,000
Non seulement nous risquons de croire les mensonges,

136
0:07:40.03,000 --> 0:07:43,000
mais aussi de douter de la vérité.

137
0:07:43.887,000 --> 0:07:47,000
Nous avons déjà vu des gens utiliser ce phénomène des deepfakes

138
0:07:47.99,000 --> 0:07:5,000
pour mettre en doute des preuves tangibles de leurs agissements.

139
0:07:51.934,000 --> 0:07:56,000
Nous avons entendu des politiciens dire de certains enregistrements :

140
0:07:57.927,000 --> 0:07:58,000
« Allez, ce sont des fake news.

141
0:07:59.697,000 --> 0:08:02,000
Vous ne pouvez pas croire ce que vos yeux et vos oreilles vous disent. »

142
0:08:04.402,000 --> 0:08:05,000
Et c'est ce risque

143
0:08:06.157,000 --> 0:08:11,000
que le professeur Robert Chesney et moi appelons le « dividende du menteur » :

144
0:08:11.617,000 --> 0:08:14,000
le risque qu'un menteur invoque les deepfakes

145
0:08:14.998,000 --> 0:08:16,000
pour échapper aux conséquences de ses malversations.

146
0:08:18.963,000 --> 0:08:21,000
Nous avons donc du pain sur la planche, sans aucun doute.

147
0:08:22.606,000 --> 0:08:25,000
Nous allons avoir besoin d'une solution proactive

148
0:08:25.955,000 --> 0:08:28,000
de la part des sociétés de technologie, des législateurs,

149
0:08:29.49,000 --> 0:08:3,000
des forces de l'ordre et des media.

150
0:08:32.093,000 --> 0:08:36,000
Et nous allons avoir besoin d'une bonne dose de résilience sociale.

151
0:08:37.506,000 --> 0:08:4,000
Nous sommes déjà engagés en ce moment dans une large conversation publique

152
0:08:41.426,000 --> 0:08:43,000
sur la responsabilité des sociétés de technologies.

153
0:08:44.926,000 --> 0:08:47,000
J'ai déjà suggéré aux réseaux sociaux

154
0:08:47.982,000 --> 0:08:5,000
de changer leurs conditions d'utilisation et leurs règles d'usage

155
0:08:51.879,000 --> 0:08:53,000
pour interdire ces deepfakes dangereux.

156
0:08:54.712,000 --> 0:08:57,000
Cette détermination va nécessiter une évaluation par des humains

157
0:08:58.696,000 --> 0:08:59,000
et va coûter cher.

158
0:09:00.673,000 --> 0:09:02,000
Mais il faut que des humains

159
0:09:02.982,000 --> 0:09:05,000
étudient le contenu et le contexte d'un deepfake

160
0:09:06.879,000 --> 0:09:09,000
pour déterminer s'il s'agit une utilisation dangereuse

161
0:09:10.585,000 --> 0:09:14,000
ou, au contraire, d'une satire, d'une œuvre d'art ou d'un contenu éducatif.

162
0:09:16.118,000 --> 0:09:17,000
Et au fait, que dit la loi ?

163
0:09:18.666,000 --> 0:09:2,000
Nous devons apprendre de la loi.

164
0:09:21.515,000 --> 0:09:25,000
Elle nous dit ce qui est dangereux et ce qui est mal.

165
0:09:25.577,000 --> 0:09:29,000
Et elle réprime les comportements interdits en punissant les coupables

166
0:09:30.156,000 --> 0:09:32,000
et en prenant soin des victimes.

167
0:09:33.148,000 --> 0:09:37,000
A ce jour, la loi n'est pas à la hauteur du défi posé par les deepfakes.

168
0:09:38.116,000 --> 0:09:39,000
Dans le monde entier,

169
0:09:39.53,000 --> 0:09:41,000
on manque de lois adaptées

170
0:09:41.998,000 --> 0:09:44,000
qui permettraient d'empêcher ces mises en scène numériques

171
0:09:45.592,000 --> 0:09:47,000
qui violent l'intimité,

172
0:09:47.847,000 --> 0:09:48,000
qui détruisent des réputations

173
0:09:49.284,000 --> 0:09:5,000
et qui causent des dommages émotionnels.

174
0:09:51.725,000 --> 0:09:54,000
Ce qui est arrivé à Rana Ayyub devient de plus en plus courant.

175
0:09:56.074,000 --> 0:09:58,000
Quand elle s'est rendue à la police à Delhi,

176
0:09:58.312,000 --> 0:10:,000
on lui a dit qu'on ne pouvait rien faire.

177
0:10:01.101,000 --> 0:10:04,000
Et la triste vérité est que c'est la même chose

178
0:10:04.308,000 --> 0:10:06,000
aux États-Unis et en Europe.

179
0:10:07.3,000 --> 0:10:11,000
Il y a là un vide juridique qu'il faut combler.

180
0:10:12.292,000 --> 0:10:16,000
Ma collègue le Dr Mary Anne Franks et moi travaillons avec le législateur étasunien

181
0:10:16.408,000 --> 0:10:17,000
pour mettre sur pied des lois

182
0:10:18.2,000 --> 0:10:21,000
qui interdiraient ces mises en scène numériques blessantes

183
0:10:21.236,000 --> 0:10:23,000
qui s'apparentent à du vol d'identité.

184
0:10:24.252,000 --> 0:10:26,000
On constate des démarches similaires

185
0:10:26.402,000 --> 0:10:29,000
en Islande, au Royaume-Uni et en Australie.

186
0:10:30.157,000 --> 0:10:33,000
Mais bien sûr, il ne s'agit que d'une petite pièce dans le puzzle réglementaire.

187
0:10:34.911,000 --> 0:10:37,000
Je sais bien que la loi n'est pas la panacée.

188
0:10:38.104,000 --> 0:10:39,000
C'est un instrument brutal.

189
0:10:40.346,000 --> 0:10:41,000
Nous devons nous en servir subtilement.

190
0:10:42.411,000 --> 0:10:44,000
Il y a également quelques problèmes pratiques.

191
0:10:45.657,000 --> 0:10:47,000
On ne peut pas exercer de sanctions contre quelqu'un

192
0:10:48.179,000 --> 0:10:5,000
qu'on ne peut pas identifier ni trouver.

193
0:10:51.463,000 --> 0:10:54,000
Et si un coupable vit en dehors du pays

194
0:10:54.773,000 --> 0:10:55,000
où vit la victime,

195
0:10:56.551,000 --> 0:10:57,000
il pourrait être impossible

196
0:10:58.204,000 --> 0:11:,000
que le coupable comparaisse devant la justice

197
0:11:00.577,000 --> 0:11:01,000
dans le pays de la victime.

198
0:11:02.236,000 --> 0:11:06,000
Nous allons donc avoir besoin d'une réponse internationale coordonnée.

199
0:11:07.819,000 --> 0:11:1,000
Sans oublier l'éducation.

200
0:11:11.803,000 --> 0:11:16,000
Les forces de l'ordre n'interviendront pas si elles ne connaissent pas ces lois

201
0:11:17.04,000 --> 0:11:19,000
et ne régleront pas des problèmes qu'elles ne comprennent pas.

202
0:11:20.376,000 --> 0:11:22,000
Dans mes recherches sur le cyberharcèlement,

203
0:11:22.591,000 --> 0:11:25,000
j'ai constaté que les forces de l'ordre n'avaient pas la formation

204
0:11:26.114,000 --> 0:11:28,000
pour comprendre les lois dont elles disposent

205
0:11:28.72,000 --> 0:11:3,000
et les problèmes sur Internet.

206
0:11:31.093,000 --> 0:11:33,000
Et elles disent souvent aux victimes :

207
0:11:33.799,000 --> 0:11:36,000
« Éteignez votre ordinateur. Ignorez ça. Ça va disparaître. »

208
0:11:38.261,000 --> 0:11:4,000
C'est qu'on a vu dans le cas de Rana Ayyub.

209
0:11:41.102,000 --> 0:11:44,000
On lui a dit : « Allez, vous faites toute une histoire.

210
0:11:44.594,000 --> 0:11:46,000
C'est juste des garçons qui s'amusent. »

211
0:11:47.268,000 --> 0:11:52,000
Il faut donc accompagner les nouvelles lois avec de la formation.

212
0:11:54.053,000 --> 0:11:57,000
Cette formation doit aussi être dispensée aux media.

213
0:11:58.18,000 --> 0:12:02,000
Les journalistes doivent être formés au phénomène des deepfakes,

214
0:12:02.464,000 --> 0:12:05,000
pour qu'ils ne relaient pas, ni ne les amplifient.

215
0:12:06.583,000 --> 0:12:08,000
Et c'est là où nous sommes tous concernés.

216
0:12:08.775,000 --> 0:12:11,000
Chacun d'entre nous doit être formé.

217
0:12:13.375,000 --> 0:12:16,000
On clique, on partage, on like sans même y réfléchir.

218
0:12:17.551,000 --> 0:12:18,000
Nous devons nous améliorer.

219
0:12:19.726,000 --> 0:12:21,000
Nous avons besoin de meilleurs radars contre les faux.

220
0:12:25.744,000 --> 0:12:28,000
Et pendant que nous essayerons d'y voir clair,

221
0:12:29.609,000 --> 0:12:31,000
il y aura encore beaucoup de souffrance générée.

222
0:12:33.093,000 --> 0:12:36,000
Rana Ayyub se bat toujours contre les conséquences du deepfake.

223
0:12:36.669,000 --> 0:12:38,000
Elle ne se sent toujours pas libre de s'exprimer -

224
0:12:39.433,000 --> 0:12:4,000
en ligne et dans la vraie vie.

225
0:12:41.566,000 --> 0:12:42,000
Elle m'a dit

226
0:12:42.955,000 --> 0:12:47,000
qu'elle avait toujours l'impression que des milliers d'yeux la voyaient nue,

227
0:12:48.053,000 --> 0:12:51,000
même si, intellectuellement, elle sait que ce n'était pas son corps.

228
0:12:52.371,000 --> 0:12:54,000
Elle a souvent des crises de panique,

229
0:12:54.744,000 --> 0:12:58,000
surtout quand quelqu'un qu'elle ne connaît pas essaie de la prendre en photo.

230
0:12:58.868,000 --> 0:13:01,000
« Est-ce qu'ils vont en faire un autre deepfake ? » se demande-t-elle.

231
0:13:03.082,000 --> 0:13:06,000
Et donc, pour le bien de personnes comme Rana Ayyub,

232
0:13:07.027,000 --> 0:13:09,000
et pour le bien de nos démocraties,

233
0:13:09.357,000 --> 0:13:11,000
nous devons agir maintenant.

234
0:13:11.563,000 --> 0:13:12,000
Merci.

235
0:13:12.738,000 --> 0:13:14,000
(Applaudissements)

