1
0:00:25,000 --> 0:00:28,000
I want to talk today about --

2
0:00:28,000 --> 0:00:34,000
I've been asked to take the long view, and I'm going to tell you what

3
0:00:34,000 --> 0:00:38,000
I think are the three biggest problems for humanity

4
0:00:38,000 --> 0:00:41,000
from this long point of view.

5
0:00:41,000 --> 0:00:44,000
Some of these have already been touched upon by other speakers,

6
0:00:44,000 --> 0:00:46,000
which is encouraging.

7
0:00:46,000 --> 0:00:48,000
It seems that there's not just one person

8
0:00:48,000 --> 0:00:5,000
who thinks that these problems are important.

9
0:00:5,000 --> 0:00:54,000
The first is -- death is a big problem.

10
0:00:54,000 --> 0:00:57,000
If you look at the statistics,

11
0:00:57,000 --> 0:00:59,000
the odds are not very favorable to us.

12
0:00:59,000 --> 0:01:03,000
So far, most people who have lived have also died.

13
0:01:03,000 --> 0:01:07,000
Roughly 90 percent of everybody who has been alive has died by now.

14
0:01:07,000 --> 0:01:13,000
So the annual death rate adds up to 150,000 --

15
0:01:13,000 --> 0:01:16,000
sorry, the daily death rate -- 150,000 people per day,

16
0:01:16,000 --> 0:01:19,000
which is a huge number by any standard.

17
0:01:19,000 --> 0:01:24,000
The annual death rate, then, becomes 56 million.

18
0:01:24,000 --> 0:01:29,000
If we just look at the single, biggest cause of death -- aging --

19
0:01:3,000 --> 0:01:35,000
it accounts for roughly two-thirds of all human people who die.

20
0:01:35,000 --> 0:01:38,000
That adds up to an annual death toll

21
0:01:38,000 --> 0:01:4,000
of greater than the population of Canada.

22
0:01:4,000 --> 0:01:42,000
Sometimes, we don't see a problem

23
0:01:42,000 --> 0:01:46,000
because either it's too familiar or it's too big.

24
0:01:46,000 --> 0:01:48,000
Can't see it because it's too big.

25
0:01:48,000 --> 0:01:51,000
I think death might be both too familiar and too big

26
0:01:51,000 --> 0:01:54,000
for most people to see it as a problem.

27
0:01:54,000 --> 0:01:56,000
Once you think about it, you see this is not statistical points;

28
0:01:56,000 --> 0:01:58,000
these are -- let's see, how far have I talked?

29
0:01:58,000 --> 0:02:01,000
I've talked for three minutes.

30
0:02:01,000 --> 0:02:08,000
So that would be, roughly, 324 people have died since I've begun speaking.

31
0:02:08,000 --> 0:02:12,000
People like -- it's roughly the population in this room has just died.

32
0:02:13,000 --> 0:02:15,000
Now, the human cost of that is obvious,

33
0:02:15,000 --> 0:02:18,000
once you start to think about it -- the suffering, the loss --

34
0:02:18,000 --> 0:02:21,000
it's also, economically, enormously wasteful.

35
0:02:21,000 --> 0:02:24,000
I just look at the information, and knowledge, and experience

36
0:02:24,000 --> 0:02:27,000
that is lost due to natural causes of death in general,

37
0:02:27,000 --> 0:02:29,000
and aging, in particular.

38
0:02:29,000 --> 0:02:32,000
Suppose we approximated one person with one book?

39
0:02:32,000 --> 0:02:34,000
Now, of course, this is an underestimation.

40
0:02:34,000 --> 0:02:4,000
A person's lifetime of learning and experience

41
0:02:4,000 --> 0:02:42,000
is a lot more than you could put into a single book.

42
0:02:42,000 --> 0:02:44,000
But let's suppose we did this.

43
0:02:45,000 --> 0:02:5,000
52 million people die of natural causes each year

44
0:02:5,000 --> 0:02:54,000
corresponds, then, to 52 million volumes destroyed.

45
0:02:54,000 --> 0:02:57,000
Library of Congress holds 18 million volumes.

46
0:02:58,000 --> 0:03:01,000
We are upset about the burning of the Library of Alexandria.

47
0:03:01,000 --> 0:03:03,000
It's one of the great cultural tragedies

48
0:03:03,000 --> 0:03:06,000
that we remember, even today.

49
0:03:07,000 --> 0:03:09,000
But this is the equivalent of three Libraries of Congress --

50
0:03:09,000 --> 0:03:12,000
burnt down, forever lost -- each year.

51
0:03:12,000 --> 0:03:14,000
So that's the first big problem.

52
0:03:14,000 --> 0:03:17,000
And I wish Godspeed to Aubrey de Grey,

53
0:03:17,000 --> 0:03:19,000
and other people like him,

54
0:03:19,000 --> 0:03:22,000
to try to do something about this as soon as possible.

55
0:03:23,000 --> 0:03:26,000
Existential risk -- the second big problem.

56
0:03:26,000 --> 0:03:33,000
Existential risk is a threat to human survival, or to the long-term potential of our species.

57
0:03:33,000 --> 0:03:35,000
Now, why do I say that this is a big problem?

58
0:03:35,000 --> 0:03:39,000
Well, let's first look at the probability --

59
0:03:39,000 --> 0:03:42,000
and this is very, very difficult to estimate --

60
0:03:42,000 --> 0:03:45,000
but there have been only four studies on this in recent years,

61
0:03:45,000 --> 0:03:47,000
which is surprising.

62
0:03:47,000 --> 0:03:5,000
You would think that it would be of some interest

63
0:03:5,000 --> 0:03:54,000
to try to find out more about this given that the stakes are so big,

64
0:03:54,000 --> 0:03:56,000
but it's a very neglected area.

65
0:03:56,000 --> 0:03:58,000
But there have been four studies --

66
0:03:58,000 --> 0:04:,000
one by John Lesley, wrote a book on this.

67
0:04:,000 --> 0:04:02,000
He estimated a probability that we will fail

68
0:04:02,000 --> 0:04:05,000
to survive the current century: 50 percent.

69
0:04:05,000 --> 0:04:1,000
Similarly, the Astronomer Royal, whom we heard speak yesterday,

70
0:04:1,000 --> 0:04:13,000
also has a 50 percent probability estimate.

71
0:04:13,000 --> 0:04:16,000
Another author doesn't give any numerical estimate,

72
0:04:16,000 --> 0:04:19,000
but says the probability is significant that it will fail.

73
0:04:19,000 --> 0:04:22,000
I wrote a long paper on this.

74
0:04:22,000 --> 0:04:26,000
I said assigning a less than 20 percent probability would be a mistake

75
0:04:26,000 --> 0:04:29,000
in light of the current evidence we have.

76
0:04:29,000 --> 0:04:31,000
Now, the exact figures here,

77
0:04:31,000 --> 0:04:33,000
we should take with a big grain of salt,

78
0:04:33,000 --> 0:04:36,000
but there seems to be a consensus that the risk is substantial.

79
0:04:36,000 --> 0:04:39,000
Everybody who has looked at this and studied it agrees.

80
0:04:39,000 --> 0:04:41,000
Now, if we think about what just reducing

81
0:04:41,000 --> 0:04:46,000
the probability of human extinction by just one percentage point --

82
0:04:46,000 --> 0:04:51,000
not very much -- so that's equivalent to 60 million lives saved,

83
0:04:51,000 --> 0:04:55,000
if we just count the currently living people, the current generation.

84
0:04:55,000 --> 0:04:59,000
Now one percent of six billion people is equivalent to 60 million.

85
0:04:59,000 --> 0:05:01,000
So that's a large number.

86
0:05:01,000 --> 0:05:04,000
If we were to take into account future generations

87
0:05:04,000 --> 0:05:09,000
that will never come into existence if we blow ourselves up,

88
0:05:09,000 --> 0:05:12,000
then the figure becomes astronomical.

89
0:05:12,000 --> 0:05:15,000
If we could eventually colonize a chunk of the universe --

90
0:05:15,000 --> 0:05:17,000
the Virgo supercluster --

91
0:05:17,000 --> 0:05:19,000
maybe it will take us 100 million years to get there,

92
0:05:19,000 --> 0:05:22,000
but if we go extinct we never will.

93
0:05:22,000 --> 0:05:25,000
Then, even a one percentage point reduction

94
0:05:25,000 --> 0:05:29,000
in the extinction risk could be equivalent

95
0:05:29,000 --> 0:05:32,000
to this astronomical number -- 10 to the power of 32.

96
0:05:32,000 --> 0:05:36,000
So if you take into account future generations as much as our own,

97
0:05:36,000 --> 0:05:41,000
every other moral imperative of philanthropic cost just becomes irrelevant.

98
0:05:41,000 --> 0:05:43,000
The only thing you should focus on

99
0:05:43,000 --> 0:05:45,000
would be to reduce existential risk

100
0:05:45,000 --> 0:05:49,000
because even the tiniest decrease in existential risk

101
0:05:49,000 --> 0:05:53,000
would just overwhelm any other benefit you could hope to achieve.

102
0:05:53,000 --> 0:05:55,000
And even if you just look at the current people,

103
0:05:55,000 --> 0:06:,000
and ignore the potential that would be lost if we went extinct,

104
0:06:,000 --> 0:06:02,000
it should still have a high priority.

105
0:06:02,000 --> 0:06:07,000
Now, let me spend the rest of my time on the third big problem,

106
0:06:07,000 --> 0:06:12,000
because it's more subtle and perhaps difficult to grasp.

107
0:06:13,000 --> 0:06:17,000
Think about some time in your life --

108
0:06:17,000 --> 0:06:2,000
some people might never have experienced it -- but some people,

109
0:06:2,000 --> 0:06:23,000
there are just those moments that you have experienced

110
0:06:23,000 --> 0:06:25,000
where life was fantastic.

111
0:06:25,000 --> 0:06:32,000
It might have been at the moment of some great, creative inspiration

112
0:06:32,000 --> 0:06:34,000
you might have had when you just entered this flow stage.

113
0:06:34,000 --> 0:06:36,000
Or when you understood something you had never done before.

114
0:06:36,000 --> 0:06:4,000
Or perhaps in the ecstasy of romantic love.

115
0:06:4,000 --> 0:06:45,000
Or an aesthetic experience -- a sunset or a great piece of art.

116
0:06:45,000 --> 0:06:47,000
Every once in a while we have these moments,

117
0:06:47,000 --> 0:06:51,000
and we realize just how good life can be when it's at its best.

118
0:06:51,000 --> 0:06:56,000
And you wonder, why can't it be like that all the time?

119
0:06:56,000 --> 0:06:58,000
You just want to cling onto this.

120
0:06:58,000 --> 0:07:02,000
And then, of course, it drifts back into ordinary life and the memory fades.

121
0:07:02,000 --> 0:07:06,000
And it's really difficult to recall, in a normal frame of mind,

122
0:07:06,000 --> 0:07:09,000
just how good life can be at its best.

123
0:07:09,000 --> 0:07:12,000
Or how bad it can be at its worst.

124
0:07:12,000 --> 0:07:15,000
The third big problem is that life isn't usually

125
0:07:15,000 --> 0:07:17,000
as wonderful as it could be.

126
0:07:17,000 --> 0:07:21,000
I think that's a big, big problem.

127
0:07:21,000 --> 0:07:23,000
It's easy to say what we don't want.

128
0:07:24,000 --> 0:07:27,000
Here are a number of things that we don't want --

129
0:07:27,000 --> 0:07:3,000
illness, involuntary death, unnecessary suffering, cruelty,

130
0:07:3,000 --> 0:07:35,000
stunted growth, memory loss, ignorance, absence of creativity.

131
0:07:36,000 --> 0:07:39,000
Suppose we fixed these things -- we did something about all of these.

132
0:07:39,000 --> 0:07:41,000
We were very successful.

133
0:07:41,000 --> 0:07:43,000
We got rid of all of these things.

134
0:07:43,000 --> 0:07:46,000
We might end up with something like this,

135
0:07:46,000 --> 0:07:5,000
which is -- I mean, it's a heck of a lot better than that.

136
0:07:5,000 --> 0:07:55,000
But is this really the best we can dream of?

137
0:07:55,000 --> 0:07:57,000
Is this the best we can do?

138
0:07:57,000 --> 0:08:03,000
Or is it possible to find something a little bit more inspiring to work towards?

139
0:08:03,000 --> 0:08:05,000
And if we think about this,

140
0:08:05,000 --> 0:08:09,000
I think it's very clear that there are ways

141
0:08:09,000 --> 0:08:12,000
in which we could change things, not just by eliminating negatives,

142
0:08:12,000 --> 0:08:14,000
but adding positives.

143
0:08:14,000 --> 0:08:16,000
On my wish list, at least, would be:

144
0:08:16,000 --> 0:08:21,000
much longer, healthier lives, greater subjective well-being,

145
0:08:21,000 --> 0:08:26,000
enhanced cognitive capacities, more knowledge and understanding,

146
0:08:26,000 --> 0:08:28,000
unlimited opportunity for personal growth

147
0:08:28,000 --> 0:08:32,000
beyond our current biological limits, better relationships,

148
0:08:32,000 --> 0:08:34,000
an unbounded potential for spiritual, moral

149
0:08:34,000 --> 0:08:36,000
and intellectual development.

150
0:08:36,000 --> 0:08:44,000
If we want to achieve this, what, in the world, would have to change?

151
0:08:44,000 --> 0:08:49,000
And this is the answer -- we would have to change.

152
0:08:49,000 --> 0:08:52,000
Not just the world around us, but we, ourselves.

153
0:08:52,000 --> 0:08:56,000
Not just the way we think about the world, but the way we are -- our very biology.

154
0:08:56,000 --> 0:08:58,000
Human nature would have to change.

155
0:08:58,000 --> 0:09:,000
Now, when we think about changing human nature,

156
0:09:,000 --> 0:09:02,000
the first thing that comes to mind

157
0:09:02,000 --> 0:09:06,000
are these human modification technologies --

158
0:09:06,000 --> 0:09:08,000
growth hormone therapy, cosmetic surgery,

159
0:09:08,000 --> 0:09:11,000
stimulants like Ritalin, Adderall, anti-depressants,

160
0:09:11,000 --> 0:09:13,000
anabolic steroids, artificial hearts.

161
0:09:13,000 --> 0:09:16,000
It's a pretty pathetic list.

162
0:09:16,000 --> 0:09:18,000
They do great things for a few people

163
0:09:18,000 --> 0:09:2,000
who suffer from some specific condition,

164
0:09:2,000 --> 0:09:25,000
but for most people, they don't really transform

165
0:09:25,000 --> 0:09:27,000
what it is to be human.

166
0:09:27,000 --> 0:09:29,000
And they also all seem a little bit --

167
0:09:29,000 --> 0:09:32,000
most people have this instinct that, well, sure,

168
0:09:32,000 --> 0:09:34,000
there needs to be anti-depressants for the really depressed people.

169
0:09:34,000 --> 0:09:36,000
But there's a kind of queasiness

170
0:09:36,000 --> 0:09:39,000
that these are unnatural in some way.

171
0:09:39,000 --> 0:09:41,000
It's worth recalling that there are a lot of other

172
0:09:41,000 --> 0:09:44,000
modification technologies and enhancement technologies that we use.

173
0:09:44,000 --> 0:09:48,000
We have skin enhancements, clothing.

174
0:09:48,000 --> 0:09:52,000
As far as I can see, all of you are users of this

175
0:09:52,000 --> 0:09:57,000
enhancement technology in this room, so that's a great thing.

176
0:09:57,000 --> 0:10:,000
Mood modifiers have been used from time immemorial --

177
0:10:,000 --> 0:10:05,000
caffeine, alcohol, nicotine, immune system enhancement,

178
0:10:05,000 --> 0:10:07,000
vision enhancement, anesthetics --

179
0:10:07,000 --> 0:10:09,000
we take that very much for granted,

180
0:10:09,000 --> 0:10:13,000
but just think about how great progress that is --

181
0:10:13,000 --> 0:10:17,000
like, having an operation before anesthetics was not fun.

182
0:10:17,000 --> 0:10:23,000
Contraceptives, cosmetics and brain reprogramming techniques --

183
0:10:23,000 --> 0:10:25,000
that sounds ominous,

184
0:10:25,000 --> 0:10:29,000
but the distinction between what is a technology --

185
0:10:29,000 --> 0:10:31,000
a gadget would be the archetype --

186
0:10:31,000 --> 0:10:35,000
and other ways of changing and rewriting human nature is quite subtle.

187
0:10:35,000 --> 0:10:39,000
So if you think about what it means to learn arithmetic or to learn to read,

188
0:10:39,000 --> 0:10:42,000
you're actually, literally rewriting your own brain.

189
0:10:42,000 --> 0:10:45,000
You're changing the microstructure of your brain as you go along.

190
0:10:46,000 --> 0:10:49,000
So in a broad sense, we don't need to think about technology

191
0:10:49,000 --> 0:10:51,000
as only little gadgets, like these things here,

192
0:10:51,000 --> 0:10:55,000
but even institutions and techniques,

193
0:10:55,000 --> 0:10:57,000
psychological methods and so forth.

194
0:10:57,000 --> 0:11:02,000
Forms of organization can have a profound impact on human nature.

195
0:11:02,000 --> 0:11:04,000
Looking ahead, there is a range of technologies

196
0:11:04,000 --> 0:11:07,000
that are almost certain to be developed sooner or later.

197
0:11:07,000 --> 0:11:11,000
We are very ignorant about what the time scale for these things are,

198
0:11:11,000 --> 0:11:13,000
but they all are consistent with everything we know

199
0:11:13,000 --> 0:11:17,000
about physical laws, laws of chemistry, etc.

200
0:11:17,000 --> 0:11:19,000
It's possible to assume,

201
0:11:19,000 --> 0:11:22,000
setting aside a possibility of catastrophe,

202
0:11:22,000 --> 0:11:25,000
that sooner or later we will develop all of these.

203
0:11:25,000 --> 0:11:28,000
And even just a couple of these would be enough

204
0:11:28,000 --> 0:11:3,000
to transform the human condition.

205
0:11:3,000 --> 0:11:35,000
So let's look at some of the dimensions of human nature

206
0:11:35,000 --> 0:11:38,000
that seem to leave room for improvement.

207
0:11:38,000 --> 0:11:4,000
Health span is a big and urgent thing,

208
0:11:4,000 --> 0:11:42,000
because if you're not alive,

209
0:11:42,000 --> 0:11:45,000
then all the other things will be to little avail.

210
0:11:45,000 --> 0:11:47,000
Intellectual capacity -- let's take that box,

211
0:11:47,000 --> 0:11:52,000
which falls into a lot of different sub-categories:

212
0:11:52,000 --> 0:11:55,000
memory, concentration, mental energy, intelligence, empathy.

213
0:11:55,000 --> 0:11:57,000
These are really great things.

214
0:11:57,000 --> 0:11:59,000
Part of the reason why we value these traits

215
0:11:59,000 --> 0:12:03,000
is that they make us better at competing with other people --

216
0:12:03,000 --> 0:12:05,000
they're positional goods.

217
0:12:05,000 --> 0:12:07,000
But part of the reason --

218
0:12:07,000 --> 0:12:11,000
and that's the reason why we have ethical ground for pursuing these --

219
0:12:11,000 --> 0:12:14,000
is that they're also intrinsically valuable.

220
0:12:14,000 --> 0:12:18,000
It's just better to be able to understand more of the world around you

221
0:12:18,000 --> 0:12:2,000
and the people that you are communicating with,

222
0:12:2,000 --> 0:12:24,000
and to remember what you have learned.

223
0:12:24,000 --> 0:12:26,000
Modalities and special faculties.

224
0:12:26,000 --> 0:12:31,000
Now, the human mind is not a single unitary information processor,

225
0:12:31,000 --> 0:12:35,000
but it has a lot of different, special, evolved modules

226
0:12:35,000 --> 0:12:37,000
that do specific things for us.

227
0:12:37,000 --> 0:12:41,000
If you think about what we normally take as giving life a lot of its meaning --

228
0:12:41,000 --> 0:12:45,000
music, humor, eroticism, spirituality, aesthetics,

229
0:12:45,000 --> 0:12:5,000
nurturing and caring, gossip, chatting with people --

230
0:12:5,000 --> 0:12:54,000
all of these, very likely, are enabled by a special circuitry

231
0:12:54,000 --> 0:12:56,000
that we humans have,

232
0:12:56,000 --> 0:12:59,000
but that you could have another intelligent life form that lacks these.

233
0:12:59,000 --> 0:13:02,000
We're just lucky that we have the requisite neural machinery

234
0:13:02,000 --> 0:13:06,000
to process music and to appreciate it and enjoy it.

235
0:13:06,000 --> 0:13:09,000
All of these would enable, in principle -- be amenable to enhancement.

236
0:13:09,000 --> 0:13:11,000
Some people have a better musical ability

237
0:13:11,000 --> 0:13:13,000
and ability to appreciate music than others have.

238
0:13:13,000 --> 0:13:16,000
It's also interesting to think about what other things are --

239
0:13:16,000 --> 0:13:19,000
so if these all enabled great values,

240
0:13:2,000 --> 0:13:23,000
why should we think that evolution has happened to provide us

241
0:13:23,000 --> 0:13:26,000
with all the modalities we would need to engage

242
0:13:26,000 --> 0:13:28,000
with other values that there might be?

243
0:13:28,000 --> 0:13:3,000
Imagine a species

244
0:13:3,000 --> 0:13:34,000
that just didn't have this neural machinery for processing music.

245
0:13:34,000 --> 0:13:37,000
And they would just stare at us with bafflement

246
0:13:37,000 --> 0:13:41,000
when we spend time listening to a beautiful performance,

247
0:13:41,000 --> 0:13:43,000
like the one we just heard -- because of people making stupid movements,

248
0:13:43,000 --> 0:13:46,000
and they would be really irritated and wouldn't see what we were up to.

249
0:13:46,000 --> 0:13:49,000
But maybe they have another faculty, something else

250
0:13:49,000 --> 0:13:52,000
that would seem equally irrational to us,

251
0:13:52,000 --> 0:13:55,000
but they actually tap into some great possible value there.

252
0:13:55,000 --> 0:13:59,000
But we are just literally deaf to that kind of value.

253
0:13:59,000 --> 0:14:01,000
So we could think of adding on different,

254
0:14:01,000 --> 0:14:05,000
new sensory capacities and mental faculties.

255
0:14:05,000 --> 0:14:1,000
Bodily functionality and morphology and affective self-control.

256
0:14:1,000 --> 0:14:12,000
Greater subjective well-being.

257
0:14:12,000 --> 0:14:15,000
Be able to switch between relaxation and activity --

258
0:14:15,000 --> 0:14:19,000
being able to go slow when you need to do that, and to speed up.

259
0:14:19,000 --> 0:14:21,000
Able to switch back and forth more easily

260
0:14:21,000 --> 0:14:23,000
would be a neat thing to be able to do --

261
0:14:23,000 --> 0:14:25,000
easier to achieve the flow state,

262
0:14:25,000 --> 0:14:29,000
when you're totally immersed in something you are doing.

263
0:14:29,000 --> 0:14:31,000
Conscientiousness and sympathy.

264
0:14:31,000 --> 0:14:34,000
The ability to -- it's another interesting application

265
0:14:34,000 --> 0:14:37,000
that would have large social ramification, perhaps.

266
0:14:37,000 --> 0:14:43,000
If you could actually choose to preserve your romantic attachments to one person,

267
0:14:43,000 --> 0:14:45,000
undiminished through time,

268
0:14:45,000 --> 0:14:48,000
so that wouldn't have to -- love would never have to fade if you didn't want it to.

269
0:14:5,000 --> 0:14:53,000
That's probably not all that difficult.

270
0:14:53,000 --> 0:14:56,000
It might just be a simple hormone or something that could do this.

271
0:14:58,000 --> 0:15:,000
It's been done in voles.

272
0:15:02,000 --> 0:15:05,000
You can engineer a prairie vole to become monogamous

273
0:15:05,000 --> 0:15:07,000
when it's naturally polygamous.

274
0:15:07,000 --> 0:15:09,000
It's just a single gene.

275
0:15:09,000 --> 0:15:11,000
Might be more complicated in humans, but perhaps not that much.

276
0:15:11,000 --> 0:15:13,000
This is the last picture that I want to --

277
0:15:14,000 --> 0:15:16,000
now we've got to use the laser pointer.

278
0:15:17,000 --> 0:15:2,000
A possible mode of being here would be a way of life --

279
0:15:2,000 --> 0:15:24,000
a way of being, experiencing, thinking, seeing,

280
0:15:24,000 --> 0:15:26,000
interacting with the world.

281
0:15:26,000 --> 0:15:31,000
Down here in this little corner, here, we have the little sub-space

282
0:15:31,000 --> 0:15:35,000
of this larger space that is accessible to human beings --

283
0:15:35,000 --> 0:15:38,000
beings with our biological capacities.

284
0:15:38,000 --> 0:15:41,000
It's a part of the space that's accessible to animals;

285
0:15:41,000 --> 0:15:44,000
since we are animals, we are a subset of that.

286
0:15:44,000 --> 0:15:48,000
And then you can imagine some enhancements of human capacities.

287
0:15:48,000 --> 0:15:51,000
There would be different modes of being you could experience

288
0:15:51,000 --> 0:15:54,000
if you were able to stay alive for, say, 200 years.

289
0:15:54,000 --> 0:15:58,000
Then you could live sorts of lives and accumulate wisdoms

290
0:15:58,000 --> 0:16:01,000
that are just not possible for humans as we currently are.

291
0:16:01,000 --> 0:16:05,000
So then, you move off to this larger sphere of "human +,"

292
0:16:05,000 --> 0:16:08,000
and you could continue that process and eventually

293
0:16:08,000 --> 0:16:12,000
explore a lot of this larger space of possible modes of being.

294
0:16:12,000 --> 0:16:14,000
Now, why is that a good thing to do?

295
0:16:14,000 --> 0:16:18,000
Well, we know already that in this little human circle there,

296
0:16:18,000 --> 0:16:22,000
there are these enormously wonderful and worthwhile modes of being --

297
0:16:22,000 --> 0:16:25,000
human life at its best is wonderful.

298
0:16:25,000 --> 0:16:3,000
We have no reason to believe that within this much, much larger space

299
0:16:3,000 --> 0:16:34,000
there would not also be extremely worthwhile modes of being,

300
0:16:34,000 --> 0:16:4,000
perhaps ones that would be way beyond our wildest ability

301
0:16:4,000 --> 0:16:42,000
even to imagine or dream about.

302
0:16:42,000 --> 0:16:44,000
And so, to fix this third problem,

303
0:16:44,000 --> 0:16:5,000
I think we need -- slowly, carefully, with ethical wisdom and constraint --

304
0:16:5,000 --> 0:16:55,000
develop the means that enable us to go out in this larger space and explore it

305
0:16:55,000 --> 0:16:57,000
and find the great values that might hide there.

306
0:16:57,000 --> 0:16:59,000
Thanks.

