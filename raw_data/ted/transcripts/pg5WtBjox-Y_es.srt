1
0:00:,000 --> 0:00:07,000
Traductor: Elisa Tortosa Revisor: Sebastian Betti

2
0:00:12.535,000 --> 0:00:14,000
[Esta charla tiene contenido adulto]

3
0:00:17.762,000 --> 0:00:19,000
Rana Ayyub es periodista en India

4
0:00:20.778,000 --> 0:00:22,000
y ha destapado casos de corrupción del gobierno

5
0:00:24.411,000 --> 0:00:26,000
y violaciones de los derechos humanos.

6
0:00:26.99,000 --> 0:00:27,000
Con los años

7
0:00:28.181,000 --> 0:00:31,000
se acostumbró a la fuerte crítica y a la polémica de su trabajo.

8
0:00:32.149,000 --> 0:00:37,000
Pero nada podría haberla preparado para lo que enfrentó en abril de 2018.

9
0:00:38.125,000 --> 0:00:41,000
Ella estaba en una cafetería con una amiga la primera vez que lo vio.

10
0:00:41.8,000 --> 0:00:45,000
Un vídeo de 2 minutos y 20 segundos de ella en un acto sexual.

11
0:00:47.188,000 --> 0:00:49,000
Y ella no se lo podía creer.

12
0:00:49.561,000 --> 0:00:51,000
Ella nunca había hecho un vídeo sexual.

13
0:00:52.506,000 --> 0:00:55,000
Pero por desgracia miles y miles de personas

14
0:00:55.995,000 --> 0:00:56,000
creerían que era ella.

15
0:00:58.673,000 --> 0:01:,000
Entrevisté a Ayyub hace tres meses

16
0:01:01.641,000 --> 0:01:03,000
en relación con mi libro de privacidad sexual.

17
0:01:04.681,000 --> 0:01:07,000
Soy catedrática en Derecho, abogada y defensora de los derechos civiles,

18
0:01:08.204,000 --> 0:01:12,000
así que es frustra mucho saber que ahora mismo

19
0:01:12.839,000 --> 0:01:14,000
la ley podría hacer muy poco para ayudarla.

20
0:01:15.458,000 --> 0:01:16,000
Y mientras hablábamos

21
0:01:17.029,000 --> 0:01:21,000
me explicó que nunca vio venir el falso vídeo sexual.

22
0:01:22.038,000 --> 0:01:27,000
Dijo "el sexo se usa muchas veces para menospreciar y avergonzar a las mujeres

23
0:01:27.658,000 --> 0:01:29,000
en particular a las mujeres minoritarias

24
0:01:30.11,000 --> 0:01:34,000
y sobre todo a las que se atreven a desafiar a hombres poderosos",

25
0:01:34.446,000 --> 0:01:35,000
como los de su trabajo.

26
0:01:37.191,000 --> 0:01:4,000
El vídeo se hizo viral en 48 horas.

27
0:01:42.064,000 --> 0:01:47,000
Todas sus redes sociales se llenaron de capturas del vídeo

28
0:01:47.395,000 --> 0:01:49,000
con amenazas de violación y amenazas de muerte

29
0:01:50.046,000 --> 0:01:52,000
insultando su fe musulmana.

30
0:01:53.426,000 --> 0:01:57,000
Había publicaciones online que insinuaban que ella estaba "disponible" para sexo.

31
0:01:58.014,000 --> 0:01:59,000
Revelaron su información personal,

32
0:01:59.648,000 --> 0:02:01,000
es decir, su dirección y su número de teléfono

33
0:02:02.45,000 --> 0:02:03,000
se difundió por Internet.

34
0:02:04.879,000 --> 0:02:08,000
El vídeo se compartió más de 40 000 veces.

35
0:02:09.76,000 --> 0:02:12,000
Cuando alguien es el objetivo de este tipo de ataques de mafias online

36
0:02:13.72,000 --> 0:02:15,000
la herida es profunda.

37
0:02:16.482,000 --> 0:02:19,000
La vida de Rana Ayyub se puso patas arriba.

38
0:02:20.211,000 --> 0:02:23,000
Durante semanas, apenas podía comer y hablar.

39
0:02:23.919,000 --> 0:02:26,000
Dejó de escribir y cerró todas sus cuentas de las redes sociales,

40
0:02:27.632,000 --> 0:02:3,000
algo muy duro cuando eres periodista.

41
0:02:31.188,000 --> 0:02:34,000
Y tenía miedo de salir de su casa.

42
0:02:34.696,000 --> 0:02:37,000
¿Y si cumplían con las amenazas?

43
0:02:38.395,000 --> 0:02:42,000
El Consejo de Derechos Humanos de la ONU afirmó que no estaba loca.

44
0:02:42.784,000 --> 0:02:46,000
Publicó una declaración que decía que estaban preocupados por su seguridad.

45
0:02:48.776,000 --> 0:02:52,000
Rana Ayyub se enfrentaba a un deepfake:

46
0:02:53.029,000 --> 0:02:55,000
Tecnología de aprendizaje automático

47
0:02:55.593,000 --> 0:02:59,000
que manipula o fabrica audio y vídeos

48
0:02:59.728,000 --> 0:03:01,000
que muestran personas diciendo o haciendo cosas

49
0:03:02.475,000 --> 0:03:03,000
que nunca han dicho o hecho.

50
0:03:04.807,000 --> 0:03:07,000
Los deepfakes parecen auténticos y realistas, pero no lo son;

51
0:03:08.192,000 --> 0:03:09,000
son mentiras absolutas.

52
0:03:11.228,000 --> 0:03:14,000
Si bien es una tecnología en desarrollo,

53
0:03:15.046,000 --> 0:03:16,000
es de fácil acceso.

54
0:03:17.371,000 --> 0:03:2,000
Y como tantas cosas en Internet,

55
0:03:20.467,000 --> 0:03:22,000
los deepfakes llaman ahora la atención

56
0:03:22.652,000 --> 0:03:23,000
por la pornografía.

57
0:03:24.498,000 --> 0:03:26,000
A principios del 2018

58
0:03:26.633,000 --> 0:03:28,000
alguien publicó una herramienta en Reddit

59
0:03:29.125,000 --> 0:03:33,000
que permitía a los usuarios insertar caras en vídeos porno.

60
0:03:33.561,000 --> 0:03:36,000
A esto le siguió una lluvia de falsos vídeos porno protagonizados

61
0:03:37.025,000 --> 0:03:39,000
por las celebridades favoritas de las personas.

62
0:03:40.712,000 --> 0:03:43,000
Y hoy en YouTube se pueden encontrar infinitos tutoriales

63
0:03:44.213,000 --> 0:03:46,000
con instrucciones que enseñan paso a paso

64
0:03:46.523,000 --> 0:03:49,000
a hacer un deepfake en tu computadora.

65
0:03:50.26,000 --> 0:03:53,000
Y quizás pronto podamos hacerlos en nuestros móviles.

66
0:03:55.072,000 --> 0:04:,000
Ahora, la interacción de algunas de nuestras debilidades humanas más básicas

67
0:04:00.478,000 --> 0:04:01,000
con las herramientas de la red

68
0:04:02.184,000 --> 0:04:04,000
pueden convertir estos vídeos en armas.

69
0:04:04.874,000 --> 0:04:05,000
Les explicaré.

70
0:04:06.875,000 --> 0:04:1,000
Los seres humanos tenemos una respuesta instintiva a los audios y vídeos.

71
0:04:11.86,000 --> 0:04:12,000
Creemos que son verdad

72
0:04:13.372,000 --> 0:04:15,000
ya que pensamos que obviamente podemos creer

73
0:04:15.474,000 --> 0:04:17,000
lo que dicen los ojos y los oídos.

74
0:04:18.476,000 --> 0:04:19,000
Y este mecanismo

75
0:04:20.199,000 --> 0:04:23,000
puede debilitar nuestro sentido de la realidad.

76
0:04:23.921,000 --> 0:04:26,000
Aunque creamos que los deepfakes son verdad, no lo son.

77
0:04:27.604,000 --> 0:04:31,000
Nos atrae lo lascivo, lo provocativo.

78
0:04:32.365,000 --> 0:04:35,000
Tendemos a creer y compartir información

79
0:04:35.436,000 --> 0:04:37,000
negativa y novedosa.

80
0:04:37.809,000 --> 0:04:42,000
Los expertos dicen que online las falacias se difunden diez veces más rápido

81
0:04:42.852,000 --> 0:04:43,000
que las historias verídicas.

82
0:04:46.015,000 --> 0:04:5,000
Además, nos atrae la información

83
0:04:50.419,000 --> 0:04:51,000
más afín a nuestro punto de vista.

84
0:04:52.95,000 --> 0:04:55,000
Los psicólogos llaman a esta tendencia "sesgo de confirmación".

85
0:04:57.3,000 --> 0:05:01,000
Y las redes sociales sobrecargan esta tendencia

86
0:05:01.711,000 --> 0:05:04,000
al permitirnos compartir información afín a nuestra idea

87
0:05:05.616,000 --> 0:05:06,000
de una manera amplia e instantánea.

88
0:05:08.735,000 --> 0:05:13,000
Pero los deepfakes tienen la posibilidad de causar un gran daño individual y social.

89
0:05:15.204,000 --> 0:05:17,000
Imaginen uno que muestre

90
0:05:17.252,000 --> 0:05:21,000
a los soldados estadounidenses quemando un Corán en Afganistán.

91
0:05:22.807,000 --> 0:05:25,000
Podemos imaginar que este vídeo puede causar violencia

92
0:05:25.855,000 --> 0:05:26,000
contra esos soldados.

93
0:05:27.847,000 --> 0:05:29,000
¿Y si justo al día siguiente

94
0:05:30.744,000 --> 0:05:32,000
aparece otro deepfake

95
0:05:33.022,000 --> 0:05:36,000
que muestra a un conocido imán de Londres

96
0:05:36.363,000 --> 0:05:38,000
instando a atacar a esos soldados?

97
0:05:39.617,000 --> 0:05:42,000
Veríamos violencia y disturbios civiles.

98
0:05:42.804,000 --> 0:05:45,000
no solo en Afganistán y en Reino Unido

99
0:05:46.077,000 --> 0:05:47,000
sino en todo el mundo.

100
0:05:48.251,000 --> 0:05:49,000
Podrán decirme

101
0:05:49.433,000 --> 0:05:51,000
"Ya, Danielle, qué rebuscado".

102
0:05:51.704,000 --> 0:05:52,000
Pero no lo es.

103
0:05:53.293,000 --> 0:05:55,000
Hemos visto difusiones de mentiras

104
0:05:55.508,000 --> 0:05:57,000
por WhatsApp y por otros servicios de mensajería online

105
0:05:58.254,000 --> 0:06:,000
que han llevado a la violencia contra las minorías étnicas.

106
0:06:01.039,000 --> 0:06:02,000
Y solo era un texto.

107
0:06:02.95,000 --> 0:06:04,000
Imaginen si fuese un vídeo.

108
0:06:06.593,000 --> 0:06:09,000
Los deepfakes tienen el potencial de corroer la confianza

109
0:06:10.524,000 --> 0:06:13,000
que tenemos en las instituciones democráticas.

110
0:06:15.006,000 --> 0:06:17,000
Imaginen la noche de antes de unas elecciones.

111
0:06:17.996,000 --> 0:06:2,000
Un deepfake muestra a uno de los candidatos de un partido principal

112
0:06:21.258,000 --> 0:06:22,000
muy enfermo.

113
0:06:23.202,000 --> 0:06:25,000
El vídeo podría modificar las elecciones

114
0:06:25.559,000 --> 0:06:28,000
y cambiar nuestra opinión sobre la validez de las elecciones.

115
0:06:30.515,000 --> 0:06:33,000
Imaginen si la noche anterior a una oferta pública de venta

116
0:06:33.865,000 --> 0:06:35,000
de un gran banco mundial

117
0:06:36.222,000 --> 0:06:39,000
hubiese un deepfake mostrando al director ejecutivo

118
0:06:39.395,000 --> 0:06:41,000
hablando borracho sobre teorías conspirativas.

119
0:06:42.887,000 --> 0:06:45,000
El vídeo podría hundir la venta

120
0:06:45.958,000 --> 0:06:49,000
y peor, cambiar nuestra opinión sobre la estabilidad del mercado financiero.

121
0:06:51.385,000 --> 0:06:56,000
Los deepfakes pueden aprovecharse para aumentar la gran desconfianza

122
0:06:56.718,000 --> 0:07:01,000
que ya tenemos en los políticos, líderes de empresas y otros influyentes.

123
0:07:02.945,000 --> 0:07:05,000
Encuentran a un público listo para creérselos.

124
0:07:07.287,000 --> 0:07:09,000
La búsqueda de la verdad también está en peligro.

125
0:07:11.077,000 --> 0:07:14,000
Expertos en tecnología creen

126
0:07:14.665,000 --> 0:07:17,000
que con los avances de Inteligencia Artificial

127
0:07:18.371,000 --> 0:07:21,000
pronto será casi imposible diferenciar un vídeo real de uno falso.

128
0:07:23.022,000 --> 0:07:28,000
¿Cómo podría emerger la verdad en un mercado lleno de deepfakes?

129
0:07:28.752,000 --> 0:07:31,000
Si seguimos por el camino de no poner resistencia

130
0:07:32.196,000 --> 0:07:34,000
creyendo lo que queramos creer

131
0:07:34.657,000 --> 0:07:35,000
¿será el fin de la verdad?

132
0:07:36.831,000 --> 0:07:39,000
Quizás no solo creamos lo falso,

133
0:07:40.03,000 --> 0:07:43,000
quizás pongamos en duda la verdad.

134
0:07:43.887,000 --> 0:07:47,000
Ya hemos visto a personas que usan este fenómeno de deepfakes

135
0:07:47.99,000 --> 0:07:5,000
para cuestionar la evidencia tangible de sus acciones.

136
0:07:51.934,000 --> 0:07:56,000
Hemos oído a políticos decir sobre audios de comentarios alarmantes

137
0:07:57.927,000 --> 0:07:58,000
"Son noticias falsas.

138
0:07:59.697,000 --> 0:08:02,000
No podemos creer los que nos dicen nuestros ojos y oídos".

139
0:08:04.402,000 --> 0:08:05,000
El profesor Robert Chesney y yo

140
0:08:06.157,000 --> 0:08:11,000
llamamos a este peligro "recompensa del mentiroso"

141
0:08:11.617,000 --> 0:08:14,000
El temor de que los mentirosos usen deepfakes

142
0:08:14.998,000 --> 0:08:16,000
para escapar de la culpabilidad del delito.

143
0:08:18.963,000 --> 0:08:21,000
Tenemos mucho trabajo por delante, de eso no hay duda.

144
0:08:22.606,000 --> 0:08:25,000
Vamos a necesitar una solución proactiva

145
0:08:25.955,000 --> 0:08:28,000
de parte de empresas tecnológicas, legisladores,

146
0:08:29.49,000 --> 0:08:3,000
poder judicial y medios de comunicación.

147
0:08:32.093,000 --> 0:08:36,000
Necesitaremos una dosis saludable de resistencia social.

148
0:08:37.506,000 --> 0:08:4,000
Así que ahora estamos inmersos en una conversación muy pública

149
0:08:41.426,000 --> 0:08:43,000
sobre las responsabilidades de las empresas tecnológicas,

150
0:08:44.926,000 --> 0:08:47,000
mi consejo para las redes sociales

151
0:08:47.982,000 --> 0:08:5,000
es que cambien sus términos de uso y las normas comunitarias

152
0:08:51.879,000 --> 0:08:53,000
para prohibir deepfakes dañinos.

153
0:08:54.712,000 --> 0:08:57,000
Esta resolución requerirá el juicio humano

154
0:08:58.696,000 --> 0:08:59,000
y será costoso.

155
0:09:00.673,000 --> 0:09:02,000
Pero necesitamos supervisión humana

156
0:09:02.982,000 --> 0:09:05,000
del contenido y el contexto de un deepfake

157
0:09:06.879,000 --> 0:09:09,000
para ver si es una imitación perjudicial

158
0:09:10.585,000 --> 0:09:14,000
o si tiene un valor satírico, artístico o educativo.

159
0:09:16.118,000 --> 0:09:17,000
Y, ¿qué pasa con la ley?

160
0:09:18.666,000 --> 0:09:2,000
Debemos aprender de la ley.

161
0:09:21.515,000 --> 0:09:25,000
Nos enseña qué es perjudicial y qué es incorrecto.

162
0:09:25.577,000 --> 0:09:29,000
Reprime comportamientos prohibidos castigando a los responsables

163
0:09:30.156,000 --> 0:09:32,000
y cuidando a las víctimas.

164
0:09:33.148,000 --> 0:09:37,000
Ahora mismo la ley no está preparada para el desafío de los deepfakes.

165
0:09:38.116,000 --> 0:09:39,000
En todo el mundo

166
0:09:39.53,000 --> 0:09:41,000
necesitamos leyes hechas a medida

167
0:09:41.998,000 --> 0:09:44,000
diseñadas para destruir las suplantaciones digitales

168
0:09:45.592,000 --> 0:09:47,000
que invaden la privacidad sexual,

169
0:09:47.847,000 --> 0:09:48,000
que dañan reputaciones

170
0:09:49.258,000 --> 0:09:5,000
y causan perjuicio emocional.

171
0:09:51.725,000 --> 0:09:54,000
Lo que le ocurrió a Rana Ayyub cada vez es más común.

172
0:09:56.074,000 --> 0:09:58,000
Incluso cuando fue a la policía en Delhi

173
0:09:58.312,000 --> 0:10:,000
le dijeron que no podía hacer nada.

174
0:10:01.101,000 --> 0:10:04,000
Y es triste pero verdad que ocurriría lo mismo

175
0:10:04.308,000 --> 0:10:06,000
en Estados Unidos y en Europa.

176
0:10:07.3,000 --> 0:10:11,000
Así que hay un vacío legal por llenar.

177
0:10:12.292,000 --> 0:10:16,000
Mi compañera la doctora Mary Anne Franks y yo trabajamos con legisladores de EE.UU.

178
0:10:16.408,000 --> 0:10:2,000
para crear leyes que prohíban las suplantaciones digitales dañinas

179
0:10:21.236,000 --> 0:10:23,000
equivalentes a la suplantación de identidad.

180
0:10:24.252,000 --> 0:10:26,000
Y hemos visto los mismos movimientos

181
0:10:26.402,000 --> 0:10:29,000
en Islandia, Reino Unido y Australia.

182
0:10:30.157,000 --> 0:10:33,000
Pero por supuesto, esto es solo una gota en el mar.

183
0:10:34.911,000 --> 0:10:37,000
Sé que la ley no es el remedio universal.

184
0:10:38.104,000 --> 0:10:39,000
Es un instrumento contundente.

185
0:10:40.346,000 --> 0:10:41,000
Y tenemos que usarlo con cabeza.

186
0:10:42.411,000 --> 0:10:44,000
También tiene algunos impedimentos prácticos.

187
0:10:45.657,000 --> 0:10:5,000
No se puede sancionar a alguien si no se lo puede identificar ni encontrar.

188
0:10:51.463,000 --> 0:10:54,000
Y si el infractor vive en otro país

189
0:10:54.773,000 --> 0:10:55,000
diferente al de la víctima

190
0:10:56.551,000 --> 0:10:57,000
quizás no se le pueda exigir

191
0:10:58.204,000 --> 0:11:,000
al infractor que acceda a cumplir las leyes

192
0:11:00.577,000 --> 0:11:01,000
de su país.

193
0:11:02.236,000 --> 0:11:06,000
Así que necesitaremos una respuesta internacional coordinada.

194
0:11:07.819,000 --> 0:11:1,000
La educación tiene que ser también parte de nuestra respuesta.

195
0:11:11.803,000 --> 0:11:14,000
Las fuerzas del orden no van a imponer leyes

196
0:11:15.558,000 --> 0:11:16,000
que no conozcan

197
0:11:17.04,000 --> 0:11:19,000
y juzgar problemas que no entiendan.

198
0:11:20.376,000 --> 0:11:22,000
En mi investigación sobre el ciberacoso

199
0:11:22.591,000 --> 0:11:25,000
he descubierto que el poder judicial carece de experiencia

200
0:11:26.114,000 --> 0:11:28,000
para entender las leyes

201
0:11:28.72,000 --> 0:11:3,000
y el problema del acoso online.

202
0:11:31.093,000 --> 0:11:33,000
Y muy a menudo le dicen a las víctimas

203
0:11:33.799,000 --> 0:11:36,000
"Simplemente apaga la computadora. Ignóralo. Ya pasará".

204
0:11:38.261,000 --> 0:11:4,000
Y eso se ve en el caso de Rana Ayyub.

205
0:11:41.102,000 --> 0:11:44,000
Le dijeron "Estás haciendo una montaña de un grano de arena.

206
0:11:44.594,000 --> 0:11:45,000
Son solo niños siendo niños".

207
0:11:47.268,000 --> 0:11:52,000
Así que tenemos que unir la nueva legislación con la experiencia.

208
0:11:54.053,000 --> 0:11:57,000
Y los medios de comunicación tienen que animar a la educación.

209
0:11:58.18,000 --> 0:12:02,000
Los periodistas necesitan instruirse en el fenómeno de deepfakes

210
0:12:02.464,000 --> 0:12:05,000
para que no aumenten ni se difundan.

211
0:12:06.583,000 --> 0:12:08,000
Y aquí todos estamos involucrados.

212
0:12:08.775,000 --> 0:12:11,000
Cada uno de nosotros necesita educación.

213
0:12:13.375,000 --> 0:12:16,000
Hacemos clic, compartimos, damos me gusta sin pensar en ello.

214
0:12:17.551,000 --> 0:12:18,000
Necesitamos hacerlo mejor.

215
0:12:19.726,000 --> 0:12:22,000
Necesitamos un radar más rápido y mejor contra la falsificación.

216
0:12:25.744,000 --> 0:12:28,000
Así que mientras trabajamos en estas soluciones

217
0:12:29.609,000 --> 0:12:31,000
habrá bastante sufrimiento.

218
0:12:33.093,000 --> 0:12:35,000
Rana Ayyub sigue lidiando con los efectos secundarios.

219
0:12:36.669,000 --> 0:12:4,000
Aún no se siente libre de expresarse online y offline.

220
0:12:41.566,000 --> 0:12:42,000
Y ella me ha dicho

221
0:12:42.955,000 --> 0:12:47,000
que aún se siente como si miles de ojos mirasen su cuerpo desnudo

222
0:12:48.053,000 --> 0:12:51,000
sabiendo perfectamente que no es su cuerpo.

223
0:12:52.371,000 --> 0:12:54,000
Y sufre frecuentes ataques de pánico

224
0:12:54.744,000 --> 0:12:58,000
en concreto cuando alguien que no conoce intenta tomarle una foto.

225
0:12:58.868,000 --> 0:13:01,000
"¿Y si van a hacer otro deepfake?" es lo que piensa.

226
0:13:03.082,000 --> 0:13:06,000
Así que por personas como Rana Ayyub

227
0:13:07.027,000 --> 0:13:09,000
y por la democracia

228
0:13:09.357,000 --> 0:13:11,000
tenemos que hacer algo ahora mismo.

229
0:13:11.563,000 --> 0:13:12,000
Gracias.

230
0:13:12.738,000 --> 0:13:14,000
(Aplausos)

