1
0:00:12.641,000 --> 0:00:14,000
I would like to tell you a story

2
0:00:14.995,000 --> 0:00:17,000
connecting the notorious privacy incident

3
0:00:18.171,000 --> 0:00:2,000
involving Adam and Eve,

4
0:00:20.94,000 --> 0:00:23,000
and the remarkable shift in the boundaries

5
0:00:24.386,000 --> 0:00:26,000
between public and private which has occurred

6
0:00:27.072,000 --> 0:00:28,000
in the past 10 years.

7
0:00:28.842,000 --> 0:00:29,000
You know the incident.

8
0:00:30.14,000 --> 0:00:33,000
Adam and Eve one day in the Garden of Eden

9
0:00:33.47,000 --> 0:00:34,000
realize they are naked.

10
0:00:35.313,000 --> 0:00:36,000
They freak out.

11
0:00:36.813,000 --> 0:00:38,000
And the rest is history.

12
0:00:39.57,000 --> 0:00:41,000
Nowadays, Adam and Eve

13
0:00:41.758,000 --> 0:00:43,000
would probably act differently.

14
0:00:44.119,000 --> 0:00:46,000
[@Adam Last nite was a blast! loved dat apple LOL]

15
0:00:46.387,000 --> 0:00:47,000
[@Eve yep.. babe, know what happened to my pants tho?]

16
0:00:48.26,000 --> 0:00:5,000
We do reveal so much more information

17
0:00:50.896,000 --> 0:00:53,000
about ourselves online than ever before,

18
0:00:54.23,000 --> 0:00:55,000
and so much information about us

19
0:00:55.934,000 --> 0:00:57,000
is being collected by organizations.

20
0:00:58.158,000 --> 0:01:01,000
Now there is much to gain and benefit

21
0:01:01.44,000 --> 0:01:03,000
from this massive analysis of personal information,

22
0:01:03.886,000 --> 0:01:04,000
or big data,

23
0:01:05.832,000 --> 0:01:07,000
but there are also complex tradeoffs that come

24
0:01:08.47,000 --> 0:01:11,000
from giving away our privacy.

25
0:01:11.568,000 --> 0:01:15,000
And my story is about these tradeoffs.

26
0:01:15.591,000 --> 0:01:17,000
We start with an observation which, in my mind,

27
0:01:18.175,000 --> 0:01:21,000
has become clearer and clearer in the past few years,

28
0:01:21.502,000 --> 0:01:23,000
that any personal information

29
0:01:23.599,000 --> 0:01:25,000
can become sensitive information.

30
0:01:25.884,000 --> 0:01:29,000
Back in the year 2000, about 100 billion photos

31
0:01:30.009,000 --> 0:01:31,000
were shot worldwide,

32
0:01:31.921,000 --> 0:01:34,000
but only a minuscule proportion of them

33
0:01:34.986,000 --> 0:01:35,000
were actually uploaded online.

34
0:01:36.869,000 --> 0:01:39,000
In 2010, only on Facebook, in a single month,

35
0:01:40.23,000 --> 0:01:43,000
2.5 billion photos were uploaded,

36
0:01:43.5,000 --> 0:01:44,000
most of them identified.

37
0:01:45.382,000 --> 0:01:46,000
In the same span of time,

38
0:01:47.262,000 --> 0:01:51,000
computers' ability to recognize people in photos

39
0:01:52.132,000 --> 0:01:55,000
improved by three orders of magnitude.

40
0:01:55.74,000 --> 0:01:56,000
What happens when you combine

41
0:01:57.622,000 --> 0:01:58,000
these technologies together:

42
0:01:59.123,000 --> 0:02:01,000
increasing availability of facial data;

43
0:02:01.781,000 --> 0:02:04,000
improving facial recognizing ability by computers;

44
0:02:05.429,000 --> 0:02:07,000
but also cloud computing,

45
0:02:07.611,000 --> 0:02:08,000
which gives anyone in this theater

46
0:02:09.499,000 --> 0:02:1,000
the kind of computational power

47
0:02:11.059,000 --> 0:02:12,000
which a few years ago was only the domain

48
0:02:12.945,000 --> 0:02:13,000
of three-letter agencies;

49
0:02:14.727,000 --> 0:02:15,000
and ubiquitous computing,

50
0:02:16.105,000 --> 0:02:18,000
which allows my phone, which is not a supercomputer,

51
0:02:18.997,000 --> 0:02:19,000
to connect to the Internet

52
0:02:20.668,000 --> 0:02:22,000
and do there hundreds of thousands

53
0:02:23.002,000 --> 0:02:25,000
of face metrics in a few seconds?

54
0:02:25.641,000 --> 0:02:27,000
Well, we conjecture that the result

55
0:02:28.269,000 --> 0:02:3,000
of this combination of technologies

56
0:02:30.333,000 --> 0:02:32,000
will be a radical change in our very notions

57
0:02:33.221,000 --> 0:02:35,000
of privacy and anonymity.

58
0:02:35.478,000 --> 0:02:36,000
To test that, we did an experiment

59
0:02:37.471,000 --> 0:02:39,000
on Carnegie Mellon University campus.

60
0:02:39.592,000 --> 0:02:41,000
We asked students who were walking by

61
0:02:41.691,000 --> 0:02:42,000
to participate in a study,

62
0:02:43.47,000 --> 0:02:45,000
and we took a shot with a webcam,

63
0:02:46.032,000 --> 0:02:48,000
and we asked them to fill out a survey on a laptop.

64
0:02:48.814,000 --> 0:02:49,000
While they were filling out the survey,

65
0:02:50.793,000 --> 0:02:52,000
we uploaded their shot to a cloud-computing cluster,

66
0:02:53.59,000 --> 0:02:54,000
and we started using a facial recognizer

67
0:02:55.317,000 --> 0:02:57,000
to match that shot to a database

68
0:02:57.722,000 --> 0:02:59,000
of some hundreds of thousands of images

69
0:03:00.115,000 --> 0:03:03,000
which we had downloaded from Facebook profiles.

70
0:03:03.711,000 --> 0:03:06,000
By the time the subject reached the last page

71
0:03:06.97,000 --> 0:03:09,000
on the survey, the page had been dynamically updated

72
0:03:10.317,000 --> 0:03:12,000
with the 10 best matching photos

73
0:03:12.63,000 --> 0:03:14,000
which the recognizer had found,

74
0:03:14.915,000 --> 0:03:15,000
and we asked the subjects to indicate

75
0:03:16.653,000 --> 0:03:2,000
whether he or she found themselves in the photo.

76
0:03:20.773,000 --> 0:03:23,000
Do you see the subject?

77
0:03:24.472,000 --> 0:03:26,000
Well, the computer did, and in fact did so

78
0:03:27.317,000 --> 0:03:29,000
for one out of three subjects.

79
0:03:29.466,000 --> 0:03:32,000
So essentially, we can start from an anonymous face,

80
0:03:32.65,000 --> 0:03:35,000
offline or online, and we can use facial recognition

81
0:03:36.134,000 --> 0:03:38,000
to give a name to that anonymous face

82
0:03:38.494,000 --> 0:03:4,000
thanks to social media data.

83
0:03:40.602,000 --> 0:03:41,000
But a few years back, we did something else.

84
0:03:42.474,000 --> 0:03:43,000
We started from social media data,

85
0:03:44.297,000 --> 0:03:47,000
we combined it statistically with data

86
0:03:47.348,000 --> 0:03:49,000
from U.S. government social security,

87
0:03:49.45,000 --> 0:03:52,000
and we ended up predicting social security numbers,

88
0:03:52.774,000 --> 0:03:53,000
which in the United States

89
0:03:54.286,000 --> 0:03:56,000
are extremely sensitive information.

90
0:03:56.326,000 --> 0:03:58,000
Do you see where I'm going with this?

91
0:03:58.419,000 --> 0:04:,000
So if you combine the two studies together,

92
0:04:01.341,000 --> 0:04:02,000
then the question becomes,

93
0:04:02.853,000 --> 0:04:04,000
can you start from a face and,

94
0:04:05.573,000 --> 0:04:07,000
using facial recognition, find a name

95
0:04:07.884,000 --> 0:04:09,000
and publicly available information

96
0:04:10.553,000 --> 0:04:11,000
about that name and that person,

97
0:04:12.485,000 --> 0:04:14,000
and from that publicly available information

98
0:04:14.733,000 --> 0:04:16,000
infer non-publicly available information,

99
0:04:16.775,000 --> 0:04:17,000
much more sensitive ones

100
0:04:18.381,000 --> 0:04:19,000
which you link back to the face?

101
0:04:19.873,000 --> 0:04:2,000
And the answer is, yes, we can, and we did.

102
0:04:21.789,000 --> 0:04:23,000
Of course, the accuracy keeps getting worse.

103
0:04:24.357,000 --> 0:04:24,000
[27% of subjects' first 5 SSN digits identified (with 4 attempts)]

104
0:04:25.301,000 --> 0:04:28,000
But in fact, we even decided to develop an iPhone app

105
0:04:29.128,000 --> 0:04:31,000
which uses the phone's internal camera

106
0:04:31.843,000 --> 0:04:32,000
to take a shot of a subject

107
0:04:33.443,000 --> 0:04:34,000
and then upload it to a cloud

108
0:04:34.93,000 --> 0:04:36,000
and then do what I just described to you in real time:

109
0:04:37.592,000 --> 0:04:39,000
looking for a match, finding public information,

110
0:04:39.68,000 --> 0:04:4,000
trying to infer sensitive information,

111
0:04:41.41,000 --> 0:04:43,000
and then sending back to the phone

112
0:04:44.001,000 --> 0:04:47,000
so that it is overlaid on the face of the subject,

113
0:04:47.61,000 --> 0:04:48,000
an example of augmented reality,

114
0:04:49.511,000 --> 0:04:51,000
probably a creepy example of augmented reality.

115
0:04:51.962,000 --> 0:04:54,000
In fact, we didn't develop the app to make it available,

116
0:04:55.301,000 --> 0:04:56,000
just as a proof of concept.

117
0:04:57.223,000 --> 0:04:59,000
In fact, take these technologies

118
0:04:59.536,000 --> 0:05:,000
and push them to their logical extreme.

119
0:05:01.373,000 --> 0:05:03,000
Imagine a future in which strangers around you

120
0:05:04.092,000 --> 0:05:06,000
will look at you through their Google Glasses

121
0:05:06.403,000 --> 0:05:08,000
or, one day, their contact lenses,

122
0:05:08.71,000 --> 0:05:12,000
and use seven or eight data points about you

123
0:05:12.73,000 --> 0:05:14,000
to infer anything else

124
0:05:15.312,000 --> 0:05:17,000
which may be known about you.

125
0:05:17.915,000 --> 0:05:21,000
What will this future without secrets look like?

126
0:05:22.709,000 --> 0:05:23,000
And should we care?

127
0:05:24.673,000 --> 0:05:25,000
We may like to believe

128
0:05:26.564,000 --> 0:05:29,000
that the future with so much wealth of data

129
0:05:29.604,000 --> 0:05:31,000
would be a future with no more biases,

130
0:05:32.118,000 --> 0:05:35,000
but in fact, having so much information

131
0:05:35.701,000 --> 0:05:37,000
doesn't mean that we will make decisions

132
0:05:37.892,000 --> 0:05:38,000
which are more objective.

133
0:05:39.598,000 --> 0:05:41,000
In another experiment, we presented to our subjects

134
0:05:42.158,000 --> 0:05:44,000
information about a potential job candidate.

135
0:05:44.404,000 --> 0:05:47,000
We included in this information some references

136
0:05:47.582,000 --> 0:05:49,000
to some funny, absolutely legal,

137
0:05:50.228,000 --> 0:05:52,000
but perhaps slightly embarrassing information

138
0:05:52.693,000 --> 0:05:54,000
that the subject had posted online.

139
0:05:54.713,000 --> 0:05:56,000
Now interestingly, among our subjects,

140
0:05:57.079,000 --> 0:06:,000
some had posted comparable information,

141
0:06:00.162,000 --> 0:06:02,000
and some had not.

142
0:06:02.524,000 --> 0:06:03,000
Which group do you think

143
0:06:04.473,000 --> 0:06:08,000
was more likely to judge harshly our subject?

144
0:06:09.025,000 --> 0:06:1,000
Paradoxically, it was the group

145
0:06:10.982,000 --> 0:06:11,000
who had posted similar information,

146
0:06:12.715,000 --> 0:06:14,000
an example of moral dissonance.

147
0:06:15.657,000 --> 0:06:16,000
Now you may be thinking,

148
0:06:17.407,000 --> 0:06:18,000
this does not apply to me,

149
0:06:19.109,000 --> 0:06:21,000
because I have nothing to hide.

150
0:06:21.271,000 --> 0:06:23,000
But in fact, privacy is not about

151
0:06:23.753,000 --> 0:06:26,000
having something negative to hide.

152
0:06:27.429,000 --> 0:06:29,000
Imagine that you are the H.R. director

153
0:06:29.783,000 --> 0:06:31,000
of a certain organization, and you receive résumés,

154
0:06:32.73,000 --> 0:06:34,000
and you decide to find more information about the candidates.

155
0:06:35.203,000 --> 0:06:37,000
Therefore, you Google their names

156
0:06:37.663,000 --> 0:06:39,000
and in a certain universe,

157
0:06:39.903,000 --> 0:06:41,000
you find this information.

158
0:06:41.911,000 --> 0:06:45,000
Or in a parallel universe, you find this information.

159
0:06:46.348,000 --> 0:06:48,000
Do you think that you would be equally likely

160
0:06:49.065,000 --> 0:06:51,000
to call either candidate for an interview?

161
0:06:51.868,000 --> 0:06:53,000
If you think so, then you are not

162
0:06:54.15,000 --> 0:06:56,000
like the U.S. employers who are, in fact,

163
0:06:56.732,000 --> 0:06:59,000
part of our experiment, meaning we did exactly that.

164
0:07:00.039,000 --> 0:07:03,000
We created Facebook profiles, manipulating traits,

165
0:07:03.221,000 --> 0:07:05,000
then we started sending out résumés to companies in the U.S.,

166
0:07:06.072,000 --> 0:07:07,000
and we detected, we monitored,

167
0:07:07.98,000 --> 0:07:09,000
whether they were searching for our candidates,

168
0:07:10.373,000 --> 0:07:11,000
and whether they were acting on the information

169
0:07:12.205,000 --> 0:07:13,000
they found on social media. And they were.

170
0:07:14.143,000 --> 0:07:16,000
Discrimination was happening through social media

171
0:07:16.244,000 --> 0:07:19,000
for equally skilled candidates.

172
0:07:19.317,000 --> 0:07:23,000
Now marketers like us to believe

173
0:07:23.892,000 --> 0:07:25,000
that all information about us will always

174
0:07:26.161,000 --> 0:07:29,000
be used in a manner which is in our favor.

175
0:07:29.434,000 --> 0:07:32,000
But think again. Why should that be always the case?

176
0:07:33.149,000 --> 0:07:35,000
In a movie which came out a few years ago,

177
0:07:35.813,000 --> 0:07:37,000
"Minority Report," a famous scene

178
0:07:38.366,000 --> 0:07:4,000
had Tom Cruise walk in a mall

179
0:07:40.942,000 --> 0:07:43,000
and holographic personalized advertising

180
0:07:44.718,000 --> 0:07:45,000
would appear around him.

181
0:07:46.553,000 --> 0:07:49,000
Now, that movie is set in 2054,

182
0:07:49.78,000 --> 0:07:5,000
about 40 years from now,

183
0:07:51.422,000 --> 0:07:53,000
and as exciting as that technology looks,

184
0:07:54.33,000 --> 0:07:56,000
it already vastly underestimates

185
0:07:56.976,000 --> 0:07:58,000
the amount of information that organizations

186
0:07:59.116,000 --> 0:08:01,000
can gather about you, and how they can use it

187
0:08:01.599,000 --> 0:08:04,000
to influence you in a way that you will not even detect.

188
0:08:04.997,000 --> 0:08:06,000
So as an example, this is another experiment

189
0:08:07.1,000 --> 0:08:09,000
actually we are running, not yet completed.

190
0:08:09.373,000 --> 0:08:11,000
Imagine that an organization has access

191
0:08:11.692,000 --> 0:08:13,000
to your list of Facebook friends,

192
0:08:13.748,000 --> 0:08:14,000
and through some kind of algorithm

193
0:08:15.52,000 --> 0:08:18,000
they can detect the two friends that you like the most.

194
0:08:19.254,000 --> 0:08:21,000
And then they create, in real time,

195
0:08:21.534,000 --> 0:08:23,000
a facial composite of these two friends.

196
0:08:24.376,000 --> 0:08:27,000
Now studies prior to ours have shown that people

197
0:08:27.445,000 --> 0:08:29,000
don't recognize any longer even themselves

198
0:08:30.33,000 --> 0:08:32,000
in facial composites, but they react

199
0:08:32.792,000 --> 0:08:34,000
to those composites in a positive manner.

200
0:08:34.909,000 --> 0:08:37,000
So next time you are looking for a certain product,

201
0:08:38.324,000 --> 0:08:4,000
and there is an ad suggesting you to buy it,

202
0:08:40.883,000 --> 0:08:42,000
it will not be just a standard spokesperson.

203
0:08:43.79,000 --> 0:08:45,000
It will be one of your friends,

204
0:08:46.103,000 --> 0:08:49,000
and you will not even know that this is happening.

205
0:08:49.406,000 --> 0:08:51,000
Now the problem is that

206
0:08:51.819,000 --> 0:08:53,000
the current policy mechanisms we have

207
0:08:54.338,000 --> 0:08:57,000
to protect ourselves from the abuses of personal information

208
0:08:57.776,000 --> 0:08:59,000
are like bringing a knife to a gunfight.

209
0:09:00.76,000 --> 0:09:02,000
One of these mechanisms is transparency,

210
0:09:03.673,000 --> 0:09:06,000
telling people what you are going to do with their data.

211
0:09:06.873,000 --> 0:09:08,000
And in principle, that's a very good thing.

212
0:09:08.979,000 --> 0:09:11,000
It's necessary, but it is not sufficient.

213
0:09:12.646,000 --> 0:09:15,000
Transparency can be misdirected.

214
0:09:16.344,000 --> 0:09:18,000
You can tell people what you are going to do,

215
0:09:18.448,000 --> 0:09:2,000
and then you still nudge them to disclose

216
0:09:20.68,000 --> 0:09:22,000
arbitrary amounts of personal information.

217
0:09:23.303,000 --> 0:09:25,000
So in yet another experiment, this one with students,

218
0:09:26.189,000 --> 0:09:29,000
we asked them to provide information

219
0:09:29.247,000 --> 0:09:3,000
about their campus behavior,

220
0:09:31.06,000 --> 0:09:33,000
including pretty sensitive questions, such as this one.

221
0:09:34,000 --> 0:09:34,000
[Have you ever cheated in an exam?]

222
0:09:34.621,000 --> 0:09:36,000
Now to one group of subjects, we told them,

223
0:09:36.921,000 --> 0:09:38,000
"Only other students will see your answers."

224
0:09:39.762,000 --> 0:09:4,000
To another group of subjects, we told them,

225
0:09:41.341,000 --> 0:09:44,000
"Students and faculty will see your answers."

226
0:09:44.902,000 --> 0:09:46,000
Transparency. Notification. And sure enough, this worked,

227
0:09:47.493,000 --> 0:09:48,000
in the sense that the first group of subjects

228
0:09:48.9,000 --> 0:09:5,000
were much more likely to disclose than the second.

229
0:09:51.468,000 --> 0:09:52,000
It makes sense, right?

230
0:09:52.988,000 --> 0:09:53,000
But then we added the misdirection.

231
0:09:54.478,000 --> 0:09:56,000
We repeated the experiment with the same two groups,

232
0:09:57.238,000 --> 0:09:59,000
this time adding a delay

233
0:09:59.665,000 --> 0:10:01,000
between the time we told subjects

234
0:10:02.6,000 --> 0:10:04,000
how we would use their data

235
0:10:04.68,000 --> 0:10:08,000
and the time we actually started answering the questions.

236
0:10:09.068,000 --> 0:10:11,000
How long a delay do you think we had to add

237
0:10:11.629,000 --> 0:10:15,000
in order to nullify the inhibitory effect

238
0:10:16.242,000 --> 0:10:19,000
of knowing that faculty would see your answers?

239
0:10:19.653,000 --> 0:10:2,000
Ten minutes?

240
0:10:21.433,000 --> 0:10:22,000
Five minutes?

241
0:10:23.224,000 --> 0:10:24,000
One minute?

242
0:10:25,000 --> 0:10:27,000
How about 15 seconds?

243
0:10:27.049,000 --> 0:10:29,000
Fifteen seconds were sufficient to have the two groups

244
0:10:29.717,000 --> 0:10:3,000
disclose the same amount of information,

245
0:10:31.285,000 --> 0:10:33,000
as if the second group now no longer cares

246
0:10:34.031,000 --> 0:10:36,000
for faculty reading their answers.

247
0:10:36.687,000 --> 0:10:39,000
Now I have to admit that this talk so far

248
0:10:40.023,000 --> 0:10:42,000
may sound exceedingly gloomy,

249
0:10:42.503,000 --> 0:10:43,000
but that is not my point.

250
0:10:44.224,000 --> 0:10:46,000
In fact, I want to share with you the fact that

251
0:10:46.923,000 --> 0:10:47,000
there are alternatives.

252
0:10:48.695,000 --> 0:10:5,000
The way we are doing things now is not the only way

253
0:10:51.194,000 --> 0:10:54,000
they can done, and certainly not the best way

254
0:10:54.231,000 --> 0:10:56,000
they can be done.

255
0:10:56.258,000 --> 0:11:,000
When someone tells you, "People don't care about privacy,"

256
0:11:00.429,000 --> 0:11:02,000
consider whether the game has been designed

257
0:11:03.071,000 --> 0:11:05,000
and rigged so that they cannot care about privacy,

258
0:11:05.795,000 --> 0:11:08,000
and coming to the realization that these manipulations occur

259
0:11:09.057,000 --> 0:11:1,000
is already halfway through the process

260
0:11:10.664,000 --> 0:11:12,000
of being able to protect yourself.

261
0:11:12.922,000 --> 0:11:15,000
When someone tells you that privacy is incompatible

262
0:11:16.632,000 --> 0:11:17,000
with the benefits of big data,

263
0:11:18.481,000 --> 0:11:2,000
consider that in the last 20 years,

264
0:11:20.954,000 --> 0:11:21,000
researchers have created technologies

265
0:11:22.871,000 --> 0:11:25,000
to allow virtually any electronic transactions

266
0:11:26.189,000 --> 0:11:29,000
to take place in a more privacy-preserving manner.

267
0:11:29.938,000 --> 0:11:31,000
We can browse the Internet anonymously.

268
0:11:32.493,000 --> 0:11:34,000
We can send emails that can only be read

269
0:11:35.171,000 --> 0:11:38,000
by the intended recipient, not even the NSA.

270
0:11:38.88,000 --> 0:11:4,000
We can have even privacy-preserving data mining.

271
0:11:41.877,000 --> 0:11:44,000
In other words, we can have the benefits of big data

272
0:11:45.771,000 --> 0:11:47,000
while protecting privacy.

273
0:11:47.903,000 --> 0:11:5,000
Of course, these technologies imply a shifting

274
0:11:51.694,000 --> 0:11:52,000
of cost and revenues

275
0:11:53.24,000 --> 0:11:55,000
between data holders and data subjects,

276
0:11:55.347,000 --> 0:11:58,000
which is why, perhaps, you don't hear more about them.

277
0:11:58.8,000 --> 0:12:01,000
Which brings me back to the Garden of Eden.

278
0:12:02.506,000 --> 0:12:04,000
There is a second privacy interpretation

279
0:12:05.286,000 --> 0:12:06,000
of the story of the Garden of Eden

280
0:12:07.095,000 --> 0:12:09,000
which doesn't have to do with the issue

281
0:12:09.191,000 --> 0:12:11,000
of Adam and Eve feeling naked

282
0:12:11.416,000 --> 0:12:13,000
and feeling ashamed.

283
0:12:13.797,000 --> 0:12:15,000
You can find echoes of this interpretation

284
0:12:16.578,000 --> 0:12:18,000
in John Milton's "Paradise Lost."

285
0:12:19.36,000 --> 0:12:23,000
In the garden, Adam and Eve are materially content.

286
0:12:23.557,000 --> 0:12:25,000
They're happy. They are satisfied.

287
0:12:25.661,000 --> 0:12:27,000
However, they also lack knowledge

288
0:12:27.954,000 --> 0:12:28,000
and self-awareness.

289
0:12:29.594,000 --> 0:12:32,000
The moment they eat the aptly named

290
0:12:32.913,000 --> 0:12:33,000
fruit of knowledge,

291
0:12:34.206,000 --> 0:12:36,000
that's when they discover themselves.

292
0:12:36.811,000 --> 0:12:4,000
They become aware. They achieve autonomy.

293
0:12:40.842,000 --> 0:12:43,000
The price to pay, however, is leaving the garden.

294
0:12:43.968,000 --> 0:12:46,000
So privacy, in a way, is both the means

295
0:12:47.849,000 --> 0:12:49,000
and the price to pay for freedom.

296
0:12:50.811,000 --> 0:12:52,000
Again, marketers tell us

297
0:12:53.581,000 --> 0:12:56,000
that big data and social media

298
0:12:56.6,000 --> 0:12:58,000
are not just a paradise of profit for them,

299
0:12:59.579,000 --> 0:13:01,000
but a Garden of Eden for the rest of us.

300
0:13:02.036,000 --> 0:13:03,000
We get free content.

301
0:13:03.274,000 --> 0:13:06,000
We get to play Angry Birds. We get targeted apps.

302
0:13:06.397,000 --> 0:13:08,000
But in fact, in a few years, organizations

303
0:13:09.294,000 --> 0:13:1,000
will know so much about us,

304
0:13:10.903,000 --> 0:13:12,000
they will be able to infer our desires

305
0:13:13.613,000 --> 0:13:15,000
before we even form them, and perhaps

306
0:13:15.817,000 --> 0:13:17,000
buy products on our behalf

307
0:13:18.264,000 --> 0:13:2,000
before we even know we need them.

308
0:13:20.538,000 --> 0:13:23,000
Now there was one English author

309
0:13:23.775,000 --> 0:13:26,000
who anticipated this kind of future

310
0:13:26.82,000 --> 0:13:27,000
where we would trade away

311
0:13:28.225,000 --> 0:13:31,000
our autonomy and freedom for comfort.

312
0:13:31.773,000 --> 0:13:33,000
Even more so than George Orwell,

313
0:13:33.934,000 --> 0:13:35,000
the author is, of course, Aldous Huxley.

314
0:13:36.695,000 --> 0:13:38,000
In "Brave New World," he imagines a society

315
0:13:39.549,000 --> 0:13:41,000
where technologies that we created

316
0:13:41.72,000 --> 0:13:42,000
originally for freedom

317
0:13:43.579,000 --> 0:13:45,000
end up coercing us.

318
0:13:46.146,000 --> 0:13:5,000
However, in the book, he also offers us a way out

319
0:13:50.937,000 --> 0:13:53,000
of that society, similar to the path

320
0:13:54.375,000 --> 0:13:57,000
that Adam and Eve had to follow to leave the garden.

321
0:13:58.33,000 --> 0:14:,000
In the words of the Savage,

322
0:14:00.477,000 --> 0:14:03,000
regaining autonomy and freedom is possible,

323
0:14:03.546,000 --> 0:14:05,000
although the price to pay is steep.

324
0:14:06.225,000 --> 0:14:11,000
So I do believe that one of the defining fights

325
0:14:11.94,000 --> 0:14:13,000
of our times will be the fight

326
0:14:14.503,000 --> 0:14:16,000
for the control over personal information,

327
0:14:16.89,000 --> 0:14:19,000
the fight over whether big data will become a force

328
0:14:20.397,000 --> 0:14:21,000
for freedom,

329
0:14:21.686,000 --> 0:14:25,000
rather than a force which will hiddenly manipulate us.

330
0:14:26.432,000 --> 0:14:28,000
Right now, many of us

331
0:14:29.025,000 --> 0:14:31,000
do not even know that the fight is going on,

332
0:14:31.778,000 --> 0:14:33,000
but it is, whether you like it or not.

333
0:14:34.45,000 --> 0:14:36,000
And at the risk of playing the serpent,

334
0:14:37.254,000 --> 0:14:39,000
I will tell you that the tools for the fight

335
0:14:40.151,000 --> 0:14:43,000
are here, the awareness of what is going on,

336
0:14:43.16,000 --> 0:14:44,000
and in your hands,

337
0:14:44.515,000 --> 0:14:47,000
just a few clicks away.

338
0:14:48.255,000 --> 0:14:49,000
Thank you.

339
0:14:49.737,000 --> 0:14:53,000
(Applause)

