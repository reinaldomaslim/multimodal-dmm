1
0:00:,000 --> 0:00:07,000
Translator: Ivana Korom Reviewer: Joanna Pietrulewicz

2
0:00:12.875,000 --> 0:00:15,000
How many decisions have been made about you today,

3
0:00:16.667,000 --> 0:00:18,000
or this week or this year,

4
0:00:19.292,000 --> 0:00:2,000
by artificial intelligence?

5
0:00:22.958,000 --> 0:00:23,000
I build AI for a living

6
0:00:24.667,000 --> 0:00:27,000
so, full disclosure, I'm kind of a nerd.

7
0:00:27.708,000 --> 0:00:29,000
And because I'm kind of a nerd,

8
0:00:30.125,000 --> 0:00:32,000
wherever some new news story comes out

9
0:00:32.5,000 --> 0:00:35,000
about artificial intelligence stealing all our jobs,

10
0:00:35.958,000 --> 0:00:39,000
or robots getting citizenship of an actual country,

11
0:00:40.167,000 --> 0:00:43,000
I'm the person my friends and followers message

12
0:00:43.333,000 --> 0:00:44,000
freaking out about the future.

13
0:00:45.833,000 --> 0:00:47,000
We see this everywhere.

14
0:00:47.958,000 --> 0:00:51,000
This media panic that our robot overlords are taking over.

15
0:00:52.875,000 --> 0:00:53,000
We could blame Hollywood for that.

16
0:00:56.125,000 --> 0:01:,000
But in reality, that's not the problem we should be focusing on.

17
0:01:01.25,000 --> 0:01:04,000
There is a more pressing danger, a bigger risk with AI,

18
0:01:04.917,000 --> 0:01:05,000
that we need to fix first.

19
0:01:07.417,000 --> 0:01:09,000
So we are back to this question:

20
0:01:09.75,000 --> 0:01:13,000
How many decisions have been made about you today by AI?

21
0:01:15.792,000 --> 0:01:16,000
And how many of these

22
0:01:17.792,000 --> 0:01:21,000
were based on your gender, your race or your background?

23
0:01:24.5,000 --> 0:01:26,000
Algorithms are being used all the time

24
0:01:27.292,000 --> 0:01:3,000
to make decisions about who we are and what we want.

25
0:01:32.208,000 --> 0:01:35,000
Some of the women in this room will know what I'm talking about

26
0:01:35.875,000 --> 0:01:38,000
if you've been made to sit through those pregnancy test adverts on YouTube

27
0:01:39.667,000 --> 0:01:41,000
like 1,000 times.

28
0:01:41.75,000 --> 0:01:43,000
Or you've scrolled past adverts of fertility clinics

29
0:01:44.625,000 --> 0:01:46,000
on your Facebook feed.

30
0:01:47.625,000 --> 0:01:49,000
Or in my case, Indian marriage bureaus.

31
0:01:50.042,000 --> 0:01:51,000
(Laughter)

32
0:01:51.333,000 --> 0:01:53,000
But AI isn't just being used to make decisions

33
0:01:54.333,000 --> 0:01:56,000
about what products we want to buy

34
0:01:56.958,000 --> 0:01:58,000
or which show we want to binge watch next.

35
0:02:01.042,000 --> 0:02:06,000
I wonder how you'd feel about someone who thought things like this:

36
0:02:06.25,000 --> 0:02:07,000
"A black or Latino person

37
0:02:08.208,000 --> 0:02:12,000
is less likely than a white person to pay off their loan on time."

38
0:02:13.542,000 --> 0:02:15,000
"A person called John makes a better programmer

39
0:02:16.375,000 --> 0:02:17,000
than a person called Mary."

40
0:02:19.25,000 --> 0:02:24,000
"A black man is more likely to be a repeat offender than a white man."

41
0:02:26.958,000 --> 0:02:27,000
You're probably thinking,

42
0:02:28.25,000 --> 0:02:31,000
"Wow, that sounds like a pretty sexist, racist person," right?

43
0:02:33,000 --> 0:02:37,000
These are some real decisions that AI has made very recently,

44
0:02:37.875,000 --> 0:02:39,000
based on the biases it has learned from us,

45
0:02:40.833,000 --> 0:02:41,000
from the humans.

46
0:02:43.75,000 --> 0:02:47,000
AI is being used to help decide whether or not you get that job interview;

47
0:02:48.583,000 --> 0:02:5,000
how much you pay for your car insurance;

48
0:02:51,000 --> 0:02:52,000
how good your credit score is;

49
0:02:52.917,000 --> 0:02:55,000
and even what rating you get in your annual performance review.

50
0:02:57.083,000 --> 0:03:,000
But these decisions are all being filtered through

51
0:03:00.25,000 --> 0:03:05,000
its assumptions about our identity, our race, our gender, our age.

52
0:03:08.25,000 --> 0:03:1,000
How is that happening?

53
0:03:10.542,000 --> 0:03:13,000
Now, imagine an AI is helping a hiring manager

54
0:03:14.083,000 --> 0:03:16,000
find the next tech leader in the company.

55
0:03:16.958,000 --> 0:03:19,000
So far, the manager has been hiring mostly men.

56
0:03:20.083,000 --> 0:03:24,000
So the AI learns men are more likely to be programmers than women.

57
0:03:25.542,000 --> 0:03:27,000
And it's a very short leap from there to:

58
0:03:28.458,000 --> 0:03:3,000
men make better programmers than women.

59
0:03:31.417,000 --> 0:03:34,000
We have reinforced our own bias into the AI.

60
0:03:35.167,000 --> 0:03:38,000
And now, it's screening out female candidates.

61
0:03:40.917,000 --> 0:03:43,000
Hang on, if a human hiring manager did that,

62
0:03:43.958,000 --> 0:03:45,000
we'd be outraged, we wouldn't allow it.

63
0:03:46.333,000 --> 0:03:49,000
This kind of gender discrimination is not OK.

64
0:03:49.833,000 --> 0:03:53,000
And yet somehow, AI has become above the law,

65
0:03:54.375,000 --> 0:03:56,000
because a machine made the decision.

66
0:03:57.833,000 --> 0:03:58,000
That's not it.

67
0:03:59.375,000 --> 0:04:03,000
We are also reinforcing our bias in how we interact with AI.

68
0:04:04.917,000 --> 0:04:09,000
How often do you use a voice assistant like Siri, Alexa or even Cortana?

69
0:04:10.917,000 --> 0:04:12,000
They all have two things in common:

70
0:04:13.5,000 --> 0:04:16,000
one, they can never get my name right,

71
0:04:16.625,000 --> 0:04:18,000
and second, they are all female.

72
0:04:20.417,000 --> 0:04:22,000
They are designed to be our obedient servants,

73
0:04:23.208,000 --> 0:04:26,000
turning your lights on and off, ordering your shopping.

74
0:04:27.125,000 --> 0:04:3,000
You get male AIs too, but they tend to be more high-powered,

75
0:04:30.458,000 --> 0:04:33,000
like IBM Watson, making business decisions,

76
0:04:33.541,000 --> 0:04:36,000
Salesforce Einstein or ROSS, the robot lawyer.

77
0:04:38.208,000 --> 0:04:42,000
So poor robots, even they suffer from sexism in the workplace.

78
0:04:42.292,000 --> 0:04:43,000
(Laughter)

79
0:04:44.542,000 --> 0:04:46,000
Think about how these two things combine

80
0:04:47.417,000 --> 0:04:52,000
and affect a kid growing up in today's world around AI.

81
0:04:52.75,000 --> 0:04:54,000
So they're doing some research for a school project

82
0:04:55.708,000 --> 0:04:58,000
and they Google images of CEO.

83
0:04:58.75,000 --> 0:05:,000
The algorithm shows them results of mostly men.

84
0:05:01.667,000 --> 0:05:03,000
And now, they Google personal assistant.

85
0:05:04.25,000 --> 0:05:07,000
As you can guess, it shows them mostly females.

86
0:05:07.708,000 --> 0:05:1,000
And then they want to put on some music, and maybe order some food,

87
0:05:11.333,000 --> 0:05:17,000
and now, they are barking orders at an obedient female voice assistant.

88
0:05:19.542,000 --> 0:05:24,000
Some of our brightest minds are creating this technology today.

89
0:05:24.875,000 --> 0:05:28,000
Technology that they could have created in any way they wanted.

90
0:05:29.083,000 --> 0:05:34,000
And yet, they have chosen to create it in the style of 1950s "Mad Man" secretary.

91
0:05:34.792,000 --> 0:05:35,000
Yay!

92
0:05:36.958,000 --> 0:05:37,000
But OK, don't worry,

93
0:05:38.292,000 --> 0:05:4,000
this is not going to end with me telling you

94
0:05:40.375,000 --> 0:05:43,000
that we are all heading towards sexist, racist machines running the world.

95
0:05:44.792,000 --> 0:05:49,000
The good news about AI is that it is entirely within our control.

96
0:05:51.333,000 --> 0:05:55,000
We get to teach the right values, the right ethics to AI.

97
0:05:56.167,000 --> 0:05:58,000
So there are three things we can do.

98
0:05:58.375,000 --> 0:06:01,000
One, we can be aware of our own biases

99
0:06:01.75,000 --> 0:06:03,000
and the bias in machines around us.

100
0:06:04.5,000 --> 0:06:08,000
Two, we can make sure that diverse teams are building this technology.

101
0:06:09.042,000 --> 0:06:13,000
And three, we have to give it diverse experiences to learn from.

102
0:06:14.875,000 --> 0:06:17,000
I can talk about the first two from personal experience.

103
0:06:18.208,000 --> 0:06:19,000
When you work in technology

104
0:06:19.667,000 --> 0:06:22,000
and you don't look like a Mark Zuckerberg or Elon Musk,

105
0:06:23.083,000 --> 0:06:26,000
your life is a little bit difficult, your ability gets questioned.

106
0:06:27.875,000 --> 0:06:28,000
Here's just one example.

107
0:06:29.292,000 --> 0:06:32,000
Like most developers, I often join online tech forums

108
0:06:33.042,000 --> 0:06:36,000
and share my knowledge to help others.

109
0:06:36.292,000 --> 0:06:37,000
And I've found,

110
0:06:37.625,000 --> 0:06:4,000
when I log on as myself, with my own photo, my own name,

111
0:06:41.625,000 --> 0:06:45,000
I tend to get questions or comments like this:

112
0:06:46.25,000 --> 0:06:49,000
"What makes you think you're qualified to talk about AI?"

113
0:06:50.458,000 --> 0:06:53,000
"What makes you think you know about machine learning?"

114
0:06:53.958,000 --> 0:06:56,000
So, as you do, I made a new profile,

115
0:06:57.417,000 --> 0:07:01,000
and this time, instead of my own picture, I chose a cat with a jet pack on it.

116
0:07:02.292,000 --> 0:07:04,000
And I chose a name that did not reveal my gender.

117
0:07:05.917,000 --> 0:07:07,000
You can probably guess where this is going, right?

118
0:07:08.667,000 --> 0:07:14,000
So, this time, I didn't get any of those patronizing comments about my ability

119
0:07:15.083,000 --> 0:07:18,000
and I was able to actually get some work done.

120
0:07:19.5,000 --> 0:07:2,000
And it sucks, guys.

121
0:07:21.375,000 --> 0:07:23,000
I've been building robots since I was 15,

122
0:07:23.875,000 --> 0:07:25,000
I have a few degrees in computer science,

123
0:07:26.167,000 --> 0:07:28,000
and yet, I had to hide my gender

124
0:07:28.625,000 --> 0:07:3,000
in order for my work to be taken seriously.

125
0:07:31.875,000 --> 0:07:32,000
So, what's going on here?

126
0:07:33.792,000 --> 0:07:36,000
Are men just better at technology than women?

127
0:07:37.917,000 --> 0:07:38,000
Another study found

128
0:07:39.5,000 --> 0:07:43,000
that when women coders on one platform hid their gender, like myself,

129
0:07:44.458,000 --> 0:07:47,000
their code was accepted four percent more than men.

130
0:07:48.542,000 --> 0:07:5,000
So this is not about the talent.

131
0:07:51.958,000 --> 0:07:53,000
This is about an elitism in AI

132
0:07:54.875,000 --> 0:07:56,000
that says a programmer needs to look like a certain person.

133
0:07:59.375,000 --> 0:08:02,000
What we really need to do to make AI better

134
0:08:02.5,000 --> 0:08:05,000
is bring people from all kinds of backgrounds.

135
0:08:06.542,000 --> 0:08:08,000
We need people who can write and tell stories

136
0:08:09.125,000 --> 0:08:11,000
to help us create personalities of AI.

137
0:08:12.208,000 --> 0:08:14,000
We need people who can solve problems.

138
0:08:15.125,000 --> 0:08:18,000
We need people who face different challenges

139
0:08:18.917,000 --> 0:08:23,000
and we need people who can tell us what are the real issues that need fixing

140
0:08:24.292,000 --> 0:08:27,000
and help us find ways that technology can actually fix it.

141
0:08:29.833,000 --> 0:08:32,000
Because, when people from diverse backgrounds come together,

142
0:08:33.583,000 --> 0:08:35,000
when we build things in the right way,

143
0:08:35.75,000 --> 0:08:37,000
the possibilities are limitless.

144
0:08:38.75,000 --> 0:08:41,000
And that's what I want to end by talking to you about.

145
0:08:42.083,000 --> 0:08:46,000
Less racist robots, less machines that are going to take our jobs --

146
0:08:46.332,000 --> 0:08:49,000
and more about what technology can actually achieve.

147
0:08:50.292,000 --> 0:08:53,000
So, yes, some of the energy in the world of AI,

148
0:08:53.75,000 --> 0:08:54,000
in the world of technology

149
0:08:55.167,000 --> 0:08:59,000
is going to be about what ads you see on your stream.

150
0:08:59.458,000 --> 0:09:04,000
But a lot of it is going towards making the world so much better.

151
0:09:05.5,000 --> 0:09:08,000
Think about a pregnant woman in the Democratic Republic of Congo,

152
0:09:09.292,000 --> 0:09:13,000
who has to walk 17 hours to her nearest rural prenatal clinic

153
0:09:13.5,000 --> 0:09:14,000
to get a checkup.

154
0:09:15.375,000 --> 0:09:17,000
What if she could get diagnosis on her phone, instead?

155
0:09:19.75,000 --> 0:09:2,000
Or think about what AI could do

156
0:09:21.583,000 --> 0:09:23,000
for those one in three women in South Africa

157
0:09:24.333,000 --> 0:09:26,000
who face domestic violence.

158
0:09:27.083,000 --> 0:09:29,000
If it wasn't safe to talk out loud,

159
0:09:29.833,000 --> 0:09:31,000
they could get an AI service to raise alarm,

160
0:09:32.333,000 --> 0:09:34,000
get financial and legal advice.

161
0:09:35.958,000 --> 0:09:4,000
These are all real examples of projects that people, including myself,

162
0:09:41,000 --> 0:09:43,000
are working on right now, using AI.

163
0:09:45.542,000 --> 0:09:48,000
So, I'm sure in the next couple of days there will be yet another news story

164
0:09:49.167,000 --> 0:09:51,000
about the existential risk,

165
0:09:51.875,000 --> 0:09:53,000
robots taking over and coming for your jobs.

166
0:09:54.333,000 --> 0:09:55,000
(Laughter)

167
0:09:55.375,000 --> 0:09:57,000
And when something like that happens,

168
0:09:57.708,000 --> 0:10:,000
I know I'll get the same messages worrying about the future.

169
0:10:01.333,000 --> 0:10:04,000
But I feel incredibly positive about this technology.

170
0:10:07.458,000 --> 0:10:12,000
This is our chance to remake the world into a much more equal place.

171
0:10:14.458,000 --> 0:10:18,000
But to do that, we need to build it the right way from the get go.

172
0:10:19.667,000 --> 0:10:24,000
We need people of different genders, races, sexualities and backgrounds.

173
0:10:26.458,000 --> 0:10:28,000
We need women to be the makers

174
0:10:28.958,000 --> 0:10:31,000
and not just the machines who do the makers' bidding.

175
0:10:33.875,000 --> 0:10:36,000
We need to think very carefully what we teach machines,

176
0:10:37.667,000 --> 0:10:38,000
what data we give them,

177
0:10:39.333,000 --> 0:10:42,000
so they don't just repeat our own past mistakes.

178
0:10:44.125,000 --> 0:10:47,000
So I hope I leave you thinking about two things.

179
0:10:48.542,000 --> 0:10:52,000
First, I hope you leave thinking about bias today.

180
0:10:53.125,000 --> 0:10:56,000
And that the next time you scroll past an advert

181
0:10:56.333,000 --> 0:10:58,000
that assumes you are interested in fertility clinics

182
0:10:59.167,000 --> 0:11:01,000
or online betting websites,

183
0:11:02.042,000 --> 0:11:04,000
that you think and remember

184
0:11:04.083,000 --> 0:11:08,000
that the same technology is assuming that a black man will reoffend.

185
0:11:09.833,000 --> 0:11:13,000
Or that a woman is more likely to be a personal assistant than a CEO.

186
0:11:14.958,000 --> 0:11:17,000
And I hope that reminds you that we need to do something about it.

187
0:11:20.917,000 --> 0:11:21,000
And second,

188
0:11:22.792,000 --> 0:11:23,000
I hope you think about the fact

189
0:11:24.708,000 --> 0:11:25,000
that you don't need to look a certain way

190
0:11:26.708,000 --> 0:11:29,000
or have a certain background in engineering or technology

191
0:11:30.583,000 --> 0:11:31,000
to create AI,

192
0:11:31.875,000 --> 0:11:33,000
which is going to be a phenomenal force for our future.

193
0:11:36.166,000 --> 0:11:38,000
You don't need to look like a Mark Zuckerberg,

194
0:11:38.333,000 --> 0:11:39,000
you can look like me.

195
0:11:41.25,000 --> 0:11:43,000
And it is up to all of us in this room

196
0:11:44.167,000 --> 0:11:46,000
to convince the governments and the corporations

197
0:11:46.917,000 --> 0:11:48,000
to build AI technology for everyone,

198
0:11:49.833,000 --> 0:11:51,000
including the edge cases.

199
0:11:52.25,000 --> 0:11:54,000
And for us all to get education

200
0:11:54.333,000 --> 0:11:56,000
about this phenomenal technology in the future.

201
0:11:58.167,000 --> 0:12:,000
Because if we do that,

202
0:12:00.208,000 --> 0:12:04,000
then we've only just scratched the surface of what we can achieve with AI.

203
0:12:05.125,000 --> 0:12:06,000
Thank you.

204
0:12:06.417,000 --> 0:12:08,000
(Applause)

