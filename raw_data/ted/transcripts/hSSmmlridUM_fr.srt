1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Claire Ghyselen

2
0:00:12.739,000 --> 0:00:15,000
Mon premier travail était programmeuse informatique

3
0:00:16.635,000 --> 0:00:18,000
durant ma première année à l'université --

4
0:00:18.881,000 --> 0:00:19,000
quand j'étais adolescente.

5
0:00:20.889,000 --> 0:00:23,000
Peu après avoir commencé à écrire des programmes en entreprise,

6
0:00:24.799,000 --> 0:00:27,000
un responsable de l'entreprise est venu me voir

7
0:00:28.458,000 --> 0:00:29,000
et m'a murmuré :

8
0:00:30.229,000 --> 0:00:32,000
« Peut-il dire si je mens ? »

9
0:00:33.806,000 --> 0:00:35,000
Il n'y avait personne d'autre dans la pièce.

10
0:00:37.032,000 --> 0:00:41,000
« Qui peut dire si vous mentez ? Et pourquoi chuchotez-vous ? »

11
0:00:42.266,000 --> 0:00:45,000
Le responsable a pointé du doigt l'ordinateur dans la pièce.

12
0:00:45.397,000 --> 0:00:48,000
« Peut-il dire si je mens ? »

13
0:00:49.613,000 --> 0:00:53,000
Ce responsable avait une aventure avec la réceptionniste.

14
0:00:53.999,000 --> 0:00:54,000
(Rires)

15
0:00:55.135,000 --> 0:00:56,000
J'étais toujours adolescente.

16
0:00:57.447,000 --> 0:00:59,000
J'ai lui ai murmuré-crié :

17
0:00:59.49,000 --> 0:01:02,000
« Oui, l'ordinateur peut dire si vous mentez. »

18
0:01:03.138,000 --> 0:01:04,000
(Rires)

19
0:01:04.968,000 --> 0:01:06,000
J'ai rigolé, mais c'est de moi qu'on peut se moquer.

20
0:01:07.915,000 --> 0:01:1,000
Il y a aujourd'hui des systèmes informatiques

21
0:01:11.207,000 --> 0:01:14,000
qui peuvent repérer les états émotionnels et les mensonges

22
0:01:14.779,000 --> 0:01:16,000
en traitant les informations du visage humain.

23
0:01:17.248,000 --> 0:01:21,000
Les publicitaires et les gouvernements sont très intéressés.

24
0:01:22.319,000 --> 0:01:24,000
J'étais devenue programmeuse informatique

25
0:01:24.321,000 --> 0:01:26,000
car j'étais l'une de ces gamines folles de maths et de sciences.

26
0:01:27.942,000 --> 0:01:3,000
Mais, en chemin, j'avais découvert les armes nucléaires

27
0:01:31.074,000 --> 0:01:33,000
et je me sentais très concernée par l'éthique de la science.

28
0:01:34.05,000 --> 0:01:35,000
J'étais troublée.

29
0:01:35.278,000 --> 0:01:37,000
Cependant, du fait de circonstances familiales,

30
0:01:37.943,000 --> 0:01:4,000
je devais aussi commencer à travailler aussi vite que possible.

31
0:01:41.265,000 --> 0:01:44,000
Je me suis dit : « Choisis un domaine technique

32
0:01:44.588,000 --> 0:01:45,000
où tu peux avoir un emploi facilement

33
0:01:46.408,000 --> 0:01:5,000
et où je n'ai pas à gérer des questions d'éthique difficiles. »

34
0:01:51.022,000 --> 0:01:52,000
J'ai donc choisi l'informatique.

35
0:01:52.575,000 --> 0:01:53,000
(Rires)

36
0:01:53.703,000 --> 0:01:56,000
Eh bien, ah ah ah ! On peut se moquer de moi.

37
0:01:57.137,000 --> 0:01:59,000
Aujourd'hui, les informaticiens construisent des plateformes

38
0:01:59.951,000 --> 0:02:03,000
qui contrôlent chaque jour ce que voient un milliard de personnes.

39
0:02:05.052,000 --> 0:02:08,000
Ils développent des voitures pouvant décider qui écraser.

40
0:02:09.707,000 --> 0:02:12,000
Ils construisent même des machines, des armes

41
0:02:12.904,000 --> 0:02:14,000
qui pourraient tuer des êtres humains dans une guerre.

42
0:02:15.449,000 --> 0:02:17,000
Il y a de l'éthique partout.

43
0:02:19.183,000 --> 0:02:21,000
L'intelligence artificielle est arrivée.

44
0:02:21.823,000 --> 0:02:24,000
Nous utilisons l'informatique pour prendre toutes sortes de décisions,

45
0:02:25.321,000 --> 0:02:26,000
y compris de nouvelles décisions.

46
0:02:27.231,000 --> 0:02:29,000
Nous posons à l'informatique

47
0:02:29.427,000 --> 0:02:32,000
des questions auxquelles il n'y a pas d'unique bonne réponse,

48
0:02:32.427,000 --> 0:02:33,000
qui sont subjectives,

49
0:02:33.653,000 --> 0:02:35,000
ouvertes et reposent sur des valeurs.

50
0:02:36.002,000 --> 0:02:37,000
Nous posons des questions comme :

51
0:02:37.784,000 --> 0:02:38,000
« Qui devrait-on embaucher ? »

52
0:02:40.096,000 --> 0:02:42,000
« Quelles nouvelles de quel ami devrait-on vous montrer ? »

53
0:02:42.879,000 --> 0:02:44,000
« Quel prisonnier va probablement récidiver ? »

54
0:02:45.514,000 --> 0:02:48,000
« Quel nouvel objet ou film devrait être recommandé aux gens ? »

55
0:02:48.592,000 --> 0:02:51,000
Cela fait un certain temps que nous utilisons des ordinateurs

56
0:02:51.988,000 --> 0:02:52,000
mais c'est différent.

57
0:02:53.529,000 --> 0:02:55,000
C'est un changement historique :

58
0:02:55.62,000 --> 0:03:,000
car on ne peut pas utiliser l'informatique pour des décisions si subjectives

59
0:03:00.981,000 --> 0:03:05,000
comme on utilise l'informatique pour piloter un avion, construire un pont,

60
0:03:06.425,000 --> 0:03:07,000
aller sur la Lune.

61
0:03:08.449,000 --> 0:03:11,000
Les avions sont-ils plus sûrs ? Un pont a-t-il bougé et est tombé ?

62
0:03:11.732,000 --> 0:03:15,000
Là, nous nous accordons sur des repères assez clairs

63
0:03:16.254,000 --> 0:03:18,000
et les lois de la nature nous guident.

64
0:03:18.517,000 --> 0:03:21,000
Nous n'avons pas de tels ancres et repères

65
0:03:21.935,000 --> 0:03:24,000
pour les décisions des affaires complexes humaines.

66
0:03:25.922,000 --> 0:03:29,000
Pour compliquer encore les choses, nos logiciels gagnent en puissance

67
0:03:30.183,000 --> 0:03:33,000
mais sont aussi moins transparents et plus complexes.

68
0:03:34.542,000 --> 0:03:36,000
Récemment, les dix dernières années,

69
0:03:36.606,000 --> 0:03:38,000
les algorithmes complexes ont fait de grandes avancées.

70
0:03:39.359,000 --> 0:03:41,000
Ils peuvent reconnaître les visages humains,

71
0:03:41.985,000 --> 0:03:43,000
déchiffrer l'écriture,

72
0:03:44.436,000 --> 0:03:46,000
détecter la fraude à la carte bancaire,

73
0:03:46.526,000 --> 0:03:47,000
bloquer le spam,

74
0:03:47.739,000 --> 0:03:49,000
traduire d'une langue à une autre,

75
0:03:49.8,000 --> 0:03:51,000
détecter les tumeurs en imagerie médicale,

76
0:03:52.398,000 --> 0:03:54,000
battre les humains aux échecs et au go.

77
0:03:55.264,000 --> 0:03:58,000
Beaucoup de ces progrès découlent d'une méthode :

78
0:03:58.315,000 --> 0:03:59,000
« l'apprentissage de la machine ».

79
0:04:00.175,000 --> 0:04:03,000
Cette méthode est différente de la programmation traditionnelle

80
0:04:03.386,000 --> 0:04:06,000
où l'on donne des instructions détaillées, exactes, méticuleuses à l'ordinateur.

81
0:04:07.378,000 --> 0:04:11,000
Cela ressemble plus à un système nourri de données,

82
0:04:11.584,000 --> 0:04:12,000
dont des données non structurées,

83
0:04:13.264,000 --> 0:04:15,000
comme celles générées par notre vie numérique.

84
0:04:15.566,000 --> 0:04:17,000
Le système apprend en parcourant ces données.

85
0:04:18.669,000 --> 0:04:19,000
Et aussi, c'est crucial,

86
0:04:20.219,000 --> 0:04:24,000
ces systèmes n'utilisent pas la logique de la réponse unique.

87
0:04:24.623,000 --> 0:04:27,000
Ils ne produisent pas une seule réponse, c'est plus probabiliste :

88
0:04:27.712,000 --> 0:04:3,000
« Celle-ci est probablement plus proche de ce que vous cherchez. »

89
0:04:32.023,000 --> 0:04:35,000
L'avantage est que cette méthode est très puissante.

90
0:04:35.117,000 --> 0:04:37,000
Le chef de l'IA chez Google l'a appelée :

91
0:04:37.217,000 --> 0:04:39,000
« l'efficacité irraisonnable des données ».

92
0:04:39.791,000 --> 0:04:4,000
L'inconvénient est :

93
0:04:41.738,000 --> 0:04:44,000
nous ne comprenons pas vraiment ce que le système a appris.

94
0:04:44.833,000 --> 0:04:45,000
En fait, c'est sa force.

95
0:04:46.946,000 --> 0:04:49,000
C'est moins comme donner des instructions à un ordinateur ;

96
0:04:51.2,000 --> 0:04:55,000
plus comme entraîner une machine-chiot-créature

97
0:04:55.288,000 --> 0:04:57,000
que nous ne comprenons ni ne contrôlons vraiment.

98
0:04:58.362,000 --> 0:04:59,000
Voilà notre problème.

99
0:05:00.427,000 --> 0:05:04,000
C'est un problème quand cette intelligence artificielle comprend mal les choses.

100
0:05:04.713,000 --> 0:05:07,000
C'est aussi un problème quand elle comprend les choses

101
0:05:08.277,000 --> 0:05:11,000
car on ne sait pas différencier ces situations pour un problème subjectif.

102
0:05:11.929,000 --> 0:05:13,000
Nous ignorons ce que pense cette chose.

103
0:05:15.493,000 --> 0:05:18,000
Considérez un algorithme d'embauche --

104
0:05:20.123,000 --> 0:05:24,000
un système utilisé pour embaucher des gens en utilisant l'apprentissage des machines.

105
0:05:25.052,000 --> 0:05:28,000
Un tel système aurait été entraîné sur les données des employés

106
0:05:28.655,000 --> 0:05:3,000
et chargé de trouver et embaucher

107
0:05:31.27,000 --> 0:05:34,000
des gens similaires à ceux les plus performants de l'entreprise.

108
0:05:34.814,000 --> 0:05:35,000
Cela semble bien.

109
0:05:35.991,000 --> 0:05:36,000
Une fois, j'ai assisté à une conférence

110
0:05:37.924,000 --> 0:05:4,000
qui réunissait responsables des ressources humaines et des dirigeants,

111
0:05:41.209,000 --> 0:05:42,000
des gens de haut niveau,

112
0:05:42.393,000 --> 0:05:43,000
avec de tels systèmes d'embauche.

113
0:05:43.982,000 --> 0:05:44,000
Ils étaient très excités.

114
0:05:45.646,000 --> 0:05:49,000
Ils pensaient que cela rendrait l'embauche plus objective, moins biaisée

115
0:05:50.323,000 --> 0:05:53,000
et donnerait plus de chances aux femmes et minorités

116
0:05:53.347,000 --> 0:05:55,000
face à des responsables RH partiaux.

117
0:05:55.559,000 --> 0:05:57,000
L'embauche humaine est partiale.

118
0:05:59.099,000 --> 0:06:,000
Je sais.

119
0:06:00.308,000 --> 0:06:03,000
Dans l'un de mes premiers postes en tant que programmeuse,

120
0:06:03.337,000 --> 0:06:06,000
ma responsable directe venait parfois me voir

121
0:06:07.229,000 --> 0:06:1,000
très tôt le matin ou très tard l'après-midi

122
0:06:11.006,000 --> 0:06:14,000
et elle disait : « Zeinep, allons déjeuner ! »

123
0:06:14.724,000 --> 0:06:16,000
L'heure étrange me laissait perplexe.

124
0:06:16.915,000 --> 0:06:18,000
Il est 16h, déjeuner ?

125
0:06:19.068,000 --> 0:06:22,000
J'étais fauchée, le déjeuner était gratuit donc j'y allais toujours.

126
0:06:22.618,000 --> 0:06:24,000
Plus tard, j'ai réalisé ce qu'il se passait.

127
0:06:24.709,000 --> 0:06:28,000
Mes responsables directs n'avaient pas dit à leurs responsables

128
0:06:29.279,000 --> 0:06:32,000
qu'ils avaient embauché pour un travail sérieux une adolescente

129
0:06:32.416,000 --> 0:06:35,000
qui portait un jeans et des baskets au travail.

130
0:06:37.174,000 --> 0:06:39,000
Je faisais du bon travail mais mon allure clochait,

131
0:06:39.606,000 --> 0:06:4,000
j'avais les mauvais âge et sexe.

132
0:06:41.139,000 --> 0:06:44,000
Embaucher d'une manière aveugle à la couleur et au sexe

133
0:06:44.493,000 --> 0:06:45,000
me semble très bien.

134
0:06:47.031,000 --> 0:06:5,000
Mais avec ces systèmes, c'est plus compliqué, voici pourquoi :

135
0:06:50.968,000 --> 0:06:55,000
actuellement, les systèmes informatiques peuvent déduire beaucoup vous concernant

136
0:06:56.783,000 --> 0:06:57,000
grâce à vos miettes numériques,

137
0:06:58.679,000 --> 0:07:,000
même si vous n'avez rien révélé.

138
0:07:01.506,000 --> 0:07:03,000
Ils peuvent déduire votre orientation sexuelle,

139
0:07:04.994,000 --> 0:07:05,000
vos traits de personnalité,

140
0:07:06.859,000 --> 0:07:07,000
vos tendances politiques.

141
0:07:08.83,000 --> 0:07:11,000
Ils ont des pouvoirs prédictifs ayant une exactitude élevée.

142
0:07:13.362,000 --> 0:07:15,000
Pour des choses que vous n'avez pas révélées.

143
0:07:15.964,000 --> 0:07:16,000
C'est de la déduction.

144
0:07:17.579,000 --> 0:07:2,000
J'ai une amie qui a développé de tels systèmes informatiques

145
0:07:20.864,000 --> 0:07:23,000
pour prévoir la probabilité d'une dépression clinique ou post-partum

146
0:07:24.529,000 --> 0:07:25,000
grâce à vos médias sociaux.

147
0:07:26.676,000 --> 0:07:27,000
Les résultats sont impressionnants.

148
0:07:28.492,000 --> 0:07:31,000
Son système peut prévoir les risques de dépression

149
0:07:31.873,000 --> 0:07:34,000
des mois avant l'apparition de tout symptôme --

150
0:07:35.8,000 --> 0:07:36,000
des mois avant.

151
0:07:37.197,000 --> 0:07:39,000
Aucun symptôme mais une prédiction.

152
0:07:39.467,000 --> 0:07:43,000
Elle espère que cela sera utilisé pour des interventions précoces, super !

153
0:07:44.911,000 --> 0:07:46,000
Mais mettez cela dans le contexte de l'embauche.

154
0:07:48.027,000 --> 0:07:51,000
Lors de cette conférence de responsables des ressources humaines,

155
0:07:51.097,000 --> 0:07:55,000
j'ai approché une responsable d'une très grande entreprise

156
0:07:55.83,000 --> 0:07:59,000
et lui ai dit : « Et si, à votre insu,

157
0:08:00.432,000 --> 0:08:06,000
votre système éliminait les gens avec de forts risques de dépression ?

158
0:08:07.761,000 --> 0:08:1,000
Ils ne sont pas en dépression mais ont plus de risques pour l'avenir.

159
0:08:11.923,000 --> 0:08:14,000
Et s'il éliminait les femmes ayant plus de chances d'être enceintes

160
0:08:15.353,000 --> 0:08:17,000
dans un ou deux ans mais ne le sont pas actuellement ?

161
0:08:18.844,000 --> 0:08:23,000
Et s'il embauchait des gens agressifs car c'est la culture de l'entreprise ? »

162
0:08:25.173,000 --> 0:08:27,000
On ne peut pas le dire en regardant la répartition par sexe.

163
0:08:27.994,000 --> 0:08:28,000
Cela peut être équilibré.

164
0:08:29.414,000 --> 0:08:32,000
Puisque c'est de l'apprentissage de la machine, non du code traditionnel,

165
0:08:32.995,000 --> 0:08:36,000
il n'y a pas de variables appelées « plus de risques de dépression »,

166
0:08:37.926,000 --> 0:08:38,000
« plus de risques d'être enceinte »,

167
0:08:39.783,000 --> 0:08:4,000
« échelle d'agressivité d'un mec ».

168
0:08:41.995,000 --> 0:08:44,000
Non seulement vous ignorez ce que votre système utilise pour choisir,

169
0:08:45.698,000 --> 0:08:47,000
mais vous ignorez où chercher.

170
0:08:48.045,000 --> 0:08:49,000
C'est une boîte noire.

171
0:08:49.315,000 --> 0:08:51,000
Elle a un pouvoir prédictif mais vous ne le comprenez pas.

172
0:08:52.486,000 --> 0:08:54,000
J'ai demandé : « Quelle garantie avez-vous

173
0:08:54.879,000 --> 0:08:57,000
pour vous assurer que votre boîte noire ne fait rien de louche ? »

174
0:09:00.863,000 --> 0:09:03,000
Elle m'a regardée comme si je venais de l'insulter.

175
0:09:04.765,000 --> 0:09:05,000
(Rires)

176
0:09:06.037,000 --> 0:09:08,000
Elle m'a fixée et m'a dit :

177
0:09:08.556,000 --> 0:09:12,000
« Je ne veux rien entendre de plus. »

178
0:09:13.458,000 --> 0:09:15,000
Puis elle s'est tournée et est partie.

179
0:09:16.064,000 --> 0:09:17,000
Elle n'était pas impolie.

180
0:09:17.574,000 --> 0:09:18,000
C'était clairement du :

181
0:09:18.906,000 --> 0:09:23,000
« ce que j'ignore n'est pas mon problème, allez-vous en, regard meurtrier ».

182
0:09:23.946,000 --> 0:09:24,000
(Rires)

183
0:09:25.862,000 --> 0:09:28,000
Un tel système pourrait être moins biaisé

184
0:09:29.725,000 --> 0:09:31,000
que les responsables humains.

185
0:09:31.852,000 --> 0:09:33,000
Et il pourrait être monétairement censé.

186
0:09:34.573,000 --> 0:09:35,000
Mais il pourrait aussi mener

187
0:09:36.247,000 --> 0:09:4,000
à une fermeture du marché du travail stable mais dissimulée

188
0:09:41.019,000 --> 0:09:43,000
pour les gens avec plus de risques de dépression.

189
0:09:43.753,000 --> 0:09:45,000
Est-ce le genre de société que nous voulons bâtir,

190
0:09:46.373,000 --> 0:09:48,000
sans même savoir que nous l'avons fait,

191
0:09:48.682,000 --> 0:09:5,000
car nous avons confié la prise de décisions à des machines

192
0:09:51.405,000 --> 0:09:52,000
que nous ne comprenons pas vraiment ?

193
0:09:53.265,000 --> 0:09:54,000
Un autre problème :

194
0:09:55.314,000 --> 0:09:59,000
ces systèmes sont souvent entraînés sur des données générées par nos actions,

195
0:09:59.79,000 --> 0:10:,000
des empreintes humaines.

196
0:10:02.188,000 --> 0:10:05,000
Elles pourraient refléter nos préjugés

197
0:10:06.02,000 --> 0:10:09,000
et ces systèmes pourraient apprendre nos préjugés,

198
0:10:09.637,000 --> 0:10:1,000
les amplifier

199
0:10:10.974,000 --> 0:10:11,000
et nous les retourner

200
0:10:12.416,000 --> 0:10:13,000
alors que nous nous disons :

201
0:10:13.902,000 --> 0:10:16,000
« Nous ne faisons que de l'informatique neutre et objective. »

202
0:10:18.314,000 --> 0:10:2,000
Des chercheurs chez Google ont découvert

203
0:10:22.134,000 --> 0:10:25,000
qu'on a moins de chances de montrer aux femmes plutôt qu'aux hommes

204
0:10:25.463,000 --> 0:10:28,000
des offres d'emploi avec un salaire élevé.

205
0:10:28.463,000 --> 0:10:3,000
Et chercher des noms afro-américains

206
0:10:31.017,000 --> 0:10:32,000
a plus de chances de retourner

207
0:10:32.747,000 --> 0:10:35,000
des publicités suggérant un historique criminel,

208
0:10:35.747,000 --> 0:10:36,000
même quand il n'y en a pas.

209
0:10:38.693,000 --> 0:10:41,000
De tels préjugés cachés et des algorithmes boîte noire

210
0:10:42.266,000 --> 0:10:45,000
qui sont parfois découverts par les chercheurs, parfois non,

211
0:10:46.263,000 --> 0:10:48,000
peuvent avoir des conséquences qui changent la vie.

212
0:10:49.958,000 --> 0:10:53,000
Dans le Wisconsin, un prévenu a été condamné à 6 ans de prison

213
0:10:54.141,000 --> 0:10:55,000
pour avoir échappé à la police.

214
0:10:56.824,000 --> 0:10:57,000
Vous l'ignorez peut-être

215
0:10:58.034,000 --> 0:11:01,000
mais des algorithmes sont utilisés pour les probations et les condamnations.

216
0:11:02.056,000 --> 0:11:04,000
Nous voulions savoir comment ce score était calculé.

217
0:11:05.795,000 --> 0:11:06,000
C'est une boîte noire commerciale.

218
0:11:07.484,000 --> 0:11:11,000
L'entreprise a refusé que l'on conteste son algorithme en audience publique.

219
0:11:12.396,000 --> 0:11:17,000
Mais ProPublica, une organisation d'enquête, a audité cet algorithme

220
0:11:17.952,000 --> 0:11:19,000
avec des données publiques

221
0:11:19.992,000 --> 0:11:21,000
et a découvert que les résultats étaient biaisés,

222
0:11:22.332,000 --> 0:11:25,000
que son pouvoir prédictif était mauvais, à peine meilleur que la chance,

223
0:11:25.985,000 --> 0:11:29,000
et qu'il étiquetait les prévenus noirs comme de futurs criminels

224
0:11:30.425,000 --> 0:11:33,000
avec un taux deux fois plus élevé que pour les prévenus blancs.

225
0:11:35.891,000 --> 0:11:36,000
Considérez ce cas :

226
0:11:38.103,000 --> 0:11:41,000
cette femme était en retard pour récupérer sa filleule

227
0:11:41.979,000 --> 0:11:43,000
à une école du comté de Broward, en Floride,

228
0:11:44.757,000 --> 0:11:46,000
elle courait dans la rue avec une amie à elle.

229
0:11:47.137,000 --> 0:11:51,000
Elles ont repéré une bécane et un vélo non attachés sur un porche

230
0:11:51.26,000 --> 0:11:52,000
et ont bêtement sauté dessus.

231
0:11:52.916,000 --> 0:11:54,000
Alors qu'elles partaient, une femme est sortie et a dit :

232
0:11:55.585,000 --> 0:11:57,000
« Hey ! C'est la bécane de mon fils ! »

233
0:11:57.768,000 --> 0:12:,000
Elles l'ont lâchée, sont parties mais ont été arrêtées.

234
0:12:01.086,000 --> 0:12:04,000
Elle avait tort, elle a été idiote mais elle n'avait que 18 ans.

235
0:12:04.747,000 --> 0:12:06,000
Adolescente, elle avait commis quelques méfaits.

236
0:12:07.808,000 --> 0:12:12,000
Pendant ce temps, cet homme a été arrêté pour vol chez Home Depot --

237
0:12:13.017,000 --> 0:12:15,000
pour une valeur de 85$, un crime mineur similaire.

238
0:12:16.766,000 --> 0:12:2,000
Mais il avait deux condamnations pour vol à main armée.

239
0:12:21.955,000 --> 0:12:24,000
L'algorithme l'a considérée elle, comme étant un risque important, pas lui.

240
0:12:26.746,000 --> 0:12:29,000
Deux ans plus tard, ProPublica a découvert qu'elle n'avait pas récidivé.

241
0:12:30.584,000 --> 0:12:32,000
Son casier judiciaire compliquait sa recherche d'emploi.

242
0:12:33.234,000 --> 0:12:35,000
Lui, d'un autre côté, avait récidivé

243
0:12:35.318,000 --> 0:12:38,000
et avait été condamné à 8 ans pour un autre crime.

244
0:12:40.088,000 --> 0:12:43,000
Clairement, nous devons auditer nos boîtes noires

245
0:12:43.481,000 --> 0:12:45,000
et ne pas leur laisser ce genre de pouvoir incontrôlé.

246
0:12:46.12,000 --> 0:12:48,000
(Applaudissements)

247
0:12:50.087,000 --> 0:12:51,000
Les audits sont importants,

248
0:12:51.383,000 --> 0:12:53,000
mais ils ne résolvent pas tous nos problèmes.

249
0:12:54.353,000 --> 0:12:56,000
Prenez le puissant algorithme du fil d'actualités Facebook,

250
0:12:57.151,000 --> 0:13:01,000
celui qui classe tout et décide quoi vous montrer

251
0:13:01.992,000 --> 0:13:03,000
des amis et des pages que vous suivez.

252
0:13:04.898,000 --> 0:13:06,000
Devrait-on vous montrer une autre photo de bébé ?

253
0:13:07.243,000 --> 0:13:08,000
(Rires)

254
0:13:08.417,000 --> 0:13:1,000
Une note maussade d'une connaissance ?

255
0:13:11.449,000 --> 0:13:12,000
Une actualité importante mais dure ?

256
0:13:13.329,000 --> 0:13:14,000
Il n'y a pas de bonne réponse.

257
0:13:14.835,000 --> 0:13:16,000
Facebook optimise pour vous engager envers le site :

258
0:13:17.518,000 --> 0:13:18,000
les j'aime, partages, commentaires.

259
0:13:20.168,000 --> 0:13:22,000
En août 2014,

260
0:13:22.818,000 --> 0:13:24,000
des manifestations ont éclaté à Ferguson, dans le Missouri,

261
0:13:25.6,000 --> 0:13:29,000
après qu'un adolescent afro-américain a été tué par un officier de police blanc

262
0:13:30.015,000 --> 0:13:31,000
dans des circonstances douteuses.

263
0:13:31.974,000 --> 0:13:35,000
La nouvelle des manifestations remplissait mon fil d'actualité Twitter non filtré

264
0:13:36.714,000 --> 0:13:37,000
mais n'était pas sur mon Facebook.

265
0:13:39.182,000 --> 0:13:4,000
Était-ce mes amis Facebook ?

266
0:13:40.94,000 --> 0:13:42,000
J'ai désactivé l'algorithme Facebook,

267
0:13:43.472,000 --> 0:13:45,000
ce qui est difficile car Facebook veut vous faire passer

268
0:13:46.344,000 --> 0:13:48,000
sous le contrôle de l'algorithme,

269
0:13:48.404,000 --> 0:13:5,000
et j'ai vu que mes amis en parlaient.

270
0:13:50.666,000 --> 0:13:52,000
C'est juste que l'algorithme ne me le montrait pas.

271
0:13:53.199,000 --> 0:13:56,000
Après des recherches, j'ai découvert que le problème est répandu.

272
0:13:56.351,000 --> 0:13:59,000
L'histoire de Ferguson ne plaisait pas à l'algorithme.

273
0:14:00.102,000 --> 0:14:03,000
Ce n'était pas « aimable », qui allait cliquer sur « j'aime » ?

274
0:14:03.5,000 --> 0:14:05,000
Ce n'est même pas facile à commenter.

275
0:14:05.73,000 --> 0:14:06,000
Sans j'aime et commentaires,

276
0:14:07.125,000 --> 0:14:1,000
l'algorithme allait le montrer à un nombre décroissant de gens,

277
0:14:10.441,000 --> 0:14:11,000
donc nous ne pouvions le voir.

278
0:14:12.946,000 --> 0:14:13,000
Cette semaine-là,

279
0:14:14.198,000 --> 0:14:16,000
Facebook a plutôt souligné ceci,

280
0:14:16.52,000 --> 0:14:18,000
le Ice Bucket Challenge.

281
0:14:18.77,000 --> 0:14:21,000
Cause méritante, lâcher d'eau glacée, donner à une charité, très bien.

282
0:14:22.536,000 --> 0:14:23,000
Cela plaisait beaucoup à l'algorithme.

283
0:14:25.219,000 --> 0:14:27,000
La machine a pris cette décision pour nous.

284
0:14:27.856,000 --> 0:14:3,000
Une conversation très importante mais difficile

285
0:14:31.377,000 --> 0:14:32,000
aurait pu être étouffée

286
0:14:32.956,000 --> 0:14:34,000
si Facebook avait été le seul canal.

287
0:14:36.117,000 --> 0:14:39,000
Finalement, ces systèmes peuvent aussi avoir tort

288
0:14:39.938,000 --> 0:14:41,000
de façons qui ne ressemblent pas aux systèmes humains.

289
0:14:42.698,000 --> 0:14:44,000
Vous souvenez-vous de Watson, le système d'IA d'IBM

290
0:14:45.644,000 --> 0:14:48,000
qui a éliminé les participants humains dans Jeopardy ?

291
0:14:49.131,000 --> 0:14:5,000
C'était un super joueur.

292
0:14:50.583,000 --> 0:14:53,000
Mais, pour la finale de Jeopardy, on a posé cette question à Watson :

293
0:14:54.659,000 --> 0:14:56,000
« Le plus grand aéroport ayant le nom d'un héros de 39-45,

294
0:14:57.565,000 --> 0:14:59,000
le second plus grand pour une bataille de 39-45. »

295
0:14:59.897,000 --> 0:15:,000
(Signal sonore de fin)

296
0:15:01.582,000 --> 0:15:02,000
Chicago.

297
0:15:02.788,000 --> 0:15:03,000
Les deux humains avaient raison.

298
0:15:04.697,000 --> 0:15:08,000
Watson, par contre, a répondu « Toronto » --

299
0:15:09.069,000 --> 0:15:11,000
à une question sur les villes des États-Unis !

300
0:15:11.597,000 --> 0:15:13,000
L'impressionnant système a aussi fait une erreur

301
0:15:14.521,000 --> 0:15:17,000
qu'un humain ne ferait jamais, qu'un CE1 ne ferait jamais.

302
0:15:18.823,000 --> 0:15:21,000
Notre intelligence artificielle peut échouer

303
0:15:21.956,000 --> 0:15:24,000
de façons ne correspondant pas aux schémas d'erreurs humaines,

304
0:15:25.08,000 --> 0:15:27,000
de façons inattendues et imprévues.

305
0:15:28.054,000 --> 0:15:31,000
Il serait lamentable de ne pas obtenir un emploi pour lequel on est qualifié

306
0:15:31.716,000 --> 0:15:34,000
mais ce serait pire si c'était à cause d'un dépassement de pile

307
0:15:35.467,000 --> 0:15:36,000
dans une sous-routine.

308
0:15:36.923,000 --> 0:15:37,000
(Rires)

309
0:15:38.526,000 --> 0:15:4,000
En mai 2010,

310
0:15:41.336,000 --> 0:15:45,000
un crash éclair sur Wall Street alimenté par une boucle de rétroaction

311
0:15:45.404,000 --> 0:15:48,000
dans un algorithme de vente de Wall Street

312
0:15:48.456,000 --> 0:15:52,000
a fait perdre mille milliards de dollars en 36 minutes.

313
0:15:53.722,000 --> 0:15:55,000
Je refuse de penser au sens du mot « erreur »

314
0:15:55.933,000 --> 0:15:58,000
dans le contexte des armes mortelles autonomes.

315
0:16:01.894,000 --> 0:16:04,000
Oui, les humains ont toujours été partiaux.

316
0:16:05.708,000 --> 0:16:07,000
Les preneurs de décision et gardiens,

317
0:16:07.908,000 --> 0:16:1,000
dans les tribunaux, les actualités, en guerre...

318
0:16:11.425,000 --> 0:16:14,000
Ils font des erreurs ; mais c'est de cela dont je parle.

319
0:16:14.487,000 --> 0:16:17,000
Nous ne pouvons pas échapper à ces questions difficiles.

320
0:16:18.596,000 --> 0:16:21,000
Nous ne pouvons pas sous-traiter nos responsabilités aux machines.

321
0:16:22.676,000 --> 0:16:26,000
(Applaudissements)

322
0:16:29.089,000 --> 0:16:33,000
L'intelligence artificielle n'offre pas une carte « sortie de l'éthique ».

323
0:16:34.742,000 --> 0:16:37,000
Le scientifique des données Fred Benenson qualifie cela de lavage des maths.

324
0:16:38.373,000 --> 0:16:39,000
Il nous faut l'opposé.

325
0:16:39.56,000 --> 0:16:44,000
Nous devons cultiver la suspicion, le contrôle et l'enquête de l'algorithme.

326
0:16:45.38,000 --> 0:16:48,000
Nous devons nous assurer de la responsabilité des algorithmes,

327
0:16:48.602,000 --> 0:16:5,000
les auditer et avoir une transparence significative.

328
0:16:51.38,000 --> 0:16:54,000
Nous devons accepter qu'apporter les maths et l'informatique

329
0:16:54.638,000 --> 0:16:57,000
dans les affaires humaines désordonnées et basées sur des valeurs

330
0:16:57.688,000 --> 0:16:59,000
n'apporte pas l'objectivité

331
0:17:00.04,000 --> 0:17:02,000
mais plutôt que la complexité des affaires humaines

332
0:17:02.448,000 --> 0:17:03,000
envahit les algorithmes.

333
0:17:04.148,000 --> 0:17:07,000
Nous devrions utiliser l'informatique

334
0:17:07.659,000 --> 0:17:09,000
pour prendre de meilleures décisions.

335
0:17:09.697,000 --> 0:17:14,000
Mais nous devons assumer notre responsabilité morale de jugement

336
0:17:15.053,000 --> 0:17:17,000
et utiliser les algorithmes dans ce cadre,

337
0:17:17.895,000 --> 0:17:21,000
pas comme un moyen d'abdiquer et sous-traiter nos responsabilités

338
0:17:22.854,000 --> 0:17:24,000
d'un humain à un autre.

339
0:17:25.807,000 --> 0:17:27,000
L'intelligence artificielle est arrivée.

340
0:17:28.44,000 --> 0:17:31,000
Cela signifie que nous devons nous accrocher encore plus

341
0:17:31.885,000 --> 0:17:33,000
aux valeurs et éthiques humaines.

342
0:17:34.056,000 --> 0:17:35,000
Merci.

343
0:17:35.234,000 --> 0:17:4,000
(Applaudissements)

