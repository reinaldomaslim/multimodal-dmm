1
0:00:,000 --> 0:00:07,000
Traductor: Claudia Viveros Revisor: Frank Zegarra

2
0:00:11.82,000 --> 0:00:15,000
Cuando era niño, era un nerd por excelencia.

3
0:00:17.14,000 --> 0:00:19,000
Creo que algunos de Uds. también.

4
0:00:19.34,000 --> 0:00:2,000
(Risas)

5
0:00:20.58,000 --> 0:00:23,000
Y Ud. señor, el que se rio más fuerte, probablemente aún lo sea.

6
0:00:23.82,000 --> 0:00:25,000
(Risas)

7
0:00:26.1,000 --> 0:00:29,000
Crecí en un pueblo pequeño en los llanos del norte de Tejas,

8
0:00:29.62,000 --> 0:00:32,000
hijo de un comisario, que fue hijo de un pastor.

9
0:00:32.98,000 --> 0:00:33,000
Meterse en problemas no era una opción.

10
0:00:35.86,000 --> 0:00:38,000
Así es que comencé a leer libros de cálculo por diversión.

11
0:00:39.14,000 --> 0:00:4,000
(Risas)

12
0:00:40.7,000 --> 0:00:41,000
Uds. también lo hicieron.

13
0:00:42.42,000 --> 0:00:45,000
Eso me llevó a crear un láser, una computadora y modelos de cohetes,

14
0:00:46.18,000 --> 0:00:49,000
y eso me llevó a hacer combustible para cohetes en mi habitación.

15
0:00:49.78,000 --> 0:00:52,000
En términos científicos,

16
0:00:53.46,000 --> 0:00:56,000
esta es una muy mala idea.

17
0:00:56.74,000 --> 0:00:57,000
(Risas)

18
0:00:57.98,000 --> 0:00:59,000
Por esos días

19
0:01:00.18,000 --> 0:01:03,000
se estrenó "2001: Una Odisea en el Espacio" de Stanley Kubrick,

20
0:01:03.42,000 --> 0:01:05,000
y mi vida cambió para siempre.

21
0:01:06.1,000 --> 0:01:08,000
Me encantó todo de esa película,

22
0:01:08.18,000 --> 0:01:1,000
especialmente el HAL 9000.

23
0:01:10.74,000 --> 0:01:12,000
HAL era un computador sensible

24
0:01:12.82,000 --> 0:01:14,000
diseñado para guiar la aeronave Discovery

25
0:01:15.3,000 --> 0:01:17,000
de la Tierra a Júpiter.

26
0:01:17.86,000 --> 0:01:19,000
HAL además era imperfecto,

27
0:01:19.94,000 --> 0:01:23,000
porque eligió el valor de la misión sobre la vida humana.

28
0:01:24.66,000 --> 0:01:26,000
HAL era un personaje ficticio,

29
0:01:26.78,000 --> 0:01:28,000
y sin embargo habla sobre nuestros temores,

30
0:01:29.46,000 --> 0:01:31,000
nuestros temores a ser subyugados

31
0:01:31.58,000 --> 0:01:34,000
por alguna insensible inteligencia artificial

32
0:01:34.62,000 --> 0:01:35,000
indiferente a nuestra humanidad.

33
0:01:37.7,000 --> 0:01:39,000
Creo que esos temores son infundados.

34
0:01:40.3,000 --> 0:01:42,000
De hecho, estamos en un tiempo notable

35
0:01:43.02,000 --> 0:01:44,000
en la historia de la humanidad,

36
0:01:44.58,000 --> 0:01:48,000
donde, motivados por la negación a aceptar los límites del cuerpo y mente,

37
0:01:49.58,000 --> 0:01:5,000
construimos máquinas

38
0:01:51.3,000 --> 0:01:54,000
de complejidad y gracia exquisita y hermosa

39
0:01:54.94,000 --> 0:01:56,000
que expanden la experiencia humana

40
0:01:57.02,000 --> 0:01:58,000
más allá de nuestra imaginación.

41
0:01:59.24,000 --> 0:02:02,000
Tras una carrera que me llevó de la Academia de la Fuerza Aérea

42
0:02:02.286,000 --> 0:02:03,000
a comandante de estación espacial,

43
0:02:04.1,000 --> 0:02:05,000
me volví ingeniero de sistemas,

44
0:02:05.82,000 --> 0:02:07,000
y recientemente me encontré con un problema de ingeniería

45
0:02:08.58,000 --> 0:02:1,000
relacionado con la misión de la NASA a Marte.

46
0:02:11.18,000 --> 0:02:13,000
En vuelos espaciales a la Luna,

47
0:02:13.7,000 --> 0:02:16,000
dependemos del control de la misión en Houston

48
0:02:16.78,000 --> 0:02:18,000
para vigilar todos los aspectos de un vuelo.

49
0:02:18.86,000 --> 0:02:21,000
Sin embargo, Marte está 200 veces más lejos,

50
0:02:22.42,000 --> 0:02:25,000
como resultado toma en promedio 13 minutos

51
0:02:25.66,000 --> 0:02:28,000
para que una señal viaje de la Tierra a Marte.

52
0:02:28.82,000 --> 0:02:31,000
Si hay algún percance, no hay suficiente tiempo.

53
0:02:32.66,000 --> 0:02:34,000
Una solución razonable de ingeniería

54
0:02:35.18,000 --> 0:02:37,000
nos incita a instalar el control de la misión

55
0:02:37.78,000 --> 0:02:4,000
dentro de las paredes de la aeronave Orión.

56
0:02:40.82,000 --> 0:02:42,000
Otra idea fascinante en el perfil de la misión

57
0:02:43.74,000 --> 0:02:45,000
es colocar robots humanoides en la superficie de Marte

58
0:02:46.62,000 --> 0:02:47,000
antes de que los humanos mismos lleguen,

59
0:02:48.6,000 --> 0:02:49,000
primero para construir instalaciones

60
0:02:50.356,000 --> 0:02:53,000
y luego para servir como colaboradores en el equipo científico.

61
0:02:55.22,000 --> 0:02:57,000
Al mirar esto desde la ingeniería,

62
0:02:57.98,000 --> 0:03:,000
me quedó claro que lo que necesitaba diseñar

63
0:03:01.18,000 --> 0:03:03,000
era una inteligencia artificial muy lista,

64
0:03:03.38,000 --> 0:03:05,000
colaborativa y social.

65
0:03:05.78,000 --> 0:03:09,000
En otras palabras, necesitaba construir algo muy parecido a HAL

66
0:03:10.1,000 --> 0:03:12,000
pero sin las tendencias homicidas.

67
0:03:12.54,000 --> 0:03:13,000
(Risas)

68
0:03:14.74,000 --> 0:03:15,000
Detengámonos un momento.

69
0:03:16.58,000 --> 0:03:19,000
¿Es realmente posible construir inteligencia artificial así?

70
0:03:20.5,000 --> 0:03:21,000
De hecho, sí lo es.

71
0:03:21.98,000 --> 0:03:22,000
En muchas maneras,

72
0:03:23.22,000 --> 0:03:25,000
este es un problema difícil de la ingeniería

73
0:03:25.316,000 --> 0:03:26,000
con elementos de IA,

74
0:03:26.74,000 --> 0:03:3,000
no es un simple problema de IA que necesite diseñarse.

75
0:03:31.46,000 --> 0:03:33,000
Parafraseando a Alan Turing,

76
0:03:34.06,000 --> 0:03:36,000
no me interesa construir una máquina sensible.

77
0:03:36.54,000 --> 0:03:37,000
No voy a construir un HAL.

78
0:03:38.14,000 --> 0:03:4,000
Todo lo que busco es un cerebro sencillo,

79
0:03:40.58,000 --> 0:03:43,000
algo que ofrezca una ilusión de inteligencia.

80
0:03:44.82,000 --> 0:03:47,000
El arte y la ciencia de la computación ha pasado por mucho

81
0:03:47.98,000 --> 0:03:48,000
desde la aparición de HAL,

82
0:03:49.5,000 --> 0:03:52,000
e imagino que si su inventor Dr. Chandra estuviera aquí hoy,

83
0:03:52.74,000 --> 0:03:54,000
tendría mucho que preguntarnos.

84
0:03:55.1,000 --> 0:03:57,000
¿Es realmente posible

85
0:03:57.22,000 --> 0:04:01,000
tomar un sistema de millones y millones de dispositivos,

86
0:04:01.26,000 --> 0:04:02,000
para leer flujos de datos,

87
0:04:02.74,000 --> 0:04:04,000
para predecir fallas y actuar con antelación?

88
0:04:05.02,000 --> 0:04:06,000
Sí.

89
0:04:06.26,000 --> 0:04:09,000
¿Podemos construir sistemas que se comuniquen con humanos?

90
0:04:09.46,000 --> 0:04:1,000
Sí.

91
0:04:10.7,000 --> 0:04:12,000
¿Podemos construir sistemas que reconozcan objetos, emociones,

92
0:04:13.7,000 --> 0:04:16,000
que se emocionen, jueguen e incluso que lean los labios?

93
0:04:17.1,000 --> 0:04:18,000
Sí.

94
0:04:18.28,000 --> 0:04:2,000
¿Podemos construir un sistema que fije objetivos,

95
0:04:20.616,000 --> 0:04:23,000
que lleve a cabo planes contra esos objetivos y siga aprendiendo más?

96
0:04:24.14,000 --> 0:04:25,000
Sí.

97
0:04:25.38,000 --> 0:04:28,000
¿Podemos construir sistemas que piensen por sí mismos?

98
0:04:28.74,000 --> 0:04:29,000
Estamos aprendiendo a hacerlo.

99
0:04:30.26,000 --> 0:04:33,000
¿Podemos construir sistemas con fundamentos éticos y morales?

100
0:04:34.3,000 --> 0:04:36,000
Debemos aprender cómo hacerlo.

101
0:04:37.18,000 --> 0:04:38,000
Aceptemos por un momento

102
0:04:38.58,000 --> 0:04:4,000
que es posible construir tal inteligencia artificial

103
0:04:41.5,000 --> 0:04:43,000
para estas misiones y otras.

104
0:04:43.66,000 --> 0:04:45,000
La siguiente pregunta que deben formularse es,

105
0:04:46.22,000 --> 0:04:47,000
¿deberíamos temerle?

106
0:04:47.7,000 --> 0:04:48,000
Toda nueva tecnología

107
0:04:49.7,000 --> 0:04:51,000
trae consigo algunos temores.

108
0:04:52.62,000 --> 0:04:53,000
Cuando vimos autos por primera vez,

109
0:04:54.34,000 --> 0:04:58,000
la gente temía que viéramos la destrucción de la familia.

110
0:04:58.38,000 --> 0:05:,000
Cuando se inventaron los teléfonos,

111
0:05:01.1,000 --> 0:05:03,000
la gente temía que arruinara la conversación civilizada.

112
0:05:04.02,000 --> 0:05:07,000
En algún momento, vimos a la palabra escrita aparecer en todos lados,

113
0:05:07.79,000 --> 0:05:09,000
la gente creyó que perderíamos la habilidad de memorizar.

114
0:05:10.5,000 --> 0:05:12,000
Todo esto es cierto hasta cierto grado,

115
0:05:12.58,000 --> 0:05:14,000
pero estas tecnologías también

116
0:05:15.02,000 --> 0:05:18,000
nos dieron cosas que expandieron la experiencia humana

117
0:05:18.42,000 --> 0:05:19,000
de manera profunda.

118
0:05:21.66,000 --> 0:05:23,000
Así que vayamos más lejos.

119
0:05:24.94,000 --> 0:05:28,000
No temo la creación de una IA así,

120
0:05:29.7,000 --> 0:05:32,000
porque finalmente encarnará algunos de nuestros valores.

121
0:05:33.54,000 --> 0:05:36,000
Consideren lo siguiente: construir un sistema cognitivo es muy distinto

122
0:05:37.06,000 --> 0:05:4,000
a construir uno tradicional intensivo en software como en el pasado.

123
0:05:40.38,000 --> 0:05:42,000
No los programamos. Les enseñamos.

124
0:05:42.86,000 --> 0:05:44,000
Para enseñarle a un sistema a reconocer flores,

125
0:05:45.54,000 --> 0:05:48,000
le muestro miles de flores que me gustan.

126
0:05:48.58,000 --> 0:05:5,000
Para enseñarle a jugar...

127
0:05:50.86,000 --> 0:05:51,000
Bueno, lo haría y Uds. también.

128
0:05:54.42,000 --> 0:05:56,000
Me gustan las flores.

129
0:05:57.26,000 --> 0:05:59,000
Para enseñarle a un sistema a jugar un juego como Go,

130
0:06:00.14,000 --> 0:06:02,000
lo pondría a jugar miles de juegos de Go,

131
0:06:02.19,000 --> 0:06:03,000
y en el proceso también le enseñaría

132
0:06:03.9,000 --> 0:06:05,000
a discernir un buen juego de uno malo.

133
0:06:06.34,000 --> 0:06:09,000
Si quiero crear un asistente legal con inteligencia artificial,

134
0:06:10.02,000 --> 0:06:11,000
le enseñaría algo del corpus legislativo

135
0:06:11.956,000 --> 0:06:13,000
combinando al mismo tiempo

136
0:06:14.74,000 --> 0:06:17,000
el sentido de la piedad y la justicia que también son parte de la ley.

137
0:06:18.38,000 --> 0:06:2,000
En términos científicos, lo llamamos verdad en tierra firme,

138
0:06:21.38,000 --> 0:06:23,000
este es el punto importante:

139
0:06:23.42,000 --> 0:06:24,000
al producir estas máquinas,

140
0:06:24.9,000 --> 0:06:27,000
les estamos enseñando también el sentido de nuestros valores.

141
0:06:28.34,000 --> 0:06:31,000
Para este fin, confío en la inteligencia artificial

142
0:06:31.5,000 --> 0:06:34,000
tanto, si no más, que en un humano bien entrenado.

143
0:06:35.9,000 --> 0:06:36,000
Quizá se pregunten

144
0:06:37.14,000 --> 0:06:39,000
¿qué hay de los agentes rebeldes,

145
0:06:39.78,000 --> 0:06:42,000
algunas organizaciones sólidas no gubernamentales?

146
0:06:43.14,000 --> 0:06:46,000
No le temo a la inteligencia artificial en manos de un lobo solitario.

147
0:06:46.98,000 --> 0:06:5,000
Claramente no podemos protegernos de todo tipo de violencia,

148
0:06:51.54,000 --> 0:06:53,000
en realidad, un sistema así

149
0:06:53.7,000 --> 0:06:56,000
requiere entrenamiento importante y sutil

150
0:06:56.82,000 --> 0:06:58,000
más allá de los recursos de un individuo.

151
0:06:59.14,000 --> 0:07:,000
Además,

152
0:07:00.38,000 --> 0:07:03,000
se trata de algo más que inyectar un virus en el internet global,

153
0:07:03.66,000 --> 0:07:06,000
donde al presionar un botón, se esparce a millones de lugares

154
0:07:06.78,000 --> 0:07:08,000
y las laptops comienzan a estallar en pedazos.

155
0:07:09.26,000 --> 0:07:11,000
Este tipo de sustancias son más grandes

156
0:07:12.1,000 --> 0:07:13,000
y se les ve venir.

157
0:07:14.34,000 --> 0:07:17,000
¿Temo que ese tipo de inteligencia artificial

158
0:07:17.42,000 --> 0:07:18,000
amenace a la humanidad?

159
0:07:20.1,000 --> 0:07:24,000
Si miran películas como "Matrix", "Metrópolis",

160
0:07:24.5,000 --> 0:07:27,000
"Terminator" y shows como "Westworld",

161
0:07:27.7,000 --> 0:07:29,000
todos hablan de este miedo.

162
0:07:29.86,000 --> 0:07:33,000
El libro "Superinteligencia" del filósofo Nick Bostrom,

163
0:07:34.18,000 --> 0:07:35,000
habla de este tema

164
0:07:35.74,000 --> 0:07:39,000
y opina que una superinteligencia no solo puede ser peligrosa,

165
0:07:39.78,000 --> 0:07:42,000
sino que podría representar una amenaza existencial para la humanidad.

166
0:07:43.66,000 --> 0:07:45,000
El argumento fundamental del Dr. Bostrom

167
0:07:45.9,000 --> 0:07:47,000
es que dichos sistemas finalmente

168
0:07:48.66,000 --> 0:07:51,000
tendrán una necesidad insaciable de información

169
0:07:51.94,000 --> 0:07:53,000
que quizá aprendan cómo aprender

170
0:07:54.86,000 --> 0:07:56,000
y finalmente descubran que pueden tener objetivos

171
0:07:57.5,000 --> 0:07:59,000
contrarios a las necesidades humanas.

172
0:07:59.82,000 --> 0:08:,000
El Dr. Bostrom tiene varios seguidores.

173
0:08:01.7,000 --> 0:08:05,000
Gente como Elon Musk y Stephen Hawking lo apoyan.

174
0:08:06.7,000 --> 0:08:08,000
Con todo respeto

175
0:08:09.98,000 --> 0:08:11,000
a estas mentes brillantes,

176
0:08:12.02,000 --> 0:08:14,000
creo que están completamente equivocados.

177
0:08:14.3,000 --> 0:08:17,000
Hay muchas partes del argumento del Dr. Bostrom que analizar,

178
0:08:17.5,000 --> 0:08:19,000
y no tengo tiempo para hacerlo,

179
0:08:19.66,000 --> 0:08:21,000
pero brevemente, piensen esto:

180
0:08:22.38,000 --> 0:08:25,000
el superconocimiento es muy distinto al superhacer.

181
0:08:25.98,000 --> 0:08:27,000
HAL era una amenaza para la flota del Discovery

182
0:08:28.196,000 --> 0:08:32,000
en la medida en que controlaba todos los aspectos del Discovery.

183
0:08:32.5,000 --> 0:08:34,000
Se necesitaría una superinteligencia.

184
0:08:35.02,000 --> 0:08:37,000
Y un dominio total de nuestro mundo.

185
0:08:37.54,000 --> 0:08:39,000
Skynet de la película "Terminator"

186
0:08:40.38,000 --> 0:08:41,000
tenía una superinteligencia

187
0:08:42.16,000 --> 0:08:43,000
que controlaba la voluntad humana,

188
0:08:43.776,000 --> 0:08:46,000
y controlaba cada dispositivo en todos los rincones del mundo.

189
0:08:47.54,000 --> 0:08:48,000
Siendo prácticos,

190
0:08:49.02,000 --> 0:08:51,000
eso no pasará.

191
0:08:51.14,000 --> 0:08:54,000
No estamos construyendo IAs que controlen el clima,

192
0:08:54.22,000 --> 0:08:55,000
que controlen las mareas,

193
0:08:55.58,000 --> 0:08:58,000
que nos controlen a nosotros, caprichosos y caóticos humanos.

194
0:08:58.98,000 --> 0:09:01,000
Además, si tal inteligencia artificial existiera,

195
0:09:02.9,000 --> 0:09:04,000
tendría que competir contra las economías del hombre,

196
0:09:05.86,000 --> 0:09:07,000
y por tanto competir por recursos contra nosotros.

197
0:09:09.02,000 --> 0:09:1,000
Y al final

198
0:09:10.26,000 --> 0:09:11,000
--no le digan a Siri--

199
0:09:12.08,000 --> 0:09:13,000
siempre podremos desconectarlos.

200
0:09:13.66,000 --> 0:09:15,000
(Risas)

201
0:09:17.18,000 --> 0:09:19,000
Estamos en una increíble aventura

202
0:09:19.66,000 --> 0:09:21,000
de coevolución con nuestras máquinas.

203
0:09:22.18,000 --> 0:09:24,000
Los humanos que somos hoy

204
0:09:24.7,000 --> 0:09:26,000
no son los humanos que seremos.

205
0:09:27.26,000 --> 0:09:3,000
Preocuparse ahorita del ascenso de una superinteligencia

206
0:09:30.42,000 --> 0:09:33,000
es una distracción peligrosa en muchos sentidos

207
0:09:33.5,000 --> 0:09:35,000
porque el ascenso de la computación

208
0:09:35.86,000 --> 0:09:38,000
trajo consigo muchos problemas humanos y sociales

209
0:09:38.9,000 --> 0:09:39,000
de los que debemos ocuparnos ahora.

210
0:09:41.18,000 --> 0:09:43,000
¿Cómo organizar a la sociedad

211
0:09:44.02,000 --> 0:09:46,000
cuando la necesidad de trabajo humano decrece?

212
0:09:46.38,000 --> 0:09:49,000
¿Cómo llevar conocimiento y educación a través del mundo

213
0:09:50.22,000 --> 0:09:51,000
respetando nuestras diferencias?

214
0:09:52.02,000 --> 0:09:56,000
¿Cómo expandir y mejorar la vida con la atención médica cognitiva?

215
0:09:56.3,000 --> 0:09:58,000
¿Cómo usar la computación

216
0:09:59.18,000 --> 0:10:,000
para ayudarnos a alcanzar las estrellas?

217
0:10:01.58,000 --> 0:10:03,000
Eso es lo emocionante.

218
0:10:04.22,000 --> 0:10:06,000
Las oportunidades para usar la computación

219
0:10:06.58,000 --> 0:10:07,000
y fomentar la experiencia humana

220
0:10:08.14,000 --> 0:10:09,000
están a nuestro alcance,

221
0:10:09.58,000 --> 0:10:1,000
aquí y ahora,

222
0:10:11.46,000 --> 0:10:12,000
y apenas hemos comenzado.

223
0:10:14.1,000 --> 0:10:15,000
Muchas gracias.

224
0:10:15.34,000 --> 0:10:19,000
(Aplausos)

