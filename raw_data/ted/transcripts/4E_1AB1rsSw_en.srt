1
0:00:13.04,000 --> 0:00:15,000
Do you remember when you were a child,

2
0:00:15.48,000 --> 0:00:18,000
you probably had a favorite toy that was a constant companion,

3
0:00:19.08,000 --> 0:00:21,000
like Christopher Robin had Winnie the Pooh,

4
0:00:21.72,000 --> 0:00:23,000
and your imagination fueled endless adventures?

5
0:00:25.64,000 --> 0:00:27,000
What could be more innocent than that?

6
0:00:28.8,000 --> 0:00:32,000
Well, let me introduce you to my friend Cayla.

7
0:00:34.6,000 --> 0:00:37,000
Cayla was voted toy of the year in countries around the world.

8
0:00:38.08,000 --> 0:00:41,000
She connects to the internet and uses speech recognition technology

9
0:00:41.68,000 --> 0:00:43,000
to answer your child's questions,

10
0:00:43.84,000 --> 0:00:44,000
respond just like a friend.

11
0:00:46.92,000 --> 0:00:49,000
But the power doesn't lie with your child's imagination.

12
0:00:50.6,000 --> 0:00:54,000
It actually lies with the company harvesting masses of personal information

13
0:00:55.16,000 --> 0:01:,000
while your family is innocently chatting away in the safety of their home,

14
0:01:00.72,000 --> 0:01:02,000
a dangerously false sense of security.

15
0:01:04.84,000 --> 0:01:06,000
This case sounded alarm bells for me,

16
0:01:07.52,000 --> 0:01:1,000
as it is my job to protect consumers' rights in my country.

17
0:01:11.8,000 --> 0:01:14,000
And with billions of devices such as cars,

18
0:01:15.32,000 --> 0:01:2,000
energy meters and even vacuum cleaners expected to come online by 2020,

19
0:01:20.44,000 --> 0:01:23,000
we thought this was a case worth investigating further.

20
0:01:24.4,000 --> 0:01:25,000
Because what was Cayla doing

21
0:01:26.32,000 --> 0:01:28,000
with all the interesting things she was learning?

22
0:01:28.88,000 --> 0:01:31,000
Did she have another friend she was loyal to and shared her information with?

23
0:01:33.64,000 --> 0:01:35,000
Yes, you guessed right. She did.

24
0:01:36.44,000 --> 0:01:38,000
In order to play with Cayla,

25
0:01:38.56,000 --> 0:01:41,000
you need to download an app to access all her features.

26
0:01:42.28,000 --> 0:01:45,000
Parents must consent to the terms being changed without notice.

27
0:01:47.28,000 --> 0:01:5,000
The recordings of the child, her friends and family,

28
0:01:51.08,000 --> 0:01:52,000
can be used for targeted advertising.

29
0:01:54.08,000 --> 0:01:58,000
And all this information can be shared with unnamed third parties.

30
0:01:59.76,000 --> 0:02:01,000
Enough? Not quite.

31
0:02:02.88,000 --> 0:02:06,000
Anyone with a smartphone can connect to Cayla

32
0:02:07.2,000 --> 0:02:08,000
within a certain distance.

33
0:02:09.56,000 --> 0:02:13,000
When we confronted the company that made and programmed Cayla,

34
0:02:14.16,000 --> 0:02:16,000
they issued a series of statements

35
0:02:16.44,000 --> 0:02:2,000
that one had to be an IT expert in order to breach the security.

36
0:02:22.039,000 --> 0:02:25,000
Shall we fact-check that statement and live hack Cayla together?

37
0:02:29.92,000 --> 0:02:3,000
Here she is.

38
0:02:32.2,000 --> 0:02:35,000
Cayla is equipped with a Bluetooth device

39
0:02:35.6,000 --> 0:02:37,000
which can transmit up to 60 feet,

40
0:02:37.84,000 --> 0:02:39,000
a bit less if there's a wall between.

41
0:02:40.48,000 --> 0:02:45,000
That means I, or any stranger, can connect to the doll

42
0:02:45.8,000 --> 0:02:48,000
while being outside the room where Cayla and her friends are.

43
0:02:49.56,000 --> 0:02:51,000
And to illustrate this,

44
0:02:51.76,000 --> 0:02:53,000
I'm going to turn Cayla on now.

45
0:02:53.92,000 --> 0:02:54,000
Let's see, one, two, three.

46
0:02:57.04,000 --> 0:02:58,000
There. She's on. And I asked a colleague

47
0:02:59.04,000 --> 0:03:01,000
to stand outside with his smartphone,

48
0:03:01.16,000 --> 0:03:02,000
and he's connected,

49
0:03:03.32,000 --> 0:03:05,000
and to make this a bit creepier ...

50
0:03:05.44,000 --> 0:03:09,000
(Laughter)

51
0:03:09.52,000 --> 0:03:13,000
let's see what kids could hear Cayla say in the safety of their room.

52
0:03:15.92,000 --> 0:03:17,000
Man: Hi. My name is Cayla. What is yours?

53
0:03:18.84,000 --> 0:03:19,000
Finn Myrstad: Uh, Finn.

54
0:03:20.96,000 --> 0:03:21,000
Man: Is your mom close by?

55
0:03:22.28,000 --> 0:03:23,000
FM: Uh, no, she's in the store.

56
0:03:24.68,000 --> 0:03:26,000
Man: Ah. Do you want to come out and play with me?

57
0:03:27.08,000 --> 0:03:28,000
FM: That's a great idea.

58
0:03:29.72,000 --> 0:03:3,000
Man: Ah, great.

59
0:03:32.48,000 --> 0:03:34,000
FM: I'm going to turn Cayla off now.

60
0:03:34.64,000 --> 0:03:35,000
(Laughter)

61
0:03:39.08,000 --> 0:03:41,000
We needed no password

62
0:03:41.84,000 --> 0:03:44,000
or to circumvent any other type of security to do this.

63
0:03:46.44,000 --> 0:03:49,000
We published a report in 20 countries around the world,

64
0:03:50.28,000 --> 0:03:52,000
exposing this significant security flaw

65
0:03:53.28,000 --> 0:03:54,000
and many other problematic issues.

66
0:03:56,000 --> 0:03:57,000
So what happened?

67
0:03:57.84,000 --> 0:03:58,000
Cayla was banned in Germany,

68
0:04:00.48,000 --> 0:04:03,000
taken off the shelves by Amazon and Wal-Mart,

69
0:04:03.72,000 --> 0:04:06,000
and she's now peacefully resting

70
0:04:06.8,000 --> 0:04:09,000
at the German Spy Museum in Berlin.

71
0:04:10.28,000 --> 0:04:12,000
(Laughter)

72
0:04:13.08,000 --> 0:04:17,000
However, Cayla was also for sale in stores around the world

73
0:04:17.4,000 --> 0:04:2,000
for more than a year after we published our report.

74
0:04:21,000 --> 0:04:25,000
What we uncovered is that there are few rules to protect us

75
0:04:25.28,000 --> 0:04:28,000
and the ones we have are not being properly enforced.

76
0:04:3,000 --> 0:04:33,000
We need to get the security and privacy of these devices right

77
0:04:33.88,000 --> 0:04:35,000
before they enter the market,

78
0:04:36.76,000 --> 0:04:39,000
because what is the point of locking a house with a key

79
0:04:40.76,000 --> 0:04:42,000
if anyone can enter it through a connected device?

80
0:04:45.64,000 --> 0:04:48,000
You may well think, "This will not happen to me.

81
0:04:48.96,000 --> 0:04:5,000
I will just stay away from these flawed devices."

82
0:04:52.6,000 --> 0:04:54,000
But that won't keep you safe,

83
0:04:54.68,000 --> 0:04:57,000
because simply by connecting to the internet,

84
0:04:57.88,000 --> 0:05:01,000
you are put in an impossible take-it-or-leave-it position.

85
0:05:02.48,000 --> 0:05:03,000
Let me show you.

86
0:05:04.4,000 --> 0:05:07,000
Like most of you, I have dozens of apps on my phone,

87
0:05:07.52,000 --> 0:05:09,000
and used properly, they can make our lives easier,

88
0:05:10.4,000 --> 0:05:12,000
more convenient and maybe even healthier.

89
0:05:13.96,000 --> 0:05:16,000
But have we been lulled into a false sense of security?

90
0:05:18.6,000 --> 0:05:2,000
It starts simply by ticking a box.

91
0:05:21.88,000 --> 0:05:22,000
Yes, we say,

92
0:05:23.68,000 --> 0:05:24,000
I've read the terms.

93
0:05:27.24,000 --> 0:05:3,000
But have you really read the terms?

94
0:05:31.2,000 --> 0:05:33,000
Are you sure they didn't look too long

95
0:05:33.52,000 --> 0:05:35,000
and your phone was running out of battery,

96
0:05:35.6,000 --> 0:05:38,000
and the last time you tried they were impossible to understand,

97
0:05:38.84,000 --> 0:05:39,000
and you needed to use the service now?

98
0:05:41.84,000 --> 0:05:44,000
And now, the power imbalance is established,

99
0:05:45.52,000 --> 0:05:48,000
because we have agreed to our personal information

100
0:05:49.2,000 --> 0:05:52,000
being gathered and used on a scale we could never imagine.

101
0:05:53.64,000 --> 0:05:56,000
This is why my colleagues and I decided to take a deeper look at this.

102
0:05:57.36,000 --> 0:06:,000
We set out to read the terms

103
0:06:00.72,000 --> 0:06:02,000
of popular apps on an average phone.

104
0:06:03.44,000 --> 0:06:06,000
And to show the world how unrealistic it is

105
0:06:07.2,000 --> 0:06:1,000
to expect consumers to actually read the terms,

106
0:06:10.44,000 --> 0:06:11,000
we printed them,

107
0:06:11.96,000 --> 0:06:12,000
more than 900 pages,

108
0:06:14.8,000 --> 0:06:17,000
and sat down in our office and read them out loud ourselves,

109
0:06:19.8,000 --> 0:06:21,000
streaming the experiment live on our websites.

110
0:06:22.36,000 --> 0:06:24,000
As you can see, it took quite a long time.

111
0:06:24.92,000 --> 0:06:28,000
It took us 31 hours, 49 minutes and 11 seconds

112
0:06:29.36,000 --> 0:06:31,000
to read the terms on an average phone.

113
0:06:31.96,000 --> 0:06:35,000
That is longer than a movie marathon of the "Harry Potter" movies

114
0:06:36.36,000 --> 0:06:38,000
and the "Godfather" movies combined.

115
0:06:38.88,000 --> 0:06:39,000
(Laughter)

116
0:06:41.6,000 --> 0:06:42,000
And reading is one thing.

117
0:06:43.56,000 --> 0:06:44,000
Understanding is another story.

118
0:06:45.56,000 --> 0:06:48,000
That would have taken us much, much longer.

119
0:06:49.16,000 --> 0:06:5,000
And this is a real problem,

120
0:06:50.96,000 --> 0:06:53,000
because companies have argued for 20 to 30 years

121
0:06:54.2,000 --> 0:06:57,000
against regulating the internet better,

122
0:06:57.28,000 --> 0:07:,000
because users have consented to the terms and conditions.

123
0:07:02.52,000 --> 0:07:03,000
As we've shown with this experiment,

124
0:07:04.52,000 --> 0:07:06,000
achieving informed consent is close to impossible.

125
0:07:09.08,000 --> 0:07:12,000
Do you think it's fair to put the burden of responsibility on the consumer?

126
0:07:14,000 --> 0:07:15,000
I don't.

127
0:07:15.76,000 --> 0:07:18,000
I think we should demand less take-it-or-leave-it

128
0:07:18.88,000 --> 0:07:21,000
and more understandable terms before we agree to them.

129
0:07:22.08,000 --> 0:07:23,000
(Applause)

130
0:07:23.64,000 --> 0:07:24,000
Thank you.

131
0:07:28.2,000 --> 0:07:32,000
Now, I would like to tell you a story about love.

132
0:07:34.08,000 --> 0:07:37,000
Some of the world's most popular apps are dating apps,

133
0:07:37.64,000 --> 0:07:41,000
an industry now worth more than, or close to, three billion dollars a year.

134
0:07:43.16,000 --> 0:07:47,000
And of course, we're OK sharing our intimate details

135
0:07:47.36,000 --> 0:07:48,000
with our other half.

136
0:07:49.24,000 --> 0:07:5,000
But who else is snooping,

137
0:07:51.24,000 --> 0:07:53,000
saving and sharing our information

138
0:07:54.2,000 --> 0:07:55,000
while we are baring our souls?

139
0:07:56.52,000 --> 0:07:58,000
My team and I decided to investigate this.

140
0:08:00.92,000 --> 0:08:03,000
And in order to understand the issue from all angles

141
0:08:03.96,000 --> 0:08:05,000
and to truly do a thorough job,

142
0:08:07.4,000 --> 0:08:08,000
I realized I had to download

143
0:08:09.4,000 --> 0:08:12,000
one of the world's most popular dating apps myself.

144
0:08:14.44,000 --> 0:08:16,000
So I went home to my wife ...

145
0:08:16.76,000 --> 0:08:17,000
(Laughter)

146
0:08:18.72,000 --> 0:08:19,000
who I had just married.

147
0:08:20.4,000 --> 0:08:24,000
"Is it OK if I establish a profile on a very popular dating app

148
0:08:25.04,000 --> 0:08:26,000
for purely scientific purposes?"

149
0:08:26.96,000 --> 0:08:27,000
(Laughter)

150
0:08:28.84,000 --> 0:08:29,000
This is what we found.

151
0:08:30.36,000 --> 0:08:33,000
Hidden behind the main menu was a preticked box

152
0:08:34.36,000 --> 0:08:4,000
that gave the dating company access to all my personal pictures on Facebook,

153
0:08:40.44,000 --> 0:08:42,000
in my case more than 2,000 of them,

154
0:08:43.32,000 --> 0:08:45,000
and some were quite personal.

155
0:08:46.4,000 --> 0:08:48,000
And to make matters worse,

156
0:08:48.64,000 --> 0:08:5,000
when we read the terms and conditions,

157
0:08:50.72,000 --> 0:08:51,000
we discovered the following,

158
0:08:52.12,000 --> 0:08:55,000
and I'm going to need to take out my reading glasses for this one.

159
0:08:56.4,000 --> 0:08:58,000
And I'm going to read it for you, because this is complicated.

160
0:08:59.36,000 --> 0:09:,000
All right.

161
0:09:01.44,000 --> 0:09:02,000
"By posting content" --

162
0:09:03,000 --> 0:09:04,000
and content refers to your pictures, chat

163
0:09:05,000 --> 0:09:07,000
and other interactions in the dating service --

164
0:09:07.24,000 --> 0:09:08,000
"as a part of the service,

165
0:09:08.52,000 --> 0:09:09,000
you automatically grant to the company,

166
0:09:10.52,000 --> 0:09:12,000
its affiliates, licensees and successors

167
0:09:12.72,000 --> 0:09:15,000
an irrevocable" -- which means you can't change your mind --

168
0:09:16.36,000 --> 0:09:18,000
"perpetual" -- which means forever --

169
0:09:19.16,000 --> 0:09:21,000
"nonexclusive, transferrable, sublicensable, fully paid-up,

170
0:09:22.08,000 --> 0:09:24,000
worldwide right and license to use, copy, store, perform,

171
0:09:24.8,000 --> 0:09:25,000
display, reproduce, record,

172
0:09:26.16,000 --> 0:09:28,000
play, adapt, modify and distribute the content,

173
0:09:28.4,000 --> 0:09:29,000
prepare derivative works of the content,

174
0:09:30.36,000 --> 0:09:32,000
or incorporate the content into other works

175
0:09:32.4,000 --> 0:09:35,000
and grant and authorize sublicenses of the foregoing in any media

176
0:09:35.48,000 --> 0:09:36,000
now known or hereafter created."

177
0:09:40.64,000 --> 0:09:43,000
That basically means that all your dating history

178
0:09:44.48,000 --> 0:09:49,000
and everything related to it can be used for any purpose for all time.

179
0:09:50.52,000 --> 0:09:54,000
Just imagine your children seeing your sassy dating photos

180
0:09:55.52,000 --> 0:09:57,000
in a birth control ad 20 years from now.

181
0:10:00.4,000 --> 0:10:01,000
But seriously, though --

182
0:10:01.64,000 --> 0:10:02,000
(Laughter)

183
0:10:04.88,000 --> 0:10:06,000
what might these commercial practices mean to you?

184
0:10:08.32,000 --> 0:10:1,000
For example, financial loss:

185
0:10:11.48,000 --> 0:10:12,000
based on your web browsing history,

186
0:10:13.2,000 --> 0:10:15,000
algorithms might decide whether you will get a mortgage or not.

187
0:10:16.84,000 --> 0:10:17,000
Subconscious manipulation:

188
0:10:19.56,000 --> 0:10:22,000
companies can analyze your emotions based on your photos and chats,

189
0:10:23.28,000 --> 0:10:26,000
targeting you with ads when you are at your most vulnerable.

190
0:10:26.56,000 --> 0:10:27,000
Discrimination:

191
0:10:28.08,000 --> 0:10:31,000
a fitness app can sell your data to a health insurance company,

192
0:10:31.12,000 --> 0:10:34,000
preventing you from getting coverage in the future.

193
0:10:34.2,000 --> 0:10:36,000
All of this is happening in the world today.

194
0:10:37.8,000 --> 0:10:4,000
But of course, not all uses of data are malign.

195
0:10:41.16,000 --> 0:10:42,000
Some are just flawed or need more work,

196
0:10:43.16,000 --> 0:10:44,000
and some are truly great.

197
0:10:47.56,000 --> 0:10:5,000
And there is some good news as well.

198
0:10:51.28,000 --> 0:10:54,000
The dating companies changed their policies globally

199
0:10:54.6,000 --> 0:10:55,000
after we filed a legal complaint.

200
0:10:57.72,000 --> 0:10:59,000
But organizations such as mine

201
0:11:00.44,000 --> 0:11:02,000
that fight for consumers' rights can't be everywhere.

202
0:11:03.44,000 --> 0:11:05,000
Nor can consumers fix this on their own,

203
0:11:06,000 --> 0:11:09,000
because if we know that something innocent we said

204
0:11:09.6,000 --> 0:11:1,000
will come back to haunt us,

205
0:11:11.08,000 --> 0:11:12,000
we will stop speaking.

206
0:11:13,000 --> 0:11:16,000
If we know that we are being watched and monitored,

207
0:11:16.4,000 --> 0:11:18,000
we will change our behavior.

208
0:11:18.52,000 --> 0:11:21,000
And if we can't control who has our data and how it is being used,

209
0:11:22.44,000 --> 0:11:23,000
we have lost the control of our lives.

210
0:11:26.4,000 --> 0:11:29,000
The stories I have told you today are not random examples.

211
0:11:29.92,000 --> 0:11:3,000
They are everywhere,

212
0:11:31.72,000 --> 0:11:33,000
and they are a sign that things need to change.

213
0:11:34.6,000 --> 0:11:36,000
And how can we achieve that change?

214
0:11:36.72,000 --> 0:11:41,000
Well, companies need to realize that by prioritizing privacy and security,

215
0:11:42.32,000 --> 0:11:44,000
they can build trust and loyalty to their users.

216
0:11:46.52,000 --> 0:11:49,000
Governments must create a safer internet

217
0:11:49.64,000 --> 0:11:51,000
by ensuring enforcement and up-to-date rules.

218
0:11:53.4,000 --> 0:11:55,000
And us, the citizens?

219
0:11:55.64,000 --> 0:11:56,000
We can use our voice

220
0:11:57.48,000 --> 0:12:02,000
to remind the world that technology can only truly benefit society

221
0:12:02.6,000 --> 0:12:04,000
if it respects basic rights.

222
0:12:05.72,000 --> 0:12:06,000
Thank you so much.

223
0:12:07.32,000 --> 0:12:11,000
(Applause)

