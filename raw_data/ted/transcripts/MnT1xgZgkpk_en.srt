1
0:00:12.57,000 --> 0:00:16,000
I work with a bunch of mathematicians, philosophers and computer scientists,

2
0:00:16.777,000 --> 0:00:21,000
and we sit around and think about the future of machine intelligence,

3
0:00:21.986,000 --> 0:00:23,000
among other things.

4
0:00:24.03,000 --> 0:00:28,000
Some people think that some of these things are sort of science fiction-y,

5
0:00:28.755,000 --> 0:00:31,000
far out there, crazy.

6
0:00:31.856,000 --> 0:00:32,000
But I like to say,

7
0:00:33.326,000 --> 0:00:36,000
okay, let's look at the modern human condition.

8
0:00:36.93,000 --> 0:00:37,000
(Laughter)

9
0:00:38.622,000 --> 0:00:4,000
This is the normal way for things to be.

10
0:00:41.024,000 --> 0:00:43,000
But if we think about it,

11
0:00:43.309,000 --> 0:00:46,000
we are actually recently arrived guests on this planet,

12
0:00:46.602,000 --> 0:00:48,000
the human species.

13
0:00:48.684,000 --> 0:00:52,000
Think about if Earth was created one year ago,

14
0:00:53.43,000 --> 0:00:56,000
the human species, then, would be 10 minutes old.

15
0:00:56.978,000 --> 0:00:59,000
The industrial era started two seconds ago.

16
0:01:01.276,000 --> 0:01:06,000
Another way to look at this is to think of world GDP over the last 10,000 years,

17
0:01:06.501,000 --> 0:01:09,000
I've actually taken the trouble to plot this for you in a graph.

18
0:01:09.53,000 --> 0:01:1,000
It looks like this.

19
0:01:11.304,000 --> 0:01:12,000
(Laughter)

20
0:01:12.667,000 --> 0:01:14,000
It's a curious shape for a normal condition.

21
0:01:14.818,000 --> 0:01:15,000
I sure wouldn't want to sit on it.

22
0:01:16.516,000 --> 0:01:18,000
(Laughter)

23
0:01:19.067,000 --> 0:01:23,000
Let's ask ourselves, what is the cause of this current anomaly?

24
0:01:23.841,000 --> 0:01:25,000
Some people would say it's technology.

25
0:01:26.393,000 --> 0:01:3,000
Now it's true, technology has accumulated through human history,

26
0:01:31.061,000 --> 0:01:35,000
and right now, technology advances extremely rapidly --

27
0:01:35.713,000 --> 0:01:36,000
that is the proximate cause,

28
0:01:37.278,000 --> 0:01:39,000
that's why we are currently so very productive.

29
0:01:40.473,000 --> 0:01:43,000
But I like to think back further to the ultimate cause.

30
0:01:45.114,000 --> 0:01:48,000
Look at these two highly distinguished gentlemen:

31
0:01:48.88,000 --> 0:01:49,000
We have Kanzi --

32
0:01:50.48,000 --> 0:01:54,000
he's mastered 200 lexical tokens, an incredible feat.

33
0:01:55.123,000 --> 0:01:58,000
And Ed Witten unleashed the second superstring revolution.

34
0:01:58.817,000 --> 0:02:,000
If we look under the hood, this is what we find:

35
0:02:01.141,000 --> 0:02:02,000
basically the same thing.

36
0:02:02.711,000 --> 0:02:03,000
One is a little larger,

37
0:02:04.524,000 --> 0:02:06,000
it maybe also has a few tricks in the exact way it's wired.

38
0:02:07.282,000 --> 0:02:1,000
These invisible differences cannot be too complicated, however,

39
0:02:11.094,000 --> 0:02:15,000
because there have only been 250,000 generations

40
0:02:15.379,000 --> 0:02:16,000
since our last common ancestor.

41
0:02:17.111,000 --> 0:02:2,000
We know that complicated mechanisms take a long time to evolve.

42
0:02:22,000 --> 0:02:24,000
So a bunch of relatively minor changes

43
0:02:24.499,000 --> 0:02:27,000
take us from Kanzi to Witten,

44
0:02:27.566,000 --> 0:02:31,000
from broken-off tree branches to intercontinental ballistic missiles.

45
0:02:32.839,000 --> 0:02:35,000
So this then seems pretty obvious that everything we've achieved,

46
0:02:36.774,000 --> 0:02:37,000
and everything we care about,

47
0:02:38.152,000 --> 0:02:43,000
depends crucially on some relatively minor changes that made the human mind.

48
0:02:44.65,000 --> 0:02:47,000
And the corollary, of course, is that any further changes

49
0:02:48.312,000 --> 0:02:51,000
that could significantly change the substrate of thinking

50
0:02:51.789,000 --> 0:02:54,000
could have potentially enormous consequences.

51
0:02:56.321,000 --> 0:02:58,000
Some of my colleagues think we're on the verge

52
0:02:59.226,000 --> 0:03:02,000
of something that could cause a profound change in that substrate,

53
0:03:03.134,000 --> 0:03:06,000
and that is machine superintelligence.

54
0:03:06.347,000 --> 0:03:1,000
Artificial intelligence used to be about putting commands in a box.

55
0:03:11.086,000 --> 0:03:12,000
You would have human programmers

56
0:03:12.751,000 --> 0:03:15,000
that would painstakingly handcraft knowledge items.

57
0:03:15.886,000 --> 0:03:17,000
You build up these expert systems,

58
0:03:17.972,000 --> 0:03:19,000
and they were kind of useful for some purposes,

59
0:03:20.296,000 --> 0:03:22,000
but they were very brittle, you couldn't scale them.

60
0:03:22.977,000 --> 0:03:25,000
Basically, you got out only what you put in.

61
0:03:26.41,000 --> 0:03:26,000
But since then,

62
0:03:27.407,000 --> 0:03:3,000
a paradigm shift has taken place in the field of artificial intelligence.

63
0:03:30.874,000 --> 0:03:32,000
Today, the action is really around machine learning.

64
0:03:34.394,000 --> 0:03:39,000
So rather than handcrafting knowledge representations and features,

65
0:03:40.511,000 --> 0:03:45,000
we create algorithms that learn, often from raw perceptual data.

66
0:03:46.065,000 --> 0:03:5,000
Basically the same thing that the human infant does.

67
0:03:51.063,000 --> 0:03:55,000
The result is A.I. that is not limited to one domain --

68
0:03:55.27,000 --> 0:03:59,000
the same system can learn to translate between any pairs of languages,

69
0:03:59.901,000 --> 0:04:04,000
or learn to play any computer game on the Atari console.

70
0:04:05.338,000 --> 0:04:06,000
Now of course,

71
0:04:07.117,000 --> 0:04:1,000
A.I. is still nowhere near having the same powerful, cross-domain

72
0:04:11.116,000 --> 0:04:14,000
ability to learn and plan as a human being has.

73
0:04:14.335,000 --> 0:04:16,000
The cortex still has some algorithmic tricks

74
0:04:16.461,000 --> 0:04:18,000
that we don't yet know how to match in machines.

75
0:04:19.886,000 --> 0:04:2,000
So the question is,

76
0:04:21.785,000 --> 0:04:24,000
how far are we from being able to match those tricks?

77
0:04:26.245,000 --> 0:04:27,000
A couple of years ago,

78
0:04:27.328,000 --> 0:04:29,000
we did a survey of some of the world's leading A.I. experts,

79
0:04:30.216,000 --> 0:04:33,000
to see what they think, and one of the questions we asked was,

80
0:04:33.44,000 --> 0:04:36,000
"By which year do you think there is a 50 percent probability

81
0:04:36.793,000 --> 0:04:39,000
that we will have achieved human-level machine intelligence?"

82
0:04:40.785,000 --> 0:04:44,000
We defined human-level here as the ability to perform

83
0:04:44.968,000 --> 0:04:46,000
almost any job at least as well as an adult human,

84
0:04:47.839,000 --> 0:04:51,000
so real human-level, not just within some limited domain.

85
0:04:51.844,000 --> 0:04:54,000
And the median answer was 2040 or 2050,

86
0:04:55.494,000 --> 0:04:57,000
depending on precisely which group of experts we asked.

87
0:04:58.3,000 --> 0:05:02,000
Now, it could happen much, much later, or sooner,

88
0:05:02.339,000 --> 0:05:03,000
the truth is nobody really knows.

89
0:05:05.259,000 --> 0:05:09,000
What we do know is that the ultimate limit to information processing

90
0:05:09.671,000 --> 0:05:13,000
in a machine substrate lies far outside the limits in biological tissue.

91
0:05:15.241,000 --> 0:05:17,000
This comes down to physics.

92
0:05:17.619,000 --> 0:05:21,000
A biological neuron fires, maybe, at 200 hertz, 200 times a second.

93
0:05:22.337,000 --> 0:05:25,000
But even a present-day transistor operates at the Gigahertz.

94
0:05:25.931,000 --> 0:05:3,000
Neurons propagate slowly in axons, 100 meters per second, tops.

95
0:05:31.228,000 --> 0:05:34,000
But in computers, signals can travel at the speed of light.

96
0:05:35.079,000 --> 0:05:36,000
There are also size limitations,

97
0:05:36.948,000 --> 0:05:39,000
like a human brain has to fit inside a cranium,

98
0:05:39.975,000 --> 0:05:43,000
but a computer can be the size of a warehouse or larger.

99
0:05:44.736,000 --> 0:05:49,000
So the potential for superintelligence lies dormant in matter,

100
0:05:50.335,000 --> 0:05:55,000
much like the power of the atom lay dormant throughout human history,

101
0:05:56.047,000 --> 0:06:,000
patiently waiting there until 1945.

102
0:06:00.452,000 --> 0:06:01,000
In this century,

103
0:06:01.7,000 --> 0:06:05,000
scientists may learn to awaken the power of artificial intelligence.

104
0:06:05.818,000 --> 0:06:09,000
And I think we might then see an intelligence explosion.

105
0:06:10.406,000 --> 0:06:13,000
Now most people, when they think about what is smart and what is dumb,

106
0:06:14.363,000 --> 0:06:17,000
I think have in mind a picture roughly like this.

107
0:06:17.386,000 --> 0:06:19,000
So at one end we have the village idiot,

108
0:06:19.984,000 --> 0:06:21,000
and then far over at the other side

109
0:06:22.467,000 --> 0:06:26,000
we have Ed Witten, or Albert Einstein, or whoever your favorite guru is.

110
0:06:27.223,000 --> 0:06:3,000
But I think that from the point of view of artificial intelligence,

111
0:06:31.057,000 --> 0:06:34,000
the true picture is actually probably more like this:

112
0:06:35.258,000 --> 0:06:38,000
AI starts out at this point here, at zero intelligence,

113
0:06:38.636,000 --> 0:06:41,000
and then, after many, many years of really hard work,

114
0:06:41.647,000 --> 0:06:44,000
maybe eventually we get to mouse-level artificial intelligence,

115
0:06:45.491,000 --> 0:06:47,000
something that can navigate cluttered environments

116
0:06:47.921,000 --> 0:06:48,000
as well as a mouse can.

117
0:06:49.908,000 --> 0:06:53,000
And then, after many, many more years of really hard work, lots of investment,

118
0:06:54.221,000 --> 0:06:58,000
maybe eventually we get to chimpanzee-level artificial intelligence.

119
0:06:58.86,000 --> 0:07:01,000
And then, after even more years of really, really hard work,

120
0:07:02.07,000 --> 0:07:04,000
we get to village idiot artificial intelligence.

121
0:07:04.983,000 --> 0:07:07,000
And a few moments later, we are beyond Ed Witten.

122
0:07:08.255,000 --> 0:07:1,000
The train doesn't stop at Humanville Station.

123
0:07:11.225,000 --> 0:07:14,000
It's likely, rather, to swoosh right by.

124
0:07:14.247,000 --> 0:07:15,000
Now this has profound implications,

125
0:07:16.231,000 --> 0:07:19,000
particularly when it comes to questions of power.

126
0:07:20.093,000 --> 0:07:21,000
For example, chimpanzees are strong --

127
0:07:21.992,000 --> 0:07:26,000
pound for pound, a chimpanzee is about twice as strong as a fit human male.

128
0:07:27.214,000 --> 0:07:31,000
And yet, the fate of Kanzi and his pals depends a lot more

129
0:07:31.828,000 --> 0:07:35,000
on what we humans do than on what the chimpanzees do themselves.

130
0:07:37.228,000 --> 0:07:39,000
Once there is superintelligence,

131
0:07:39.542,000 --> 0:07:42,000
the fate of humanity may depend on what the superintelligence does.

132
0:07:44.451,000 --> 0:07:45,000
Think about it:

133
0:07:45.508,000 --> 0:07:5,000
Machine intelligence is the last invention that humanity will ever need to make.

134
0:07:50.552,000 --> 0:07:52,000
Machines will then be better at inventing than we are,

135
0:07:53.525,000 --> 0:07:55,000
and they'll be doing so on digital timescales.

136
0:07:56.065,000 --> 0:08:,000
What this means is basically a telescoping of the future.

137
0:08:00.966,000 --> 0:08:03,000
Think of all the crazy technologies that you could have imagined

138
0:08:04.524,000 --> 0:08:06,000
maybe humans could have developed in the fullness of time:

139
0:08:07.322,000 --> 0:08:1,000
cures for aging, space colonization,

140
0:08:10.58,000 --> 0:08:13,000
self-replicating nanobots or uploading of minds into computers,

141
0:08:14.311,000 --> 0:08:16,000
all kinds of science fiction-y stuff

142
0:08:16.47,000 --> 0:08:18,000
that's nevertheless consistent with the laws of physics.

143
0:08:19.207,000 --> 0:08:23,000
All of this superintelligence could develop, and possibly quite rapidly.

144
0:08:24.449,000 --> 0:08:27,000
Now, a superintelligence with such technological maturity

145
0:08:28.007,000 --> 0:08:3,000
would be extremely powerful,

146
0:08:30.186,000 --> 0:08:34,000
and at least in some scenarios, it would be able to get what it wants.

147
0:08:34.732,000 --> 0:08:39,000
We would then have a future that would be shaped by the preferences of this A.I.

148
0:08:41.855,000 --> 0:08:44,000
Now a good question is, what are those preferences?

149
0:08:46.244,000 --> 0:08:47,000
Here it gets trickier.

150
0:08:48.013,000 --> 0:08:49,000
To make any headway with this,

151
0:08:49.448,000 --> 0:08:52,000
we must first of all avoid anthropomorphizing.

152
0:08:53.934,000 --> 0:08:56,000
And this is ironic because every newspaper article

153
0:08:57.235,000 --> 0:09:,000
about the future of A.I. has a picture of this:

154
0:09:02.28,000 --> 0:09:06,000
So I think what we need to do is to conceive of the issue more abstractly,

155
0:09:06.414,000 --> 0:09:08,000
not in terms of vivid Hollywood scenarios.

156
0:09:09.204,000 --> 0:09:12,000
We need to think of intelligence as an optimization process,

157
0:09:12.821,000 --> 0:09:17,000
a process that steers the future into a particular set of configurations.

158
0:09:18.47,000 --> 0:09:21,000
A superintelligence is a really strong optimization process.

159
0:09:21.981,000 --> 0:09:25,000
It's extremely good at using available means to achieve a state

160
0:09:26.098,000 --> 0:09:27,000
in which its goal is realized.

161
0:09:28.447,000 --> 0:09:3,000
This means that there is no necessary connection between

162
0:09:31.119,000 --> 0:09:33,000
being highly intelligent in this sense,

163
0:09:33.853,000 --> 0:09:37,000
and having an objective that we humans would find worthwhile or meaningful.

164
0:09:39.321,000 --> 0:09:42,000
Suppose we give an A.I. the goal to make humans smile.

165
0:09:43.115,000 --> 0:09:45,000
When the A.I. is weak, it performs useful or amusing actions

166
0:09:46.097,000 --> 0:09:48,000
that cause its user to smile.

167
0:09:48.614,000 --> 0:09:5,000
When the A.I. becomes superintelligent,

168
0:09:51.031,000 --> 0:09:54,000
it realizes that there is a more effective way to achieve this goal:

169
0:09:54.554,000 --> 0:09:55,000
take control of the world

170
0:09:56.476,000 --> 0:09:59,000
and stick electrodes into the facial muscles of humans

171
0:09:59.638,000 --> 0:10:01,000
to cause constant, beaming grins.

172
0:10:02.579,000 --> 0:10:03,000
Another example,

173
0:10:03.614,000 --> 0:10:06,000
suppose we give A.I. the goal to solve a difficult mathematical problem.

174
0:10:06.997,000 --> 0:10:07,000
When the A.I. becomes superintelligent,

175
0:10:08.934,000 --> 0:10:12,000
it realizes that the most effective way to get the solution to this problem

176
0:10:13.105,000 --> 0:10:15,000
is by transforming the planet into a giant computer,

177
0:10:16.035,000 --> 0:10:18,000
so as to increase its thinking capacity.

178
0:10:18.281,000 --> 0:10:2,000
And notice that this gives the A.I.s an instrumental reason

179
0:10:21.045,000 --> 0:10:23,000
to do things to us that we might not approve of.

180
0:10:23.561,000 --> 0:10:24,000
Human beings in this model are threats,

181
0:10:25.496,000 --> 0:10:27,000
we could prevent the mathematical problem from being solved.

182
0:10:29.207,000 --> 0:10:32,000
Of course, perceivably things won't go wrong in these particular ways;

183
0:10:32.701,000 --> 0:10:33,000
these are cartoon examples.

184
0:10:34.454,000 --> 0:10:35,000
But the general point here is important:

185
0:10:36.393,000 --> 0:10:38,000
if you create a really powerful optimization process

186
0:10:39.266,000 --> 0:10:41,000
to maximize for objective x,

187
0:10:41.5,000 --> 0:10:43,000
you better make sure that your definition of x

188
0:10:43.776,000 --> 0:10:45,000
incorporates everything you care about.

189
0:10:46.835,000 --> 0:10:5,000
This is a lesson that's also taught in many a myth.

190
0:10:51.219,000 --> 0:10:56,000
King Midas wishes that everything he touches be turned into gold.

191
0:10:56.517,000 --> 0:10:58,000
He touches his daughter, she turns into gold.

192
0:10:59.378,000 --> 0:11:01,000
He touches his food, it turns into gold.

193
0:11:01.931,000 --> 0:11:03,000
This could become practically relevant,

194
0:11:04.52,000 --> 0:11:06,000
not just as a metaphor for greed,

195
0:11:06.59,000 --> 0:11:07,000
but as an illustration of what happens

196
0:11:08.485,000 --> 0:11:1,000
if you create a powerful optimization process

197
0:11:11.322,000 --> 0:11:15,000
and give it misconceived or poorly specified goals.

198
0:11:16.111,000 --> 0:11:21,000
Now you might say, if a computer starts sticking electrodes into people's faces,

199
0:11:21.3,000 --> 0:11:23,000
we'd just shut it off.

200
0:11:24.555,000 --> 0:11:29,000
A, this is not necessarily so easy to do if we've grown dependent on the system --

201
0:11:29.895,000 --> 0:11:31,000
like, where is the off switch to the Internet?

202
0:11:32.627,000 --> 0:11:37,000
B, why haven't the chimpanzees flicked the off switch to humanity,

203
0:11:37.747,000 --> 0:11:38,000
or the Neanderthals?

204
0:11:39.298,000 --> 0:11:41,000
They certainly had reasons.

205
0:11:41.964,000 --> 0:11:43,000
We have an off switch, for example, right here.

206
0:11:44.759,000 --> 0:11:45,000
(Choking)

207
0:11:46.313,000 --> 0:11:48,000
The reason is that we are an intelligent adversary;

208
0:11:49.238,000 --> 0:11:51,000
we can anticipate threats and plan around them.

209
0:11:51.966,000 --> 0:11:53,000
But so could a superintelligent agent,

210
0:11:54.47,000 --> 0:11:57,000
and it would be much better at that than we are.

211
0:11:57.724,000 --> 0:12:04,000
The point is, we should not be confident that we have this under control here.

212
0:12:04.911,000 --> 0:12:07,000
And we could try to make our job a little bit easier by, say,

213
0:12:08.358,000 --> 0:12:09,000
putting the A.I. in a box,

214
0:12:09.948,000 --> 0:12:1,000
like a secure software environment,

215
0:12:11.744,000 --> 0:12:14,000
a virtual reality simulation from which it cannot escape.

216
0:12:14.766,000 --> 0:12:18,000
But how confident can we be that the A.I. couldn't find a bug.

217
0:12:18.912,000 --> 0:12:21,000
Given that merely human hackers find bugs all the time,

218
0:12:22.081,000 --> 0:12:25,000
I'd say, probably not very confident.

219
0:12:26.237,000 --> 0:12:3,000
So we disconnect the ethernet cable to create an air gap,

220
0:12:30.785,000 --> 0:12:32,000
but again, like merely human hackers

221
0:12:33.453,000 --> 0:12:36,000
routinely transgress air gaps using social engineering.

222
0:12:36.834,000 --> 0:12:37,000
Right now, as I speak,

223
0:12:38.093,000 --> 0:12:4,000
I'm sure there is some employee out there somewhere

224
0:12:40.482,000 --> 0:12:43,000
who has been talked into handing out her account details

225
0:12:43.828,000 --> 0:12:45,000
by somebody claiming to be from the I.T. department.

226
0:12:46.574,000 --> 0:12:48,000
More creative scenarios are also possible,

227
0:12:48.701,000 --> 0:12:49,000
like if you're the A.I.,

228
0:12:50.016,000 --> 0:12:53,000
you can imagine wiggling electrodes around in your internal circuitry

229
0:12:53.548,000 --> 0:12:56,000
to create radio waves that you can use to communicate.

230
0:12:57.01,000 --> 0:12:59,000
Or maybe you could pretend to malfunction,

231
0:12:59.434,000 --> 0:13:02,000
and then when the programmers open you up to see what went wrong with you,

232
0:13:02.931,000 --> 0:13:03,000
they look at the source code -- Bam! --

233
0:13:04.867,000 --> 0:13:06,000
the manipulation can take place.

234
0:13:07.314,000 --> 0:13:1,000
Or it could output the blueprint to a really nifty technology,

235
0:13:10.744,000 --> 0:13:11,000
and when we implement it,

236
0:13:12.142,000 --> 0:13:16,000
it has some surreptitious side effect that the A.I. had planned.

237
0:13:16.539,000 --> 0:13:19,000
The point here is that we should not be confident in our ability

238
0:13:20.002,000 --> 0:13:23,000
to keep a superintelligent genie locked up in its bottle forever.

239
0:13:23.81,000 --> 0:13:25,000
Sooner or later, it will out.

240
0:13:27.034,000 --> 0:13:3,000
I believe that the answer here is to figure out

241
0:13:30.137,000 --> 0:13:35,000
how to create superintelligent A.I. such that even if -- when -- it escapes,

242
0:13:35.161,000 --> 0:13:38,000
it is still safe because it is fundamentally on our side

243
0:13:38.438,000 --> 0:13:39,000
because it shares our values.

244
0:13:40.337,000 --> 0:13:43,000
I see no way around this difficult problem.

245
0:13:44.557,000 --> 0:13:47,000
Now, I'm actually fairly optimistic that this problem can be solved.

246
0:13:48.391,000 --> 0:13:51,000
We wouldn't have to write down a long list of everything we care about,

247
0:13:52.294,000 --> 0:13:55,000
or worse yet, spell it out in some computer language

248
0:13:55.937,000 --> 0:13:56,000
like C++ or Python,

249
0:13:57.391,000 --> 0:13:59,000
that would be a task beyond hopeless.

250
0:14:00.158,000 --> 0:14:04,000
Instead, we would create an A.I. that uses its intelligence

251
0:14:04.455,000 --> 0:14:06,000
to learn what we value,

252
0:14:07.226,000 --> 0:14:12,000
and its motivation system is constructed in such a way that it is motivated

253
0:14:12.506,000 --> 0:14:17,000
to pursue our values or to perform actions that it predicts we would approve of.

254
0:14:17.738,000 --> 0:14:2,000
We would thus leverage its intelligence as much as possible

255
0:14:21.152,000 --> 0:14:23,000
to solve the problem of value-loading.

256
0:14:24.727,000 --> 0:14:25,000
This can happen,

257
0:14:26.239,000 --> 0:14:29,000
and the outcome could be very good for humanity.

258
0:14:29.835,000 --> 0:14:32,000
But it doesn't happen automatically.

259
0:14:33.792,000 --> 0:14:35,000
The initial conditions for the intelligence explosion

260
0:14:36.79,000 --> 0:14:38,000
might need to be set up in just the right way

261
0:14:39.653,000 --> 0:14:42,000
if we are to have a controlled detonation.

262
0:14:43.183,000 --> 0:14:45,000
The values that the A.I. has need to match ours,

263
0:14:45.801,000 --> 0:14:46,000
not just in the familiar context,

264
0:14:47.561,000 --> 0:14:49,000
like where we can easily check how the A.I. behaves,

265
0:14:49.999,000 --> 0:14:52,000
but also in all novel contexts that the A.I. might encounter

266
0:14:53.233,000 --> 0:14:54,000
in the indefinite future.

267
0:14:54.79,000 --> 0:14:58,000
And there are also some esoteric issues that would need to be solved, sorted out:

268
0:14:59.527,000 --> 0:15:01,000
the exact details of its decision theory,

269
0:15:01.616,000 --> 0:15:03,000
how to deal with logical uncertainty and so forth.

270
0:15:05.33,000 --> 0:15:08,000
So the technical problems that need to be solved to make this work

271
0:15:08.432,000 --> 0:15:09,000
look quite difficult --

272
0:15:09.545,000 --> 0:15:12,000
not as difficult as making a superintelligent A.I.,

273
0:15:12.925,000 --> 0:15:14,000
but fairly difficult.

274
0:15:15.793,000 --> 0:15:16,000
Here is the worry:

275
0:15:17.488,000 --> 0:15:21,000
Making superintelligent A.I. is a really hard challenge.

276
0:15:22.172,000 --> 0:15:24,000
Making superintelligent A.I. that is safe

277
0:15:24.72,000 --> 0:15:26,000
involves some additional challenge on top of that.

278
0:15:28.216,000 --> 0:15:31,000
The risk is that if somebody figures out how to crack the first challenge

279
0:15:31.703,000 --> 0:15:34,000
without also having cracked the additional challenge

280
0:15:34.704,000 --> 0:15:35,000
of ensuring perfect safety.

281
0:15:37.375,000 --> 0:15:4,000
So I think that we should work out a solution

282
0:15:40.706,000 --> 0:15:42,000
to the control problem in advance,

283
0:15:43.528,000 --> 0:15:45,000
so that we have it available by the time it is needed.

284
0:15:46.768,000 --> 0:15:49,000
Now it might be that we cannot solve the entire control problem in advance

285
0:15:50.275,000 --> 0:15:53,000
because maybe some elements can only be put in place

286
0:15:53.299,000 --> 0:15:56,000
once you know the details of the architecture where it will be implemented.

287
0:15:57.296,000 --> 0:16:,000
But the more of the control problem that we solve in advance,

288
0:16:00.676,000 --> 0:16:04,000
the better the odds that the transition to the machine intelligence era

289
0:16:04.766,000 --> 0:16:05,000
will go well.

290
0:16:06.306,000 --> 0:16:1,000
This to me looks like a thing that is well worth doing

291
0:16:10.95,000 --> 0:16:13,000
and I can imagine that if things turn out okay,

292
0:16:14.282,000 --> 0:16:18,000
that people a million years from now look back at this century

293
0:16:18.94,000 --> 0:16:22,000
and it might well be that they say that the one thing we did that really mattered

294
0:16:22.942,000 --> 0:16:23,000
was to get this thing right.

295
0:16:24.509,000 --> 0:16:25,000
Thank you.

296
0:16:26.198,000 --> 0:16:28,000
(Applause)

