1
0:00:,000 --> 0:00:07,000
Traductor: Lidia Cámara de la Fuente Revisor: Ciro Gomez

2
0:00:13.047,000 --> 0:00:15,000
Un día, hace unos 10 años,

3
0:00:15.578,000 --> 0:00:18,000
pedí a un amigo que sostuviera un robot dinosaurio bebé boca abajo.

4
0:00:21.889,000 --> 0:00:24,000
Era el juguete llamado Pleo que había encargado,

5
0:00:25.359,000 --> 0:00:28,000
y estaba muy emocionada porque siempre me han gustado los robots.

6
0:00:29.164,000 --> 0:00:31,000
Y este tiene características técnicas realmente geniales.

7
0:00:32.087,000 --> 0:00:34,000
Tenía motores y sensores táctiles

8
0:00:34.23,000 --> 0:00:36,000
y una cámara infrarroja.

9
0:00:36.498,000 --> 0:00:38,000
Y también tenía un sensor de inclinación,

10
0:00:39.285,000 --> 0:00:41,000
por lo que sabía en qué dirección estaba orientado.

11
0:00:42.095,000 --> 0:00:44,000
Y cuando lo poníamos boca abajo,

12
0:00:44.253,000 --> 0:00:45,000
empezaba a llorar.

13
0:00:46.527,000 --> 0:00:49,000
Y pensé que esto era genial, por eso se lo estaba mostrando a mi amigo,

14
0:00:50.047,000 --> 0:00:52,000
y dije: "Levántalo por la cola. Mira lo que hace".

15
0:00:55.268,000 --> 0:00:58,000
Así que estamos viendo la teatralidad de este robot.

16
0:00:58.917,000 --> 0:01:,000
luchando y llorando.

17
0:01:02.767,000 --> 0:01:04,000
Y después de unos segundos,

18
0:01:04.838,000 --> 0:01:05,000
esto me empezó a agobiar un poco,

19
0:01:07.744,000 --> 0:01:1,000
y dije, "Es suficiente por ahora.

20
0:01:11.93,000 --> 0:01:13,000
Vamos a ponerlo de nuevo hacia abajo".

21
0:01:14.259,000 --> 0:01:16,000
Y luego acaricié el robot para que dejara de llorar.

22
0:01:18.973,000 --> 0:01:2,000
Y esa fue una experiencia extraña para mí.

23
0:01:22.084,000 --> 0:01:26,000
Por un lado, yo no era la persona más maternal en ese momento.

24
0:01:26.677,000 --> 0:01:28,000
Aunque ahora desde hace nueve meses, soy madre,

25
0:01:29.432,000 --> 0:01:32,000
y he aprendido que los bebés también se retuercen al sostenerlos boca abajo.

26
0:01:33.295,000 --> 0:01:34,000
(Risas)

27
0:01:34.703,000 --> 0:01:36,000
Pero mi respuesta a este robot también fue interesante.

28
0:01:37.405,000 --> 0:01:41,000
porque sabía exactamente cómo funcionaba esta máquina,

29
0:01:41.53,000 --> 0:01:44,000
y aun así me sentía obligada a ser amable con ella.

30
0:01:46.45,000 --> 0:01:48,000
Y esa observación despertó una curiosidad

31
0:01:49.181,000 --> 0:01:51,000
a la que he dedicado la última década.

32
0:01:52.911,000 --> 0:01:53,000
¿Por qué he de consolar a este robot?

33
0:01:56.228,000 --> 0:01:59,000
Y una de las cosas que descubrí fue que mi trato a esta máquina

34
0:01:59.831,000 --> 0:02:02,000
era algo más que un momento incómodo en mi sala de estar,

35
0:02:03.556,000 --> 0:02:08,000
que en un mundo donde integramos cada vez más robots en nuestras vidas,

36
0:02:09,000 --> 0:02:12,000
un instinto así podría tener consecuencias,

37
0:02:13.452,000 --> 0:02:16,000
porque lo primero que descubrí es que no soy solo yo.

38
0:02:19.249,000 --> 0:02:23,000
En 2007 el Washington Post informó que los militares de EE. UU.

39
0:02:24.075,000 --> 0:02:27,000
estaban probando el robot que desactivaba minas terrestres.

40
0:02:27.329,000 --> 0:02:29,000
Y funcionaba con la forma de un insecto palo,

41
0:02:30.035,000 --> 0:02:32,000
caminando sobre sus piernas alrededor de un campo minado

42
0:02:32.94,000 --> 0:02:35,000
y cada vez que pisaba una mina, una de las piernas explotaba,

43
0:02:36.17,000 --> 0:02:39,000
y seguía sobre las restantes piernas para hacer explotar más minas.

44
0:02:39.487,000 --> 0:02:42,000
Y el coronel a cargo de este ejercicio de prueba

45
0:02:43.061,000 --> 0:02:45,000
terminó suspendiendo,

46
0:02:45.203,000 --> 0:02:47,000
porque decía que era demasiado inhumano

47
0:02:47.662,000 --> 0:02:51,000
ver a ese robot dañado arrastrarse por el campo minado.

48
0:02:54.978,000 --> 0:02:57,000
Y ¿qué causaría que un oficial militar endurecido

49
0:02:58.899,000 --> 0:03:,000
y alguien como yo

50
0:03:00.966,000 --> 0:03:01,000
tuviéramos esa reacción a los robots?

51
0:03:03.287,000 --> 0:03:06,000
Por supuesto, estamos preparados por la ciencia ficción y la cultura pop

52
0:03:06.871,000 --> 0:03:08,000
a querer de verdad personificar estas cosas,

53
0:03:09.474,000 --> 0:03:11,000
pero esto es un poco más profundo que eso.

54
0:03:12.287,000 --> 0:03:15,000
Resulta que estamos biológicamente programados

55
0:03:15.58,000 --> 0:03:17,000
para proyectar intencionalidad y vida

56
0:03:17.62,000 --> 0:03:21,000
a cualquier movimiento en nuestro espacio físico que nos parezca autónomo.

57
0:03:23.174,000 --> 0:03:26,000
Así que la gente tratará todo tipo de robots como si estuvieran vivos.

58
0:03:26.463,000 --> 0:03:28,000
Estas unidades de eliminación de bombas obtienen nombres.

59
0:03:29.41,000 --> 0:03:3,000
Obtienen medallas de honor.

60
0:03:31.116,000 --> 0:03:33,000
Les han homenajeado con funerales con salvas.

61
0:03:34.11,000 --> 0:03:38,000
Y la investigación muestra que lo hacemos incluso con robots domésticos muy simples,

62
0:03:38.237,000 --> 0:03:4,000
como la aspiradora Roomba.

63
0:03:40.396,000 --> 0:03:41,000
(Risas)

64
0:03:41.711,000 --> 0:03:43,000
Es solo un disco que vaga por el piso para limpiarlo,

65
0:03:44.404,000 --> 0:03:46,000
pero el hecho de que se está moviendo de forma autónoma

66
0:03:47.154,000 --> 0:03:49,000
hará que la gente le denomine el Roomba

67
0:03:49.345,000 --> 0:03:52,000
y se sienta mal por el Roomba cuando se atasca debajo del sofá.

68
0:03:52.551,000 --> 0:03:53,000
(Risas)

69
0:03:54.44,000 --> 0:03:57,000
Y podemos diseñar concretamente robots para evocar esta respuesta,

70
0:03:57.804,000 --> 0:04:,000
usando ojos y caras o movimientos.

71
0:04:01.289,000 --> 0:04:04,000
para que la gente automáticamente y subconscientemente lo asocie

72
0:04:04.572,000 --> 0:04:05,000
con estados de ánimo.

73
0:04:05.916,000 --> 0:04:08,000
Y hay todo un cuerpo de investigación denominado interacción humano-robot

74
0:04:09.539,000 --> 0:04:11,000
que muestra realmente lo bien que funciona esto.

75
0:04:11.869,000 --> 0:04:14,000
Por ejemplo, los investigadores de la Universidad de Stanford descubrieron

76
0:04:15.489,000 --> 0:04:17,000
que la gente se siente realmente incómoda

77
0:04:17.564,000 --> 0:04:19,000
cuando se les pide tocar las partes íntimas de un robot.

78
0:04:20.27,000 --> 0:04:21,000
(Risas)

79
0:04:21.488,000 --> 0:04:23,000
A partir de esto y a partir de muchos otros estudios,

80
0:04:24.181,000 --> 0:04:27,000
sabemos que las personas responden a las señales que reciben

81
0:04:27.892,000 --> 0:04:29,000
a través de estas máquinas realistas,

82
0:04:29.938,000 --> 0:04:31,000
incluso sabiendo que no son reales.

83
0:04:33.654,000 --> 0:04:36,000
Nos dirigimos hacia un mundo donde los robots estarán en todas partes.

84
0:04:37.574,000 --> 0:04:4,000
La robótica se está moviendo desde detrás de las paredes de la fábrica.

85
0:04:41.269,000 --> 0:04:43,000
Está entrando a los lugares de trabajo, a los hogares.

86
0:04:43.86,000 --> 0:04:49,000
Y como esas máquinas pueden percibir y tomar decisiones autónomas y aprender,

87
0:04:50.093,000 --> 0:04:52,000
entran en estos espacios compartidos.

88
0:04:52.669,000 --> 0:04:54,000
Creo que tal vez la mejor analogía que tenemos para esto

89
0:04:55.445,000 --> 0:04:56,000
es nuestra relación con los animales.

90
0:04:57.523,000 --> 0:05:,000
Hace miles de años comenzamos a domesticar animales,

91
0:05:01.435,000 --> 0:05:05,000
y los entrenamos para el trabajo, la guerra y la compañía.

92
0:05:05.504,000 --> 0:05:08,000
Y a lo largo de la historia hemos tratado a algunos animales

93
0:05:08.523,000 --> 0:05:09,000
como herramientas o productos similares.

94
0:05:10.513,000 --> 0:05:12,000
y a otros animales los hemos tratado con amabilidad.

95
0:05:13.107,000 --> 0:05:16,000
Y les hemos dado un lugar en la sociedad como nuestros compañeros.

96
0:05:16.359,000 --> 0:05:19,000
Creo entonces plausible comenzar a integrar robots de manera similar.

97
0:05:21.484,000 --> 0:05:24,000
Y claro, los animales están vivos.

98
0:05:24.604,000 --> 0:05:25,000
Los robots no.

99
0:05:27.626,000 --> 0:05:29,000
Y les puedo decir, por mi trabajo en robótica.

100
0:05:30.23,000 --> 0:05:33,000
que estamos bastante lejos de desarrollar robots que puedan sentir.

101
0:05:35.072,000 --> 0:05:36,000
Pero sentimos por ellos,

102
0:05:37.835,000 --> 0:05:38,000
y eso importa,

103
0:05:39.066,000 --> 0:05:42,000
Porque si intentamos integrar robots en estos espacios compartidos,

104
0:05:42.537,000 --> 0:05:43,000
hay que entender que

105
0:05:44.189,000 --> 0:05:47,000
las personas los tratarán de manera diferente a otros dispositivos,

106
0:05:47.609,000 --> 0:05:48,000
y que, en algunos casos,

107
0:05:49.237,000 --> 0:05:52,000
por ejemplo, el caso de un soldado que se une emocionalmente

108
0:05:52.433,000 --> 0:05:54,000
al robot con el que trabaja,

109
0:05:54.504,000 --> 0:05:56,000
puede ser desde ineficiente hasta peligroso.

110
0:05:58.111,000 --> 0:06:,000
Pero en otros casos, en realidad, puede ser útil

111
0:06:00.713,000 --> 0:06:02,000
fomentar esta conexión emocional con los robots.

112
0:06:03.844,000 --> 0:06:05,000
Ya estamos viendo algunos buenos casos de uso,

113
0:06:06.342,000 --> 0:06:08,000
Por ejemplo, robots que trabajan con niños autistas.

114
0:06:08.97,000 --> 0:06:11,000
para involucrarlos en formas que no se han visto antes o

115
0:06:12.628,000 --> 0:06:13,000
robots que trabajan con maestros

116
0:06:14.313,000 --> 0:06:17,000
para involucrar a los niños en el aprendizaje con nuevos resultados.

117
0:06:17.663,000 --> 0:06:18,000
Y no solo para niños.

118
0:06:19.75,000 --> 0:06:22,000
Los primeros estudios muestran que los robots pueden ayudar

119
0:06:22.947,000 --> 0:06:24,000
a médicos y pacientes en entornos sanitarios.

120
0:06:25.535,000 --> 0:06:26,000
Este es el robot de foca bebé PARO.

121
0:06:27.359,000 --> 0:06:3,000
Se utilizan en hogares de ancianos y con pacientes con demencia

122
0:06:30.418,000 --> 0:06:31,000
y ya se aplican hace algún tiempo.

123
0:06:32.272,000 --> 0:06:35,000
Y recuerdo, hace años estar en una fiesta

124
0:06:35.621,000 --> 0:06:37,000
contándole a alguien sobre este robot,

125
0:06:38.216,000 --> 0:06:4,000
y su respuesta fue,

126
0:06:40.366,000 --> 0:06:41,000
"Oh Dios mío.

127
0:06:42.508,000 --> 0:06:43,000
Esto es horrible.

128
0:06:45.056,000 --> 0:06:49,000
No puedo creer estar dando a las personas robots en vez de cuidado humano".

129
0:06:50.54,000 --> 0:06:51,000
Y esta es una respuesta muy común,

130
0:06:52.439,000 --> 0:06:54,000
y creo que es absolutamente correcto,

131
0:06:54.962,000 --> 0:06:56,000
porque eso sería terrible.

132
0:06:57.795,000 --> 0:06:59,000
Pero en este caso, no es lo que reemplaza este robot.

133
0:07:00.858,000 --> 0:07:03,000
Lo que este robot reemplaza es la terapia animal.

134
0:07:04.002,000 --> 0:07:06,000
En contextos donde no podemos usar animales reales,

135
0:07:06.644,000 --> 0:07:07,000
podemos usar robots, porque la gente

136
0:07:08.416,000 --> 0:07:13,000
los tratará de forma más parecida a un animal que a un dispositivo.

137
0:07:15.332,000 --> 0:07:17,000
Reconocer esta conexión emocional con los robots

138
0:07:17.806,000 --> 0:07:19,000
también nos puede ayudar a anticipar desafíos, porque

139
0:07:20.325,000 --> 0:07:24,000
estos dispositivos se mueven hacia áreas más íntimas de la vida de las personas.

140
0:07:24.34,000 --> 0:07:27,000
Por ejemplo, ¿está bien si el robot de oso de peluche de su hijo

141
0:07:27.539,000 --> 0:07:29,000
graba conversaciones privadas?

142
0:07:29.8,000 --> 0:07:33,000
¿Está bien si su robot sexual tiene compras atractivas dentro de la app?

143
0:07:33.887,000 --> 0:07:34,000
(Risas)

144
0:07:35.307,000 --> 0:07:37,000
Porque los robots más capitalismo es igual a

145
0:07:37.832,000 --> 0:07:4,000
preguntas sobre la protección del consumidor y la privacidad.

146
0:07:41.939,000 --> 0:07:43,000
Y esas no son las únicas razones para que

147
0:07:44.185,000 --> 0:07:47,000
nuestro comportamiento relacionado con estas máquinas pueda ser relevante.

148
0:07:48.747,000 --> 0:07:51,000
Unos años antes de esa primera experiencia inicial

149
0:07:52.041,000 --> 0:07:54,000
con este robot dinosaurio bebé,

150
0:07:54.376,000 --> 0:07:56,000
hice un taller con mi amigo Hannes Gassert.

151
0:07:56.901,000 --> 0:07:58,000
Y tomamos cinco de estos robots bebé dinosaurio

152
0:07:59.822,000 --> 0:08:01,000
y se los dimos a cinco equipos de personas.

153
0:08:02.299,000 --> 0:08:03,000
Y les pedimos que los nombraran

154
0:08:04.02,000 --> 0:08:07,000
y jugaran e interactuaran con ellos durante aproximadamente una hora.

155
0:08:08.707,000 --> 0:08:1,000
Y luego les dimos un martillo y un hacha

156
0:08:10.937,000 --> 0:08:12,000
y les dijimos que torturaran y mataran a los robots.

157
0:08:13.695,000 --> 0:08:15,000
(Risas)

158
0:08:16.857,000 --> 0:08:18,000
Y esto resultó ser un poco más dramático.

159
0:08:19.175,000 --> 0:08:2,000
de lo que esperábamos,

160
0:08:20.477,000 --> 0:08:22,000
porque ninguno de los participantes lo hizo eso

161
0:08:23.343,000 --> 0:08:24,000
a estos robots dinosaurios bebé,

162
0:08:24.904,000 --> 0:08:29,000
así que tuvimos que improvisar un poco, y en un momento dijimos,

163
0:08:30.078,000 --> 0:08:34,000
"Bien, pueden salvar el robot de su equipo si destruyen el robot de otro equipo".

164
0:08:34.539,000 --> 0:08:35,000
(Risas)

165
0:08:36.249,000 --> 0:08:38,000
E incluso eso no funcionó. No querían hacerlo.

166
0:08:38.838,000 --> 0:08:39,000
Así que finalmente, dijimos,

167
0:08:40.233,000 --> 0:08:42,000
"Vamos a destruir todos los robots

168
0:08:42.289,000 --> 0:08:44,000
a no ser que alguien destruya con un hacha uno de ellos".

169
0:08:45.586,000 --> 0:08:48,000
Y un tipo se puso de pie y tomó el hacha,

170
0:08:48.738,000 --> 0:08:51,000
y toda la habitación se estremeció cuando dio un hachazo

171
0:08:51.919,000 --> 0:08:52,000
en el cuello del robot,

172
0:08:53.722,000 --> 0:08:59,000
y hubo un momento de silencio medio jocoso, medio serio en la habitación

173
0:09:00.085,000 --> 0:09:01,000
por este robot muerto.

174
0:09:01.807,000 --> 0:09:02,000
(Risas)

175
0:09:03.237,000 --> 0:09:06,000
Fue una experiencia realmente interesante.

176
0:09:06.955,000 --> 0:09:08,000
No era un estudio controlado, obviamente,

177
0:09:09.438,000 --> 0:09:12,000
pero condujo a una investigación posterior que hice en el MIT

178
0:09:12.488,000 --> 0:09:14,000
con Palash Nandy y Cynthia Breazeal,

179
0:09:14.564,000 --> 0:09:17,000
donde hubo personas que vinieron al laboratorio y aplastaron a estos HEXBUGs

180
0:09:18.215,000 --> 0:09:21,000
que se mueven de manera muy realista, como los insectos.

181
0:09:21.326,000 --> 0:09:24,000
Así que en lugar de elegir algo lindo que atrae a la gente,

182
0:09:24.484,000 --> 0:09:26,000
elegimos algo más básico.

183
0:09:26.601,000 --> 0:09:29,000
Y lo que descubrimos fue que las personas de alta empatía

184
0:09:30.105,000 --> 0:09:32,000
dudaban más en golpear a los HEXBUGS.

185
0:09:33.575,000 --> 0:09:34,000
Esto es solo un pequeño estudio,

186
0:09:35.163,000 --> 0:09:37,000
pero es parte de un gran cuerpo de investigación.

187
0:09:37.576,000 --> 0:09:39,000
Eso está empezando a indicar que puede haber una conexión

188
0:09:40.544,000 --> 0:09:42,000
entre las tendencias de empatía de las personas

189
0:09:42.941,000 --> 0:09:44,000
y su comportamiento en torno a los robots.

190
0:09:45.721,000 --> 0:09:48,000
Pero mi pregunta para la era venidera de la interacción humano-robot

191
0:09:49.372,000 --> 0:09:52,000
no es: "¿Simpatizamos con los robots?",

192
0:09:53.041,000 --> 0:09:56,000
sino: "¿Pueden los robots cambiar la empatía de las personas?".

193
0:09:57.489,000 --> 0:09:59,000
¿Hay alguna razón para, por ejemplo,

194
0:09:59.8,000 --> 0:10:01,000
evitar que su hijo patee a un perro robótico,

195
0:10:03.228,000 --> 0:10:05,000
no solo por respeto a la propiedad, sino porque

196
0:10:06.166,000 --> 0:10:09,000
el niño podría ser más propenso a patear a un perro de verdad?

197
0:10:10.507,000 --> 0:10:11,000
Y de nuevo, no son solo niños.

198
0:10:13.044,000 --> 0:10:15,000
Esta es la pregunta sobre los videojuegos violentos,

199
0:10:15.644,000 --> 0:10:16,000
pero a un nivel completamente nuevo,

200
0:10:17.644,000 --> 0:10:21,000
debido a esta fisicalidad visceral a la que respondemos más intensamente

201
0:10:22.428,000 --> 0:10:24,000
que a las imágenes en una pantalla.

202
0:10:25.674,000 --> 0:10:27,000
Cuando nos comportamos violentamente hacia los robots,

203
0:10:28.276,000 --> 0:10:31,000
concretamente robots diseñados para imitar la vida,

204
0:10:31.42,000 --> 0:10:34,000
¿es esta una salida saludable para el comportamiento violento

205
0:10:35.336,000 --> 0:10:38,000
o significa entrenar nuestros músculos para la crueldad?

206
0:10:39.511,000 --> 0:10:4,000
Nosotros no lo sabemos.

207
0:10:42.182,000 --> 0:10:44,000
Sin embargo, la respuesta a esta pregunta

208
0:10:44.731,000 --> 0:10:46,000
tiene el potencial de impactar el comportamiento humano,

209
0:10:47.489,000 --> 0:10:49,000
tiene el potencial de impactar normas sociales,

210
0:10:50.007,000 --> 0:10:52,000
tiene el potencial de inspirar reglas sobre lo que podemos

211
0:10:52.986,000 --> 0:10:54,000
y no podemos hacer con ciertos robots,

212
0:10:54.997,000 --> 0:10:56,000
similar a nuestras leyes de crueldad animal.

213
0:10:57.239,000 --> 0:10:59,000
Porque incluso si los robots no pueden sentir,

214
0:10:59.676,000 --> 0:11:02,000
nuestro comportamiento hacia ellos puede ser importante para nosotros.

215
0:11:04.889,000 --> 0:11:06,000
Y sin importar si terminamos cambiando nuestras reglas,

216
0:11:08.926,000 --> 0:11:12,000
los robots podrían ayudarnos a llegar a una nueva comprensión de nosotros mismos.

217
0:11:13.556,000 --> 0:11:16,000
La mayor parte de lo que he aprendido en los últimos 10 años,

218
0:11:17.002,000 --> 0:11:18,000
no es tecnología en absoluto,

219
0:11:18.878,000 --> 0:11:2,000
sino psicología humana

220
0:11:21.405,000 --> 0:11:23,000
empatía y cómo nos relacionamos con los demás.

221
0:11:25.524,000 --> 0:11:27,000
Porque cuando un niño es amable con un Roomba,

222
0:11:29.262,000 --> 0:11:33,000
cuando un soldado trata de salvar a un robot en el campo de batalla,

223
0:11:33.301,000 --> 0:11:36,000
o cuando un grupo de personas se niega a dañar a un bebé dinosaurio robótico,

224
0:11:38.248,000 --> 0:11:41,000
esos robots no son solo motores, engranajes y algoritmos,

225
0:11:42.501,000 --> 0:11:44,000
son reflejos de nuestra propia humanidad.

226
0:11:45.523,000 --> 0:11:46,000
Gracias.

227
0:11:46.698,000 --> 0:11:49,000
(Aplausos)

