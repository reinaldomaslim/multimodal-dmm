1
0:00:,000 --> 0:00:07,000
Traducteur: Elisabeth Buffard Relecteur: Antoniu Gugu

2
0:00:18.33,000 --> 0:00:24,000
Si vous demandez aux gens ce qu'ils trouvent difficile dans la psychologie,

3
0:00:24.33,000 --> 0:00:27,000
et vous dites, eh bien, par exemple la pensée et les émotions,

4
0:00:27.33,000 --> 0:00:3,000
la plupart des gens diront: «Les émotions c'est très dur.

5
0:00:30.33,000 --> 0:00:36,000
Elles sont incroyablement complexes, elles ne peuvent pas - je n'ai aucune idée de comment elles fonctionnent.

6
0:00:36.33,000 --> 0:00:38,000
Mais la pensée c'est vraiment très simple:

7
0:00:38.33,000 --> 0:00:42,000
c'est juste une sorte de raisonnement logique ou quelque chose comme ça.

8
0:00:42.33,000 --> 0:00:45,000
Mais ce n'est pas la partie difficile. "

9
0:00:45.33,000 --> 0:00:47,000
Donc, voici une liste des problèmes qui ressortent.

10
0:00:47.33,000 --> 0:00:5,000
Un bon problème est: que faisons-nous sur la santé?

11
0:00:50.33,000 --> 0:00:54,000
L'autre jour, je lisais quelque chose, et l'auteur disait

12
0:00:54.33,000 --> 0:01:,000
que probablement la plus importante cause de maladie est la poignée de main en occident.

13
0:01:00.33,000 --> 0:01:04,000
Et il y avait une petite d'étude sur les personnes qui ne se serrent pas la main,

14
0:01:04.33,000 --> 0:01:07,000
et qui les comparait avec ceux qui se serraient la main,

15
0:01:07.33,000 --> 0:01:12,000
et je n'ai pas la moindre idée d'où vous trouvez celles qui ne le font pas

16
0:01:12.33,000 --> 0:01:15,000
parce qu'ils doivent se cacher.

17
0:01:15.33,000 --> 0:01:19,000
Et les gens qui évitent ça

18
0:01:19.33,000 --> 0:01:23,000
ont quelque chose comme 30 % en moins de maladies infectieuses.

19
0:01:23.33,000 --> 0:01:26,000
Ou peut-être était-ce 31 % et un quart.

20
0:01:26.33,000 --> 0:01:3,000
Donc, si vous voulez vraiment résoudre le problème des épidémies et ainsi de suite,

21
0:01:30.33,000 --> 0:01:34,000
commençons par cela. Et depuis que j'ai eu cette idée,

22
0:01:34.33,000 --> 0:01:38,000
j'ai dû serrer des centaines de mains.

23
0:01:38.33,000 --> 0:01:43,000
Et je pense que la seule façon de l'éviter

24
0:01:43.33,000 --> 0:01:45,000
est d'avoir une horrible maladie visible,

25
0:01:45.33,000 --> 0:01:48,000
et ensuite vous n'avez pas à expliquer.

26
0:01:48.33,000 --> 0:01:52,000
L'éducation: comment pouvons-nous améliorer l'éducation?

27
0:01:52.33,000 --> 0:01:56,000
Eh bien, la meilleure façon simple est de leur faire comprendre

28
0:01:56.33,000 --> 0:01:59,000
que ce qu'on leur dit, c'est un tas de bêtises.

29
0:01:59.33,000 --> 0:02:01,000
Et puis, bien sûr, vous devez faire quelque chose

30
0:02:01.33,000 --> 0:02:06,000
pour modérerça, pour que n'importe qui puisse vous écouter.

31
0:02:06.33,000 --> 0:02:1,000
La pollution, la pénurie d'énergie, la diversité de l'environnement, la pauvreté -

32
0:02:10.33,000 --> 0:02:14,000
comment faire une société stable? La longévité.

33
0:02:14.33,000 --> 0:02:17,000
Bon, il y a beaucoup de problèmes à craindre.

34
0:02:17.33,000 --> 0:02:19,000
Quoi qu'il en soit, la question dont je pense que les gens devraient parler -

35
0:02:19.33,000 --> 0:02:24,000
et c'est absolument tabou - c'est, combien de personnes devrait-il y avoir?

36
0:02:24.33,000 --> 0:02:31,000
Et je pense que ça devrait être environ 100 millions ou peut-être 500 millions.

37
0:02:31.33,000 --> 0:02:36,000
Et alors notez qu'un grand nombre de ces problèmes disparaissent.

38
0:02:36.33,000 --> 0:02:38,000
Si vous aviez 100 millions de personnes

39
0:02:38.33,000 --> 0:02:44,000
bien réparties, ensuite, s'il y a des ordures,

40
0:02:44.33,000 --> 0:02:51,000
vous les jetez, de préférence là où vous ne pouvez pas les voir, et où elles vont pourrir.

41
0:02:51.33,000 --> 0:02:56,000
Ou alors vous les jetez dans l'océan et des poissons en bénéficieront.

42
0:02:56.33,000 --> 0:02:58,000
Le problème est, combien de personnes faut-il être?

43
0:02:58.33,000 --> 0:03:01,000
Et c'est une sorte de choix que nous avons à faire.

44
0:03:01.33,000 --> 0:03:04,000
La plupart des gens font environ 1,55 mètre de haut ou plus,

45
0:03:04.33,000 --> 0:03:08,000
et il y a une perte cube. Donc, si vous les faites grands comme ça -

46
0:03:08.33,000 --> 0:03:11,000
en utilisant la nanotechnologie, je suppose -

47
0:03:11.33,000 --> 0:03:12,000
(Rires)

48
0:03:12.33,000 --> 0:03:14,000
- Alors vous pourriez enavoir mille fois plus.

49
0:03:14.33,000 --> 0:03:16,000
Cela réglerait le problème, mais je ne vois personne

50
0:03:16.33,000 --> 0:03:19,000
faire des recherches sur comment rendre les personnes plus petites.

51
0:03:19.33,000 --> 0:03:24,000
Maintenant, il est bon de réduire la population, mais beaucoup de gens veulent avoir des enfants.

52
0:03:24.33,000 --> 0:03:27,000
Et il y a une solution qui apparaitra probablement dans quelques années.

53
0:03:27.33,000 --> 0:03:32,000
Vous savez que vous avez 46 chromosomes. Si vous êtes chanceux, vous en avez obtenu 23

54
0:03:32.33,000 --> 0:03:38,000
de chaque parent, parfois vous obtenez un de plus ou un de moins,

55
0:03:38.33,000 --> 0:03:42,000
mais - donc vous pouvez sauter l'étape des grands-parents et arrière grands-parents

56
0:03:42.33,000 --> 0:03:47,000
et aller droit aux arrière-arrière-grands-parents. Et vous avez 46 personnes

57
0:03:47.33,000 --> 0:03:5,000
et vous leur faites passer un scanner, ou ce dont vous avez besoin,

58
0:03:50.33,000 --> 0:03:54,000
et ils regardent leurs chromosomes et chacun d'eux dit

59
0:03:54.33,000 --> 0:03:59,000
celui qu'il aime le mieux, ou elle - plus aucune raison d'avoir seulement deux sexes

60
0:03:59.33,000 --> 0:04:04,000
en fait. Donc, chaque enfant dispose de 46 parents,

61
0:04:04.33,000 --> 0:04:1,000
et je suppose que vous pourriez laisser chaque groupe de 46 parents avoir 15 enfants -

62
0:04:10.33,000 --> 0:04:12,000
ne serait-ce pas suffisant? Et puis les enfants

63
0:04:12.33,000 --> 0:04:16,000
recevraient beaucoup de soutien et de soin et de tutorat

64
0:04:16.33,000 --> 0:04:18,000
et la population mondiale déclinerait très rapidement

65
0:04:18.33,000 --> 0:04:21,000
et tout le monde serait tout à fait heureux.

66
0:04:21.33,000 --> 0:04:24,000
La multipropriété est un peu plus loin dans l'avenir.

67
0:04:24.33,000 --> 0:04:27,000
Et il y a ce grand roman qu'Arthur Clarke a écrit deux fois,

68
0:04:27.33,000 --> 0:04:31,000
appelé Contre la Tombée de la Nuit et La Ville et les Etoiles.

69
0:04:31.33,000 --> 0:04:34,000
Les deux sont merveilleux et largement identiques

70
0:04:34.33,000 --> 0:04:36,000
sauf que les ordinateurs sont apparus entre les deux.

71
0:04:36.33,000 --> 0:04:41,000
Et Arthur regardait ce vieux livre, et il a dit ce qui n'allait pas.

72
0:04:41.33,000 --> 0:04:43,000
L'avenir doit avoir des ordinateurs.

73
0:04:43.33,000 --> 0:04:48,000
Donc, dans la deuxième version du livre, il y a 100 milliards

74
0:04:48.33,000 --> 0:04:56,000
ou 1.000 milliards de personnes sur Terre, mais ils sont tous stockés sur des disques durs ou des disquettes,

75
0:04:56.33,000 --> 0:04:58,000
ou ce qu'ils ont dans l'avenir.

76
0:04:58.33,000 --> 0:05:02,000
Et vous laissez quelques millions d'entre eux sortir à la fois.

77
0:05:02.33,000 --> 0:05:06,000
Une personne sort, elle vit un millier d'années

78
0:05:06.33,000 --> 0:05:12,000
fait ce qu'elle fait, et puis, quand il est temps de revenir

79
0:05:12.33,000 --> 0:05:16,000
pour un milliard d'années - ou un million, je ne sais plus, les chiffres n'ont pas d'importance -

80
0:05:16.33,000 --> 0:05:2,000
mais il y a vraiment très peu de gens sur la terre à la fois.

81
0:05:20.33,000 --> 0:05:22,000
Et vous vous mettez à réfléchir sur vous-même et vos souvenirs,

82
0:05:22.33,000 --> 0:05:27,000
et avant de retourner en suspension, vous modifier vos souvenirs

83
0:05:27.33,000 --> 0:05:3,000
et vous modifiez votre personnalité et ainsi de suite.

84
0:05:30.33,000 --> 0:05:36,000
L'intrigue du livre est qu'il n'y a pas assez de diversité,

85
0:05:36.33,000 --> 0:05:39,000
et donc les gens qui ont conçu la ville

86
0:05:39.33,000 --> 0:05:43,000
se sont assurés que de temps à autre une personne entièrement nouvelle est créée.

87
0:05:43.33,000 --> 0:05:49,000
Et dans le roman, un individu nommé Alvin est créé. Et il dit:

88
0:05:49.33,000 --> 0:05:53,000
c'est peut-être pas la meilleure façon, et des épaves de l'ensemble du système.

89
0:05:53.33,000 --> 0:05:55,000
Je ne pense pas que les solutions que j'ai proposées

90
0:05:55.33,000 --> 0:05:58,000
soient assez bonnes ou assez intelligentes.

91
0:05:58.33,000 --> 0:06:02,000
Je pense que le gros problème est que nous ne sommes pas assez intelligents

92
0:06:02.33,000 --> 0:06:06,000
pour comprendre quels sont les problèmes assez bons parmis ceux auxquels nous sommes confrontés.

93
0:06:06.33,000 --> 0:06:1,000
Par conséquent, nous devons construire des machines super intelligentes comme HAL.

94
0:06:10.33,000 --> 0:06:15,000
Comme vous le savez, à un moment donné dans le livre pour l'année 2001,

95
0:06:15.33,000 --> 0:06:2,000
HAL se rend compte que l'univers est trop grand et grandiose et profond

96
0:06:20.33,000 --> 0:06:24,000
pour ces astronautes vraiment stupide. Si vous comparez le comportement de HAL

97
0:06:24.33,000 --> 0:06:28,000
avec la trivialité de la population sur le vaisseau spatial,

98
0:06:28.33,000 --> 0:06:31,000
vous pouvez voir ce qui est écrit entre les lignes.

99
0:06:31.33,000 --> 0:06:34,000
Eh bien, qu'allons-nous faire à ce sujet? Nous pourrions devenir plus intelligent.

100
0:06:34.33,000 --> 0:06:39,000
Je pense que nous sommes assez intelligents, comparativement à des chimpanzés,

101
0:06:39.33,000 --> 0:06:45,000
mais nous ne sommes pas assez intelligents pour faire face aux énormes problèmes auxquels nous sommes confrontés,

102
0:06:45.33,000 --> 0:06:47,000
que ce soit en mathématiques abstraites

103
0:06:47.33,000 --> 0:06:52,000
ou pour comprendre les économies ou l'équilibre du monde qui nous entoure.

104
0:06:52.33,000 --> 0:06:55,000
Donc, une chose que nous pouvons faire, c'est vivre plus longtemps.

105
0:06:55.33,000 --> 0:06:57,000
Et personne ne sait combien c'est difficile,

106
0:06:57.33,000 --> 0:07:,000
mais nous allons probablement le découvrir dans quelques années.

107
0:07:00.33,000 --> 0:07:03,000
Vous voyez, il y a deux embranchements de la route. Nous savons que les gens vivent

108
0:07:03.33,000 --> 0:07:07,000
deux fois plus longtemps que les chimpanzés, ou presque,

109
0:07:07.33,000 --> 0:07:11,000
et personne ne vit plus de 120 ans,

110
0:07:11.33,000 --> 0:07:14,000
pour des raisons qu'on ne comprend pas très bien.

111
0:07:14.33,000 --> 0:07:17,000
Mais beaucoup de gens vivent maintenant à 90 ou 100,

112
0:07:17.33,000 --> 0:07:21,000
à moins qu'ils ne se serrent la main trop souvent ou quelque chose comme ça.

113
0:07:21.33,000 --> 0:07:26,000
Et alors peut-être si nous vivions 200 ans, nous pourrions accumuler suffisamment de compétences

114
0:07:26.33,000 --> 0:07:31,000
et de connaissances pour résoudre certains problèmes.

115
0:07:31.33,000 --> 0:07:33,000
Donc, c'est une façon de s'y prendre.

116
0:07:33.33,000 --> 0:07:36,000
Et comme je l'ai dit, nous ne savons pas combien c'est difficile. Ce serait peut-être -

117
0:07:36.33,000 --> 0:07:42,000
Après tout, la plupart des autres mammifères vivent moitié moins long que le chimpanzé,

118
0:07:42.33,000 --> 0:07:45,000
Nous avons donc trois fois et demi ou quatre fois - disont quatre fois

119
0:07:45.33,000 --> 0:07:51,000
la longévité de la plupart des mammifères. Et dans le cas des primates,

120
0:07:51.33,000 --> 0:07:55,000
nous avons presque les mêmes gènes. Nous ne différons des chimpanzés

121
0:07:55.33,000 --> 0:08:01,000
dans l'état actuel des connaissances - ce qui est une foutaise absolue -

122
0:08:01.33,000 --> 0:08:03,000
peut-être par quelques centaines de gènes.

123
0:08:03.33,000 --> 0:08:06,000
Ce que je pense, c'est que les compteurs de gène ne savent pas ce qu'ils font pour le moment.

124
0:08:06.33,000 --> 0:08:09,000
Et quoi que vous fassiez, ne lisez rien sur la génétique

125
0:08:09.33,000 --> 0:08:12,000
qu'on publiera de votre vivant.

126
0:08:12.33,000 --> 0:08:15,000
(Rires)

127
0:08:15.33,000 --> 0:08:19,000
C'est un sujet qui a une demi-vie très courte, pareil pour la science du cerveau.

128
0:08:19.33,000 --> 0:08:25,000
Et donc peut-être que si nous réparons quatre ou cinq gènes,

129
0:08:25.33,000 --> 0:08:27,000
nous pouvons vivre 200 ans.

130
0:08:27.33,000 --> 0:08:3,000
Ou il se pourrait que ce soit juste 30 ou 40,

131
0:08:30.33,000 --> 0:08:32,000
et je doute que ce soit plusieurs centaines.

132
0:08:32.33,000 --> 0:08:36,000
Donc, c'est quelque chose dont les gens vont débattre

133
0:08:36.33,000 --> 0:08:39,000
et beaucoup de spécialistes de l'éthique - vous savez, un éthicien est quelqu'un

134
0:08:39.33,000 --> 0:08:42,000
qui voit quelque chose de mal avec ce que vous avez à l'esprit.

135
0:08:42.33,000 --> 0:08:45,000
(Rires)

136
0:08:45.33,000 --> 0:08:49,000
Et il est très difficile de trouver un éthicien qui considère qu'un changement quelconque

137
0:08:49.33,000 --> 0:08:53,000
vaille la peine, parce que, dit-il, qu'en est-il des conséquences?

138
0:08:53.33,000 --> 0:08:56,000
Et bien sûr, nous ne sommes pas responsables des conséquences

139
0:08:56.33,000 --> 0:09:02,000
de ce que nous faisons maintenant, n'est-ce pas? Comme toutes ces plaintes au sujet des clones.

140
0:09:02.33,000 --> 0:09:05,000
Et pourtant, deux personnes au hasard s'accoupleront et feront cet enfant,

141
0:09:05.33,000 --> 0:09:09,000
et tous les deux ont des gènes assez pourris,

142
0:09:09.33,000 --> 0:09:13,000
et l'enfant est susceptible de n'être que moyen.

143
0:09:13.33,000 --> 0:09:19,000
Et selon les normes chimpanzé, c'est vraiment très bien.

144
0:09:19.33,000 --> 0:09:22,000
Si nous avons la longévité, alors nous aurons à faire face à la croissance de la population

145
0:09:22.33,000 --> 0:09:26,000
de toute façon. Parce que si les gens vivent 200 ou 1000 ans,

146
0:09:26.33,000 --> 0:09:32,000
nous ne pouvons leur laisser faire un enfant q'environ une fois tous les 200 ou 1.000 ans.

147
0:09:32.33,000 --> 0:09:35,000
Et il n'y aura pas de main-d'œuvre.

148
0:09:35.33,000 --> 0:09:39,000
Et une des choses que Laurie Garrett et d'autres ont souligné,

149
0:09:39.33,000 --> 0:09:44,000
est qu'une société qui ne possède pas de gens

150
0:09:44.33,000 --> 0:09:47,000
en âge de travailler est en réelle difficulté. Et les choses vont s'aggraver,

151
0:09:47.33,000 --> 0:09:53,000
car il n'y a personne pour éduquer les enfants ou pour nourrir les personnes âgées.

152
0:09:53.33,000 --> 0:09:55,000
Et quand je parle d'une longue durée de vie, bien sûr,

153
0:09:55.33,000 --> 0:10:01,000
je ne veux pas que quelqu'un qui a 200 ans ressemble à l'image

154
0:10:01.33,000 --> 0:10:05,000
de quelqu'un de 200 ans - c'est-à-dire mort, en fait.

155
0:10:05.33,000 --> 0:10:07,000
Vous savez, il y a environ 400 différentes parties du cerveau

156
0:10:07.33,000 --> 0:10:09,000
qui semblent avoir des fonctions différentes.

157
0:10:09.33,000 --> 0:10:12,000
Personne ne sait comment la plupart d'entre elles travaillent dans le détail,

158
0:10:12.33,000 --> 0:10:16,000
mais nous savons qu'il y a beaucoup de différentes choses là-dedans.

159
0:10:16.33,000 --> 0:10:18,000
Et elles ne travaillent pas toujours ensemble. J'aime la théorie de Freud

160
0:10:18.33,000 --> 0:10:22,000
que la plupart d'entre elles s'annulent mutuellement.

161
0:10:22.33,000 --> 0:10:26,000
Et si vous pensez que vous-même êtes comme une sorte de ville

162
0:10:26.33,000 --> 0:10:32,000
avec une centaine de ressources, puis, quand vous avez peur, par exemple,

163
0:10:32.33,000 --> 0:10:36,000
vous pouvez laisser tomber vos objectifs à long terme, mais vous pouvez penser profondément

164
0:10:36.33,000 --> 0:10:4,000
et vous concentrer sur comment atteindre cet objectif particulier exactement .

165
0:10:40.33,000 --> 0:10:43,000
Vous laissez tomber tout le reste de suite. Vous devenez monomaniaque -

166
0:10:43.33,000 --> 0:10:47,000
Tout ce qui vous intéresse est de ne pas descendre de ce quai.

167
0:10:47.33,000 --> 0:10:51,000
Et quand vous avez faim, la nourriture devient plus attrayante, et ainsi de suite.

168
0:10:51.33,000 --> 0:10:57,000
Je vois donc les émotions comme des sous-ensembles très évolués de vos capacités.

169
0:10:57.33,000 --> 0:11:01,000
L'émotion n'est pas quelque chose en plus de la pensée. Un état émotionnel

170
0:11:01.33,000 --> 0:11:05,000
est ce que vous obtenez lorsque vous supprimez 100 ou 200

171
0:11:05.33,000 --> 0:11:08,000
de vos ressources normalement disponibles.

172
0:11:08.33,000 --> 0:11:11,000
Ainsi penser que les émotions sont le contraire de quelque chose

173
0:11:11.33,000 --> 0:11:15,000
inférieur à la pensée est extrêmement productive. Et j'espère,

174
0:11:15.33,000 --> 0:11:19,000
dans les années qui viennent, montrer que cela conduira à des machines intelligentes.

175
0:11:19.33,000 --> 0:11:22,000
Et je suppose que je ferais mieux de sauter tout le reste, ce sont des détails

176
0:11:22.33,000 --> 0:11:27,000
sur la façon dont nous pourrions rendre ces machines «intelligentes» et -

177
0:11:27.33,000 --> 0:11:32,000
(Rires)

178
0:11:32.33,000 --> 0:11:37,000
- et l'idée principale est en fait que le noyau d'une machine vraiment intelligente

179
0:11:37.33,000 --> 0:11:42,000
est celle qui reconnaît qu'un certain type de problème se pose à vous.

180
0:11:42.33,000 --> 0:11:45,000
Il s'agit d'un problème de tel ou tel type,

181
0:11:45.33,000 --> 0:11:5,000
et donc il y a une certaine façon ou plusieurs certaines façons de penser

182
0:11:50.33,000 --> 0:11:52,000
qui sont bonnes pour ce problème.

183
0:11:52.33,000 --> 0:11:56,000
Je pense donc que le problème principal à venir de la psychologie est de classer

184
0:11:56.33,000 --> 0:12:,000
les types de situations difficiles, les types de situations, les types d'obstacles

185
0:12:00.33,000 --> 0:12:06,000
et aussi de classer les moyens disponibles et possibles de penser et de les apparier.

186
0:12:06.33,000 --> 0:12:09,000
Donc, vous voyez, c'est presque comme un réflexe pavlovien -

187
0:12:09.33,000 --> 0:12:11,000
nous avons perdu les cent premières années de la psychologie

188
0:12:11.33,000 --> 0:12:14,000
avec des théories vraiment triviales où vous dites,

189
0:12:14.33,000 --> 0:12:2,000
comment les gens apprennent à réagir à une situation? Ce que je dis c'est,

190
0:12:20.33,000 --> 0:12:25,000
qu'une fois ce niveau passé, y compris la conception

191
0:12:25.33,000 --> 0:12:28,000
d'un immense système désordonné avec des milliers de pièces,

192
0:12:28.33,000 --> 0:12:32,000
nous finirons à nouveau avec le problème central de la psychologie.

193
0:12:32.33,000 --> 0:12:35,000
En disant, non pas ce que sont les situations,

194
0:12:35.33,000 --> 0:12:37,000
mais quels sont les types de problèmes

195
0:12:37.33,000 --> 0:12:4,000
et quels sont les types de stratégies, comment les apprendre,

196
0:12:40.33,000 --> 0:12:43,000
comment les connecter, comment une personne très créative

197
0:12:43.33,000 --> 0:12:48,000
invente une nouvelle façon de penser à partir des ressources disponibles et ainsi de suite.

198
0:12:48.33,000 --> 0:12:5,000
Je pense donc que dans les 20 prochaines années,

199
0:12:50.33,000 --> 0:12:55,000
si nous pouvons nous débarrasser de toutes les approches traditionnelles de l'intelligence artificielle,

200
0:12:55.33,000 --> 0:12:57,000
comme les réseaux neuronaux et algorithmes génétiques

201
0:12:57.33,000 --> 0:13:03,000
et les systèmes à base de règles, et qu'on se contente de viser un peu plus haut pour dire,

202
0:13:03.33,000 --> 0:13:05,000
pouvons-nous faire un système qui puisse utiliser toutes ces choses

203
0:13:05.33,000 --> 0:13:09,000
pour le bon type de problème? Certains problèmes sont bons pour les réseaux neuronaux;

204
0:13:09.33,000 --> 0:13:12,000
Nous savons que d'autres, des réseaux neuronaux ne leur servent à rien.

205
0:13:12.33,000 --> 0:13:15,000
Les algorithmes génétiques sont super pour certaines choses;

206
0:13:15.33,000 --> 0:13:19,000
Je pense savoir dans quels cas ils sont mauvais à et je ne vous le dirai pas.

207
0:13:19.33,000 --> 0:13:2,000
(Rires)

208
0:13:20.33,000 --> 0:13:22,000
Merci.

209
0:13:22.33,000 --> 0:13:28,000
(Applaudissements)

