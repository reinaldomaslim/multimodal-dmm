1
0:00:12.857,000 --> 0:00:13,000
This is a graph

2
0:00:14.36,000 --> 0:00:17,000
that represents the economic history of human civilization.

3
0:00:18.043,000 --> 0:00:2,000
[World GDP per capita over the last 200,000 years]

4
0:00:23.757,000 --> 0:00:25,000
There's not much going on, is there.

5
0:00:26.751,000 --> 0:00:28,000
For the vast majority of human history,

6
0:00:29.835,000 --> 0:00:33,000
pretty much everyone lived on the equivalent of one dollar per day,

7
0:00:33.907,000 --> 0:00:34,000
and not much changed.

8
0:00:36.757,000 --> 0:00:38,000
But then, something extraordinary happened:

9
0:00:40.677,000 --> 0:00:42,000
the Scientific and Industrial Revolutions.

10
0:00:43.512,000 --> 0:00:45,000
And the basically flat graph you just saw

11
0:00:46.321,000 --> 0:00:48,000
transforms into this.

12
0:00:50.612,000 --> 0:00:54,000
What this graph means is that, in terms of power to change the world,

13
0:00:55.271,000 --> 0:00:58,000
we live in an unprecedented time in human history,

14
0:00:58.733,000 --> 0:01:01,000
and I believe our ethical understanding hasn't yet caught up with this fact.

15
0:01:03.716,000 --> 0:01:04,000
The Scientific and Industrial Revolutions

16
0:01:05.724,000 --> 0:01:07,000
transformed both our understanding of the world

17
0:01:08.657,000 --> 0:01:09,000
and our ability to alter it.

18
0:01:11.505,000 --> 0:01:14,000
What we need is an ethical revolution

19
0:01:15.196,000 --> 0:01:16,000
so that we can work out

20
0:01:16.768,000 --> 0:01:19,000
how do we use this tremendous bounty of resources

21
0:01:19.944,000 --> 0:01:2,000
to improve the world.

22
0:01:22.249,000 --> 0:01:23,000
For the last 10 years,

23
0:01:23.864,000 --> 0:01:26,000
my colleagues and I have developed a philosophy and research program

24
0:01:27.721,000 --> 0:01:28,000
that we call effective altruism.

25
0:01:30.366,000 --> 0:01:33,000
It tries to respond to these radical changes in our world,

26
0:01:33.985,000 --> 0:01:37,000
uses evidence and careful reasoning to try to answer this question:

27
0:01:40.173,000 --> 0:01:42,000
How can we do the most good?

28
0:01:44.265,000 --> 0:01:47,000
Now, there are many issues you've got to address

29
0:01:47.51,000 --> 0:01:49,000
if you want to tackle this problem:

30
0:01:49.797,000 --> 0:01:51,000
whether to do good through your charity

31
0:01:51.852,000 --> 0:01:53,000
or your career or your political engagement,

32
0:01:54.028,000 --> 0:01:56,000
what programs to focus on, who to work with.

33
0:01:57.624,000 --> 0:01:58,000
But what I want to talk about

34
0:01:59.124,000 --> 0:02:01,000
is what I think is the most fundamental problem.

35
0:02:02.02,000 --> 0:02:04,000
Of all the many problems that the world faces,

36
0:02:05.962,000 --> 0:02:07,000
which should we be focused on trying to solve first?

37
0:02:10.668,000 --> 0:02:13,000
Now, I'm going to give you a framework for thinking about this question,

38
0:02:14.16,000 --> 0:02:15,000
and the framework is very simple.

39
0:02:16.842,000 --> 0:02:17,000
A problem's higher priority,

40
0:02:19.416,000 --> 0:02:23,000
the bigger, the more easily solvable and the more neglected it is.

41
0:02:24.694,000 --> 0:02:25,000
Bigger is better,

42
0:02:26.36,000 --> 0:02:28,000
because we've got more to gain if we do solve the problem.

43
0:02:30.221,000 --> 0:02:31,000
More easily solvable is better

44
0:02:31.814,000 --> 0:02:33,000
because I can solve the problem with less time or money.

45
0:02:35.737,000 --> 0:02:37,000
And most subtly,

46
0:02:38.681,000 --> 0:02:4,000
more neglected is better, because of diminishing returns.

47
0:02:42.285,000 --> 0:02:45,000
The more resources that have already been invested into solving a problem,

48
0:02:46.023,000 --> 0:02:48,000
the harder it will be to make additional progress.

49
0:02:50.56,000 --> 0:02:54,000
Now, the key thing that I want to leave with you is this framework,

50
0:02:54.643,000 --> 0:02:55,000
so that you can think for yourself

51
0:02:56.651,000 --> 0:02:58,000
what are the highest global priorities.

52
0:02:59.954,000 --> 0:03:01,000
But I and others in the effective altruism community

53
0:03:02.67,000 --> 0:03:07,000
have converged on three moral issues that we believe are unusually important,

54
0:03:08.573,000 --> 0:03:1,000
score unusually well in this framework.

55
0:03:11.151,000 --> 0:03:13,000
First is global health.

56
0:03:13.988,000 --> 0:03:15,000
This is supersolvable.

57
0:03:16.423,000 --> 0:03:19,000
We have an amazing track record in global health.

58
0:03:19.844,000 --> 0:03:24,000
Rates of death from measles, malaria, diarrheal disease

59
0:03:25.288,000 --> 0:03:27,000
are down by over 70 percent.

60
0:03:29.534,000 --> 0:03:31,000
And in 1980, we eradicated smallpox.

61
0:03:33.815,000 --> 0:03:36,000
I estimate we thereby saved over 60 million lives.

62
0:03:37.506,000 --> 0:03:4,000
That's more lives saved than if we'd achieved world peace

63
0:03:40.594,000 --> 0:03:41,000
in that same time period.

64
0:03:43.893,000 --> 0:03:45,000
On our current best estimates,

65
0:03:46.242,000 --> 0:03:5,000
we can save a life by distributing long-lasting insecticide-treated bed nets

66
0:03:50.394,000 --> 0:03:51,000
for just a few thousand dollars.

67
0:03:52.911,000 --> 0:03:53,000
This is an amazing opportunity.

68
0:03:55.594,000 --> 0:03:57,000
The second big priority is factory farming.

69
0:03:58.681,000 --> 0:03:59,000
This is superneglected.

70
0:04:00.768,000 --> 0:04:04,000
There are 50 billion land animals used every year for food,

71
0:04:05.625,000 --> 0:04:07,000
and the vast majority of them are factory farmed,

72
0:04:08.197,000 --> 0:04:1,000
living in conditions of horrific suffering.

73
0:04:10.601,000 --> 0:04:13,000
They're probably among the worst-off creatures on this planet,

74
0:04:13.776,000 --> 0:04:15,000
and in many cases, we could significantly improve their lives

75
0:04:16.658,000 --> 0:04:17,000
for just pennies per animal.

76
0:04:19.123,000 --> 0:04:21,000
Yet this is hugely neglected.

77
0:04:21.229,000 --> 0:04:24,000
There are 3,000 times more animals in factory farms

78
0:04:25.063,000 --> 0:04:26,000
than there are stray pets,

79
0:04:28.6,000 --> 0:04:32,000
but yet, factory farming gets one fiftieth of the philanthropic funding.

80
0:04:34.211,000 --> 0:04:36,000
That means additional resources in this area

81
0:04:36.363,000 --> 0:04:38,000
could have a truly transformative impact.

82
0:04:39.458,000 --> 0:04:41,000
Now the third area is the one that I want to focus on the most,

83
0:04:42.467,000 --> 0:04:44,000
and that's the category of existential risks:

84
0:04:45.475,000 --> 0:04:48,000
events like a nuclear war or a global pandemic

85
0:04:50.824,000 --> 0:04:52,000
that could permanently derail civilization

86
0:04:54.156,000 --> 0:04:56,000
or even lead to the extinction of the human race.

87
0:04:57.882,000 --> 0:04:59,000
Let me explain why I think this is such a big priority

88
0:05:00.446,000 --> 0:05:01,000
in terms of this framework.

89
0:05:02.992,000 --> 0:05:03,000
First, size.

90
0:05:05.341,000 --> 0:05:08,000
How bad would it be if there were a truly existential catastrophe?

91
0:05:10.92,000 --> 0:05:16,000
Well, it would involve the deaths of all seven billion people on this planet

92
0:05:17.286,000 --> 0:05:2,000
and that means you and everyone you know and love.

93
0:05:21.214,000 --> 0:05:23,000
That's just a tragedy of unimaginable size.

94
0:05:25.684,000 --> 0:05:26,000
But then, what's more,

95
0:05:27.684,000 --> 0:05:3,000
it would also mean the curtailment of humanity's future potential,

96
0:05:31.313,000 --> 0:05:33,000
and I believe that humanity's potential is vast.

97
0:05:35.551,000 --> 0:05:38,000
The human race has been around for about 200,000 years,

98
0:05:39.026,000 --> 0:05:41,000
and if she lives as long as a typical mammalian species,

99
0:05:41.933,000 --> 0:05:43,000
she would last for about two million years.

100
0:05:46.884,000 --> 0:05:48,000
If the human race were a single individual,

101
0:05:49.599,000 --> 0:05:51,000
she would be just 10 years old today.

102
0:05:53.526,000 --> 0:05:57,000
And what's more, the human race isn't a typical mammalian species.

103
0:05:58.95,000 --> 0:05:59,000
There's no reason why, if we're careful,

104
0:06:00.88,000 --> 0:06:02,000
we should die off after only two million years.

105
0:06:03.839,000 --> 0:06:07,000
The earth will remain habitable for 500 million years to come.

106
0:06:08.696,000 --> 0:06:09,000
And if someday, we took to the stars,

107
0:06:11.64,000 --> 0:06:13,000
the civilization could continue for billions more.

108
0:06:16.193,000 --> 0:06:18,000
So I think the future is going to be really big,

109
0:06:19.669,000 --> 0:06:2,000
but is it going to be good?

110
0:06:21.495,000 --> 0:06:23,000
Is the human race even really worth preserving?

111
0:06:26.54,000 --> 0:06:29,000
Well, we hear all the time about how things have been getting worse,

112
0:06:31.459,000 --> 0:06:33,000
but I think that when we take the long run,

113
0:06:34.176,000 --> 0:06:36,000
things have been getting radically better.

114
0:06:37.453,000 --> 0:06:39,000
Here, for example, is life expectancy over time.

115
0:06:40.892,000 --> 0:06:43,000
Here's the proportion of people not living in extreme poverty.

116
0:06:45.106,000 --> 0:06:49,000
Here's the number of countries over time that have decriminalized homosexuality.

117
0:06:50.848,000 --> 0:06:53,000
Here's the number of countries over time that have become democratic.

118
0:06:55.015,000 --> 0:06:59,000
Then, when we look to the future, there could be so much more to gain again.

119
0:06:59.658,000 --> 0:07:,000
We'll be so much richer,

120
0:07:00.91,000 --> 0:07:03,000
we can solve so many problems that are intractable today.

121
0:07:05.389,000 --> 0:07:09,000
So if this is kind of a graph of how humanity has progressed

122
0:07:09.858,000 --> 0:07:11,000
in terms of total human flourishing over time,

123
0:07:12.772,000 --> 0:07:15,000
well, this is what we would expect future progress to look like.

124
0:07:16.881,000 --> 0:07:17,000
It's vast.

125
0:07:18.953,000 --> 0:07:19,000
Here, for example,

126
0:07:20.175,000 --> 0:07:23,000
is where we would expect no one to live in extreme poverty.

127
0:07:25.93,000 --> 0:07:28,000
Here is where we would expect everyone to be better off

128
0:07:29.156,000 --> 0:07:3,000
than the richest person alive today.

129
0:07:32.081,000 --> 0:07:35,000
Perhaps here is where we would discover the fundamental natural laws

130
0:07:35.297,000 --> 0:07:36,000
that govern our world.

131
0:07:37.516,000 --> 0:07:4,000
Perhaps here is where we discover an entirely new form of art,

132
0:07:41.245,000 --> 0:07:44,000
a form of music we currently lack the ears to hear.

133
0:07:45.072,000 --> 0:07:47,000
And this is just the next few thousand years.

134
0:07:47.827,000 --> 0:07:49,000
Once we think past that,

135
0:07:50.056,000 --> 0:07:54,000
well, we can't even imagine the heights that human accomplishment might reach.

136
0:07:54.247,000 --> 0:07:57,000
So the future could be very big and it could be very good,

137
0:07:57.311,000 --> 0:07:59,000
but are there ways we could lose this value?

138
0:08:00.366,000 --> 0:08:01,000
And sadly, I think there are.

139
0:08:02.216,000 --> 0:08:06,000
The last two centuries brought tremendous technological progress,

140
0:08:06.293,000 --> 0:08:08,000
but they also brought the global risks of nuclear war

141
0:08:08.939,000 --> 0:08:1,000
and the possibility of extreme climate change.

142
0:08:11.725,000 --> 0:08:12,000
When we look to the coming centuries,

143
0:08:13.516,000 --> 0:08:15,000
we should expect to see the same pattern again.

144
0:08:16.187,000 --> 0:08:19,000
And we can see some radically powerful technologies on the horizon.

145
0:08:20.132,000 --> 0:08:22,000
Synthetic biology might give us the power to create viruses

146
0:08:23.005,000 --> 0:08:26,000
of unprecedented contagiousness and lethality.

147
0:08:27.131,000 --> 0:08:31,000
Geoengineering might give us the power to dramatically alter the earth's climate.

148
0:08:31.798,000 --> 0:08:35,000
Artificial intelligence might give us the power to create intelligent agents

149
0:08:36.021,000 --> 0:08:38,000
with abilities greater than our own.

150
0:08:40.222,000 --> 0:08:43,000
Now, I'm not saying that any of these risks are particularly likely,

151
0:08:44.134,000 --> 0:08:45,000
but when there's so much at stake,

152
0:08:45.802,000 --> 0:08:47,000
even small probabilities matter a great deal.

153
0:08:49.568,000 --> 0:08:52,000
Imagine if you're getting on a plane and you're kind of nervous,

154
0:08:52.593,000 --> 0:08:55,000
and the pilot reassures you by saying,

155
0:08:56.061,000 --> 0:09:,000
"There's only a one-in-a-thousand chance of crashing. Don't worry."

156
0:09:02.157,000 --> 0:09:03,000
Would you feel reassured?

157
0:09:04.509,000 --> 0:09:08,000
For these reasons, I think that preserving the future of humanity

158
0:09:08.621,000 --> 0:09:1,000
is among the most important problems that we currently face.

159
0:09:12.546,000 --> 0:09:14,000
But let's keep using this framework.

160
0:09:14.72,000 --> 0:09:15,000
Is this problem neglected?

161
0:09:18.085,000 --> 0:09:2,000
And I think the answer is yes,

162
0:09:20.391,000 --> 0:09:23,000
and that's because problems that affect future generations

163
0:09:23.74,000 --> 0:09:24,000
are often hugely neglected.

164
0:09:26.93,000 --> 0:09:27,000
Why?

165
0:09:28.36,000 --> 0:09:31,000
Because future people don't participate in markets today.

166
0:09:31.862,000 --> 0:09:32,000
They don't have a vote.

167
0:09:33.931,000 --> 0:09:35,000
It's not like there's a lobby representing the interests

168
0:09:36.628,000 --> 0:09:38,000
of those born in 2300 AD.

169
0:09:40.313,000 --> 0:09:43,000
They don't get to influence the decisions we make today.

170
0:09:43.995,000 --> 0:09:44,000
They're voiceless.

171
0:09:46.49,000 --> 0:09:49,000
And that means we still spend a paltry amount on these issues:

172
0:09:49.959,000 --> 0:09:5,000
nuclear nonproliferation,

173
0:09:51.782,000 --> 0:09:53,000
geoengineering, biorisk,

174
0:09:55.414,000 --> 0:09:56,000
artificial intelligence safety.

175
0:09:57.923,000 --> 0:09:59,000
All of these receive only a few tens of millions of dollars

176
0:10:00.821,000 --> 0:10:01,000
of philanthropic funding every year.

177
0:10:04.044,000 --> 0:10:07,000
That's tiny compared to the 390 billion dollars

178
0:10:08.79,000 --> 0:10:1,000
that's spent on US philanthropy in total.

179
0:10:13.885,000 --> 0:10:15,000
The final aspect of our framework then:

180
0:10:17.083,000 --> 0:10:18,000
Is this solvable?

181
0:10:19.289,000 --> 0:10:2,000
I believe it is.

182
0:10:21.014,000 --> 0:10:24,000
You can contribute with your money,

183
0:10:24.085,000 --> 0:10:26,000
your career or your political engagement.

184
0:10:28.225,000 --> 0:10:3,000
With your money, you can support organizations

185
0:10:30.424,000 --> 0:10:31,000
that focus on these risks,

186
0:10:31.75,000 --> 0:10:33,000
like the Nuclear Threat Initiative,

187
0:10:34.329,000 --> 0:10:37,000
which campaigns to take nuclear weapons off hair-trigger alert,

188
0:10:38.013,000 --> 0:10:41,000
or the Blue Ribbon Panel, which develops policy to minimize the damage

189
0:10:41.608,000 --> 0:10:43,000
from natural and man-made pandemics,

190
0:10:45.158,000 --> 0:10:48,000
or the Center for Human-Compatible AI, which does technical research

191
0:10:48.442,000 --> 0:10:5,000
to ensure that AI systems are safe and reliable.

192
0:10:52.652,000 --> 0:10:53,000
With your political engagement,

193
0:10:54.192,000 --> 0:10:57,000
you can vote for candidates that care about these risks,

194
0:10:57.312,000 --> 0:10:59,000
and you can support greater international cooperation.

195
0:11:01.767,000 --> 0:11:04,000
And then with your career, there is so much that you can do.

196
0:11:05.333,000 --> 0:11:08,000
Of course, we need scientists and policymakers and organization leaders,

197
0:11:09.865,000 --> 0:11:1,000
but just as importantly,

198
0:11:11.041,000 --> 0:11:15,000
we also need accountants and managers and assistants

199
0:11:16.691,000 --> 0:11:19,000
to work in these organizations that are tackling these problems.

200
0:11:20.469,000 --> 0:11:23,000
Now, the research program of effective altruism

201
0:11:25.191,000 --> 0:11:26,000
is still in its infancy,

202
0:11:27.262,000 --> 0:11:29,000
and there's still a huge amount that we don't know.

203
0:11:31.173,000 --> 0:11:33,000
But even with what we've learned so far,

204
0:11:34.748,000 --> 0:11:36,000
we can see that by thinking carefully

205
0:11:37.494,000 --> 0:11:41,000
and by focusing on those problems that are big, solvable and neglected,

206
0:11:43.152,000 --> 0:11:45,000
we can make a truly tremendous difference to the world

207
0:11:45.884,000 --> 0:11:46,000
for thousands of years to come.

208
0:11:47.963,000 --> 0:11:48,000
Thank you.

209
0:11:49.138,000 --> 0:11:53,000
(Applause)

