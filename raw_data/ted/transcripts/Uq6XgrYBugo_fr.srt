1
0:00:,000 --> 0:00:07,000
Traducteur: Claire Ghyselen Relecteur: Morgane Quilfen

2
0:00:13.047,000 --> 0:00:15,000
Un jour, il y a environ 10 ans,

3
0:00:15.579,000 --> 0:00:18,000
j'ai demandé à un ami de tenir à l'envers un robot en forme de petit dinosaure.

4
0:00:21.889,000 --> 0:00:24,000
J'avais commandé ce jouet qui répond au nom de Pleo.

5
0:00:25.359,000 --> 0:00:29,000
J'étais vraiment ravie car j'ai toujours aimé les robots.

6
0:00:29.76,000 --> 0:00:31,000
Celui-ci avait des caractéristiques vraiment cool.

7
0:00:32.089,000 --> 0:00:34,000
Il avait des moteurs, des capteurs haptiques

8
0:00:34.23,000 --> 0:00:36,000
et une caméra infrarouge.

9
0:00:36.498,000 --> 0:00:38,000
Il avait aussi un capteur d'inclinaison

10
0:00:39.285,000 --> 0:00:41,000
qui lui permettait de savoir dans quelle direction il était.

11
0:00:42.104,000 --> 0:00:44,000
Si on le tenait la tête en bas,

12
0:00:44.253,000 --> 0:00:45,000
il se mettait à pleurer.

13
0:00:46.527,000 --> 0:00:49,000
Je trouvais ça super et donc j'étais fière de le montrer à mon ami.

14
0:00:50.023,000 --> 0:00:53,000
Je lui ai proposé de le tenir par la queue pour voir sa réaction.

15
0:00:55.268,000 --> 0:00:58,000
On observe les gesticulations du robot

16
0:00:58.917,000 --> 0:01:,000
qui se débat et pleure,

17
0:01:02.767,000 --> 0:01:04,000
mais après quelques secondes,

18
0:01:04.838,000 --> 0:01:05,000
ça a commencé à me perturber un peu.

19
0:01:07.744,000 --> 0:01:1,000
Alors j'ai décrété que c'était assez

20
0:01:11.93,000 --> 0:01:13,000
et que nous devrions le remettre à l'endroit.

21
0:01:14.235,000 --> 0:01:17,000
J'ai alors caressé le robot jusqu'à ce qu'il arrête de pleurer.

22
0:01:18.973,000 --> 0:01:2,000
Ce fut une expérience troublante pour moi.

23
0:01:22.084,000 --> 0:01:26,000
D'abord, je n'étais pas particulièrement maternelle à l'époque.

24
0:01:26.677,000 --> 0:01:28,000
Je suis devenue une maman il y a neuf mois,

25
0:01:29.408,000 --> 0:01:33,000
et j'ai appris que les enfants aussi sont contrariés si on les tient la tête en bas.

26
0:01:33.461,000 --> 0:01:34,000
(Rires)

27
0:01:35.023,000 --> 0:01:37,000
Mais ma réaction vis-à-vis du robot est intéressante

28
0:01:37.621,000 --> 0:01:39,000
car, en dépit du fait que je savais exactement

29
0:01:39.77,000 --> 0:01:4,000
comment la machine fonctionnait,

30
0:01:41.53,000 --> 0:01:44,000
j'ai quand-même ressenti le besoin de lui montrer de l'affection.

31
0:01:46.45,000 --> 0:01:48,000
Cette observation a piqué ma curiosité

32
0:01:49.181,000 --> 0:01:51,000
et cela fait une dizaine d'années que je m'y intéresse.

33
0:01:52.911,000 --> 0:01:53,000
Pourquoi ai-je consolé ce robot ?

34
0:01:56.228,000 --> 0:01:59,000
Une des choses que j'ai découvertes est que ma réaction avec cette machine

35
0:01:59.831,000 --> 0:02:02,000
est davantage qu'un moment bizarre dans mon salon

36
0:02:03.556,000 --> 0:02:08,000
et que dans un monde où nous intégrons de plus en plus les robots dans nos vies,

37
0:02:09,000 --> 0:02:12,000
un tel instinct peut avoir des conséquences concrètes,

38
0:02:13.452,000 --> 0:02:16,000
car la première chose que j'ai découverte, c'est que je ne suis pas la seule.

39
0:02:19.249,000 --> 0:02:23,000
En 2007, le Washington Post a publié un reportage sur l'armée américaine

40
0:02:24.051,000 --> 0:02:27,000
qui faisait des essais avec un robot détonateur de mines antipersonnel.

41
0:02:27.511,000 --> 0:02:29,000
Le robot avait la forme d'un phasme.

42
0:02:30.265,000 --> 0:02:32,000
Il marchait dans les champs de mines

43
0:02:32.94,000 --> 0:02:35,000
et chaque fois qu'une de ses pattes en touchait une, elle explosait.

44
0:02:36.146,000 --> 0:02:39,000
Le robot continuait de chercher d'autres mines sur ses pattes restantes.

45
0:02:39.525,000 --> 0:02:42,000
Le colonel responsable de cet essai

46
0:02:43.061,000 --> 0:02:45,000
a fini par arrêter le programme,

47
0:02:45.203,000 --> 0:02:47,000
car, selon lui, c'était trop inhumain

48
0:02:47.662,000 --> 0:02:51,000
d'observer le robot endommagé se traîner dans les champs de mines.

49
0:02:54.978,000 --> 0:02:57,000
Qu'est-ce qui peut bien susciter chez un militaire endurci

50
0:02:58.899,000 --> 0:03:,000
et quelqu'un comme moi

51
0:03:00.942,000 --> 0:03:02,000
une telle réaction vis-à-vis des robots ?

52
0:03:03.537,000 --> 0:03:06,000
Il est vrai que nous sommes poussés par la science-fiction et la culture pop

53
0:03:07.107,000 --> 0:03:09,000
à concevoir de l'anthropomorphisme pour ces objets.

54
0:03:09.52,000 --> 0:03:11,000
Mais c'est un peu plus compliqué que ça.

55
0:03:12.287,000 --> 0:03:17,000
En fait, nous sommes biologiquement conçus pour projeter intention et vie

56
0:03:17.62,000 --> 0:03:21,000
dans toutes les choses animées dans notre espace physique qui paraissent autonomes.

57
0:03:23.214,000 --> 0:03:26,000
Dès lors, on traite toutes sortes de robots comme s'ils étaient vivants.

58
0:03:26.703,000 --> 0:03:28,000
On donne des noms aux engins de déminage

59
0:03:29.41,000 --> 0:03:3,000
et on leur remettait des médailles.

60
0:03:31.092,000 --> 0:03:33,000
Ils ont même eu des funérailles avec salve d'honneur.

61
0:03:34.38,000 --> 0:03:37,000
Les recherches éclairent le fait que nous agissons ainsi avec nos robots ménagers

62
0:03:38.237,000 --> 0:03:4,000
comme l'aspirateur Roomba.

63
0:03:40.396,000 --> 0:03:41,000
(Rires)

64
0:03:41.711,000 --> 0:03:44,000
C'est un simple disque qui déambule sur le sol pour le nettoyer,

65
0:03:44.824,000 --> 0:03:46,000
mais le seul fait de se mouvoir autours de nous

66
0:03:47.154,000 --> 0:03:49,000
incite les gens à lui donner un nom

67
0:03:49.321,000 --> 0:03:52,000
et à ressentir de la peine pour le Roomba quand celui-ci est coincé.

68
0:03:52.643,000 --> 0:03:53,000
(Rires)

69
0:03:54.44,000 --> 0:03:57,000
On peut concevoir des robots précisément pour susciter cette réaction,

70
0:03:57.804,000 --> 0:04:,000
en utilisant des yeux et un visage ou des mouvements

71
0:04:01.289,000 --> 0:04:04,000
que les gens vont automatiquement associer inconsciemment

72
0:04:04.572,000 --> 0:04:06,000
à des états d'esprit.

73
0:04:06.616,000 --> 0:04:09,000
Il y a un champ de recherche entier, l'interaction humains - robots,

74
0:04:09.909,000 --> 0:04:1,000
qui explique ces mécanismes.

75
0:04:11.835,000 --> 0:04:14,000
Par exemple, des chercheurs de l'Université de Stanford ont découvert

76
0:04:15.109,000 --> 0:04:16,000
que ça met les gens très mal à l'aise

77
0:04:16.934,000 --> 0:04:19,000
quand on leur demande de toucher les parties intimes d'un robot.

78
0:04:19.966,000 --> 0:04:2,000
(Rires)

79
0:04:21.598,000 --> 0:04:23,000
De ces réactions, et d'autres recherches,

80
0:04:23.645,000 --> 0:04:27,000
nous savons que les gens réagissent à des indices qui leur sont tendus

81
0:04:27.892,000 --> 0:04:29,000
par ces engins mimant la vie,

82
0:04:29.914,000 --> 0:04:32,000
même quand ils savent que ces engins ne sont pas vivants.

83
0:04:33.654,000 --> 0:04:37,000
Notre avenir sera rempli de robots.

84
0:04:37.734,000 --> 0:04:4,000
La technologie robotique est en train de sortir des usines.

85
0:04:40.823,000 --> 0:04:43,000
Elle intègre le lieu de travail, le domicile.

86
0:04:43.86,000 --> 0:04:46,000
Alors que ces machines autonomes qui peuvent sentir,

87
0:04:47.503,000 --> 0:04:52,000
prendre des décisions et apprendre, intègrent notre environnement de vie,

88
0:04:52.669,000 --> 0:04:54,000
je pense que la meilleure analogie à disposition

89
0:04:55.189,000 --> 0:04:56,000
est notre relation avec les animaux.

90
0:04:57.523,000 --> 0:05:,000
Il y a des milliers d'années, on a domestiqué les animaux

91
0:05:01.435,000 --> 0:05:05,000
et on les a entraînés au travail, à la guerre et à devenir nos compagnons.

92
0:05:05.504,000 --> 0:05:09,000
Durant l'histoire, on a traité certaines bêtes comme des outils ou des marchandises

93
0:05:10.489,000 --> 0:05:12,000
et on a traité d'autres animaux avec affection,

94
0:05:12.723,000 --> 0:05:15,000
on leur a donné une place dans la société en tant que compagnons.

95
0:05:15.813,000 --> 0:05:18,000
Je pense plausible que nous commencions à intégrer les robots de la même manière.

96
0:05:21.484,000 --> 0:05:24,000
Certes, les animaux sont vivants.

97
0:05:24.58,000 --> 0:05:25,000
Les robots ne le sont pas.

98
0:05:27.626,000 --> 0:05:3,000
Et mon expérience avec des ingénieurs en robotique me permet d'affirmer

99
0:05:30.956,000 --> 0:05:33,000
qu'on est loin de concevoir des robots qui ressentent quoi que ce soit.

100
0:05:35.072,000 --> 0:05:37,000
Mais nous éprouvons des sentiments à leur égard

101
0:05:37.835,000 --> 0:05:38,000
et c'est crucial.

102
0:05:39.066,000 --> 0:05:42,000
Car si nous tentons d'intégrer des robots dans nos espaces de vie,

103
0:05:42.717,000 --> 0:05:46,000
il faut comprendre qu'on ne va pas les traiter comme d'autres objets

104
0:05:47.369,000 --> 0:05:48,000
et que dans certains cas,

105
0:05:49.237,000 --> 0:05:52,000
par exemple, celui du soldat qui s'est attaché affectivement

106
0:05:52.433,000 --> 0:05:54,000
au robot avec lequel il travaille,

107
0:05:54.504,000 --> 0:05:56,000
ça peut devenir improductif, voire dangereux.

108
0:05:58.551,000 --> 0:06:,000
Dans d'autres cas toutefois, ça peut être utile

109
0:06:00.819,000 --> 0:06:02,000
de nourrir une connexion émotionnelle avec un robot.

110
0:06:04.184,000 --> 0:06:06,000
Il y a des usages fabuleux,

111
0:06:06.318,000 --> 0:06:09,000
comme par exemple les robots qui travaillent avec des enfants autistes.

112
0:06:09.732,000 --> 0:06:11,000
Ils créent un lien avec eux jamais constaté auparavant.

113
0:06:12.628,000 --> 0:06:14,000
Il y a aussi des robots aux côtés des enseignants

114
0:06:14.983,000 --> 0:06:16,000
pour pousser, avec succès, les enfants à apprendre.

115
0:06:17.433,000 --> 0:06:18,000
Il n'y a pas que les enfants.

116
0:06:19.75,000 --> 0:06:22,000
Des études montrent que les robots peuvent assister médecins et infirmières

117
0:06:23.283,000 --> 0:06:25,000
dans le milieu des soins de la santé.

118
0:06:25.535,000 --> 0:06:26,000
PARO est un robot bébé phoque.

119
0:06:27.369,000 --> 0:06:3,000
Il est utilisé dans des maisons pour patients atteints de démence.

120
0:06:30.654,000 --> 0:06:31,000
Ça fait déjà un certain temps.

121
0:06:32.564,000 --> 0:06:35,000
Je me souviens d'une soirée, il y a des années,

122
0:06:35.621,000 --> 0:06:37,000
où j'avais parlé de ce robot à quelqu'un.

123
0:06:38.216,000 --> 0:06:4,000
Et cette personne avait réagi ainsi :

124
0:06:40.366,000 --> 0:06:41,000
« Oh mon dieu !

125
0:06:42.508,000 --> 0:06:43,000
C'est horrible.

126
0:06:45.056,000 --> 0:06:49,000
Je ne peux pas croire qu'on donne aux gens des robots à la place d'attention. »

127
0:06:50.54,000 --> 0:06:51,000
C'était une réaction habituelle

128
0:06:52.439,000 --> 0:06:54,000
et qui est correcte, je pense,

129
0:06:54.962,000 --> 0:06:56,000
car en effet, ce serait terrible.

130
0:06:57.795,000 --> 0:07:,000
Dans ce cas, le robot ne remplace pas l'homme.

131
0:07:00.849,000 --> 0:07:03,000
C'est de la thérapie animale

132
0:07:04.002,000 --> 0:07:07,000
dans un contexte où on ne peut pas utiliser des animaux.

133
0:07:07.2,000 --> 0:07:08,000
Mais on peut utiliser des robots

134
0:07:08.788,000 --> 0:07:12,000
car les gens vont les traiter davantage comme un animal que comme un robot.

135
0:07:15.502,000 --> 0:07:17,000
Reconnaître cette connexion émotionnelle aux robots

136
0:07:17.882,000 --> 0:07:19,000
peut aussi nous aider à anticiper les défis

137
0:07:19.921,000 --> 0:07:22,000
liés au fait que ces engins intègrent les espaces intimes de nos vies.

138
0:07:24.111,000 --> 0:07:27,000
Par exemple, serait-il acceptable que le robot nounours de votre enfant

139
0:07:27.539,000 --> 0:07:29,000
enregistre des conversations privées ?

140
0:07:29.8,000 --> 0:07:33,000
Serait-il tolérable que votre robot sexuel ait une fonction d'achat attractive ?

141
0:07:33.887,000 --> 0:07:34,000
(Rires)

142
0:07:35.307,000 --> 0:07:37,000
Car robots plus capitalisme

143
0:07:37.832,000 --> 0:07:4,000
égalent les questions sur la protection du consommateur et la vie privée.

144
0:07:42.549,000 --> 0:07:43,000
Ce ne sont pas les seules raisons

145
0:07:44.185,000 --> 0:07:47,000
pour lesquelles nos comportements avec ces engins sont importants.

146
0:07:48.747,000 --> 0:07:51,000
Quelques années après ma première expérience

147
0:07:52.041,000 --> 0:07:54,000
avec le robot bébé dinosaure,

148
0:07:54.376,000 --> 0:07:56,000
j'ai organisé un atelier avec mon ami Hannes Gassert.

149
0:07:56.901,000 --> 0:07:58,000
On a pris cinq robots bébés dinosaures

150
0:07:59.822,000 --> 0:08:01,000
que l'on a remis à cinq équipes.

151
0:08:02.299,000 --> 0:08:03,000
Elles devaient leur donner un nom,

152
0:08:04.02,000 --> 0:08:07,000
jouer avec et interagir avec eux pendant une heure environ.

153
0:08:08.727,000 --> 0:08:1,000
Ensuite, on leur a remis un marteau et une hache

154
0:08:10.973,000 --> 0:08:12,000
et on leur a demandé de torturer et tuer les robots.

155
0:08:13.715,000 --> 0:08:14,000
(Rires)

156
0:08:16.857,000 --> 0:08:18,000
La situation est devenue beaucoup plus tendue

157
0:08:19.151,000 --> 0:08:2,000
que ce que nous avions pensé

158
0:08:20.579,000 --> 0:08:22,000
car aucun participant n'a voulu faire de mal

159
0:08:23.513,000 --> 0:08:24,000
aux robots bébés dinosaures.

160
0:08:24.92,000 --> 0:08:29,000
Du coup, on a dû improviser et leur dire ceci :

161
0:08:30.078,000 --> 0:08:34,000
« D'accord. Vous pouvez sauver votre robot si vous détruisez celui des autres. »

162
0:08:34.539,000 --> 0:08:35,000
(Rires)

163
0:08:36.839,000 --> 0:08:38,000
Même ça, ça n'a pas marché. Ils n'y arrivaient pas.

164
0:08:39.254,000 --> 0:08:4,000
Alors on leur a dit :

165
0:08:40.539,000 --> 0:08:41,000
« On va détruire tous les robots

166
0:08:42.289,000 --> 0:08:44,000
si personne n'en casse un à la hache. »

167
0:08:45.586,000 --> 0:08:48,000
Alors, un type s'est levé et a pris la hache.

168
0:08:49.189,000 --> 0:08:51,000
Toute l'assemblée a grimacé quand il l'a abattue

169
0:08:51.919,000 --> 0:08:52,000
sur le cou du robot.

170
0:08:53.723,000 --> 0:08:59,000
Un silence s'est installé dans la salle, mi-sérieux, mi-plaisanterie,

171
0:09:00.085,000 --> 0:09:01,000
pour le robot tombé.

172
0:09:01.807,000 --> 0:09:02,000
(Rires)

173
0:09:03.237,000 --> 0:09:06,000
C'était une expérience vraiment intéressante.

174
0:09:06.955,000 --> 0:09:08,000
Ce n'était pas une expérience contrôlée

175
0:09:09.438,000 --> 0:09:11,000
mais elle a mené à des recherches ultérieures au MIT

176
0:09:12.288,000 --> 0:09:14,000
que j'ai conduites avec Palash Nandy et Cynthia Breazeal.

177
0:09:14.956,000 --> 0:09:17,000
Nous invitions des gens dans le labo pour écraser ces HEXBUGs

178
0:09:18.215,000 --> 0:09:21,000
qui se meuvent très réalistement, comme des insectes.

179
0:09:21.326,000 --> 0:09:24,000
Au lieu de choisir des objets mignons qui attirent les gens,

180
0:09:24.484,000 --> 0:09:26,000
on a choisi quelque chose de plus basique.

181
0:09:26.601,000 --> 0:09:29,000
Nous avons découvert que les gens avec une grande empathie

182
0:09:30.105,000 --> 0:09:32,000
hésitaient davantage à frapper les HEXBUGs.

183
0:09:33.575,000 --> 0:09:34,000
C'est une petite étude

184
0:09:35.163,000 --> 0:09:37,000
qui fait partie d'un grand champ de recherches

185
0:09:37.576,000 --> 0:09:39,000
qui semble indiquer qu'il pourrait y avoir un lien

186
0:09:40.544,000 --> 0:09:42,000
entre les tendances pour l'empathie des gens

187
0:09:42.941,000 --> 0:09:43,000
et leur comportement avec des robots.

188
0:09:45.721,000 --> 0:09:48,000
Ma question cruciale pour l'ère prochaine d'interactions hommes - robots

189
0:09:49.372,000 --> 0:09:52,000
n'est pas : « Ressentons-nous de l'empathie pour les robots ? »

190
0:09:53.211,000 --> 0:09:56,000
C'est : « Les robots peuvent-ils changer l'empathie des gens ? »

191
0:09:57.489,000 --> 0:09:59,000
Y a-t-il une bonne raison pour, par exemple,

192
0:09:59.8,000 --> 0:10:01,000
empêcher votre enfant de frapper un robot-chien,

193
0:10:03.228,000 --> 0:10:05,000
non pas pour le respect des biens,

194
0:10:06.166,000 --> 0:10:09,000
mais parce que ça pourrait le rendre plus susceptible de frapper un vrai chien ?

195
0:10:10.507,000 --> 0:10:12,000
À nouveau, il n'y a pas que les enfants.

196
0:10:13.564,000 --> 0:10:17,000
C'est le débat des jeux vidéo violents, mais à un tout autre niveau,

197
0:10:17.644,000 --> 0:10:21,000
à cause de cette réaction physique et viscérale plus intense

198
0:10:22.428,000 --> 0:10:24,000
que celle ressentie face à des images sur écran.

199
0:10:25.674,000 --> 0:10:27,000
Si nous agissons avec violence vis-à-vis des robots,

200
0:10:28.276,000 --> 0:10:31,000
des robots conçus pour imiter la vie,

201
0:10:31.42,000 --> 0:10:34,000
est-ce un exutoire sain des comportements violents

202
0:10:35.336,000 --> 0:10:37,000
ou bien est-ce en train de nourrir notre cruauté ?

203
0:10:39.511,000 --> 0:10:4,000
On ne sait pas.

204
0:10:42.622,000 --> 0:10:45,000
Toutefois la réponse peut influencer les comportements humains,

205
0:10:46.591,000 --> 0:10:48,000
elle peut influencer les normes sociales,

206
0:10:49.383,000 --> 0:10:52,000
elle peut inspirer des règles sur ce qu'on peut et ne peut pas faire

207
0:10:53.256,000 --> 0:10:54,000
avec certains robots,

208
0:10:54.407,000 --> 0:10:56,000
à l'instar des lois contre la cruauté envers les animaux.

209
0:10:57.235,000 --> 0:10:59,000
Même si les robots ne ressentent pas,

210
0:11:00.116,000 --> 0:11:03,000
notre comportement à leur égard est important pour nous-mêmes.

211
0:11:04.889,000 --> 0:11:06,000
Que nous changions ou pas nos réglementations,

212
0:11:08.926,000 --> 0:11:12,000
les robots pourraient nous offrir une compréhension nouvelle de nous-mêmes.

213
0:11:14.276,000 --> 0:11:16,000
Tout ce que j'ai appris ces dix dernières années,

214
0:11:16.616,000 --> 0:11:18,000
ne concerne pas la technologie.

215
0:11:18.878,000 --> 0:11:2,000
Il s'agit de la psychologie de l'homme,

216
0:11:21.405,000 --> 0:11:24,000
de l'empathie et de la façon dont nous nous lions avec autrui.

217
0:11:25.524,000 --> 0:11:27,000
Car quand un enfant est gentil avec Roomba,

218
0:11:29.262,000 --> 0:11:33,000
qu'un soldat tente de sauver un robot sur le champ de bataille

219
0:11:33.301,000 --> 0:11:36,000
ou qu'un groupe de personnes refuse d'abîmer un robot bébé dinosaure,

220
0:11:38.248,000 --> 0:11:41,000
ces robots ne sont plus que des moteurs, des engrenages et des algorithmes.

221
0:11:42.501,000 --> 0:11:44,000
Ils deviennent le reflet de notre propre humanité.

222
0:11:45.523,000 --> 0:11:46,000
Merci.

223
0:11:46.698,000 --> 0:11:49,000
(Applaudissements)

