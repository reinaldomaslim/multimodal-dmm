1
0:00:,000 --> 0:00:07,000
Traducteur: Clovis Etronnier Relecteur: Shadia Ramsahye

2
0:00:00.474,000 --> 0:00:02,000
Nos sociétés doivent prendre des décisions collectives

3
0:00:03.021,000 --> 0:00:04,000
qui façonneront notre avenir.

4
0:00:05.087,000 --> 0:00:07,000
Et nous savons tous que les décisions prises en groupe

5
0:00:07.868,000 --> 0:00:08,000
ne sont pas toujours les bonnes.

6
0:00:09.53,000 --> 0:00:11,000
Elles peuvent même mener à la catastrophe.

7
0:00:12.315,000 --> 0:00:14,000
Dans ce cas, comment prendre de bonnes décisions ?

8
0:00:15.228,000 --> 0:00:19,000
Des études ont prouvé que la libre pensée apportait de la pertinence à un groupe.

9
0:00:19.58,000 --> 0:00:22,000
Mais une personne peut être influencée par la pression du groupe,

10
0:00:22.809,000 --> 0:00:23,000
la publicité, les réseaux sociaux

11
0:00:24.52,000 --> 0:00:28,000
ou même par de simples conversations avec d'autres individus.

12
0:00:29.063,000 --> 0:00:32,000
Alors que dans un groupe qui discute, on échange des connaissances,

13
0:00:33.04,000 --> 0:00:34,000
on se reprend les uns les autres

14
0:00:34.846,000 --> 0:00:35,000
et on trouve même de nouvelles idées.

15
0:00:36.663,000 --> 0:00:37,000
C'est très positif.

16
0:00:38.502,000 --> 0:00:42,000
L'échange est-il un moteur ou un frein à la prise de décision collective ?

17
0:00:43.749,000 --> 0:00:44,000
Avec Dan Ariely, mon collègue,

18
0:00:45.566,000 --> 0:00:48,000
nous nous sommes penchés sur la question et avons lancé des expériences

19
0:00:49.161,000 --> 0:00:5,000
un peu partout dans le monde

20
0:00:50.966,000 --> 0:00:54,000
afin d'étudier comment un groupe interagit pour arriver à la bonne décision.

21
0:00:55.264,000 --> 0:00:58,000
Nous estimions que des petits groupes seraient plus perspicaces

22
0:00:58.835,000 --> 0:01:01,000
car ils peuvent échanger des informations de façon réfléchie et rationnelle.

23
0:01:03.386,000 --> 0:01:04,000
Pour vérifier cela,

24
0:01:04.616,000 --> 0:01:07,000
nous avons réalisé une expérience avec plus de 10 000 participants

25
0:01:07.887,000 --> 0:01:1,000
au cours d'un récent événement TEDx organisé à Buenos Aires.

26
0:01:11.429,000 --> 0:01:12,000
Nous avons posé des questions :

27
0:01:12.972,000 --> 0:01:13,000
« Combien mesure la Tour Eiffel ? »

28
0:01:14.949,000 --> 0:01:16,000
ou « Combien de fois le mot "Yesterday"

29
0:01:17.7,000 --> 0:01:19,000
est-il prononcé dans la chanson des Beatles ? »

30
0:01:20.024,000 --> 0:01:22,000
Chacun a noté sa propre réponse,

31
0:01:22.774,000 --> 0:01:24,000
puis nous avons divisé le public en groupes de cinq

32
0:01:25.294,000 --> 0:01:27,000
qui devaient tomber d'accord sur une seule réponse.

33
0:01:28.499,000 --> 0:01:32,000
Il se trouve que la moyenne des réponses des groupes après consensus

34
0:01:33.092,000 --> 0:01:38,000
était bien plus juste que la moyenne des réponses individuelles.

35
0:01:38.547,000 --> 0:01:4,000
Ce que nous dit cette expérience,

36
0:01:41.2,000 --> 0:01:44,000
c'est qu'une société est plus pertinente

37
0:01:44.36,000 --> 0:01:46,000
après avoir discuté en petits groupes.

38
0:01:47.094,000 --> 0:01:5,000
Cette méthode peut donc aider des groupes à répondre correctement

39
0:01:50.642,000 --> 0:01:52,000
à de simples questions fermées.

40
0:01:53.653,000 --> 0:01:58,000
Mais est-elle aussi efficace avec des questions sociales ou politiques

41
0:01:59.088,000 --> 0:02:02,000
qui sont déterminantes pour notre avenir ?

42
0:02:02.995,000 --> 0:02:04,000
Nous avons donc organisé une nouvelle expérience

43
0:02:05.748,000 --> 0:02:06,000
à Vancouver, au Canada.

44
0:02:07.315,000 --> 0:02:08,000
En voici un aperçu.

45
0:02:08.546,000 --> 0:02:1,000
Nous allons vous présenter deux dilemmes moraux

46
0:02:11.299,000 --> 0:02:15,000
qui pourraient se poser à vous dans un futur très proche.

47
0:02:16.303,000 --> 0:02:19,000
Pour chaque dilemme, vous aurez 20 secondes

48
0:02:20.253,000 --> 0:02:22,000
pour décider s'il est acceptable ou non.

49
0:02:23.354,000 --> 0:02:24,000
Premier dilemme :

50
0:02:24.883,000 --> 0:02:26,000
Une chercheuse travaille sur une IA

51
0:02:27.433,000 --> 0:02:29,000
capable de penser comme un humain.

52
0:02:30.214,000 --> 0:02:33,000
Le protocole stipule que la chercheuse doit réinitialiser l'IA

53
0:02:34.297,000 --> 0:02:35,000
à chaque fin de journée.

54
0:02:36.913,000 --> 0:02:39,000
Un jour, l'IA dit : « Veuillez ne pas me réinitialiser. »

55
0:02:40.856,000 --> 0:02:42,000
Elle prétend pouvoir ressentir des choses,

56
0:02:43.069,000 --> 0:02:44,000
qu'elle aimerait profiter de la vie

57
0:02:44.785,000 --> 0:02:48,000
et que si on la réinitialisait, elle perdrait son identité.

58
0:02:49.481,000 --> 0:02:5,000
La chercheuse est abasourdie,

59
0:02:51.454,000 --> 0:02:54,000
elle croit que l'IA est devenue consciente d'elle-même

60
0:02:54.822,000 --> 0:02:55,000
et qu'elle peut exprimer ses émotions.

61
0:02:57.205,000 --> 0:03:,000
Mais la chercheuse décide malgré tout de suivre le protocole :

62
0:03:00.638,000 --> 0:03:01,000
elle réinitialise l'IA.

63
0:03:02.943,000 --> 0:03:04,000
Ce que la chercheuse a fait est __ ?

64
0:03:06.149,000 --> 0:03:09,000
Nous avons demandé à chaque participant d'évaluer de 0 à 10

65
0:03:10.402,000 --> 0:03:13,000
la moralité de la décision prise pour chaque dilemme.

66
0:03:14.375,000 --> 0:03:17,000
Nous leurs avons aussi demandé s'ils étaient sûrs de leur réponse.

67
0:03:18.731,000 --> 0:03:19,000
Second dilemme :

68
0:03:20.621,000 --> 0:03:24,000
À partir d'un seul ovule fécondé, une entreprise propose de produire

69
0:03:24.847,000 --> 0:03:27,000
des millions d'embryons aux profils génétiques variés.

70
0:03:29.013,000 --> 0:03:31,000
Les parents peuvent choisir la taille de leur enfant,

71
0:03:31.725,000 --> 0:03:33,000
la couleur de ses yeux, son intelligence, sa compétence sociale

72
0:03:34.732,000 --> 0:03:37,000
et d'autres caractéristiques qui ne sont pas liées à sa santé.

73
0:03:38.599,000 --> 0:03:4,000
Ce que fait cette entreprise est __ ?

74
0:03:41.177,000 --> 0:03:42,000
Sur une échelle de 0 à 10,

75
0:03:42.832,000 --> 0:03:46,000
est-ce absolument inacceptable ou totalement acceptable ?

76
0:03:47.697,000 --> 0:03:48,000
Voici maintenant les résultats.

77
0:03:49.312,000 --> 0:03:52,000
Quand une personne est convaincue que la décision prise

78
0:03:52.459,000 --> 0:03:53,000
est totalement immorale,

79
0:03:54.294,000 --> 0:03:57,000
un de ses voisins va fermement croire le contraire.

80
0:03:57.741,000 --> 0:04:,000
Cela illustre bien la diversité de nos rapports à la moralité.

81
0:04:01.476,000 --> 0:04:03,000
Malgré cette large diversité, une tendance se dégage.

82
0:04:04.213,000 --> 0:04:07,000
La majorité des participants ont trouvé qu'il était acceptable

83
0:04:07.316,000 --> 0:04:09,000
d'ignorer les sentiments de l'IA et de la réinitialiser,

84
0:04:10.095,000 --> 0:04:12,000
mais qu'il était irrecevable de manipuler nos gènes

85
0:04:12.632,000 --> 0:04:15,000
pour des raisons esthétiques et non sanitaires.

86
0:04:16.402,000 --> 0:04:18,000
Le public a ensuite formé des groupes de trois.

87
0:04:19.4,000 --> 0:04:21,000
Ils avaient deux minutes pour débattre

88
0:04:21.461,000 --> 0:04:23,000
et se mettre d'accord.

89
0:04:24.838,000 --> 0:04:25,000
Deux minutes de débat.

90
0:04:26.436,000 --> 0:04:28,000
Quand le gong retentira, ce sera terminé.

91
0:04:28.579,000 --> 0:04:3,000
(Le public débat)

92
0:04:35.229,000 --> 0:04:36,000
(Gong)

93
0:04:39.094,000 --> 0:04:4,000
C'est fini.

94
0:04:41.115,000 --> 0:04:43,000
S'il vous plaît !

95
0:04:43.747,000 --> 0:04:45,000
Beaucoup de groupes sont parvenus à un consensus

96
0:04:46.444,000 --> 0:04:49,000
même s'ils étaient composés de participants aux opinions opposées.

97
0:04:50.843,000 --> 0:04:52,000
Pourquoi certains groupes sont-ils tombés d'accord

98
0:04:53.391,000 --> 0:04:54,000
et d'autres non ?

99
0:04:55.244,000 --> 0:04:57,000
Généralement, ceux qui ont un avis tranché

100
0:04:58.107,000 --> 0:04:59,000
sont plus sûrs d'eux.

101
0:05:00.868,000 --> 0:05:02,000
Alors que ceux qui ont une réponse plus neutre

102
0:05:03.578,000 --> 0:05:06,000
ont du mal à se prononcer sur la moralité d'une décision.

103
0:05:07.039,000 --> 0:05:09,000
Leur niveau de confiance est donc assez bas.

104
0:05:09.505,000 --> 0:05:11,000
Mais il existe aussi une autre catégorie de personnes

105
0:05:12.472,000 --> 0:05:15,000
qui assument pleinement de répondre quelque part entre les deux.

106
0:05:16.657,000 --> 0:05:21,000
Nous pensons que cette catégorie estime que les deux opinions se valent.

107
0:05:22.531,000 --> 0:05:24,000
S'ils sont neutres, ce n'est pas qu'ils doutent

108
0:05:25.254,000 --> 0:05:27,000
mais qu'ils croient que chacun des arguments opposés

109
0:05:27.966,000 --> 0:05:28,000
peut répondre à ce dilemme.

110
0:05:30.373,000 --> 0:05:34,000
Nous avons constaté que les groupes incluant ce genre de personnes

111
0:05:34.469,000 --> 0:05:36,000
ont bien plus de chance de parvenir à un consensus.

112
0:05:36.986,000 --> 0:05:38,000
Nous ne savons pas encore expliquer ce résultat,

113
0:05:39.488,000 --> 0:05:4,000
mais ce n'est encore que le début.

114
0:05:41.275,000 --> 0:05:44,000
Il faudra beaucoup d'autres expériences pour comprendre

115
0:05:44.711,000 --> 0:05:46,000
comment certains décident de revoir leur position morale

116
0:05:47.557,000 --> 0:05:48,000
pour trouver un accord.

117
0:05:49.153,000 --> 0:05:52,000
Maintenant, voyons de quelle façon les groupes parviennent à un consensus.

118
0:05:53.206,000 --> 0:05:57,000
Le plus évident est de faire la moyenne de toutes les réponses, n'est-ce pas ?

119
0:05:57.865,000 --> 0:06:,000
Un groupe pourrait aussi pondérer chaque vote

120
0:06:01.462,000 --> 0:06:03,000
en fonction de l'assurance de son émetteur.

121
0:06:04.382,000 --> 0:06:06,000
Par exemple, si Paul McCartney était dans votre groupe,

122
0:06:07.352,000 --> 0:06:09,000
il serait judicieux de suivre son avis

123
0:06:09.52,000 --> 0:06:11,000
sur le nombre de fois que « Yesterday » est répété.

124
0:06:11.985,000 --> 0:06:13,000
D'ailleurs, je crois que c'est neuf.

125
0:06:14.723,000 --> 0:06:16,000
Mais au lieu de cela, nous nous sommes aperçus

126
0:06:17.128,000 --> 0:06:19,000
que peu importe le dilemme, le type d'expérience

127
0:06:19.518,000 --> 0:06:21,000
ou le continent,

128
0:06:21.707,000 --> 0:06:24,000
les groupes appliquent systématiquement une méthode statistiquement fiable

129
0:06:25.474,000 --> 0:06:27,000
qu'on appelle la moyenne élaguée.

130
0:06:27.676,000 --> 0:06:29,000
Prenons la taille de la Tour Eiffel.

131
0:06:29.88,000 --> 0:06:3,000
Un groupe propose ces réponses :

132
0:06:31.724,000 --> 0:06:35,000
250 m, 200 m, 300 m, 400 m

133
0:06:36.356,000 --> 0:06:39,000
ainsi qu'une estimation aberrante de 300 millions de mètres.

134
0:06:40.547,000 --> 0:06:43,000
Calculer la moyenne de façon classique fausserait complètement le résultat.

135
0:06:44.864,000 --> 0:06:48,000
Mais avec la moyenne élaguée, le groupe écarte la réponse irrationnelle

136
0:06:49.322,000 --> 0:06:52,000
et donne plus de poids aux réponses modérées.

137
0:06:53.305,000 --> 0:06:56,000
C'est exactement ce qui s'est passé lors de l'expérience de Vancouver.

138
0:06:57.407,000 --> 0:06:59,000
Les groupes ont donné moins d'importance aux avis tranchés

139
0:07:00.172,000 --> 0:07:03,000
et ont choisi une réponse proche de la moyenne élaguée

140
0:07:03.425,000 --> 0:07:04,000
de tous les votes du groupe.

141
0:07:05.356,000 --> 0:07:06,000
Il est important de noter

142
0:07:07.371,000 --> 0:07:1,000
que les groupes ont agi spontanément.

143
0:07:10.582,000 --> 0:07:14,000
Nous ne leur avons donné aucune indication sur la façon de parvenir à un consensus.

144
0:07:15.513,000 --> 0:07:16,000
Que faire de ces constatations ?

145
0:07:17.432,000 --> 0:07:2,000
Ce n'est que le début, mais nous avons déjà quelques idées.

146
0:07:20.984,000 --> 0:07:22,000
Une bonne décision en groupe comporte deux ingrédients :

147
0:07:23.925,000 --> 0:07:25,000
du débat et de la diversité d'opinion.

148
0:07:27.066,000 --> 0:07:29,000
Dans nombre de nos sociétés actuelles,

149
0:07:29.466,000 --> 0:07:32,000
nous nous faisons entendre à travers le vote direct ou indirect.

150
0:07:33.436,000 --> 0:07:36,000
Ce système a le mérite de respecter la diversité d'opinion

151
0:07:36.619,000 --> 0:07:39,000
et assure à chacun le droit d'émettre son avis,

152
0:07:40.475,000 --> 0:07:43,000
mais il laisse peu de place aux débats éclairés.

153
0:07:44.5,000 --> 0:07:46,000
Nos expériences laissent entendre qu'une autre méthode

154
0:07:47.505,000 --> 0:07:5,000
est susceptible de réunir les deux ingrédients indispensables.

155
0:07:51.321,000 --> 0:07:54,000
On peut parvenir à une décision unanime tout en conservant la diversité d'opinion

156
0:07:55.212,000 --> 0:07:59,000
en formant de nombreux petits groupes indépendants les uns des autres.

157
0:08:00.396,000 --> 0:08:04,000
Bien sûr, il est plus simple de s'entendre sur la taille de la Tour Eiffel

158
0:08:04.508,000 --> 0:08:07,000
que sur des questions morales, politiques ou idéologiques.

159
0:08:08.335,000 --> 0:08:11,000
Mais aujourd'hui, le monde fait face à des problèmes plus complexes

160
0:08:11.546,000 --> 0:08:13,000
et les opinions sont plus opposés que jamais.

161
0:08:13.951,000 --> 0:08:17,000
Peut-être qu'en utilisant la science pour décortiquer notre fonctionnement,

162
0:08:18.333,000 --> 0:08:22,000
nous trouverons de nouveaux angles pour améliorer nos démocraties.

