1
0:00:,000 --> 0:00:07,000
Traductor: Ruth Alonso Revisor: Karla V. Leal

2
0:00:18.33,000 --> 0:00:24,000
Si le preguntan a la gente qué parte de la psicología creen que es más difícil,

3
0:00:24.33,000 --> 0:00:27,000
y dices, bueno, "¿qué hay del pensamiento y las emociones?"

4
0:00:27.33,000 --> 0:00:3,000
La mayoría de la gente dirá, "las emociones son muy complicadas.

5
0:00:30.33,000 --> 0:00:36,000
Son increíblemente complejas, no pueden... No tengo ni idea de cómo funcionan¨.

6
0:00:36.33,000 --> 0:00:38,000
Pero el pensamiento es muy directo:

7
0:00:38.33,000 --> 0:00:42,000
es simplemente una especie de razonamiento lógico o algo así.

8
0:00:42.33,000 --> 0:00:45,000
Pero eso no es lo difícil.

9
0:00:45.33,000 --> 0:00:47,000
Así que aquí hay una lista de problemas que surgen.

10
0:00:47.33,000 --> 0:00:5,000
Un problema fino es: ¿qué haremos con respecto a la salud?

11
0:00:50.33,000 --> 0:00:54,000
El otro día estaba leyendo algo y la persona decía que probablemente

12
0:00:54.33,000 --> 0:01:,000
dar la mano en occidente es el principal causante de enfermedades.

13
0:01:00.33,000 --> 0:01:04,000
Y hubo un pequeño estudio sobre la gente que no da la mano,

14
0:01:04.33,000 --> 0:01:07,000
que los comparaba con los que sí la dan,

15
0:01:07.33,000 --> 0:01:12,000
y no tengo ni remota idea de dónde se encuentran los que no dan la mano,

16
0:01:12.33,000 --> 0:01:15,000
porque deben de estar escondidos.

17
0:01:15.33,000 --> 0:01:19,000
Y la gente que evita eso tiene un 30% menos

18
0:01:19.33,000 --> 0:01:23,000
de posibilidades de contraer enfermedades contagiosas.

19
0:01:23.33,000 --> 0:01:26,000
O quizás fuera un 31% y un cuarto.

20
0:01:26.33,000 --> 0:01:3,000
Así que si realmente queremos solucionar el problema de las epidemias y demás,

21
0:01:30.33,000 --> 0:01:34,000
empecemos con eso. Y desde que se me ocurrió esa idea,

22
0:01:34.33,000 --> 0:01:38,000
he dado la mano cientos de veces.

23
0:01:38.33,000 --> 0:01:43,000
Creo que la única manera de evitarlo

24
0:01:43.33,000 --> 0:01:45,000
es tener algún tipo de enfermedad visiblemente horrorosa

25
0:01:45.33,000 --> 0:01:48,000
y así no tienes que explicar nada.

26
0:01:48.33,000 --> 0:01:52,000
Educación: ¿cómo mejorar la educación?

27
0:01:52.33,000 --> 0:01:56,000
Bueno, la mejor manera es hacer que entiendan

28
0:01:56.33,000 --> 0:01:59,000
que lo que se les cuenta son tonterías.

29
0:01:59.33,000 --> 0:02:01,000
Claro, entonces, tienes que hacer algo

30
0:02:01.33,000 --> 0:02:06,000
para moderar eso y que así de alguna manera te escuchen a ti.

31
0:02:06.33,000 --> 0:02:1,000
Polución, carencia de energía, diversidad ambiental, pobreza...

32
0:02:10.33,000 --> 0:02:14,000
¿Cómo crear sociedades estables? Longevidad.

33
0:02:14.33,000 --> 0:02:17,000
Hay muchos problemas de los que preocuparse.

34
0:02:17.33,000 --> 0:02:19,000
En cualquier caso, la pregunta que creo que la gente debe hacerse -

35
0:02:19.33,000 --> 0:02:24,000
y es completamente tabú- es, ¿cuántas personas debería haber?

36
0:02:24.33,000 --> 0:02:31,000
Creo que debería haber sobre 100 millones, o quizá 500.

37
0:02:31.33,000 --> 0:02:36,000
Y entonces nos damos cuenta de que muchos de estos problemas desaparecen.

38
0:02:36.33,000 --> 0:02:38,000
Si tienes 100 millones de personas

39
0:02:38.33,000 --> 0:02:44,000
bien esparcidas, si hay algo de basura

40
0:02:44.33,000 --> 0:02:51,000
la tiras, preferentemente donde no se pueda ver, y se pudrirá.

41
0:02:51.33,000 --> 0:02:56,000
O la tiras al océano y algunos peces se beneficiarán.

42
0:02:56.33,000 --> 0:02:58,000
El problema es, ¿cuánta gente debería haber?

43
0:02:58.33,000 --> 0:03:01,000
Es una decisión que tenemos que tomar.

44
0:03:01.33,000 --> 0:03:04,000
La mayoría de la gente mide 60 pulgadas o más,

45
0:03:04.33,000 --> 0:03:08,000
y hay esta pérdida al cubo si los haces así de grandes -

46
0:03:08.33,000 --> 0:03:11,000
usando nanotecnología, supongo-

47
0:03:11.33,000 --> 0:03:12,000
(Risas)

48
0:03:12.33,000 --> 0:03:14,000
entonces podrían tener mil veces más.

49
0:03:14.33,000 --> 0:03:16,000
Eso solucionaría el problema, pero no veo a nadie

50
0:03:16.33,000 --> 0:03:19,000
investigando cómo hacer a la gente más pequeña.

51
0:03:19.33,000 --> 0:03:24,000
Claro que está bien reducir la población, pero mucha gente quiere tener hijos.

52
0:03:24.33,000 --> 0:03:27,000
Hay una solución que probablemente sólo está desfasada unos años.

53
0:03:27.33,000 --> 0:03:32,000
Saben que tienen 46 cromosomas. Si tienen suerte, tienen 23

54
0:03:32.33,000 --> 0:03:38,000
de cada padre; a veces tienes uno extra o uno menos,

55
0:03:38.33,000 --> 0:03:42,000
pero -de modo que saltarías la etapa de abuelo y bisabuelo

56
0:03:42.33,000 --> 0:03:47,000
e irías directamente al tatarabuelo. Si tienes 46 personas

57
0:03:47.33,000 --> 0:03:5,000
y les das un escáner, o lo que necesites,

58
0:03:50.33,000 --> 0:03:54,000
y miran sus cromosomas y cada uno dice

59
0:03:54.33,000 --> 0:03:59,000
cuál le gusta más, o ella- ya no hay razón para tener sólo dos sexos.

60
0:03:59.33,000 --> 0:04:04,000
Así que cada hijo tiene 46 padres,

61
0:04:04.33,000 --> 0:04:1,000
y supongo que se puede dejar a cada grupo de 46 padres tener 15 hijos,

62
0:04:10.33,000 --> 0:04:12,000
¿no sería eso suficiente? De ese modo los niños

63
0:04:12.33,000 --> 0:04:16,000
tendrían suficiente apoyo, amor y modelos

64
0:04:16.33,000 --> 0:04:18,000
y la población mundial se reduciría rápidamente

65
0:04:18.33,000 --> 0:04:21,000
y todos serían totalmente felices.

66
0:04:21.33,000 --> 0:04:24,000
El tiempo compartido está más alejado en el futuro.

67
0:04:24.33,000 --> 0:04:27,000
Hay una gran novela que Arthur Clark escribió dos veces,

68
0:04:27.33,000 --> 0:04:31,000
llamada "Tras la caída de la noche" y "La ciudad y las estrellas".

69
0:04:31.33,000 --> 0:04:34,000
Las dos son maravillosas y básicamente la misma,

70
0:04:34.33,000 --> 0:04:36,000
sólo que las computadoras surgieron entre ambas,

71
0:04:36.33,000 --> 0:04:41,000
y Arthur estaba mirando la novela más vieja, y dijo, "bueno, eso fue un error.

72
0:04:41.33,000 --> 0:04:43,000
El futuro ha de tener ordenadores".

73
0:04:43.33,000 --> 0:04:48,000
Así que en la segunda versión hay 100 millardos,

74
0:04:48.33,000 --> 0:04:56,000
o 1.000 millardos de gente en la tierra, guardados en discos duros o disquetes,

75
0:04:56.33,000 --> 0:04:58,000
o lo que sea que tengan en el futuro.

76
0:04:58.33,000 --> 0:05:02,000
Así que dejas salir a unos cuantos millones cada vez.

77
0:05:02.33,000 --> 0:05:06,000
Sale una persona, vive mil años

78
0:05:06.33,000 --> 0:05:12,000
haciendo lo que sea, y entonces, cuando hay que retroceder

79
0:05:12.33,000 --> 0:05:16,000
un millardo de años -o un millón, lo olvido, las cifras no importan-

80
0:05:16.33,000 --> 0:05:2,000
en realidad no hay mucha gente en la tierra a la vez.

81
0:05:20.33,000 --> 0:05:22,000
Puedes pensar en ti mismo y tus recuerdos,

82
0:05:22.33,000 --> 0:05:27,000
y antes de volver a estar en suspensión editas tus recuerdos

83
0:05:27.33,000 --> 0:05:3,000
y cambias tu personalidad, y así sucesivamente.

84
0:05:30.33,000 --> 0:05:36,000
La trama del libro es que no hay suficiente diversidad,

85
0:05:36.33,000 --> 0:05:39,000
así que la gente que diseñó la ciudad

86
0:05:39.33,000 --> 0:05:43,000
se asegura de que cada cierto tiempo se cree una persona nueva.

87
0:05:43.33,000 --> 0:05:49,000
En la novela, se crea una persona llamada Alvin, que dice:

88
0:05:49.33,000 --> 0:05:53,000
"tal vez esta no es la mejor manera", y estropea todo el sistema.

89
0:05:53.33,000 --> 0:05:55,000
No creo que las soluciones que propuse

90
0:05:55.33,000 --> 0:05:58,000
sean lo suficientemente buenas o inteligentes.

91
0:05:58.33,000 --> 0:06:02,000
Creo que el gran problema es que no somos lo suficientemente listos

92
0:06:02.33,000 --> 0:06:06,000
para entender cuáles de los problemas ante nosotros son lo suficientemente relevantes.

93
0:06:06.33,000 --> 0:06:1,000
Así que tenemos que construir máquinas sumamente inteligentes como HAL.

94
0:06:10.33,000 --> 0:06:15,000
Como recordarán, en un momento del libro para 2001,

95
0:06:15.33,000 --> 0:06:2,000
HAL se da cuenta de que el universo es demasiado grande, maravilloso y lleno de significado

96
0:06:20.33,000 --> 0:06:24,000
para unos astronautas tan estúpidos. Si comparan el comportamiento de HAL

97
0:06:24.33,000 --> 0:06:28,000
con la trivialidad de la gente en la nave,

98
0:06:28.33,000 --> 0:06:31,000
verán lo que está escrito entre líneas.

99
0:06:31.33,000 --> 0:06:34,000
Y sobre eso, ¿qué vamos a hacer? Podríamos ser más listos.

100
0:06:34.33,000 --> 0:06:39,000
Creo que somos bastante listos, comparados con los chimpancés.

101
0:06:39.33,000 --> 0:06:45,000
pero no lo bastante para lidiar con los colosales problemas ante nosotros,

102
0:06:45.33,000 --> 0:06:47,000
sea en matemáticas abstractas,

103
0:06:47.33,000 --> 0:06:52,000
en economía, o en equilibrar el mundo.

104
0:06:52.33,000 --> 0:06:55,000
Algo que podemos hacer es vivir más.

105
0:06:55.33,000 --> 0:06:57,000
Y nadie sabe lo difícil que es eso,

106
0:06:57.33,000 --> 0:07:,000
pero probablemente lo sabremos en unos años.

107
0:07:00.33,000 --> 0:07:03,000
La carretera se bifurca. Sabemos que la gente vive

108
0:07:03.33,000 --> 0:07:07,000
casi el doble que los chimpancés,

109
0:07:07.33,000 --> 0:07:11,000
y que nadie vive más de 120 años,

110
0:07:11.33,000 --> 0:07:14,000
por razones que no entendemos bien.

111
0:07:14.33,000 --> 0:07:17,000
Pero mucha gente vive 90 ó 100 años,

112
0:07:17.33,000 --> 0:07:21,000
a menos que den demasiado la mano o algo así.

113
0:07:21.33,000 --> 0:07:26,000
Así que tal vez si viviéramos 200 años, acumularíamos suficientes destrezas

114
0:07:26.33,000 --> 0:07:31,000
y conocimientos para solucionar algunos problemas.

115
0:07:31.33,000 --> 0:07:33,000
Esa es una forma de actuar.

116
0:07:33.33,000 --> 0:07:36,000
Y, como dije, no sabemos qué tan difícil es. Al fin y al cabo,

117
0:07:36.33,000 --> 0:07:42,000
la mayoría de los otros mamíferos viven la mitad que los chimpancés,

118
0:07:42.33,000 --> 0:07:45,000
así que vivimos tres veces y media o cuatro... vivimos cuatro veces más

119
0:07:45.33,000 --> 0:07:51,000
que la mayoría de los mamíferos. En el caso de los primates,

120
0:07:51.33,000 --> 0:07:55,000
tenemos casi los mismos genes. Lo que nos separa de los chimpancés

121
0:07:55.33,000 --> 0:08:01,000
es el estado actual del saber, que es un total disparate,

122
0:08:01.33,000 --> 0:08:03,000
tal vez unas centenas de genes.

123
0:08:03.33,000 --> 0:08:06,000
Creo que los contadores de genes aún no saben lo que están haciendo.

124
0:08:06.33,000 --> 0:08:09,000
Y hagan lo que hagan, no lean nada sobre genética

125
0:08:09.33,000 --> 0:08:12,000
que se publique mientras vivan.

126
0:08:12.33,000 --> 0:08:15,000
(Risas)

127
0:08:15.33,000 --> 0:08:19,000
Esas ideas tienen una esperanza de vida corta, al igual que las ciencias del cerebro.

128
0:08:19.33,000 --> 0:08:25,000
Así que tal vez si arreglamos cuatro o cinco genes,

129
0:08:25.33,000 --> 0:08:27,000
podremos vivir 200 años.

130
0:08:27.33,000 --> 0:08:3,000
O tal vez sólo 30 ó 40,

131
0:08:30.33,000 --> 0:08:32,000
dudo que varios centenares.

132
0:08:32.33,000 --> 0:08:36,000
Esto es algo que la gente discutirá

133
0:08:36.33,000 --> 0:08:39,000
y muchos éticos -un ético es alguien

134
0:08:39.33,000 --> 0:08:42,000
que encuentra algo malo en todo lo que piensas.

135
0:08:42.33,000 --> 0:08:45,000
(Risas)

136
0:08:45.33,000 --> 0:08:49,000
Es difícil encontrar un experto en ética que considere cualquier cambio

137
0:08:49.33,000 --> 0:08:53,000
digno de hacerse, porque dice, "¿y las consecuencias?"

138
0:08:53.33,000 --> 0:08:56,000
Y claro, no somos responsables de las consecuencias

139
0:08:56.33,000 --> 0:09:02,000
de lo que estamos haciendo ahora, ¿no? Como esta protesta sobre los clones.

140
0:09:02.33,000 --> 0:09:05,000
Y sin embargo dos personas al azar se aparearán y tendrán un hijo,

141
0:09:05.33,000 --> 0:09:09,000
y aunque ambos tienen genes bastante podridos,

142
0:09:09.33,000 --> 0:09:13,000
es probable que el niño salga normal.

143
0:09:13.33,000 --> 0:09:19,000
Lo cual, para estándares chimpancés, está pero que muy bien.

144
0:09:19.33,000 --> 0:09:22,000
Si ganamos en longevidad, tendremos que afrontar de todos modos el problema

145
0:09:22.33,000 --> 0:09:26,000
del crecimiento problacional porque si la gente vive 200 ó 1.000 años,

146
0:09:26.33,000 --> 0:09:32,000
no podemos dejar que tengan más de un hijo cada 200 ó 1.000 años.

147
0:09:32.33,000 --> 0:09:35,000
Así no habrá población activa.

148
0:09:35.33,000 --> 0:09:39,000
Una de las cosas que Laurie Garrett, y otros, han señalado

149
0:09:39.33,000 --> 0:09:44,000
es que una sociedad sin población activa

150
0:09:44.33,000 --> 0:09:47,000
es un problema grave. Y las cosas van a empeorar, porque

151
0:09:47.33,000 --> 0:09:53,000
no hay nadie para educar a los niños o alimentar a los ancianos.

152
0:09:53.33,000 --> 0:09:55,000
Y cuando hablo de vidas largas, claro,

153
0:09:55.33,000 --> 0:10:01,000
no quiero que alguien con 200 años tenga la imagen que tenemos

154
0:10:01.33,000 --> 0:10:05,000
de alguien con 200 años, es decir, muerto.

155
0:10:05.33,000 --> 0:10:07,000
Hay cerca de 400 partes diferentes en el cerebro

156
0:10:07.33,000 --> 0:10:09,000
que parecen tener funciones diferentes.

157
0:10:09.33,000 --> 0:10:12,000
Nadie sabe los detalles de cómo funcionan muchas,

158
0:10:12.33,000 --> 0:10:16,000
pero sabemos que ahí hay muchas cosas diferentes,

159
0:10:16.33,000 --> 0:10:18,000
y no siempre trabajan juntas. Me gusta la teoría de Freud

160
0:10:18.33,000 --> 0:10:22,000
de que la mayoría se anulan.

161
0:10:22.33,000 --> 0:10:26,000
Si piensas en ti mismo como en una ciudad

162
0:10:26.33,000 --> 0:10:32,000
con cien recursos, entonces, cuando tienes miedo, por ejemplo,

163
0:10:32.33,000 --> 0:10:36,000
tal vez descartes objetivos a largo plazo, pero puede que pienses en serio

164
0:10:36.33,000 --> 0:10:4,000
y te centres exactamente en cómo conseguir un objetivo concreto.

165
0:10:40.33,000 --> 0:10:43,000
Dejas todo lo demás de lado, te conviertes en un mononaníaco -

166
0:10:43.33,000 --> 0:10:47,000
lo único que te preocupa es no salirte de esa plataforma.

167
0:10:47.33,000 --> 0:10:51,000
Y cuando tienes hambre, la comida se hace más apetecible y así sucesivamente.

168
0:10:51.33,000 --> 0:10:57,000
Veo las emociones como subgrupos muy evolucionados de la capacidad de ustedes.

169
0:10:57.33,000 --> 0:11:01,000
La emoción no es algo que se añade al pensamiento. Un estado emocional

170
0:11:01.33,000 --> 0:11:05,000
es lo que te queda cuando quitas 100 ó 200

171
0:11:05.33,000 --> 0:11:08,000
de tus recursos disponibles habitualmente.

172
0:11:08.33,000 --> 0:11:11,000
Pensar en las emociones como algo opuesto, como algo

173
0:11:11.33,000 --> 0:11:15,000
menos que el pensamiento es muy productivo, y espero,

174
0:11:15.33,000 --> 0:11:19,000
en los próximos años, que esto nos lleve a máquinas inteligentes.

175
0:11:19.33,000 --> 0:11:22,000
Supongo que lo mejor es que me salte el resto, son detalles sobre

176
0:11:22.33,000 --> 0:11:27,000
cómo hacer esas máquinas inteligentes -

177
0:11:27.33,000 --> 0:11:32,000
(Risas)

178
0:11:32.33,000 --> 0:11:37,000
- la idea principal es que de hecho el corazón de una máquina inteligente

179
0:11:37.33,000 --> 0:11:42,000
es una máquina que reconoce cuándo te estás enfrentando a algún problema:

180
0:11:42.33,000 --> 0:11:45,000
«Este es un problema de tal o cual tipo».

181
0:11:45.33,000 --> 0:11:5,000
Consecuentemente, hay ciertas maneras de pensar

182
0:11:50.33,000 --> 0:11:52,000
que son buenas para ese problema.

183
0:11:52.33,000 --> 0:11:56,000
Creo que el problema más importante para la psicología futura es clasificar

184
0:11:56.33,000 --> 0:12:,000
tipos de problemas, de situaciones, de obstáculos

185
0:12:00.33,000 --> 0:12:06,000
y también clasificar maneras de pensar disponibles y emparejarlos.

186
0:12:06.33,000 --> 0:12:09,000
Así que ya ven, es casi como de Pavlov -

187
0:12:09.33,000 --> 0:12:11,000
perdimos los primeros cien años de psicología

188
0:12:11.33,000 --> 0:12:14,000
en teorías realmente triviales que hablan de

189
0:12:14.33,000 --> 0:12:2,000
cómo la gente aprende a reaccionar ante una situación. Lo que digo es,

190
0:12:20.33,000 --> 0:12:25,000
tras pasar por muchos niveles, incluyendo el diseño de un sistema

191
0:12:25.33,000 --> 0:12:28,000
enorme y desordenado con miles de partes,

192
0:12:28.33,000 --> 0:12:32,000
terminaremos otra vez en el problema central de la psicología.

193
0:12:32.33,000 --> 0:12:35,000
No nos preguntaremos: ¿cúales son las situaciones?,

194
0:12:35.33,000 --> 0:12:37,000
sino: ¿cúales son los tipos de problemas?

195
0:12:37.33,000 --> 0:12:4,000
¿Cúales son los tipos de estrategias? ¿Cómo se aprenden?

196
0:12:40.33,000 --> 0:12:43,000
¿Cómo se conectan? ¿Cómo inventa una persona muy creativa

197
0:12:43.33,000 --> 0:12:48,000
una forma nueva de pensar a partir de los recursos disponibles? Y así sucesivamente.

198
0:12:48.33,000 --> 0:12:5,000
Creo que en los próximos 20 años,

199
0:12:50.33,000 --> 0:12:55,000
si nos podemos librar de los acercamientos tradicionales a la inteligencia artificial,

200
0:12:55.33,000 --> 0:12:57,000
como redes neuronales, algoritmos genéticos

201
0:12:57.33,000 --> 0:13:03,000
y sistemas expertos, tendremos las miras más altas y nos preguntaremos

202
0:13:03.33,000 --> 0:13:05,000
si podemos crear un sistema que pueda usar esas cosas para

203
0:13:05.33,000 --> 0:13:09,000
el problema adecuado. Algunos problemas son buenos para redes neuronales;

204
0:13:09.33,000 --> 0:13:12,000
sabemos que para otros las redes neurolanes son inútiles.

205
0:13:12.33,000 --> 0:13:15,000
los algoritmos genéticos son estupendos para ciertas cosas;

206
0:13:15.33,000 --> 0:13:19,000
sospecho saber para qué son malos y no se lo diré.

207
0:13:19.33,000 --> 0:13:2,000
(Risas)

208
0:13:20.33,000 --> 0:13:22,000
Gracias.

209
0:13:22.33,000 --> 0:13:28,000
(Aplausos)

