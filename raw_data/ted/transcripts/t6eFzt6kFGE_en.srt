1
0:00:12.76,000 --> 0:00:13,000
It's 1996

2
0:00:14.64,000 --> 0:00:16,000
in Uvira in eastern Congo.

3
0:00:16.68,000 --> 0:00:17,000
This is Bukeni.

4
0:00:18.04,000 --> 0:00:2,000
Militia commanders walk into his village,

5
0:00:20.76,000 --> 0:00:21,000
knock on his neighbors' doors

6
0:00:22.56,000 --> 0:00:25,000
and whisk their children away to training camps.

7
0:00:26.32,000 --> 0:00:3,000
Bukeni borrows a video camera from a local wedding photographer,

8
0:00:30.6,000 --> 0:00:31,000
he disguises as a journalist

9
0:00:32.4,000 --> 0:00:35,000
and he walks into the camps to negotiate the release of the children.

10
0:00:36.56,000 --> 0:00:39,000
He filmed footage of the children being trained as soldiers.

11
0:00:40.568,000 --> 0:00:42,000
[Soldiers don't worry!]

12
0:00:43.202,000 --> 0:00:44,000
[You'll wear uniforms!]

13
0:00:45.22,000 --> 0:00:46,000
[You'll have free cars!]

14
0:00:46.92,000 --> 0:00:46,000
[Free beans!]

15
0:00:47.68,000 --> 0:00:5,000
Many of these children are under 15 years old,

16
0:00:51.24,000 --> 0:00:52,000
and that is a war crime.

17
0:00:53.27,000 --> 0:00:54,000
[Free!]

18
0:00:55.28,000 --> 0:00:58,000
But you don't have to go to eastern Congo to find human rights abuses.

19
0:00:59.76,000 --> 0:01:02,000
In America, a country with a rapidly aging population,

20
0:01:03.56,000 --> 0:01:07,000
experts estimate that one in 10 people over 60

21
0:01:07.96,000 --> 0:01:09,000
will experience abuse.

22
0:01:10.64,000 --> 0:01:12,000
It's a hidden epidemic,

23
0:01:13,000 --> 0:01:15,000
and most of that abuse actually happens at the hands

24
0:01:15.8,000 --> 0:01:17,000
of close caretakers or family.

25
0:01:19.2,000 --> 0:01:2,000
This is Vicky.

26
0:01:20.56,000 --> 0:01:24,000
Vicky put an iron gate on her bedroom door

27
0:01:25.04,000 --> 0:01:28,000
and she became a prisoner, in fact, in her own house,

28
0:01:28.16,000 --> 0:01:32,000
out of fear for her nephew who had taken over her home as a drug den.

29
0:01:33.76,000 --> 0:01:34,000
And this is Mary.

30
0:01:35.08,000 --> 0:01:38,000
Mary picked up a video camera for the first time in her life

31
0:01:38.24,000 --> 0:01:39,000
when she was 65 years old,

32
0:01:40.2,000 --> 0:01:44,000
and she asked Vicky and 99 other older people

33
0:01:44.64,000 --> 0:01:47,000
who had experienced abuse to tell their stories on camera.

34
0:01:50.16,000 --> 0:01:51,000
And I am Dutch,

35
0:01:52.04,000 --> 0:01:54,000
so in the Netherlands we are obsessed with the truth.

36
0:01:55.04,000 --> 0:01:57,000
Now, when you are a child, that's a great thing,

37
0:01:57.52,000 --> 0:01:59,000
because you can basically get away with anything,

38
0:02:00.16,000 --> 0:02:03,000
like "Yes, Mama, it was me who smoked the cigars."

39
0:02:03.48,000 --> 0:02:04,000
(Laughter)

40
0:02:05.92,000 --> 0:02:08,000
But I think this is why I have dedicated my life

41
0:02:09.92,000 --> 0:02:13,000
to promoting citizen video to expose human rights violations,

42
0:02:14.2,000 --> 0:02:18,000
because I believe in the power of video to create undeniable truths.

43
0:02:19.4,000 --> 0:02:2,000
And my organization, WITNESS,

44
0:02:21.12,000 --> 0:02:23,000
helped use the Congolese videos

45
0:02:23.88,000 --> 0:02:29,000
to help convict and send a notorious warlord called Thomas Lubanga to jail.

46
0:02:31.44,000 --> 0:02:33,000
And the videos that Mary shot,

47
0:02:33.92,000 --> 0:02:35,000
we trained Mary and many other elder justice advocates,

48
0:02:36.92,000 --> 0:02:38,000
to make sure that the stories of elder abuse

49
0:02:39.76,000 --> 0:02:4,000
reached lawmakers,

50
0:02:41.4,000 --> 0:02:44,000
and those stories helped convince lawmakers

51
0:02:45.28,000 --> 0:02:48,000
to pass landmark legislation to protect older Americans.

52
0:02:50.28,000 --> 0:02:51,000
So I wonder,

53
0:02:52.24,000 --> 0:02:56,000
billions of us now have this powerful tool right at our fingertips.

54
0:02:56.92,000 --> 0:02:57,000
It's a camera.

55
0:02:58.44,000 --> 0:03:02,000
So why are all of us not a more powerful army of civic witnesses,

56
0:03:03.28,000 --> 0:03:05,000
like Mary and Bukeni?

57
0:03:05.36,000 --> 0:03:07,000
Why is it that so much more video

58
0:03:08.24,000 --> 0:03:11,000
is not leading to more rights and more justice?

59
0:03:13,000 --> 0:03:17,000
And I think it is because being an eyewitness is hard.

60
0:03:17.56,000 --> 0:03:19,000
Your story will get denied,

61
0:03:20.04,000 --> 0:03:22,000
your video will get lost in a sea of images,

62
0:03:22.92,000 --> 0:03:25,000
your story will not be trusted, and you will be targeted.

63
0:03:27.92,000 --> 0:03:29,000
So how do we help witnesses?

64
0:03:31.56,000 --> 0:03:32,000
In Oaxaca, in Mexico,

65
0:03:33.28,000 --> 0:03:35,000
the teachers' movement organized a protest

66
0:03:35.88,000 --> 0:03:38,000
after the president pushed down very undemocratic reforms.

67
0:03:40.16,000 --> 0:03:43,000
The federal police came down in buses and started shooting at the protesters.

68
0:03:44.64,000 --> 0:03:47,000
At least seven people died and many, many more were wounded.

69
0:03:48.28,000 --> 0:03:51,000
Images started circulating of the shootings,

70
0:03:51.68,000 --> 0:03:53,000
and the Mexican government did what it always does.

71
0:03:54.12,000 --> 0:03:55,000
It issued a formal statement,

72
0:03:55.56,000 --> 0:03:58,000
and the statement basically accused the independent media

73
0:03:58.96,000 --> 0:04:,000
of creating fake news.

74
0:04:01.04,000 --> 0:04:02,000
It said, "We were not there,

75
0:04:02.84,000 --> 0:04:04,000
that was not us doing the shooting,

76
0:04:05.6,000 --> 0:04:06,000
this did not happen."

77
0:04:08.76,000 --> 0:04:11,000
But we had just trained activists in Mexico

78
0:04:12,000 --> 0:04:15,000
to use metadata strategically with their images.

79
0:04:15.8,000 --> 0:04:18,000
Now, metadata is the kind of information that your camera captures

80
0:04:19.56,000 --> 0:04:22,000
that shows the date, the location,

81
0:04:22.6,000 --> 0:04:23,000
the temperature, the weather.

82
0:04:24.36,000 --> 0:04:27,000
It can even show the very unique way you hold your camera

83
0:04:27.8,000 --> 0:04:28,000
when you capture something.

84
0:04:29.56,000 --> 0:04:31,000
So the images started recirculating,

85
0:04:31.68,000 --> 0:04:34,000
and this time with the very verifying,

86
0:04:34.8,000 --> 0:04:36,000
validating information on top of them.

87
0:04:37.96,000 --> 0:04:39,000
And the federal government had to retract their statement.

88
0:04:41.52,000 --> 0:04:44,000
Now, justice for the people for Oaxaca

89
0:04:45.12,000 --> 0:04:46,000
is still far off,

90
0:04:46.56,000 --> 0:04:49,000
but their stories, their truths, can no longer be denied.

91
0:04:51.24,000 --> 0:04:52,000
So we started thinking:

92
0:04:53.04,000 --> 0:04:54,000
What if you had "Proof Mode?"

93
0:04:54.76,000 --> 0:04:56,000
What if everybody had a camera in their hands

94
0:04:56.92,000 --> 0:04:59,000
and all the platforms had that kind of validating ability.

95
0:05:00.44,000 --> 0:05:01,000
So we developed --

96
0:05:01.72,000 --> 0:05:05,000
together with amazing Android developers called the Guardian Project,

97
0:05:06.2,000 --> 0:05:09,000
we developed something called a technology that's called Proof Mode,

98
0:05:09.52,000 --> 0:05:12,000
that marries those metadata together with your image,

99
0:05:12.72,000 --> 0:05:14,000
and it validates and it verifies your video.

100
0:05:16.52,000 --> 0:05:2,000
Now, imagine there is a deluge of images

101
0:05:20.56,000 --> 0:05:22,000
coming from the world's camera phones.

102
0:05:23.64,000 --> 0:05:27,000
Imagine if that information could be trusted just a little bit more,

103
0:05:28.52,000 --> 0:05:3,000
what the potential would be for journalists,

104
0:05:30.88,000 --> 0:05:31,000
for human rights investigators,

105
0:05:32.72,000 --> 0:05:33,000
for human rights lawyers.

106
0:05:35.44,000 --> 0:05:38,000
So we started sharing Proof Mode with our partners in Brazil

107
0:05:39.04,000 --> 0:05:42,000
who are an amazing media collective called Coletivo Papo Reto.

108
0:05:44.28,000 --> 0:05:46,000
Brazil is a tough place for human rights.

109
0:05:47.68,000 --> 0:05:5,000
The Brazilian police kills thousands of people every year.

110
0:05:52.4,000 --> 0:05:54,000
The only time that there's an investigation,

111
0:05:55.92,000 --> 0:05:56,000
guess when?

112
0:05:57.44,000 --> 0:05:58,000
When there's video.

113
0:06:00.24,000 --> 0:06:03,000
Seventeen-year-old Eduardo was killed in broad daylight

114
0:06:04.64,000 --> 0:06:05,000
by the Rio police,

115
0:06:06.24,000 --> 0:06:08,000
and look what happens after they kill him.

116
0:06:10.32,000 --> 0:06:12,000
They put a gun in the dead boy's hand,

117
0:06:12.88,000 --> 0:06:13,000
they shoot the gun twice --

118
0:06:15.84,000 --> 0:06:16,000
(Shot)

119
0:06:17.6,000 --> 0:06:21,000
to fabricate their story of self-defense.

120
0:06:22.44,000 --> 0:06:25,000
The woman who filmed this was a very, very courageous eyewitness,

121
0:06:25.92,000 --> 0:06:28,000
and she had to go into hiding after she posted her video

122
0:06:29.32,000 --> 0:06:3,000
for fear of her life.

123
0:06:31.24,000 --> 0:06:34,000
But people are filming, and they're not going to stop filming,

124
0:06:34.72,000 --> 0:06:37,000
so we're now working together with media collectives

125
0:06:37.76,000 --> 0:06:39,000
so the residents on their WhatsApp

126
0:06:40.28,000 --> 0:06:42,000
frequently get guidance and tips,

127
0:06:43.04,000 --> 0:06:44,000
how to film safely,

128
0:06:44.8,000 --> 0:06:46,000
how to upload the video that you shoot safely,

129
0:06:47.76,000 --> 0:06:5,000
how to capture a scene so that it can actually count as evidence.

130
0:06:52.36,000 --> 0:06:54,000
And here is an inspiration

131
0:06:54.48,000 --> 0:06:56,000
from a group called Mídia Ninja in Brazil.

132
0:06:58.48,000 --> 0:07:01,000
The man on left is a heavily armed military policeman.

133
0:07:04,000 --> 0:07:05,000
He walks up to a protester --

134
0:07:05.52,000 --> 0:07:08,000
when you protest in Brazil, you can be arrested or worse --

135
0:07:08.76,000 --> 0:07:1,000
and he says to the protester, "Watch me,

136
0:07:11.36,000 --> 0:07:13,000
I am going to search you right now."

137
0:07:15.08,000 --> 0:07:18,000
And the protester is a live-streaming activist --

138
0:07:18.84,000 --> 0:07:19,000
he wears a little camera --

139
0:07:20.32,000 --> 0:07:23,000
and he says to the military policeman, he says, "I am watching you,

140
0:07:24.28,000 --> 0:07:27,000
and there are 5,000 people watching you with me."

141
0:07:28.4,000 --> 0:07:3,000
Now, the tables are turned.

142
0:07:30.88,000 --> 0:07:33,000
The distant witnesses, the watching audience, they matter.

143
0:07:35.24,000 --> 0:07:36,000
So we started thinking,

144
0:07:36.92,000 --> 0:07:38,000
what if you could tap into that power,

145
0:07:39.52,000 --> 0:07:41,000
the power of distant witnesses?

146
0:07:41.72,000 --> 0:07:43,000
What if you could pull in their expertise, their leverage,

147
0:07:44.48,000 --> 0:07:46,000
their solidarity, their skills

148
0:07:46.64,000 --> 0:07:49,000
when a frontline community needs them to be there?

149
0:07:50.96,000 --> 0:07:55,000
And we started developing a project that's called Mobilize Us,

150
0:07:56.32,000 --> 0:07:59,000
because many of us, I would assume,

151
0:07:59.56,000 --> 0:08:,000
want to help

152
0:08:01.24,000 --> 0:08:03,000
and lend our skills and our expertise,

153
0:08:03.88,000 --> 0:08:05,000
but we are often not there when a frontline community

154
0:08:06.76,000 --> 0:08:08,000
or a single individual faces an abuse.

155
0:08:10.76,000 --> 0:08:13,000
And it could be as simple as this little app that we created

156
0:08:14.44,000 --> 0:08:17,000
that just shows the perpetrator on the other side of the phone

157
0:08:17.52,000 --> 0:08:19,000
how many people are watching him.

158
0:08:20.88,000 --> 0:08:24,000
But now, imagine that you could put a layer of computer task routing

159
0:08:25.24,000 --> 0:08:26,000
on top of that.

160
0:08:26.68,000 --> 0:08:3,000
Imagine that you're a community facing an immigration raid,

161
0:08:31.52,000 --> 0:08:34,000
and at that very moment, at that right moment, via livestream,

162
0:08:35.28,000 --> 0:08:37,000
you could pull in a hundred legal observers.

163
0:08:38.4,000 --> 0:08:39,000
How would that change the situation?

164
0:08:41.32,000 --> 0:08:44,000
So we started piloting this with our partner communities in Brazil.

165
0:08:45.039,000 --> 0:08:47,000
This is a woman called Camilla,

166
0:08:47.2,000 --> 0:08:5,000
and she was able -- she's the leader in a favela called Favela Skol --

167
0:08:51.919,000 --> 0:08:55,000
she was able to pull in distant witnesses

168
0:08:56.08,000 --> 0:08:58,000
via livestream

169
0:08:58.16,000 --> 0:08:59,000
to help translation,

170
0:08:59.92,000 --> 0:09:,000
to help distribution,

171
0:09:01.56,000 --> 0:09:03,000
to help amplify her story

172
0:09:04.36,000 --> 0:09:06,000
after her community was forcibly evicted

173
0:09:07.08,000 --> 0:09:1,000
to make room for a very glossy Olympic event last summer.

174
0:09:12.92,000 --> 0:09:14,000
So we're talking about good witnessing,

175
0:09:15.92,000 --> 0:09:18,000
but what happens if the perpetrators are filming?

176
0:09:19.12,000 --> 0:09:22,000
What happens if a bystander films and doesn't do anything?

177
0:09:23.68,000 --> 0:09:25,000
This is the story of Chrissy.

178
0:09:25.92,000 --> 0:09:27,000
Chrissy is a transgender woman

179
0:09:28.28,000 --> 0:09:3,000
who walked into a McDonald's in Maryland

180
0:09:30.92,000 --> 0:09:31,000
to use the women's bathroom.

181
0:09:32.96,000 --> 0:09:37,000
Two teens viciously beat her for using that woman's bathroom,

182
0:09:38.4,000 --> 0:09:41,000
and the McDonald's employee filmed this on his mobile phone.

183
0:09:42.76,000 --> 0:09:43,000
And he posted his video,

184
0:09:44.92,000 --> 0:09:45,000
and it has garnered

185
0:09:46.64,000 --> 0:09:49,000
thousands of racist and transphobic comments.

186
0:09:52.92,000 --> 0:09:54,000
So we started a project that's called Capturing Hate.

187
0:09:56.84,000 --> 0:09:59,000
We took a very, very small sample of eyewitness videos

188
0:10:00.8,000 --> 0:10:05,000
that showed abuse against transgender and gender-nonconforming people.

189
0:10:06.2,000 --> 0:10:1,000
We searched two words, "tranny fight" and "stud fight."

190
0:10:11.24,000 --> 0:10:16,000
And those 329 videos were watched and are still being watched

191
0:10:16.32,000 --> 0:10:18,000
as we sit here in this theater,

192
0:10:18.64,000 --> 0:10:21,000
a stunning almost 90 million times,

193
0:10:22.2,000 --> 0:10:25,000
and there are hundreds of thousands of comments with these videos,

194
0:10:25.44,000 --> 0:10:27,000
egging on to more violence and more hate.

195
0:10:30.24,000 --> 0:10:32,000
So we started developing a methodology

196
0:10:32.68,000 --> 0:10:36,000
that took all that unquantified visual evidence

197
0:10:37,000 --> 0:10:41,000
and turned it into data, turning video into data,

198
0:10:41.44,000 --> 0:10:42,000
and with that tool,

199
0:10:42.72,000 --> 0:10:46,000
LGBT organizations are now using that data

200
0:10:47.16,000 --> 0:10:48,000
to fight for rights.

201
0:10:49.4,000 --> 0:10:52,000
And we take that data and we take it back to Silicon Valley,

202
0:10:52.64,000 --> 0:10:53,000
and we say to them:

203
0:10:54.16,000 --> 0:10:55,000
"How is it possible

204
0:10:56.6,000 --> 0:10:59,000
that these videos are still out there

205
0:11:00.6,000 --> 0:11:01,000
in a climate of hate

206
0:11:02.52,000 --> 0:11:03,000
egging on more hate,

207
0:11:04.4,000 --> 0:11:06,000
summoning more violence,

208
0:11:06.44,000 --> 0:11:08,000
when you have policies that actually say

209
0:11:08.88,000 --> 0:11:11,000
you do not allow this kind of content? --

210
0:11:12.12,000 --> 0:11:14,000
urging them to change their policies.

211
0:11:16.48,000 --> 0:11:18,000
So I have hope.

212
0:11:19.16,000 --> 0:11:23,000
I have hope that we can turn more video into more rights and more justice.

213
0:11:24.36,000 --> 0:11:29,000
Ten billion video views on Snapchat,

214
0:11:30.76,000 --> 0:11:31,000
per day.

215
0:11:32.32,000 --> 0:11:35,000
So what if we could turn that Snapchat generation

216
0:11:35.64,000 --> 0:11:38,000
into effective and safe civic witnesses?

217
0:11:38.88,000 --> 0:11:41,000
What if they could become the Bukenis of this new generation?

218
0:11:44.92,000 --> 0:11:48,000
In India, women have already started using Snapchat filters

219
0:11:48.96,000 --> 0:11:51,000
to protect their identity when they speak out about domestic violence.

220
0:11:52.86,000 --> 0:11:54,000
[They tortured me at home and never let me go out.]

221
0:11:55.84,000 --> 0:11:58,000
The truth is, the real truth, the truth that doesn't fit into any TED Talk,

222
0:11:59.44,000 --> 0:12:01,000
is fighting human rights abuse is hard.

223
0:12:02.32,000 --> 0:12:05,000
There are no easy solutions for human rights abuse.

224
0:12:05.4,000 --> 0:12:07,000
And there's not a single piece of technology

225
0:12:08.16,000 --> 0:12:09,000
that can ever stop the perpetrators.

226
0:12:11.4,000 --> 0:12:12,000
But for the survivors,

227
0:12:13.92,000 --> 0:12:14,000
for the victims,

228
0:12:15.56,000 --> 0:12:16,000
for the marginalized communities,

229
0:12:18.16,000 --> 0:12:22,000
their stories, their truths, matter.

230
0:12:22.64,000 --> 0:12:24,000
And that is where justice begins.

231
0:12:25.48,000 --> 0:12:26,000
Thank you.

232
0:12:26.72,000 --> 0:12:28,000
(Applause)

