1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Emmanuel Parfond

2
0:00:12.76,000 --> 0:00:15,000
L’anxiété au sujet de l’automatisation s’est récemment répandue,

3
0:00:16.16,000 --> 0:00:18,000
une peur qu’à l’avenir

4
0:00:18.84,000 --> 0:00:2,000
de nombreuses tâches soient faites par des machines

5
0:00:21.336,000 --> 0:00:22,000
plutôt que des êtres humains,

6
0:00:22.716,000 --> 0:00:24,000
étant données les remarquables avancées qui se produisent

7
0:00:25.64,000 --> 0:00:27,000
en intelligence artificielle et en robotique.

8
0:00:28.44,000 --> 0:00:3,000
Ce qui est clair, c’est qu’il y aura d’importants changements.

9
0:00:31.346,000 --> 0:00:34,000
Ce qui est moins clair, c’est à quoi ces changements ressembleront.

10
0:00:34.92,000 --> 0:00:38,000
Mes recherches suggèrent que l’avenir est à la fois inquiétant et prometteur.

11
0:00:39.88,000 --> 0:00:42,000
La menace du chômage technologique est réelle,

12
0:00:43.64,000 --> 0:00:45,000
mais c’est un bon problème à avoir.

13
0:00:45.72,000 --> 0:00:48,000
Pour expliquer comment j’en suis venu à cette conclusion,

14
0:00:48.96,000 --> 0:00:5,000
je veux confronter trois mythes

15
0:00:51.52,000 --> 0:00:55,000
qui, à mon avis, obscurcissent notre vision de cet avenir automatisé.

16
0:00:56.88,000 --> 0:00:58,000
Une image que nous voyons sur nos écrans de télé,

17
0:00:59.24,000 --> 0:01:01,000
dans les livres, les films, les commentaires

18
0:01:01.476,000 --> 0:01:04,000
est celle d’une armée de robots envahissant le monde du travail

19
0:01:05.2,000 --> 0:01:06,000
avec un objectif en tête :

20
0:01:06.6,000 --> 0:01:08,000
remplacer les êtres humains dans leur travail.

21
0:01:09.12,000 --> 0:01:11,000
J’appelle cela le mythe de Terminator.

22
0:01:11.84,000 --> 0:01:14,000
Oui, les machines remplacent les humains dans des tâches particulières,

23
0:01:15.84,000 --> 0:01:17,000
mais elles ne remplacent pas les êtres humains.

24
0:01:18.12,000 --> 0:01:2,000
Elles les complètent dans d’autres tâches,

25
0:01:20.126,000 --> 0:01:23,000
augmentant la valeur et l’importance de ce travail.

26
0:01:23.76,000 --> 0:01:26,000
Parfois, elles complètent les êtres humains directement,

27
0:01:27.12,000 --> 0:01:31,000
en les rendant plus productifs, plus efficaces pour une tâche en particulier.

28
0:01:31.16,000 --> 0:01:34,000
Un chauffeur de taxi peut utiliser le système de navigation satellite

29
0:01:34.43,000 --> 0:01:35,000
sur des routes inconnues.

30
0:01:35.8,000 --> 0:01:38,000
Un architecte peut utiliser un logiciel de conception assistée

31
0:01:39.16,000 --> 0:01:42,000
pour concevoir des bâtiments plus grands et plus compliqués.

32
0:01:42.23,000 --> 0:01:44,000
Le progrès technologique ne complémente pas toujours

33
0:01:44.67,000 --> 0:01:45,000
les humains de façon directe.

34
0:01:46.07,000 --> 0:01:49,000
Il les complémente indirectement et le fait de deux façons.

35
0:01:49.36,000 --> 0:01:52,000
Si nous voyons l’économie comme un gâteau,

36
0:01:52.72,000 --> 0:01:54,000
le progrès technologique agrandit le gâteau.

37
0:01:55.64,000 --> 0:01:58,000
Quand la productivité augmente, les revenus et la demande augmentent.

38
0:01:59.52,000 --> 0:02:,000
Le gâteau britannique, par exemple,

39
0:02:01.346,000 --> 0:02:04,000
est plus de 100 fois plus gros qu’il y a 300 ans.

40
0:02:05.92,000 --> 0:02:08,000
Les gens évincés de tâches dans l’ancien gâteau

41
0:02:09.16,000 --> 0:02:11,000
peuvent trouver des tâches dans le nouveau gâteau.

42
0:02:12.8,000 --> 0:02:15,000
Mais le progrès technologique ne fait pas qu’agrandir le gâteau.

43
0:02:16.76,000 --> 0:02:18,000
Il en change aussi les ingrédients.

44
0:02:19.64,000 --> 0:02:22,000
Le temps passant, les gens dépensent leurs revenus différemment,

45
0:02:23.12,000 --> 0:02:25,000
changeant leur dispersion parmi les produits existants

46
0:02:25.96,000 --> 0:02:28,000
et développant des goûts pour de nouveaux produits.

47
0:02:29.2,000 --> 0:02:3,000
De nouvelles industries sont créées,

48
0:02:31,000 --> 0:02:32,000
de nouvelles tâches doivent être réalisées

49
0:02:32.956,000 --> 0:02:34,000
et souvent, de nouveaux rôles doivent être remplis.

50
0:02:35.4,000 --> 0:02:36,000
Le gâteau britannique :

51
0:02:36.92,000 --> 0:02:38,000
il y a 300 ans, les gens travaillaient surtout dans des fermes ;

52
0:02:39.92,000 --> 0:02:41,000
il y a 150 ans, dans des usines ;

53
0:02:42.28,000 --> 0:02:44,000
aujourd’hui, la plupart des gens travaillent dans des bureaux.

54
0:02:45.196,000 --> 0:02:49,000
Les gens évincés de tâches dans l’ancien gâteau

55
0:02:49.24,000 --> 0:02:51,000
pouvaient tomber sur des tâches dans le nouveau.

56
0:02:52.72,000 --> 0:02:55,000
Les économistes appellent ces effets des complémentarités,

57
0:02:56.08,000 --> 0:02:59,000
mais ce n’est qu’un mot raffiné pour capturer la façon différente

58
0:02:59.36,000 --> 0:03:02,000
dont le progrès technologique aide les êtres humains.

59
0:03:02.52,000 --> 0:03:04,000
Résoudre ce mythe de Terminator

60
0:03:04.64,000 --> 0:03:06,000
nous montre qu’il y a deux forces en jeu :

61
0:03:07,000 --> 0:03:1,000
la substitution par les machines qui nuit aux travailleurs,

62
0:03:10.56,000 --> 0:03:12,000
mais aussi ces complémentarités qui font le contraire.

63
0:03:13.96,000 --> 0:03:14,000
Le second mythe

64
0:03:15.36,000 --> 0:03:17,000
que j’appelle le mythe de l’intelligence.

65
0:03:18.44,000 --> 0:03:21,000
Qu’ont en commun les tâches de conduite d’une voiture,

66
0:03:21.74,000 --> 0:03:25,000
d’établissement d’un diagnostic et d’identification éclair d’un oiseau ?

67
0:03:27.28,000 --> 0:03:29,000
Ce sont des tâches que, jusqu’à très récemment,

68
0:03:30.28,000 --> 0:03:33,000
d’éminents économiques pensaient ne pas être facilement automatisables.

69
0:03:33.64,000 --> 0:03:36,000
Pourtant, aujourd’hui, toutes ces tâches peuvent être automatisées.

70
0:03:36.84,000 --> 0:03:39,000
Les grands constructeurs automobiles ont des programmes de voitures autonomes.

71
0:03:40.506,000 --> 0:03:43,000
Il y a d’innombrables systèmes pouvant diagnostiquer des problèmes médicaux.

72
0:03:44.36,000 --> 0:03:46,000
Il y a même une application qui peut identifier un oiseau

73
0:03:47.026,000 --> 0:03:47,000
en un éclair.

74
0:03:48.92,000 --> 0:03:52,000
Ce n’était pas de la malchance de la part des économistes.

75
0:03:53.32,000 --> 0:03:54,000
Ils avaient tort

76
0:03:54.64,000 --> 0:03:56,000
et la raison à cela est très importante.

77
0:03:57.16,000 --> 0:03:59,000
Ils ont cru au mythe de l’intelligence,

78
0:03:59.44,000 --> 0:04:01,000
la croyance selon laquelle les machines doivent copier la façon

79
0:04:02.396,000 --> 0:04:04,000
de penser et de raisonner des êtres humains

80
0:04:04.44,000 --> 0:04:05,000
afin de les surpasser.

81
0:04:06.24,000 --> 0:04:08,000
Quand ces économistes essayaient de déterminer

82
0:04:08.39,000 --> 0:04:1,000
quelle tâche les machines ne savaient pas faire,

83
0:04:10.644,000 --> 0:04:12,000
ils pensaient que pour automatiser une tâche,

84
0:04:12.743,000 --> 0:04:13,000
il fallait prendre un être humain,

85
0:04:14.384,000 --> 0:04:17,000
qu’il vous explique comment il effectuait une tâche

86
0:04:17.92,000 --> 0:04:19,000
puis essayer de refléter cette explication

87
0:04:20.6,000 --> 0:04:22,000
dans un jeu d’instructions qu’une machine suivrait.

88
0:04:23.4,000 --> 0:04:27,000
Cette vision a aussi été populaire en intelligence artificielle.

89
0:04:27.6,000 --> 0:04:29,000
Je le sais car Richard Susskind,

90
0:04:29.8,000 --> 0:04:31,000
qui est mon père et mon coauteur,

91
0:04:32.67,000 --> 0:04:33,000
a eu son doctorat dans les années 80

92
0:04:34.39,000 --> 0:04:36,000
sur le sujet de l’intelligence artificielle et la loi

93
0:04:36.88,000 --> 0:04:37,000
à l’université d’Oxford

94
0:04:38.2,000 --> 0:04:39,000
et était un avant-gardiste.

95
0:04:39.8,000 --> 0:04:41,000
Avec un professeur, Phillip Capper,

96
0:04:42.08,000 --> 0:04:44,000
et un éditeur juridique, Butterworths,

97
0:04:44.2,000 --> 0:04:49,000
ils ont créé la première intelligence artificielle en droit

98
0:04:50.12,000 --> 0:04:52,000
disponible sur le marché.

99
0:04:52.92,000 --> 0:04:54,000
C’était le design de la page d’accueil.

100
0:04:55.56,000 --> 0:04:57,000
Il m’a assuré que c’était un design cool à l’époque.

101
0:04:58.28,000 --> 0:04:59,000
(Rires)

102
0:04:59.32,000 --> 0:05:,000
Il ne m'a jamais convaincu.

103
0:05:01.04,000 --> 0:05:03,000
Il l’a publié sous la forme de deux disquettes,

104
0:05:03.68,000 --> 0:05:06,000
à l’époque où les disquettes étaient molles,

105
0:05:07.24,000 --> 0:05:09,000
et il avait la même approche que les économistes :

106
0:05:09.6,000 --> 0:05:1,000
prenez un avocat,

107
0:05:10.88,000 --> 0:05:13,000
demandez-lui de vous expliquer comment résoudre un problème légal

108
0:05:14.08,000 --> 0:05:19,000
et essayez de refléter cette explication dans un jeu de règles que la machine suit.

109
0:05:19.48,000 --> 0:05:22,000
En économie, si les êtres humains savent s’expliquer ainsi,

110
0:05:23.12,000 --> 0:05:26,000
les tâches sont appelées des routines et peuvent être automatisées.

111
0:05:26.44,000 --> 0:05:28,000
Mais si les êtres humains ne savent pas s’expliquer,

112
0:05:28.916,000 --> 0:05:32,000
les tâches sont des non routines et sont vues comme hors d’atteinte.

113
0:05:33.02,000 --> 0:05:36,000
Aujourd’hui, cette distinction entre routine et non routine est répandue.

114
0:05:36.446,000 --> 0:05:38,000
Pensez à combien vous entendez des gens dire

115
0:05:38.496,000 --> 0:05:41,000
que les machines effectuent juste des tâches prévisibles ou répétitives,

116
0:05:41.876,000 --> 0:05:42,000
suivant des règles ou bien définies.

117
0:05:43.68,000 --> 0:05:45,000
Ce ne sont que des mots différents pour désigner une routine.

118
0:05:46.64,000 --> 0:05:49,000
Ce sont des cas classiques de tâches de non routine.

119
0:05:50.64,000 --> 0:05:52,000
Revenons-en à ces trois cas que j’ai évoqués au début.

120
0:05:53.56,000 --> 0:05:55,000
Demandez à un médecin comment il établit un diagnostic médical

121
0:05:56.56,000 --> 0:05:58,000
et il pourra vous donner quelques règles de bon sens,

122
0:05:59.24,000 --> 0:06:,000
mais il aura du mal.

123
0:06:00.92,000 --> 0:06:04,000
Il dira que cela requiert créativité, jugement et intuition.

124
0:06:05.76,000 --> 0:06:07,000
Ce sont des choses difficiles à formuler

125
0:06:08.16,000 --> 0:06:11,000
alors nous pensions que ces tâches seraient difficiles à automatiser.

126
0:06:11.396,000 --> 0:06:13,000
Si un être humain ne sait pas l’expliquer,

127
0:06:13.84,000 --> 0:06:15,000
où commencer pour écrire un jeu d’instructions

128
0:06:16.76,000 --> 0:06:17,000
que la machine puisse suivre ?

129
0:06:18.64,000 --> 0:06:2,000
Il y a 30 ans, cette perspective était correcte,

130
0:06:21.24,000 --> 0:06:23,000
mais aujourd’hui, elle semble douteuse

131
0:06:23.4,000 --> 0:06:25,000
et à l’avenir, elle sera fausse.

132
0:06:25.68,000 --> 0:06:28,000
Les progrès en puissance de traitement, en capacité de stockage de données

133
0:06:29.166,000 --> 0:06:3,000
et en conception algorithmique

134
0:06:30.64,000 --> 0:06:32,000
rendent cette distinction entre routine et non routine

135
0:06:33.216,000 --> 0:06:34,000
de moins en moins utile.

136
0:06:34.92,000 --> 0:06:37,000
Pour voir cela, revenez-en à l’établissement d’un diagnostic médial.

137
0:06:38.2,000 --> 0:06:39,000
Cette année,

138
0:06:39.6,000 --> 0:06:42,000
une équipe de chercheurs à Stanford a annoncé avoir développé un système

139
0:06:43.086,000 --> 0:06:45,000
qui peut vous dire si une tache de rousseur est cancéreuse

140
0:06:46,000 --> 0:06:48,000
avec la même exactitude que les meilleurs dermatologues.

141
0:06:49.28,000 --> 0:06:5,000
Comment cela marche-t-il ?

142
0:06:50.56,000 --> 0:06:55,000
Il n’essaye pas de copier le jugement ou l’intuition d’un docteur.

143
0:06:55.88,000 --> 0:06:58,000
Il ne connaît ni ne comprend rien du tout à la médecine.

144
0:06:59.04,000 --> 0:07:01,000
Il exécute un algorithme de reconnaissance de formes,

145
0:07:01.64,000 --> 0:07:05,000
parcourant 129 450 cas déjà traités

146
0:07:06.32,000 --> 0:07:09,000
à la recherche de similarités entre ces cas-là

147
0:07:09.44,000 --> 0:07:11,000
et la lésion en question.

148
0:07:12.08,000 --> 0:07:15,000
Il effectue ces tâches de façon non humaine,

149
0:07:15.32,000 --> 0:07:17,000
en se reposant sur l’analyse de plus de cas possibles

150
0:07:17.796,000 --> 0:07:19,000
qu’un docteur ne peut espérer examiner durant sa vie.

151
0:07:20.32,000 --> 0:07:21,000
Peu importe que cet être humain,

152
0:07:22.24,000 --> 0:07:25,000
ce docteur, n’ait pas su expliquer comment il a effectué la tâche.

153
0:07:25.64,000 --> 0:07:27,000
Il y a ceux qui s’attardent sur le fait

154
0:07:28,000 --> 0:07:3,000
que ces machines ne sont pas conçues à notre image.

155
0:07:30.386,000 --> 0:07:31,000
Par exemple, prenez Watson d’IBM,

156
0:07:32.4,000 --> 0:07:36,000
le superordinateur qui a participé à un jeu télévisé américain en 2011

157
0:07:37.28,000 --> 0:07:4,000
et a battu deux champions humains du jeu.

158
0:07:40.32,000 --> 0:07:41,000
Le jour après sa victoire,

159
0:07:42.04,000 --> 0:07:45,000
le Wall Street Journal a publié un article du philosophe John Searle

160
0:07:45.36,000 --> 0:07:48,000
ayant pour titre « Watson ignore qu’il a gagné à Jeopardy »

161
0:07:48.76,000 --> 0:07:49,000
C’est brillant et c’est vrai.

162
0:07:50.76,000 --> 0:07:52,000
Watson n’a pas crié de joie.

163
0:07:53.24,000 --> 0:07:56,000
Il n’a pas appelé ses parents pour dire ce qu’il avait accompli.

164
0:07:56.36,000 --> 0:07:58,000
Il n’est pas allé boire un verre dans un bar.

165
0:07:58.72,000 --> 0:08:02,000
Ce système n’essayait pas d’imiter la manière de jouer des humains,

166
0:08:03.2,000 --> 0:08:04,000
mais peu importe.

167
0:08:04.48,000 --> 0:08:05,000
Il les a quand même battus.

168
0:08:06.48,000 --> 0:08:07,000
Résoudre le mythe de l’intelligence

169
0:08:08.166,000 --> 0:08:11,000
montre que notre compréhension limitée de l’intelligence humaine,

170
0:08:11.48,000 --> 0:08:12,000
de notre façon de penser et raisonner,

171
0:08:13.4,000 --> 0:08:16,000
est moins une contrainte en automatisation qu’elle ne l’était auparavant.

172
0:08:16.88,000 --> 0:08:17,000
De plus, nous l’avons vu,

173
0:08:18.4,000 --> 0:08:21,000
quand ces machines réalisent des tâches différemment des êtres humains,

174
0:08:21.77,000 --> 0:08:22,000
il n’y a pas de raison de penser

175
0:08:23.296,000 --> 0:08:25,000
que ce que les êtres humains sont capables de faire

176
0:08:25.696,000 --> 0:08:26,000
représente le sommet

177
0:08:27.16,000 --> 0:08:3,000
de ce que ces machines pourraient être capables de faire à l’avenir.

178
0:08:31.04,000 --> 0:08:32,000
Le troisième mythe

179
0:08:32.32,000 --> 0:08:34,000
est ce que j’appelle le mythe de la supériorité.

180
0:08:34.8,000 --> 0:08:36,000
On dit souvent que ceux qui oublient

181
0:08:37.04,000 --> 0:08:39,000
le côté utile du progrès technologique,

182
0:08:39.52,000 --> 0:08:41,000
ces complémentarités mentionnées auparavant,

183
0:08:42.04,000 --> 0:08:45,000
se prêtent au sophisme de la masse fixe de travail.

184
0:08:45.84,000 --> 0:08:47,000
Le sophisme de la masse fixe de travail

185
0:08:48.159,000 --> 0:08:49,000
est lui-même un sophisme

186
0:08:49.679,000 --> 0:08:52,000
que j’appelle le sophisme du sophisme de la masse fixe de travail,

187
0:08:52.775,000 --> 0:08:54,000
ou SSMFT, pour faire court.

188
0:08:56,000 --> 0:08:57,000
Laissez-moi expliquer.

189
0:08:57.44,000 --> 0:08:59,000
Ce sophisme est une idée très ancienne.

190
0:08:59.6,000 --> 0:09:01,000
C’est un économiste britannique, David Schloss,

191
0:09:01.83,000 --> 0:09:03,000
qui lui a donné son nom en 1892.

192
0:09:03.84,000 --> 0:09:05,000
Il était perplexe d’avoir rencontré un docker

193
0:09:06.68,000 --> 0:09:08,000
qui utilisait une machine pour faire des rondelles,

194
0:09:09.076,000 --> 0:09:12,000
les petits disques en métal se plaçant au bout des vis.

195
0:09:13,000 --> 0:09:16,000
Ce docker se sentait coupable d’être plus productif.

196
0:09:17.56,000 --> 0:09:19,000
La plupart du temps, on s’attend au contraire,

197
0:09:19.76,000 --> 0:09:21,000
se sentir coupable de ne pas être productif,

198
0:09:22,000 --> 0:09:25,000
un peu trop de temps passé sur Facebook ou Twitter au travail.

199
0:09:25.04,000 --> 0:09:27,000
Mais ce docker se sentait coupable d’être plus productif

200
0:09:27.696,000 --> 0:09:29,000
et quant au pourquoi, il a dit : « J’agis mal.

201
0:09:29.92,000 --> 0:09:31,000
Je prends le travail d’un autre homme. »

202
0:09:32.76,000 --> 0:09:34,000
Dans son esprit, il y avait une masse de travail fixe

203
0:09:35.76,000 --> 0:09:37,000
à diviser entre lui et ses camarades

204
0:09:37.92,000 --> 0:09:39,000
et en utilisant la machine pour faire plus,

205
0:09:4,000 --> 0:09:42,000
ses camarades auraient moins à faire.

206
0:09:42.04,000 --> 0:09:43,000
Schloss a vu l’erreur.

207
0:09:43.92,000 --> 0:09:44,000
La masse de travail n’était pas fixe.

208
0:09:45.8,000 --> 0:09:47,000
Ce docker utilisant la machine et devenant plus productif,

209
0:09:48.64,000 --> 0:09:5,000
le prix des rondelles chuterait, la demande augmenterait,

210
0:09:51.64,000 --> 0:09:52,000
il faudrait faire plus de rondelles

211
0:09:53.36,000 --> 0:09:55,000
et ses camarades auraient plus de travail.

212
0:09:55.48,000 --> 0:09:56,000
La masse de travail augmenterait.

213
0:09:57.2,000 --> 0:10:,000
Schloss a appelé cela « le sophisme de la masse fixe de travail ».

214
0:10:00.56,000 --> 0:10:02,000
Les gens parlent du sophisme de la masse fixe de travail

215
0:10:03.52,000 --> 0:10:05,000
pour l’avenir de tous types de travaux.

216
0:10:05.76,000 --> 0:10:07,000
Il n’y a pas de masse fixe de travail à diviser

217
0:10:08.44,000 --> 0:10:09,000
entre les gens et les machines.

218
0:10:09.926,000 --> 0:10:13,000
Oui, les machines substituent les humains, diminuant la masse de travail initiale,

219
0:10:14.52,000 --> 0:10:15,000
mais elles complémentent les humains

220
0:10:16.4,000 --> 0:10:18,000
et la masse de travail augmente et change.

221
0:10:19.76,000 --> 0:10:2,000
Mais SSMFT.

222
0:10:21.4,000 --> 0:10:22,000
Voici l’erreur :

223
0:10:22.8,000 --> 0:10:24,000
le progrès technologique

224
0:10:25.04,000 --> 0:10:26,000
n’augmente pas la masse de travail.

225
0:10:27.04,000 --> 0:10:3,000
Certaines tâches ont plus de valeur, de nouvelles tâches émergent.

226
0:10:30.136,000 --> 0:10:31,000
Il est faux de penser

227
0:10:31.35,000 --> 0:10:35,000
que les humains seront nécessairement mieux placés pour réaliser ces tâches.

228
0:10:35.92,000 --> 0:10:36,000
C’est le mythe de la supériorité.

229
0:10:37.56,000 --> 0:10:4,000
Oui, la masse de travail peut augmenter et changer,

230
0:10:41,000 --> 0:10:43,000
mais les machines devenant plus compétentes,

231
0:10:43.056,000 --> 0:10:46,000
il est probable qu’elles assument cette masse de travail supplémentaire.

232
0:10:46.92,000 --> 0:10:49,000
Le progrès technologique, au lieu de complémenter les humains,

233
0:10:50.2,000 --> 0:10:51,000
complémente les machines.

234
0:10:52.92,000 --> 0:10:55,000
Pour l’observer, revenez-en au fait de conduire une voiture.

235
0:10:55.96,000 --> 0:10:59,000
Les systèmes de navigation par satellite complémentent les êtres humains.

236
0:11:00.08,000 --> 0:11:02,000
Ils font de certains humains de meilleurs conducteurs.

237
0:11:02.92,000 --> 0:11:03,000
Mais à l’avenir,

238
0:11:04.2,000 --> 0:11:07,000
le logiciel va remplacer les êtres humains sur le siège conducteur

239
0:11:07.32,000 --> 0:11:1,000
ces systèmes de navigation ne complémenteront pas les humains,

240
0:11:10.336,000 --> 0:11:12,000
mais amélioreront les voitures sans conducteur,

241
0:11:12.84,000 --> 0:11:13,000
aidant plutôt les machines.

242
0:11:14.4,000 --> 0:11:18,000
Revenez-en à ces complémentarités indirectes également mentionnées.

243
0:11:18.48,000 --> 0:11:19,000
Le gâteau économique s’agrandira,

244
0:11:20.28,000 --> 0:11:21,000
mais les machines s’améliorant,

245
0:11:22.04,000 --> 0:11:25,000
les nouvelles demandes pourront être des biens que les machines,

246
0:11:25.207,000 --> 0:11:27,000
et non les humains, seront les mieux placées pour produire.

247
0:11:27.966,000 --> 0:11:28,000
Le gâteau pourrait changer,

248
0:11:29.826,000 --> 0:11:3,000
mais, les machines s’améliorant,

249
0:11:31.72,000 --> 0:11:35,000
elles pourraient être mieux placées pour réaliser les tâches nécessaires.

250
0:11:36.6,000 --> 0:11:37,000
En bref, la demande pour des tâches

251
0:11:38.27,000 --> 0:11:4,000
n’est pas la demande pour du travail humain.

252
0:11:40.32,000 --> 0:11:41,000
Les humains n’en bénéficieront

253
0:11:42.28,000 --> 0:11:45,000
que s’ils gardent la main sur ces tâches complémentées,

254
0:11:46.12,000 --> 0:11:49,000
mais les machines s’améliorant, cette probabilité diminue.

255
0:11:50.76,000 --> 0:11:52,000
Que nous disent ces trois mythes ?

256
0:11:52.8,000 --> 0:11:53,000
Résoudre le mythe de Terminator

257
0:11:54.52,000 --> 0:11:57,000
nous montre que l’avenir du travail dépend de cet équilibre entre deux forces :

258
0:11:58.256,000 --> 0:12:01,000
la substitution par les machines nuisant aux travailleurs,

259
0:12:01.4,000 --> 0:12:03,000
mais aussi ces complémentarités qui font le contraire.

260
0:12:04,000 --> 0:12:08,000
Jusqu’à maintenant, cet équilibre est en faveur des êtres humains.

261
0:12:09.12,000 --> 0:12:1,000
Résoudre le mythe de l’intelligence

262
0:12:10.88,000 --> 0:12:12,000
montre que la substitution par les machines,

263
0:12:13.646,000 --> 0:12:14,000
monte en puissance.

264
0:12:14.72,000 --> 0:12:15,000
Les machines ne savent pas tout faire,

265
0:12:16.72,000 --> 0:12:17,000
mais elles peuvent faire plus,

266
0:12:18.166,000 --> 0:12:22,000
empiétant plus sur le royaume des tâches réalisées par des êtres humains.

267
0:12:22.6,000 --> 0:12:23,000
Il n’y a pas de raison de penser que

268
0:12:24.52,000 --> 0:12:26,000
ce que les humains peuvent faire actuellement

269
0:12:26.766,000 --> 0:12:27,000
représente une ligne d’arrivée,

270
0:12:28.64,000 --> 0:12:3,000
que les machines s’arrêteront poliment

271
0:12:30.92,000 --> 0:12:31,000
une fois aussi compétentes que nous.

272
0:12:32.76,000 --> 0:12:33,000
Rien de tout cela n’importe

273
0:12:34.32,000 --> 0:12:36,000
tant que ces vents bénéfiques de la complémentarité

274
0:12:37.16,000 --> 0:12:38,000
soufflent assez fort,

275
0:12:38.92,000 --> 0:12:39,000
mais résoudre le mythe de la supériorité

276
0:12:40.88,000 --> 0:12:43,000
montre que ce processus d’empiétement sur les tâches

277
0:12:44,000 --> 0:12:47,000
ne fait pas que renforcer la force de substitution par les machines,

278
0:12:47.96,000 --> 0:12:5,000
mais use également ces complémentarités bénéfiques.

279
0:12:51.32,000 --> 0:12:52,000
Réunissez ces trois mythes

280
0:12:53.28,000 --> 0:12:55,000
et nous aurons un aperçu de cet avenir inquiétant.

281
0:12:56.24,000 --> 0:12:58,000
Les machines continuent de s’améliorer,

282
0:12:58.28,000 --> 0:13:01,000
empiétant de plus en plus sur les tâches réalisées par les êtres humains,

283
0:13:01.96,000 --> 0:13:03,000
augmentant la force de substitution par les machines,

284
0:13:04.56,000 --> 0:13:07,000
amenuisant la force de la complémentarité avec les machines.

285
0:13:08.2,000 --> 0:13:12,000
À un moment donné, cet équilibre sera en saveur des machines

286
0:13:12.52,000 --> 0:13:14,000
plutôt que des êtres humains.

287
0:13:14.6,000 --> 0:13:15,000
C’est le chemin que nous suivons.

288
0:13:16.36,000 --> 0:13:19,000
Je dis « chemin » délibérément car nous n’y sommes pas encore,

289
0:13:19.56,000 --> 0:13:22,000
mais il est dur d’éviter de conclure que c’est la direction que nous prenons.

290
0:13:24.64,000 --> 0:13:25,000
C’est la partie inquiétante.

291
0:13:26.12,000 --> 0:13:29,000
Laissez-moi vous dire pourquoi je crois que c’est un bon problème à avoir.

292
0:13:30.52,000 --> 0:13:33,000
Durant la plupart de l’histoire humaine, un problème économique a dominé :

293
0:13:34.08,000 --> 0:13:38,000
comment agrandir assez le gâteau économique pour que tout le monde en vive.

294
0:13:38.16,000 --> 0:13:4,000
Au début du premier siècle après J.C.,

295
0:13:40.36,000 --> 0:13:42,000
en prenant le gâteau économique mondial

296
0:13:42.48,000 --> 0:13:45,000
et en le divisant en parts égales,

297
0:13:45.8,000 --> 0:13:47,000
chacun aurait quelques centaines de dollars.

298
0:13:47.96,000 --> 0:13:5,000
Presque tout le monde vivait sous ou autour du seuil de pauvreté.

299
0:13:51.32,000 --> 0:13:53,000
En avançant de 1 000 ans,

300
0:13:53.52,000 --> 0:13:54,000
c’est toujours à peu près vrai.

301
0:13:55.68,000 --> 0:13:58,000
Mais durant les derniers siècles, la croissance économique a décollé.

302
0:13:59.28,000 --> 0:14:01,000
La taille de ces gâteaux économiques a explosé.

303
0:14:01.68,000 --> 0:14:03,000
Le PIB mondial par habitant,

304
0:14:03.76,000 --> 0:14:06,000
la valeur de ces parts individuelles du gâteau d’aujourd’hui

305
0:14:07.16,000 --> 0:14:09,000
est d’environ 10 150 dollars.

306
0:14:1,000 --> 0:14:12,000
Si la croissance économique continue à 2 %,

307
0:14:12.72,000 --> 0:14:14,000
nos enfants auront deux fois nos richesses.

308
0:14:14.8,000 --> 0:14:16,000
Si elle continue au rythme d’un pour cent,

309
0:14:17.12,000 --> 0:14:19,000
nos petits-enfants auront deux fois nos richesses.

310
0:14:19.8,000 --> 0:14:22,000
Nous avons résolu le problème économique traditionnel.

311
0:14:24.2,000 --> 0:14:27,000
Le chômage technologique, s’il se produit,

312
0:14:27.24,000 --> 0:14:3,000
sera un symptôme de ce succès.

313
0:14:30.48,000 --> 0:14:33,000
Nous aurons résolu un problème, agrandir le gâteau,

314
0:14:34.36,000 --> 0:14:35,000
mais l’aurons remplacé par un autre,

315
0:14:36.2,000 --> 0:14:38,000
comment s’assurer que chacun ait une part.

316
0:14:39.84,000 --> 0:14:4,000
D’autres économistes ont remarqué

317
0:14:41.44,000 --> 0:14:43,000
que résoudre ce problème ne sera pas simple.

318
0:14:43.5,000 --> 0:14:44,000
Pour la plupart des gens,

319
0:14:45.04,000 --> 0:14:47,000
leur emploi est leur place à la table économique

320
0:14:47.56,000 --> 0:14:49,000
et, dans un monde avec moins ou pas de travail,

321
0:14:5,000 --> 0:14:52,000
il ne sera pas clair comment obtenir sa part.

322
0:14:52.106,000 --> 0:14:54,000
Il y a par exemple beaucoup de discussions

323
0:14:54.44,000 --> 0:14:56,000
sur diverses formes de revenu de base universel

324
0:14:57.16,000 --> 0:14:58,000
en tant qu’approche possible

325
0:14:58.506,000 --> 0:14:59,000
et il y a des essais en cours

326
0:15:00.04,000 --> 0:15:02,000
aux États-Unis, en Finlande et au Kenya.

327
0:15:03,000 --> 0:15:06,000
C’est le défi collectif auquel nous faisons face :

328
0:15:06.2,000 --> 0:15:08,000
découvrir comment cette prospérité matérielle

329
0:15:08.36,000 --> 0:15:1,000
générée par notre système économique

330
0:15:11.28,000 --> 0:15:12,000
peut bénéficier à tous

331
0:15:13.28,000 --> 0:15:15,000
dans un monde où notre mécanisme traditionnel

332
0:15:15.72,000 --> 0:15:16,000
pour partager le gâteau,

333
0:15:17.6,000 --> 0:15:18,000
le travail effectué par les gens,

334
0:15:19.56,000 --> 0:15:21,000
s’étiole voire disparaît.

335
0:15:22.28,000 --> 0:15:26,000
Résoudre ce problème va nécessiter que nous pensions différemment.

336
0:15:27.4,000 --> 0:15:31,000
Il y aura beaucoup de désaccords sur ce qu’il faut faire,

337
0:15:31.6,000 --> 0:15:34,000
mais nous devons nous souvenir que c’est un meilleur problème à avoir

338
0:15:35.04,000 --> 0:15:37,000
que celui qui a hanté nos ancêtres durant des siècles :

339
0:15:37.88,000 --> 0:15:4,000
comment faire grossir ce gâteau ?

340
0:15:41.28,000 --> 0:15:42,000
Merci beaucoup.

341
0:15:42.56,000 --> 0:15:45,000
(Applaudissements)

