1
0:00:,000 --> 0:00:07,000
Traductor: Lidia Cámara de la Fuente Revisor: Ciro Gomez

2
0:00:12.787,000 --> 0:00:15,000
¿Pastel favorito en EEUU?

3
0:00:16.632,000 --> 0:00:19,000
Audiencia: El de manzana. Kenneth Cukier: De manzana. Por supuesto.

4
0:00:20.138,000 --> 0:00:21,000
¿Cómo lo sabemos?

5
0:00:21.369,000 --> 0:00:23,000
Por los datos.

6
0:00:24.122,000 --> 0:00:26,000
Se miran las ventas en supermercados.

7
0:00:26.188,000 --> 0:00:28,000
Se miran las ventas en supermercados de pasteles de 30 cm

8
0:00:29.054,000 --> 0:00:33,000
congelados, y los de manzana ganan, sin rival.

9
0:00:33.129,000 --> 0:00:38,000
La mayoría de las ventas son los de manzana.

10
0:00:38.309,000 --> 0:00:4,000
Pero los supermercados comenzaron a vender

11
0:00:41.273,000 --> 0:00:43,000
pasteles más pequeños, de 11 cm,

12
0:00:43.856,000 --> 0:00:47,000
y de repente, el de manzana cayó al 4º o 5º lugar.

13
0:00:48.03,000 --> 0:00:5,000
¿Por qué? ¿Qué paso?

14
0:00:50.905,000 --> 0:00:52,000
Bueno, piensen en ello.

15
0:00:53.723,000 --> 0:00:56,000
Cuando compramos un pastel de 30 cm,

16
0:00:57.571,000 --> 0:00:59,000
toda la familia tiene que estar de acuerdo,

17
0:00:59.832,000 --> 0:01:02,000
y el de manzana es el segundo favorito de todos.

18
0:01:03.623,000 --> 0:01:04,000
(Risas)

19
0:01:05.558,000 --> 0:01:08,000
Pero si uno compra un pastel de 11 cm individual,

20
0:01:09.173,000 --> 0:01:12,000
puede comprar el que desee.

21
0:01:12.918,000 --> 0:01:16,000
Puede comprar su primera opción.

22
0:01:16.933,000 --> 0:01:17,000
Tenemos más datos.

23
0:01:18.574,000 --> 0:01:19,000
Podemos ver algo

24
0:01:20.128,000 --> 0:01:21,000
que no se podía ver

25
0:01:21.26,000 --> 0:01:24,000
cuando solo había menor cantidad de datos.

26
0:01:25.213,000 --> 0:01:27,000
Ahora, el punto es que muchos más datos

27
0:01:27.688,000 --> 0:01:29,000
no solo nos permiten ver más,

28
0:01:29.971,000 --> 0:01:3,000
más de lo mismo que ya veíamos.

29
0:01:31.825,000 --> 0:01:34,000
Más datos nos permiten ver cosas nuevas.

30
0:01:35.438,000 --> 0:01:38,000
Nos permiten ver mejor.

31
0:01:38.532,000 --> 0:01:41,000
Nos permiten ver de forma diferente.

32
0:01:42.188,000 --> 0:01:45,000
En este caso, nos permiten ver

33
0:01:45.361,000 --> 0:01:47,000
que el pastel favorito de EEUU es:

34
0:01:48.274,000 --> 0:01:5,000
no el de manzana.

35
0:01:50.816,000 --> 0:01:53,000
Puede que todos hayan oído escuchado el término "Datos masivos".

36
0:01:54.223,000 --> 0:01:56,000
De hecho, es probable que estén hartos de escucharlo

37
0:01:56.927,000 --> 0:01:57,000
"Datos masivos".

38
0:01:58.117,000 --> 0:02:01,000
Es cierto que se exagera mucho el término,

39
0:02:01.447,000 --> 0:02:03,000
y eso es muy lamentable,

40
0:02:03.779,000 --> 0:02:06,000
porque los datos masivos son una herramienta muy importante

41
0:02:06.825,000 --> 0:02:09,000
para que la sociedad avance.

42
0:02:10.559,000 --> 0:02:13,000
En el pasado, solíamos observar pequeñas cantidades de datos

43
0:02:14.12,000 --> 0:02:15,000
y pensar qué significarían

44
0:02:15.824,000 --> 0:02:16,000
para tratar de entender el mundo.

45
0:02:17.406,000 --> 0:02:18,000
Ahora tenemos mucho más de ello,

46
0:02:19.311,000 --> 0:02:21,000
más de lo que podía existir antes.

47
0:02:22.032,000 --> 0:02:23,000
Lo que encontramos es que cuando tenemos

48
0:02:23.996,000 --> 0:02:25,000
una gran cantidad de datos, podemos hacer cosas

49
0:02:26.74,000 --> 0:02:29,000
que no podíamos hacer teniendo solo cantidades más pequeñas.

50
0:02:29.91,000 --> 0:02:31,000
Los datos masivos son importantes y es algo nuevo,

51
0:02:32.551,000 --> 0:02:33,000
y cuando se piensa en ello,

52
0:02:34.328,000 --> 0:02:36,000
la única forma en que este planeta afronte

53
0:02:36.544,000 --> 0:02:37,000
sus desafíos mundiales, esto es,

54
0:02:38.399,000 --> 0:02:41,000
alimentar a la gente, ofrecer atención médica,

55
0:02:41.87,000 --> 0:02:43,000
suministrar energía, electricidad,

56
0:02:44.68,000 --> 0:02:45,000
y asegurarse de que no nos achicharramos

57
0:02:46.589,000 --> 0:02:47,000
debido al calentamiento global,

58
0:02:48.326,000 --> 0:02:51,000
es utilizando de forma eficaz los datos.

59
0:02:51.902,000 --> 0:02:54,000
Entonces, ¿qué es lo nuevo de los datos masivos? ¿Cuál es la gran cosa?

60
0:02:55.772,000 --> 0:02:57,000
Bueno, para responder a esto, pensaremos en

61
0:02:58.289,000 --> 0:02:59,000
cómo se veía la información,

62
0:03:00.185,000 --> 0:03:03,000
físicamente en el pasado.

63
0:03:03.219,000 --> 0:03:06,000
En 1908 en la isla de Creta,

64
0:03:06.83,000 --> 0:03:1,000
los arqueólogos descubrieron un disco de arcilla.

65
0:03:11.565,000 --> 0:03:15,000
Datan del año 2000 aC, así que tienen 4000 años de antigüedad.

66
0:03:15.684,000 --> 0:03:16,000
Hay inscripciones en este disco,

67
0:03:17.628,000 --> 0:03:18,000
pero, no sabemos qué significan.

68
0:03:19.168,000 --> 0:03:21,000
Es un completo misterio, pero el punto es que

69
0:03:21.426,000 --> 0:03:22,000
así solía verse la información

70
0:03:23.061,000 --> 0:03:25,000
hace 4000 años.

71
0:03:25.07,000 --> 0:03:27,000
Esta es la forma en que la sociedad almacenaba

72
0:03:27.618,000 --> 0:03:3,000
y transmitía la información.

73
0:03:31.142,000 --> 0:03:35,000
Ahora, la sociedad no ha avanzado tanto.

74
0:03:35.302,000 --> 0:03:38,000
Todavía guardamos la información en discos,

75
0:03:38.776,000 --> 0:03:41,000
pero ahora podemos almacenar mucha más información,

76
0:03:41.96,000 --> 0:03:42,000
más que nunca.

77
0:03:43.22,000 --> 0:03:46,000
Buscar es más fácil. Copiar es más fácil.

78
0:03:46.313,000 --> 0:03:49,000
El compartir es más fácil. El procesamiento es más fácil.

79
0:03:49.813,000 --> 0:03:51,000
Y podemos volver a utilizar esta información

80
0:03:52.579,000 --> 0:03:53,000
para usos que nunca nos imaginamos

81
0:03:54.413,000 --> 0:03:57,000
cuando se recogieron los primeros datos.

82
0:03:57.608,000 --> 0:03:59,000
A este respecto, los datos han evolucionado

83
0:03:59.86,000 --> 0:04:02,000
de un almacén a un flujo,

84
0:04:03.392,000 --> 0:04:06,000
de algo que es estacionario y estático

85
0:04:07.33,000 --> 0:04:1,000
a algo que es fluido y dinámico.

86
0:04:10.939,000 --> 0:04:14,000
Hay, si quieren, una liquidez de información.

87
0:04:15.502,000 --> 0:04:17,000
El disco descubierto fuera de Creta

88
0:04:18.436,000 --> 0:04:21,000
que tiene 4000 años de antigüedad, es pesado,

89
0:04:22.2,000 --> 0:04:23,000
no almacena gran cantidad de información,

90
0:04:24.162,000 --> 0:04:27,000
y esa información no es modificable.

91
0:04:27.278,000 --> 0:04:31,000
Por el contrario, todos los archivos

92
0:04:31.289,000 --> 0:04:32,000
que Edward Snowden tomó

93
0:04:33.149,000 --> 0:04:35,000
de la Agencia de Seguridad Nacional de EEUU

94
0:04:35.771,000 --> 0:04:37,000
caben en un dispositivo de memoria extraíble

95
0:04:38.19,000 --> 0:04:41,000
del tamaño de una uña,

96
0:04:41.2,000 --> 0:04:45,000
y pueden compartirse a la velocidad de la luz.

97
0:04:45.945,000 --> 0:04:49,000
Más datos. Más.

98
0:04:50.74,000 --> 0:04:52,000
Una razón para tener tantos datos hoy en el mundo

99
0:04:53.174,000 --> 0:04:54,000
es que recolectamos cosas

100
0:04:54.606,000 --> 0:04:57,000
sobre las que siempre hemos recopilado información,

101
0:04:57.886,000 --> 0:04:59,000
pero otra razón es que estamos tomando cosas

102
0:05:00.542,000 --> 0:05:02,000
que siempre han sido informacionales

103
0:05:03.354,000 --> 0:05:05,000
pero nunca se habían convertido a un formato de datos

104
0:05:05.893,000 --> 0:05:07,000
y las estamos convirtiendo en datos.

105
0:05:08.259,000 --> 0:05:11,000
Piensen, por ejemplo, en la cuestión de la ubicación.

106
0:05:11.567,000 --> 0:05:13,000
Tomemos, por ejemplo, Martín Lutero.

107
0:05:13.816,000 --> 0:05:14,000
Si hubiéramos querido saber en 1500

108
0:05:15.653,000 --> 0:05:17,000
donde estaba Martín Lutero,

109
0:05:18.08,000 --> 0:05:2,000
habríamos tenido que seguirlo en todo momento,

110
0:05:20.512,000 --> 0:05:21,000
quizá con pluma y tintero,

111
0:05:22.362,000 --> 0:05:23,000
y anotarlo.

112
0:05:23.985,000 --> 0:05:25,000
Pero piensen cómo es hoy en día.

113
0:05:26.168,000 --> 0:05:27,000
Uds. saben que en algún lugar,

114
0:05:28.01,000 --> 0:05:31,000
quizá en la base de datos de una empresa de telecomunicaciones,

115
0:05:31.116,000 --> 0:05:33,000
hay una hoja de cálculo o entrada de base de datos

116
0:05:33.772,000 --> 0:05:35,000
donde se registra su información

117
0:05:35.86,000 --> 0:05:37,000
de donde han estado en todo momento.

118
0:05:37.923,000 --> 0:05:38,000
Si tienen celular,

119
0:05:39.283,000 --> 0:05:41,000
y el teléfono tiene GPS, pero incluso si no tiene GPS,

120
0:05:42.13,000 --> 0:05:44,000
se puede registrar su información.

121
0:05:44.515,000 --> 0:05:48,000
En este sentido, la localización ha sido un campo de datos.

122
0:05:48.599,000 --> 0:05:52,000
Ahora piensen, por ejemplo, en el tema de la postura,

123
0:05:53.2,000 --> 0:05:55,000
la forma en que están sentados ahora,

124
0:05:55.291,000 --> 0:05:56,000
la forma en Ud. está sentado,

125
0:05:56.788,000 --> 0:05:58,000
la de Ud., la de Ud.

126
0:05:59.259,000 --> 0:06:02,000
Todas diferentes, en función de la longitud de las piernas,

127
0:06:02.76,000 --> 0:06:03,000
la espalda y su contorno,

128
0:06:03.989,000 --> 0:06:04,000
y si pusiera censores, tal vez 100

129
0:06:05.987,000 --> 0:06:06,000
en todos los asientos ahora,

130
0:06:07.753,000 --> 0:06:1,000
podría crear un índice que es único para cada uno,

131
0:06:11.353,000 --> 0:06:15,000
algo así como una huella digital, que no es del dedo.

132
0:06:15.762,000 --> 0:06:17,000
Y entonces, ¿qué podemos hacer con esto?

133
0:06:18.731,000 --> 0:06:2,000
Los investigadores en Tokio están utilizando

134
0:06:21.128,000 --> 0:06:25,000
como un dispositivo potencial antirobo en los autos.

135
0:06:25.516,000 --> 0:06:27,000
La idea es que el ladrón se siente al volante,

136
0:06:28.44,000 --> 0:06:3,000
intente encenderlo, pero el auto reconoce

137
0:06:30.544,000 --> 0:06:32,000
que un conductor no autorizado está en el auto

138
0:06:32.906,000 --> 0:06:34,000
y, tal vez el motor se detiene, a menos que

139
0:06:35.07,000 --> 0:06:38,000
escriba una contraseña en el salpicadero

140
0:06:38.247,000 --> 0:06:42,000
para decir, "Tengo la autorización para conducir". Estupendo.

141
0:06:42.905,000 --> 0:06:44,000
¿Qué pasaría si cada automóvil en Europa

142
0:06:45.458,000 --> 0:06:46,000
tuviera esta tecnología?

143
0:06:46.915,000 --> 0:06:49,000
¿Qué podemos hacer entonces?

144
0:06:50.08,000 --> 0:06:52,000
Tal vez, si agregamos los datos,

145
0:06:52.32,000 --> 0:06:55,000
tal vez podríamos identificar signos reveladores

146
0:06:56.134,000 --> 0:06:58,000
que predijeran mejor que un accidente de auto

147
0:06:58.843,000 --> 0:07:03,000
tendrá lugar en los próximos cinco segundos.

148
0:07:04.736,000 --> 0:07:06,000
Y entonces, la base de datos que tendremos

149
0:07:07.293,000 --> 0:07:08,000
es la fatiga del conductor,

150
0:07:09.076,000 --> 0:07:12,000
y el servicio se activaría cuando los sensores del automóvil

151
0:07:12.16,000 --> 0:07:14,000
detectaran que la persona reposa en esa posición,

152
0:07:14.847,000 --> 0:07:17,000
y automáticamente se activa una alarma interna

153
0:07:18.841,000 --> 0:07:2,000
que haría vibrar el volante, sonar una alarma

154
0:07:21.12,000 --> 0:07:22,000
para decir, "Despierta,

155
0:07:22.587,000 --> 0:07:23,000
presta más atención a la carretera".

156
0:07:24.491,000 --> 0:07:26,000
Este es el tipo de cosas que podemos hacer

157
0:07:26.51,000 --> 0:07:28,000
cuando tomamos datos en más aspectos de nuestras vidas.

158
0:07:29.451,000 --> 0:07:32,000
Entonces, ¿cuál es el valor de los datos masivos?

159
0:07:32.84,000 --> 0:07:34,000
Bueno, piensen en ello.

160
0:07:35.03,000 --> 0:07:37,000
Tienen más información.

161
0:07:37.442,000 --> 0:07:4,000
Pueden hacer cosas que antes no se podían hacer.

162
0:07:40.783,000 --> 0:07:41,000
Una de las zonas más impresionantes

163
0:07:42.459,000 --> 0:07:43,000
donde este concepto se ve aplicado

164
0:07:44.234,000 --> 0:07:47,000
es en el área del aprendizaje automático.

165
0:07:47.495,000 --> 0:07:5,000
El aprendizaje automático es una rama de la inteligencia artificial,

166
0:07:50.758,000 --> 0:07:53,000
que en sí es una rama de la informática.

167
0:07:53.95,000 --> 0:07:54,000
La idea general es que en lugar de

168
0:07:55.579,000 --> 0:07:57,000
enseñar a un equipo algo,

169
0:07:57.61,000 --> 0:07:59,000
simplemente transferiremos datos al problema

170
0:08:00.23,000 --> 0:08:03,000
para decirle a la computadora que lo averigüe sola.

171
0:08:03.436,000 --> 0:08:04,000
Y nos ayude a entenderlo

172
0:08:05.213,000 --> 0:08:08,000
al ver sus orígenes.

173
0:08:08.765,000 --> 0:08:1,000
En la década de 1950, un científico de computación

174
0:08:11.279,000 --> 0:08:14,000
en IBM llamado Arthur Samuel al que le gustaba jugar a damas,

175
0:08:14.745,000 --> 0:08:15,000
por eso escribió un programa

176
0:08:16.147,000 --> 0:08:18,000
para poder jugar contra la computadora.

177
0:08:18.96,000 --> 0:08:2,000
Jugó. Ganó.

178
0:08:21.671,000 --> 0:08:23,000
Jugó. Ganó.

179
0:08:23.774,000 --> 0:08:26,000
Jugó. Ganó,

180
0:08:26.789,000 --> 0:08:27,000
porque el equipo solo sabía

181
0:08:28.567,000 --> 0:08:3,000
lo que era un movimiento legal.

182
0:08:30.794,000 --> 0:08:32,000
Arthur Samuel sabía algo más.

183
0:08:32.881,000 --> 0:08:36,000
Arthur Samuel sabía estrategia.

184
0:08:37.51,000 --> 0:08:39,000
Así que escribió un pequeño subprograma

185
0:08:39.906,000 --> 0:08:4,000
operando en el fondo. Y todo lo que hizo

186
0:08:41.88,000 --> 0:08:42,000
fue anotar la probabilidad

187
0:08:43.697,000 --> 0:08:45,000
de que una configuración del tablero condujera

188
0:08:46.26,000 --> 0:08:48,000
a un tablero ganador frente a un tablero perdedor

189
0:08:49.169,000 --> 0:08:51,000
después de cada movimiento.

190
0:08:51.678,000 --> 0:08:54,000
Él jugó contra el equipo. Él ganó.

191
0:08:54.828,000 --> 0:08:56,000
Él jugó contra el equipo. Él ganó.

192
0:08:57.336,000 --> 0:09:,000
Él jugó contra el equipo. Él ganó.

193
0:09:01.067,000 --> 0:09:03,000
Y luego Arthur Samuel dejó que la computadora

194
0:09:03.344,000 --> 0:09:05,000
jugara sola.

195
0:09:05.571,000 --> 0:09:08,000
Juega sola. Y recoge más datos.

196
0:09:09.08,000 --> 0:09:13,000
Recoge más datos. Aumenta la precisión de su predicción.

197
0:09:13.389,000 --> 0:09:15,000
Y luego Arthur Samuel vuelve al equipo

198
0:09:15.493,000 --> 0:09:17,000
juega y pierde.

199
0:09:17.811,000 --> 0:09:19,000
Y juega y pierde.

200
0:09:19.88,000 --> 0:09:21,000
Y juega y pierde.

201
0:09:21.927,000 --> 0:09:23,000
Y Arthur Samuel ha creado una máquina

202
0:09:24.526,000 --> 0:09:3,000
que supera su capacidad en una tarea que él enseñó.

203
0:09:30.814,000 --> 0:09:32,000
Y esta idea de aprendizaje automático

204
0:09:33.312,000 --> 0:09:36,000
irá a todas partes.

205
0:09:37.239,000 --> 0:09:4,000
¿Cómo creen que tenemos autos autodirigidos?

206
0:09:40.388,000 --> 0:09:42,000
¿Estamos mejor como sociedad

207
0:09:42.525,000 --> 0:09:45,000
almacenando todas las reglas de la carretera en un software?

208
0:09:45.81,000 --> 0:09:47,000
No. La memoria es más barata. No.

209
0:09:48.408,000 --> 0:09:51,000
Los algoritmos son más rápidos. No. Los procesadores son mejores. No.

210
0:09:52.402,000 --> 0:09:54,000
Todas esas cosas importan, pero no es por eso.

211
0:09:55.174,000 --> 0:09:58,000
Es porque hemos cambiado la naturaleza del problema.

212
0:09:58.315,000 --> 0:09:59,000
Hemos cambiado el problema de uno

213
0:09:59.991,000 --> 0:10:01,000
en el que intentábamos abierta y explícitamente

214
0:10:02.343,000 --> 0:10:04,000
explicar a la computadora cómo conducir,

215
0:10:04.671,000 --> 0:10:05,000
a uno en la que decimos,

216
0:10:05.987,000 --> 0:10:07,000
"Aquí hay una gran cantidad de datos del vehículo.

217
0:10:08.396,000 --> 0:10:09,000
Haz los números.

218
0:10:09.396,000 --> 0:10:11,000
Te diste cuenta de que eso es un semáforo,

219
0:10:11.396,000 --> 0:10:12,000
que está en rojo y no verde,

220
0:10:13.344,000 --> 0:10:15,000
eso significa que tienes que detenerte

221
0:10:15.358,000 --> 0:10:18,000
y no seguir".

222
0:10:18.401,000 --> 0:10:19,000
El aprendizaje automático está en la base

223
0:10:20.398,000 --> 0:10:22,000
de muchas cosas que hacemos en línea:

224
0:10:22.826,000 --> 0:10:22,000
motores de búsqueda,

225
0:10:23.807,000 --> 0:10:26,000
el algoritmo de personalización de Amazon,

226
0:10:27.608,000 --> 0:10:29,000
la traducción automática por computadora,

227
0:10:29.82,000 --> 0:10:33,000
los sistemas de reconocimiento de voz.

228
0:10:34.11,000 --> 0:10:37,000
Recientemente, los investigadores han examinado

229
0:10:37.231,000 --> 0:10:39,000
la cuestión de biopsias,

230
0:10:40.14,000 --> 0:10:42,000
biopsias de cáncer,

231
0:10:42.907,000 --> 0:10:44,000
y han usado la computadora para identificar,

232
0:10:45.222,000 --> 0:10:47,000
mirando los datos y las tasas de supervivencia,

233
0:10:47.693,000 --> 0:10:51,000
si las células son en realidad

234
0:10:52.36,000 --> 0:10:54,000
cancerosas o no,

235
0:10:54.904,000 --> 0:10:55,000
y claro, al trasferir los datos

236
0:10:56.682,000 --> 0:10:58,000
por un algoritmo de aprendizaje automático,

237
0:10:58.729,000 --> 0:10:59,000
la máquina fue capaz de identificar

238
0:11:00.606,000 --> 0:11:02,000
los 12 signos reveladores que mejor predicen

239
0:11:02.968,000 --> 0:11:05,000
si en esta biopsia de células de cáncer de mama,

240
0:11:06.167,000 --> 0:11:09,000
hay, en efecto, cáncer.

241
0:11:09.385,000 --> 0:11:11,000
El problema: la literatura médica

242
0:11:11.883,000 --> 0:11:13,000
solo sabía nueve de ellos.

243
0:11:14.672,000 --> 0:11:15,000
Tres de los rasgos eran de

244
0:11:16.472,000 --> 0:11:18,000
los que las personas no buscan,

245
0:11:19.447,000 --> 0:11:22,000
pero que la máquina descubrió.

246
0:11:26.351,000 --> 0:11:3,000
También hay lados oscuros en los datos masivos.

247
0:11:30.829,000 --> 0:11:32,000
Mejorará nuestras vidas, pero hay problemas

248
0:11:32.977,000 --> 0:11:34,000
de los que tenemos que ser conscientes,

249
0:11:35.617,000 --> 0:11:37,000
y el primero es la idea

250
0:11:38.24,000 --> 0:11:4,000
de que podemos ser castigados por las predicciones,

251
0:11:40.926,000 --> 0:11:43,000
que la policía puede utilizar datos masivos para sus fines,

252
0:11:44.796,000 --> 0:11:46,000
un poco como "Minority Report".

253
0:11:47.147,000 --> 0:11:49,000
Es un término conocido como policial predictiva,

254
0:11:49.554,000 --> 0:11:51,000
o criminología algorítmica,

255
0:11:51.824,000 --> 0:11:53,000
y la idea es que, con gran cantidad de datos,

256
0:11:54.13,000 --> 0:11:56,000
por ejemplo, donde hubo crímenes antes,

257
0:11:56.219,000 --> 0:11:58,000
sabremos dónde enviar a las patrullas.

258
0:11:58.669,000 --> 0:12:,000
Tiene sentido, pero, el problema, claro,

259
0:12:00.804,000 --> 0:12:04,000
es que no solo se quedarán en los datos de ubicación,

260
0:12:05.348,000 --> 0:12:07,000
irán al nivel del individuo.

261
0:12:08.307,000 --> 0:12:1,000
¿Por qué no usamos los datos de personas

262
0:12:10.557,000 --> 0:12:12,000
con un alto expediente académico?

263
0:12:12.785,000 --> 0:12:13,000
Tal vez utilizar el hecho de que

264
0:12:14.319,000 --> 0:12:16,000
estén sin empleo, su record crediticio,

265
0:12:16.5,000 --> 0:12:17,000
su comportamiento en la web,

266
0:12:17.972,000 --> 0:12:18,000
si están despiertos tarde en la noche.

267
0:12:19.804,000 --> 0:12:23,000
Su controlador físico digital, cuando identifique datos bioquímicos,

268
0:12:23.91,000 --> 0:12:26,000
mostrará si tienen pensamientos agresivos.

269
0:12:27.201,000 --> 0:12:29,000
Podemos tener algoritmos que pueden predecir

270
0:12:29.422,000 --> 0:12:3,000
lo que estamos a punto de hacer,

271
0:12:31.055,000 --> 0:12:32,000
y podemos ser responsables

272
0:12:32.299,000 --> 0:12:34,000
antes de que realmente hayamos actuado.

273
0:12:34.889,000 --> 0:12:35,000
la privacidad era el desafío principal

274
0:12:36.734,000 --> 0:12:38,000
en la era de los datos pequeños.

275
0:12:39.501,000 --> 0:12:41,000
En la era de los datos masivos,

276
0:12:41.65,000 --> 0:12:45,000
el reto será salvaguardar el libre albedrío,

277
0:12:46.173,000 --> 0:12:49,000
la elección moral, la voluntad humana,

278
0:12:49.952,000 --> 0:12:52,000
la acción humana.

279
0:12:54.54,000 --> 0:12:56,000
Hay otro problema:

280
0:12:56.765,000 --> 0:12:59,000
los datos masivos nos quitarán nuestros puestos de trabajo.

281
0:13:00.321,000 --> 0:13:03,000
Los datos masivos y algoritmos desafiarán

282
0:13:03.833,000 --> 0:13:06,000
los conocimientos profesionales de gestión

283
0:13:06.894,000 --> 0:13:07,000
en el siglo XXI

284
0:13:08.547,000 --> 0:13:1,000
de la misma manera que la automatización de las fábricas

285
0:13:11.334,000 --> 0:13:12,000
y las cadenas de montaje

286
0:13:13.196,000 --> 0:13:16,000
desafiaron el trabajo de los obreros en el siglo XX.

287
0:13:16.196,000 --> 0:13:18,000
Piensen en un técnico de laboratorio

288
0:13:18.288,000 --> 0:13:19,000
que mira en un microscopio

289
0:13:19.783,000 --> 0:13:2,000
una biopsia de cáncer

290
0:13:21.321,000 --> 0:13:23,000
para determinar si es cáncer o no.

291
0:13:23.958,000 --> 0:13:24,000
La persona que fue a la universidad.

292
0:13:25.93,000 --> 0:13:26,000
En el que compra propiedades.

293
0:13:27.36,000 --> 0:13:28,000
Él o ella vota.

294
0:13:29.101,000 --> 0:13:32,000
Él o ella es un constituyente de la sociedad.

295
0:13:32.767,000 --> 0:13:33,000
Y el trabajo de esa persona,

296
0:13:34.161,000 --> 0:13:35,000
así como toda una flota

297
0:13:35.77,000 --> 0:13:36,000
de profesionales como esa persona,

298
0:13:37.739,000 --> 0:13:4,000
se encontrará que sus puestos de trabajo han cambiado radicalmente

299
0:13:40.889,000 --> 0:13:42,000
o, en realidad, se han eliminado completamente.

300
0:13:43.246,000 --> 0:13:44,000
Ahora, nos gusta pensar

301
0:13:44.53,000 --> 0:13:47,000
que la tecnología crea puestos de trabajo

302
0:13:47.717,000 --> 0:13:5,000
después de un corto período de dislocación temporal,

303
0:13:51.182,000 --> 0:13:52,000
y es cierto para el marco de referencia

304
0:13:53.123,000 --> 0:13:55,000
de la Revolución Industrial, que vivimos,

305
0:13:55.265,000 --> 0:13:57,000
porque eso es precisamente lo que ocurrió.

306
0:13:57.593,000 --> 0:13:59,000
Pero nos olvidamos de algo en el análisis:

307
0:13:59.926,000 --> 0:14:,000
Hay algunas categorías de empleos

308
0:14:01.756,000 --> 0:14:04,000
que simplemente se eliminan y no se crean nunca más.

309
0:14:05.176,000 --> 0:14:07,000
La Revolución Industrial no era muy buena

310
0:14:07.18,000 --> 0:14:1,000
si eras un caballo.

311
0:14:11.182,000 --> 0:14:13,000
Así que tendremos que tener cuidado

312
0:14:13.237,000 --> 0:14:16,000
y tomar datos masivos y ajustarlos a nuestras necesidades,

313
0:14:16.751,000 --> 0:14:19,000
a nuestras necesidades muy humanas.

314
0:14:19.936,000 --> 0:14:21,000
Tenemos que ser los dueños de esta tecnología,

315
0:14:22.249,000 --> 0:14:23,000
no sus siervos.

316
0:14:23.546,000 --> 0:14:25,000
Estamos justo en el comienzo de la era de los datos masivos,

317
0:14:26.504,000 --> 0:14:29,000
y honestamente, no somos muy buenos

318
0:14:29.654,000 --> 0:14:33,000
en el manejo de todos los datos que ahora podemos recoger.

319
0:14:33.861,000 --> 0:14:36,000
No es solo un problema para la Agencia de Seguridad Nacional.

320
0:14:37.117,000 --> 0:14:4,000
Las empresas recogen muchos datos, y también, hacen mal uso de ellos,

321
0:14:40.415,000 --> 0:14:43,000
y tenemos que mejorar en esto, y esto tomará tiempo.

322
0:14:43.896,000 --> 0:14:44,000
Es un poco como el desafío que enfrentó

323
0:14:45.891,000 --> 0:14:47,000
el hombre primitivo y el fuego.

324
0:14:48.125,000 --> 0:14:49,000
Es una herramienta, pero que,

325
0:14:50.01,000 --> 0:14:53,000
a menos que seamos cuidadosos, nos va a quemar.

326
0:14:56.008,000 --> 0:14:59,000
Los datos masivos transformarán la manera en que vivimos,

327
0:14:59.128,000 --> 0:15:01,000
cómo trabajamos y cómo pensamos.

328
0:15:01.929,000 --> 0:15:02,000
Nos ayudarán con nuestras carreras

329
0:15:03.818,000 --> 0:15:06,000
y a llevar una vida de satisfacción y esperanza

330
0:15:07.452,000 --> 0:15:09,000
y felicidad y salud,

331
0:15:10.444,000 --> 0:15:13,000
pero en el pasado, frecuentemente, vimos esa tecnología

332
0:15:13.75,000 --> 0:15:15,000
y nuestros ojos solo han visto la T

333
0:15:15.958,000 --> 0:15:16,000
la tecnología, el hardware,

334
0:15:17.644,000 --> 0:15:19,000
porque eso es físico.

335
0:15:19.906,000 --> 0:15:21,000
Ahora tenemos que reformular nuestra mirada a la I,

336
0:15:22.83,000 --> 0:15:23,000
la información,

337
0:15:24.21,000 --> 0:15:25,000
que es menos tangible,

338
0:15:25.583,000 --> 0:15:28,000
pero en algunos aspectos mucho más importante.

339
0:15:29.692,000 --> 0:15:32,000
La humanidad finalmente puede aprender de la información

340
0:15:33.157,000 --> 0:15:35,000
que puede recoger,

341
0:15:35.575,000 --> 0:15:37,000
como parte de nuestra búsqueda eterna

342
0:15:37.69,000 --> 0:15:4,000
para entender el mundo y nuestro lugar en él,

343
0:15:40.849,000 --> 0:15:44,000
y por eso los datos masivos es un gran asunto.

344
0:15:46.681,000 --> 0:15:49,000
(Aplausos)

