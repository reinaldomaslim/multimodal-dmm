1
0:00:12.96,000 --> 0:00:13,000
I want you to imagine

2
0:00:15,000 --> 0:00:16,000
walking into a room,

3
0:00:17.48,000 --> 0:00:19,000
a control room with a bunch of people,

4
0:00:19.64,000 --> 0:00:21,000
a hundred people, hunched over a desk with little dials,

5
0:00:23.28,000 --> 0:00:24,000
and that that control room

6
0:00:25.68,000 --> 0:00:28,000
will shape the thoughts and feelings

7
0:00:29.4,000 --> 0:00:3,000
of a billion people.

8
0:00:32.56,000 --> 0:00:33,000
This might sound like science fiction,

9
0:00:35.32,000 --> 0:00:37,000
but this actually exists

10
0:00:37.56,000 --> 0:00:38,000
right now, today.

11
0:00:40.04,000 --> 0:00:43,000
I know because I used to be in one of those control rooms.

12
0:00:44.159,000 --> 0:00:46,000
I was a design ethicist at Google,

13
0:00:46.48,000 --> 0:00:49,000
where I studied how do you ethically steer people's thoughts?

14
0:00:50.56,000 --> 0:00:52,000
Because what we don't talk about is how the handful of people

15
0:00:53.48,000 --> 0:00:55,000
working at a handful of technology companies

16
0:00:56.04,000 --> 0:01:01,000
through their choices will steer what a billion people are thinking today.

17
0:01:02.4,000 --> 0:01:03,000
Because when you pull out your phone

18
0:01:04.16,000 --> 0:01:07,000
and they design how this works or what's on the feed,

19
0:01:07.28,000 --> 0:01:1,000
it's scheduling little blocks of time in our minds.

20
0:01:10.52,000 --> 0:01:13,000
If you see a notification, it schedules you to have thoughts

21
0:01:13.68,000 --> 0:01:15,000
that maybe you didn't intend to have.

22
0:01:16.4,000 --> 0:01:18,000
If you swipe over that notification,

23
0:01:19.04,000 --> 0:01:21,000
it schedules you into spending a little bit of time

24
0:01:21.445,000 --> 0:01:22,000
getting sucked into something

25
0:01:22.85,000 --> 0:01:24,000
that maybe you didn't intend to get sucked into.

26
0:01:27.32,000 --> 0:01:28,000
When we talk about technology,

27
0:01:30.04,000 --> 0:01:32,000
we tend to talk about it as this blue sky opportunity.

28
0:01:32.76,000 --> 0:01:33,000
It could go any direction.

29
0:01:35.4,000 --> 0:01:36,000
And I want to get serious for a moment

30
0:01:37.28,000 --> 0:01:39,000
and tell you why it's going in a very specific direction.

31
0:01:40.84,000 --> 0:01:42,000
Because it's not evolving randomly.

32
0:01:44,000 --> 0:01:46,000
There's a hidden goal driving the direction

33
0:01:46.04,000 --> 0:01:48,000
of all of the technology we make,

34
0:01:48.2,000 --> 0:01:5,000
and that goal is the race for our attention.

35
0:01:52.84,000 --> 0:01:54,000
Because every news site,

36
0:01:55.6,000 --> 0:01:57,000
TED, elections, politicians,

37
0:01:58.36,000 --> 0:01:59,000
games, even meditation apps

38
0:02:00.36,000 --> 0:02:01,000
have to compete for one thing,

39
0:02:03.16,000 --> 0:02:04,000
which is our attention,

40
0:02:04.92,000 --> 0:02:05,000
and there's only so much of it.

41
0:02:08.44,000 --> 0:02:1,000
And the best way to get people's attention

42
0:02:10.88,000 --> 0:02:12,000
is to know how someone's mind works.

43
0:02:13.8,000 --> 0:02:15,000
And there's a whole bunch of persuasive techniques

44
0:02:16.16,000 --> 0:02:19,000
that I learned in college at a lab called the Persuasive Technology Lab

45
0:02:19.68,000 --> 0:02:2,000
to get people's attention.

46
0:02:21.88,000 --> 0:02:22,000
A simple example is YouTube.

47
0:02:24,000 --> 0:02:26,000
YouTube wants to maximize how much time you spend.

48
0:02:26.96,000 --> 0:02:27,000
And so what do they do?

49
0:02:28.84,000 --> 0:02:3,000
They autoplay the next video.

50
0:02:31.76,000 --> 0:02:32,000
And let's say that works really well.

51
0:02:33.6,000 --> 0:02:35,000
They're getting a little bit more of people's time.

52
0:02:36.04,000 --> 0:02:38,000
Well, if you're Netflix, you look at that and say,

53
0:02:38.44,000 --> 0:02:39,000
well, that's shrinking my market share,

54
0:02:40.322,000 --> 0:02:42,000
so I'm going to autoplay the next episode.

55
0:02:43.32,000 --> 0:02:44,000
But then if you're Facebook,

56
0:02:44.72,000 --> 0:02:46,000
you say, that's shrinking all of my market share,

57
0:02:47.08,000 --> 0:02:49,000
so now I have to autoplay all the videos in the newsfeed

58
0:02:49.76,000 --> 0:02:5,000
before waiting for you to click play.

59
0:02:52.32,000 --> 0:02:55,000
So the internet is not evolving at random.

60
0:02:56.32,000 --> 0:03:,000
The reason it feels like it's sucking us in the way it is

61
0:03:00.76,000 --> 0:03:02,000
is because of this race for attention.

62
0:03:03.16,000 --> 0:03:04,000
We know where this is going.

63
0:03:04.6,000 --> 0:03:05,000
Technology is not neutral,

64
0:03:07.32,000 --> 0:03:1,000
and it becomes this race to the bottom of the brain stem

65
0:03:10.76,000 --> 0:03:12,000
of who can go lower to get it.

66
0:03:13.92,000 --> 0:03:15,000
Let me give you an example of Snapchat.

67
0:03:16.28,000 --> 0:03:19,000
If you didn't know, Snapchat is the number one way

68
0:03:2,000 --> 0:03:22,000
that teenagers in the United States communicate.

69
0:03:22.28,000 --> 0:03:26,000
So if you're like me, and you use text messages to communicate,

70
0:03:26.48,000 --> 0:03:27,000
Snapchat is that for teenagers,

71
0:03:28.28,000 --> 0:03:3,000
and there's, like, a hundred million of them that use it.

72
0:03:31,000 --> 0:03:33,000
And they invented a feature called Snapstreaks,

73
0:03:33.24,000 --> 0:03:34,000
which shows the number of days in a row

74
0:03:35.16,000 --> 0:03:37,000
that two people have communicated with each other.

75
0:03:37.8,000 --> 0:03:38,000
In other words, what they just did

76
0:03:39.68,000 --> 0:03:41,000
is they gave two people something they don't want to lose.

77
0:03:44,000 --> 0:03:47,000
Because if you're a teenager, and you have 150 days in a row,

78
0:03:47.48,000 --> 0:03:48,000
you don't want that to go away.

79
0:03:49.48,000 --> 0:03:53,000
And so think of the little blocks of time that that schedules in kids' minds.

80
0:03:54.16,000 --> 0:03:56,000
This isn't theoretical: when kids go on vacation,

81
0:03:56.52,000 --> 0:03:59,000
it's been shown they give their passwords to up to five other friends

82
0:03:59.8,000 --> 0:04:01,000
to keep their Snapstreaks going,

83
0:04:02.04,000 --> 0:04:04,000
even when they can't do it.

84
0:04:04.08,000 --> 0:04:05,000
And they have, like, 30 of these things,

85
0:04:06.04,000 --> 0:04:09,000
and so they have to get through taking photos of just pictures or walls

86
0:04:09.44,000 --> 0:04:11,000
or ceilings just to get through their day.

87
0:04:13.2,000 --> 0:04:15,000
So it's not even like they're having real conversations.

88
0:04:15.92,000 --> 0:04:16,000
We have a temptation to think about this

89
0:04:17.88,000 --> 0:04:19,000
as, oh, they're just using Snapchat

90
0:04:20.6,000 --> 0:04:22,000
the way we used to gossip on the telephone.

91
0:04:22.64,000 --> 0:04:23,000
It's probably OK.

92
0:04:24.48,000 --> 0:04:26,000
Well, what this misses is that in the 1970s,

93
0:04:26.76,000 --> 0:04:28,000
when you were just gossiping on the telephone,

94
0:04:29.399,000 --> 0:04:32,000
there wasn't a hundred engineers on the other side of the screen

95
0:04:32.44,000 --> 0:04:34,000
who knew exactly how your psychology worked

96
0:04:34.52,000 --> 0:04:36,000
and orchestrated you into a double bind with each other.

97
0:04:38.44,000 --> 0:04:41,000
Now, if this is making you feel a little bit of outrage,

98
0:04:42.68,000 --> 0:04:44,000
notice that that thought just comes over you.

99
0:04:45.28,000 --> 0:04:48,000
Outrage is a really good way also of getting your attention,

100
0:04:49.88,000 --> 0:04:5,000
because we don't choose outrage.

101
0:04:51.48,000 --> 0:04:52,000
It happens to us.

102
0:04:52.92,000 --> 0:04:53,000
And if you're the Facebook newsfeed,

103
0:04:54.8,000 --> 0:04:55,000
whether you'd want to or not,

104
0:04:56.24,000 --> 0:04:58,000
you actually benefit when there's outrage.

105
0:04:59,000 --> 0:05:01,000
Because outrage doesn't just schedule a reaction

106
0:05:01.96,000 --> 0:05:03,000
in emotional time, space, for you.

107
0:05:05.44,000 --> 0:05:07,000
We want to share that outrage with other people.

108
0:05:07.88,000 --> 0:05:08,000
So we want to hit share and say,

109
0:05:09.48,000 --> 0:05:11,000
"Can you believe the thing that they said?"

110
0:05:12.52,000 --> 0:05:15,000
And so outrage works really well at getting attention,

111
0:05:15.92,000 --> 0:05:18,000
such that if Facebook had a choice between showing you the outrage feed

112
0:05:19.84,000 --> 0:05:2,000
and a calm newsfeed,

113
0:05:22.12,000 --> 0:05:24,000
they would want to show you the outrage feed,

114
0:05:24.28,000 --> 0:05:26,000
not because someone consciously chose that,

115
0:05:26.36,000 --> 0:05:28,000
but because that worked better at getting your attention.

116
0:05:31.12,000 --> 0:05:36,000
And the newsfeed control room is not accountable to us.

117
0:05:37.04,000 --> 0:05:39,000
It's only accountable to maximizing attention.

118
0:05:39.36,000 --> 0:05:4,000
It's also accountable,

119
0:05:40.6,000 --> 0:05:42,000
because of the business model of advertising,

120
0:05:43,000 --> 0:05:46,000
for anybody who can pay the most to actually walk into the control room

121
0:05:46.36,000 --> 0:05:47,000
and say, "That group over there,

122
0:05:47.96,000 --> 0:05:49,000
I want to schedule these thoughts into their minds."

123
0:05:51.76,000 --> 0:05:52,000
So you can target,

124
0:05:54.04,000 --> 0:05:55,000
you can precisely target a lie

125
0:05:56,000 --> 0:05:58,000
directly to the people who are most susceptible.

126
0:06:00.08,000 --> 0:06:02,000
And because this is profitable, it's only going to get worse.

127
0:06:05.04,000 --> 0:06:06,000
So I'm here today

128
0:06:08.16,000 --> 0:06:1,000
because the costs are so obvious.

129
0:06:12.28,000 --> 0:06:14,000
I don't know a more urgent problem than this,

130
0:06:14.44,000 --> 0:06:17,000
because this problem is underneath all other problems.

131
0:06:18.72,000 --> 0:06:21,000
It's not just taking away our agency

132
0:06:21.92,000 --> 0:06:23,000
to spend our attention and live the lives that we want,

133
0:06:25.72,000 --> 0:06:28,000
it's changing the way that we have our conversations,

134
0:06:29.28,000 --> 0:06:3,000
it's changing our democracy,

135
0:06:31.04,000 --> 0:06:33,000
and it's changing our ability to have the conversations

136
0:06:33.68,000 --> 0:06:35,000
and relationships we want with each other.

137
0:06:37.16,000 --> 0:06:38,000
And it affects everyone,

138
0:06:38.96,000 --> 0:06:41,000
because a billion people have one of these in their pocket.

139
0:06:45.36,000 --> 0:06:46,000
So how do we fix this?

140
0:06:49.08,000 --> 0:06:51,000
We need to make three radical changes

141
0:06:52.04,000 --> 0:06:53,000
to technology and to our society.

142
0:06:55.72,000 --> 0:06:58,000
The first is we need to acknowledge that we are persuadable.

143
0:07:00.84,000 --> 0:07:01,000
Once you start understanding

144
0:07:02.24,000 --> 0:07:04,000
that your mind can be scheduled into having little thoughts

145
0:07:05.04,000 --> 0:07:07,000
or little blocks of time that you didn't choose,

146
0:07:07.64,000 --> 0:07:09,000
wouldn't we want to use that understanding

147
0:07:09.72,000 --> 0:07:11,000
and protect against the way that that happens?

148
0:07:12.6,000 --> 0:07:15,000
I think we need to see ourselves fundamentally in a new way.

149
0:07:15.92,000 --> 0:07:17,000
It's almost like a new period of human history,

150
0:07:18.16,000 --> 0:07:19,000
like the Enlightenment,

151
0:07:19.4,000 --> 0:07:21,000
but almost a kind of self-aware Enlightenment,

152
0:07:21.64,000 --> 0:07:23,000
that we can be persuaded,

153
0:07:24.32,000 --> 0:07:26,000
and there might be something we want to protect.

154
0:07:27.4,000 --> 0:07:31,000
The second is we need new models and accountability systems

155
0:07:32,000 --> 0:07:35,000
so that as the world gets better and more and more persuasive over time --

156
0:07:35.52,000 --> 0:07:37,000
because it's only going to get more persuasive --

157
0:07:37.88,000 --> 0:07:38,000
that the people in those control rooms

158
0:07:39.76,000 --> 0:07:41,000
are accountable and transparent to what we want.

159
0:07:42.24,000 --> 0:07:44,000
The only form of ethical persuasion that exists

160
0:07:44.96,000 --> 0:07:45,000
is when the goals of the persuader

161
0:07:46.92,000 --> 0:07:48,000
are aligned with the goals of the persuadee.

162
0:07:49.64,000 --> 0:07:52,000
And that involves questioning big things, like the business model of advertising.

163
0:07:54.72,000 --> 0:07:55,000
Lastly,

164
0:07:56.32,000 --> 0:07:57,000
we need a design renaissance,

165
0:07:59.08,000 --> 0:08:02,000
because once you have this view of human nature,

166
0:08:02.16,000 --> 0:08:04,000
that you can steer the timelines of a billion people --

167
0:08:05.16,000 --> 0:08:07,000
just imagine, there's people who have some desire

168
0:08:07.92,000 --> 0:08:09,000
about what they want to do and what they want to be thinking

169
0:08:10.8,000 --> 0:08:13,000
and what they want to be feeling and how they want to be informed,

170
0:08:13.96,000 --> 0:08:15,000
and we're all just tugged into these other directions.

171
0:08:16.52,000 --> 0:08:19,000
And you have a billion people just tugged into all these different directions.

172
0:08:20.24,000 --> 0:08:22,000
Well, imagine an entire design renaissance

173
0:08:22.32,000 --> 0:08:25,000
that tried to orchestrate the exact and most empowering

174
0:08:25.44,000 --> 0:08:28,000
time-well-spent way for those timelines to happen.

175
0:08:28.6,000 --> 0:08:29,000
And that would involve two things:

176
0:08:30.28,000 --> 0:08:32,000
one would be protecting against the timelines

177
0:08:32.44,000 --> 0:08:33,000
that we don't want to be experiencing,

178
0:08:34.32,000 --> 0:08:36,000
the thoughts that we wouldn't want to be happening,

179
0:08:36.76,000 --> 0:08:39,000
so that when that ding happens, not having the ding that sends us away;

180
0:08:40.12,000 --> 0:08:43,000
and the second would be empowering us to live out the timeline that we want.

181
0:08:43.76,000 --> 0:08:44,000
So let me give you a concrete example.

182
0:08:46.28,000 --> 0:08:48,000
Today, let's say your friend cancels dinner on you,

183
0:08:48.76,000 --> 0:08:51,000
and you are feeling a little bit lonely.

184
0:08:52.559,000 --> 0:08:53,000
And so what do you do in that moment?

185
0:08:54.4,000 --> 0:08:55,000
You open up Facebook.

186
0:08:56.96,000 --> 0:08:57,000
And in that moment,

187
0:08:58.68,000 --> 0:09:01,000
the designers in the control room want to schedule exactly one thing,

188
0:09:02.08,000 --> 0:09:05,000
which is to maximize how much time you spend on the screen.

189
0:09:06.64,000 --> 0:09:09,000
Now, instead, imagine if those designers created a different timeline

190
0:09:10.56,000 --> 0:09:13,000
that was the easiest way, using all of their data,

191
0:09:14.08,000 --> 0:09:17,000
to actually help you get out with the people that you care about?

192
0:09:17.2,000 --> 0:09:22,000
Just think, alleviating all loneliness in society,

193
0:09:22.64,000 --> 0:09:25,000
if that was the timeline that Facebook wanted to make possible for people.

194
0:09:26.16,000 --> 0:09:27,000
Or imagine a different conversation.

195
0:09:27.899,000 --> 0:09:3,000
Let's say you wanted to post something supercontroversial on Facebook,

196
0:09:31.24,000 --> 0:09:33,000
which is a really important thing to be able to do,

197
0:09:33.68,000 --> 0:09:34,000
to talk about controversial topics.

198
0:09:35.4,000 --> 0:09:37,000
And right now, when there's that big comment box,

199
0:09:37.76,000 --> 0:09:4,000
it's almost asking you, what key do you want to type?

200
0:09:41.16,000 --> 0:09:43,000
In other words, it's scheduling a little timeline of things

201
0:09:44,000 --> 0:09:46,000
you're going to continue to do on the screen.

202
0:09:46.16,000 --> 0:09:48,000
And imagine instead that there was another button there saying,

203
0:09:49.16,000 --> 0:09:51,000
what would be most time well spent for you?

204
0:09:51.24,000 --> 0:09:52,000
And you click "host a dinner."

205
0:09:52.84,000 --> 0:09:54,000
And right there underneath the item it said,

206
0:09:54.96,000 --> 0:09:55,000
"Who wants to RSVP for the dinner?"

207
0:09:56.68,000 --> 0:09:59,000
And so you'd still have a conversation about something controversial,

208
0:09:59.96,000 --> 0:10:02,000
but you'd be having it in the most empowering place on your timeline,

209
0:10:03.72,000 --> 0:10:06,000
which would be at home that night with a bunch of a friends over

210
0:10:06.76,000 --> 0:10:07,000
to talk about it.

211
0:10:09,000 --> 0:10:12,000
So imagine we're running, like, a find and replace

212
0:10:13,000 --> 0:10:15,000
on all of the timelines that are currently steering us

213
0:10:15.6,000 --> 0:10:17,000
towards more and more screen time persuasively

214
0:10:19.08,000 --> 0:10:21,000
and replacing all of those timelines

215
0:10:21.64,000 --> 0:10:22,000
with what do we want in our lives.

216
0:10:26.96,000 --> 0:10:27,000
It doesn't have to be this way.

217
0:10:30.36,000 --> 0:10:32,000
Instead of handicapping our attention,

218
0:10:32.64,000 --> 0:10:34,000
imagine if we used all of this data and all of this power

219
0:10:35.48,000 --> 0:10:36,000
and this new view of human nature

220
0:10:37.12,000 --> 0:10:39,000
to give us a superhuman ability to focus

221
0:10:4,000 --> 0:10:44,000
and a superhuman ability to put our attention to what we cared about

222
0:10:44.16,000 --> 0:10:46,000
and a superhuman ability to have the conversations

223
0:10:46.8,000 --> 0:10:48,000
that we need to have for democracy.

224
0:10:51.6,000 --> 0:10:53,000
The most complex challenges in the world

225
0:10:56.28,000 --> 0:10:59,000
require not just us to use our attention individually.

226
0:11:00.44,000 --> 0:11:03,000
They require us to use our attention and coordinate it together.

227
0:11:04.44,000 --> 0:11:06,000
Climate change is going to require that a lot of people

228
0:11:07.28,000 --> 0:11:09,000
are being able to coordinate their attention

229
0:11:09.4,000 --> 0:11:1,000
in the most empowering way together.

230
0:11:11.32,000 --> 0:11:14,000
And imagine creating a superhuman ability to do that.

231
0:11:19,000 --> 0:11:23,000
Sometimes the world's most pressing and important problems

232
0:11:24.04,000 --> 0:11:27,000
are not these hypothetical future things that we could create in the future.

233
0:11:28.56,000 --> 0:11:29,000
Sometimes the most pressing problems

234
0:11:30.32,000 --> 0:11:32,000
are the ones that are right underneath our noses,

235
0:11:32.68,000 --> 0:11:35,000
the things that are already directing a billion people's thoughts.

236
0:11:36.6,000 --> 0:11:39,000
And maybe instead of getting excited about the new augmented reality

237
0:11:4,000 --> 0:11:43,000
and virtual reality and these cool things that could happen,

238
0:11:43.32,000 --> 0:11:46,000
which are going to be susceptible to the same race for attention,

239
0:11:46.64,000 --> 0:11:48,000
if we could fix the race for attention

240
0:11:48.84,000 --> 0:11:5,000
on the thing that's already in a billion people's pockets.

241
0:11:52.04,000 --> 0:11:53,000
Maybe instead of getting excited

242
0:11:53.64,000 --> 0:11:57,000
about the most exciting new cool fancy education apps,

243
0:11:57.84,000 --> 0:11:59,000
we could fix the way kids' minds are getting manipulated

244
0:12:00.76,000 --> 0:12:02,000
into sending empty messages back and forth.

245
0:12:04.04,000 --> 0:12:08,000
(Applause)

246
0:12:08.36,000 --> 0:12:09,000
Maybe instead of worrying

247
0:12:09.64,000 --> 0:12:12,000
about hypothetical future runaway artificial intelligences

248
0:12:13.44,000 --> 0:12:14,000
that are maximizing for one goal,

249
0:12:16.68,000 --> 0:12:18,000
we could solve the runaway artificial intelligence

250
0:12:19.36,000 --> 0:12:21,000
that already exists right now,

251
0:12:21.44,000 --> 0:12:23,000
which are these newsfeeds maximizing for one thing.

252
0:12:26.08,000 --> 0:12:29,000
It's almost like instead of running away to colonize new planets,

253
0:12:29.92,000 --> 0:12:31,000
we could fix the one that we're already on.

254
0:12:32,000 --> 0:12:36,000
(Applause)

255
0:12:40.04,000 --> 0:12:41,000
Solving this problem

256
0:12:41.84,000 --> 0:12:44,000
is critical infrastructure for solving every other problem.

257
0:12:46.6,000 --> 0:12:5,000
There's nothing in your life or in our collective problems

258
0:12:50.64,000 --> 0:12:53,000
that does not require our ability to put our attention where we care about.

259
0:12:55.8,000 --> 0:12:56,000
At the end of our lives,

260
0:12:58.24,000 --> 0:13:,000
all we have is our attention and our time.

261
0:13:01.8,000 --> 0:13:02,000
What will be time well spent for ours?

262
0:13:03.72,000 --> 0:13:04,000
Thank you.

263
0:13:04.96,000 --> 0:13:07,000
(Applause)

264
0:13:17.76,000 --> 0:13:19,000
Chris Anderson: Tristan, thank you. Hey, stay up here a sec.

265
0:13:20.72,000 --> 0:13:21,000
First of all, thank you.

266
0:13:22.08,000 --> 0:13:24,000
I know we asked you to do this talk on pretty short notice,

267
0:13:24.88,000 --> 0:13:26,000
and you've had quite a stressful week

268
0:13:27.12,000 --> 0:13:29,000
getting this thing together, so thank you.

269
0:13:30.68,000 --> 0:13:33,000
Some people listening might say, what you complain about is addiction,

270
0:13:34.68,000 --> 0:13:37,000
and all these people doing this stuff, for them it's actually interesting.

271
0:13:38.2,000 --> 0:13:39,000
All these design decisions

272
0:13:39.48,000 --> 0:13:42,000
have built user content that is fantastically interesting.

273
0:13:42.6,000 --> 0:13:44,000
The world's more interesting than it ever has been.

274
0:13:45.04,000 --> 0:13:46,000
What's wrong with that?

275
0:13:46.32,000 --> 0:13:48,000
Tristan Harris: I think it's really interesting.

276
0:13:48.6,000 --> 0:13:52,000
One way to see this is if you're just YouTube, for example,

277
0:13:52.64,000 --> 0:13:54,000
you want to always show the more interesting next video.

278
0:13:55.32,000 --> 0:13:58,000
You want to get better and better at suggesting that next video,

279
0:13:58.36,000 --> 0:14:,000
but even if you could propose the perfect next video

280
0:14:00.84,000 --> 0:14:01,000
that everyone would want to watch,

281
0:14:02.52,000 --> 0:14:05,000
it would just be better and better at keeping you hooked on the screen.

282
0:14:05.88,000 --> 0:14:06,000
So what's missing in that equation

283
0:14:07.56,000 --> 0:14:09,000
is figuring out what our boundaries would be.

284
0:14:09.72,000 --> 0:14:12,000
You would want YouTube to know something about, say, falling asleep.

285
0:14:12.96,000 --> 0:14:13,000
The CEO of Netflix recently said,

286
0:14:14.6,000 --> 0:14:16,000
"our biggest competitors are Facebook, YouTube and sleep."

287
0:14:17.36,000 --> 0:14:21,000
And so what we need to recognize is that the human architecture is limited

288
0:14:21.84,000 --> 0:14:23,000
and that we have certain boundaries or dimensions of our lives

289
0:14:24.84,000 --> 0:14:25,000
that we want to be honored and respected,

290
0:14:26.84,000 --> 0:14:27,000
and technology could help do that.

291
0:14:28.68,000 --> 0:14:3,000
(Applause)

292
0:14:31.32,000 --> 0:14:32,000
CA: I mean, could you make the case

293
0:14:33.04,000 --> 0:14:39,000
that part of the problem here is that we've got a naïve model of human nature?

294
0:14:39.12,000 --> 0:14:41,000
So much of this is justified in terms of human preference,

295
0:14:41.88,000 --> 0:14:43,000
where we've got these algorithms that do an amazing job

296
0:14:44.52,000 --> 0:14:45,000
of optimizing for human preference,

297
0:14:46.24,000 --> 0:14:47,000
but which preference?

298
0:14:47.6,000 --> 0:14:5,000
There's the preferences of things that we really care about

299
0:14:51.12,000 --> 0:14:52,000
when we think about them

300
0:14:52.52,000 --> 0:14:55,000
versus the preferences of what we just instinctively click on.

301
0:14:55.6,000 --> 0:14:59,000
If we could implant that more nuanced view of human nature in every design,

302
0:15:00.28,000 --> 0:15:01,000
would that be a step forward?

303
0:15:01.76,000 --> 0:15:02,000
TH: Absolutely. I mean, I think right now

304
0:15:03.76,000 --> 0:15:06,000
it's as if all of our technology is basically only asking our lizard brain

305
0:15:07.28,000 --> 0:15:09,000
what's the best way to just impulsively get you to do

306
0:15:09.8,000 --> 0:15:11,000
the next tiniest thing with your time,

307
0:15:11.96,000 --> 0:15:12,000
instead of asking you in your life

308
0:15:13.64,000 --> 0:15:15,000
what we would be most time well spent for you?

309
0:15:15.84,000 --> 0:15:18,000
What would be the perfect timeline that might include something later,

310
0:15:19.16,000 --> 0:15:22,000
would be time well spent for you here at TED in your last day here?

311
0:15:22.36,000 --> 0:15:24,000
CA: So if Facebook and Google and everyone said to us first up,

312
0:15:25.36,000 --> 0:15:27,000
"Hey, would you like us to optimize for your reflective brain

313
0:15:28.28,000 --> 0:15:29,000
or your lizard brain? You choose."

314
0:15:29.96,000 --> 0:15:31,000
TH: Right. That would be one way. Yes.

315
0:15:34.358,000 --> 0:15:36,000
CA: You said persuadability, that's an interesting word to me

316
0:15:37.24,000 --> 0:15:39,000
because to me there's two different types of persuadability.

317
0:15:40.12,000 --> 0:15:42,000
There's the persuadability that we're trying right now

318
0:15:42.68,000 --> 0:15:44,000
of reason and thinking and making an argument,

319
0:15:44.88,000 --> 0:15:46,000
but I think you're almost talking about a different kind,

320
0:15:47.59,000 --> 0:15:48,000
a more visceral type of persuadability,

321
0:15:49.52,000 --> 0:15:51,000
of being persuaded without even knowing that you're thinking.

322
0:15:52.44,000 --> 0:15:54,000
TH: Exactly. The reason I care about this problem so much is

323
0:15:55.32,000 --> 0:15:58,000
I studied at a lab called the Persuasive Technology Lab at Stanford

324
0:15:58.52,000 --> 0:16:,000
that taught [students how to recognize] exactly these techniques.

325
0:16:01.106,000 --> 0:16:03,000
There's conferences and workshops that teach people all these covert ways

326
0:16:04.12,000 --> 0:16:06,000
of getting people's attention and orchestrating people's lives.

327
0:16:07.12,000 --> 0:16:09,000
And it's because most people don't know that that exists

328
0:16:09.8,000 --> 0:16:1,000
that this conversation is so important.

329
0:16:11.72,000 --> 0:16:14,000
CA: Tristan, you and I, we both know so many people from all these companies.

330
0:16:15.52,000 --> 0:16:16,000
There are actually many here in the room,

331
0:16:17.52,000 --> 0:16:19,000
and I don't know about you, but my experience of them

332
0:16:20.021,000 --> 0:16:22,000
is that there is no shortage of good intent.

333
0:16:22.12,000 --> 0:16:24,000
People want a better world.

334
0:16:24.32,000 --> 0:16:27,000
They are actually -- they really want it.

335
0:16:28.32,000 --> 0:16:32,000
And I don't think anything you're saying is that these are evil people.

336
0:16:32.52,000 --> 0:16:35,000
It's a system where there's these unintended consequences

337
0:16:36.24,000 --> 0:16:37,000
that have really got out of control --

338
0:16:38.12,000 --> 0:16:39,000
TH: Of this race for attention.

339
0:16:39.64,000 --> 0:16:42,000
It's the classic race to the bottom when you have to get attention,

340
0:16:42.84,000 --> 0:16:43,000
and it's so tense.

341
0:16:44.08,000 --> 0:16:46,000
The only way to get more is to go lower on the brain stem,

342
0:16:46.84,000 --> 0:16:48,000
to go lower into outrage, to go lower into emotion,

343
0:16:49.28,000 --> 0:16:5,000
to go lower into the lizard brain.

344
0:16:51,000 --> 0:16:54,000
CA: Well, thank you so much for helping us all get a little bit wiser about this.

345
0:16:54.84,000 --> 0:16:56,000
Tristan Harris, thank you. TH: Thank you very much.

346
0:16:57.28,000 --> 0:16:59,000
(Applause)

