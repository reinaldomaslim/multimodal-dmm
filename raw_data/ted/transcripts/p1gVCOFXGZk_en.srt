1
0:00:13.04,000 --> 0:00:15,000
I want to tell you guys something about neuroscience.

2
0:00:16.04,000 --> 0:00:17,000
I'm a physicist by training.

3
0:00:18.23,000 --> 0:00:2,000
About three years ago, I left physics

4
0:00:20.46,000 --> 0:00:22,000
to come and try to understand how the brain works.

5
0:00:22.833,000 --> 0:00:23,000
And this is what I found.

6
0:00:24.331,000 --> 0:00:26,000
Lots of people are working on depression.

7
0:00:26.419,000 --> 0:00:27,000
And that's really good,

8
0:00:27.602,000 --> 0:00:29,000
depression is something that we really want to understand.

9
0:00:30.347,000 --> 0:00:31,000
Here's how you do it:

10
0:00:31.538,000 --> 0:00:35,000
you take a jar and you fill it up, about halfway, with water.

11
0:00:35.723,000 --> 0:00:39,000
And then you take a mouse, and you put the mouse in the jar, OK?

12
0:00:39.929,000 --> 0:00:41,000
And the mouse swims around for a little while

13
0:00:42.303,000 --> 0:00:44,000
and then at some point, the mouse gets tired

14
0:00:44.715,000 --> 0:00:45,000
and decides to stop swimming.

15
0:00:46.673,000 --> 0:00:49,000
And when it stops swimming, that's depression.

16
0:00:50.696,000 --> 0:00:51,000
OK?

17
0:00:52.291,000 --> 0:00:55,000
And I'm from theoretical physics,

18
0:00:55.695,000 --> 0:00:58,000
so I'm used to people making very sophisticated mathematical models

19
0:00:59.387,000 --> 0:01:01,000
to precisely describe physical phenomena,

20
0:01:02.292,000 --> 0:01:04,000
so when I saw that this is the model for depression,

21
0:01:04.768,000 --> 0:01:06,000
I though to myself, "Oh my God, we have a lot of work to do."

22
0:01:07.729,000 --> 0:01:08,000
(Laughter)

23
0:01:09.123,000 --> 0:01:11,000
But this is a kind of general problem in neuroscience.

24
0:01:12.377,000 --> 0:01:14,000
So for example, take emotion.

25
0:01:14.512,000 --> 0:01:16,000
Lots of people want to understand emotion.

26
0:01:17.352,000 --> 0:01:2,000
But you can't study emotion in mice or monkeys

27
0:01:20.689,000 --> 0:01:21,000
because you can't ask them

28
0:01:21.967,000 --> 0:01:23,000
how they're feeling or what they're experiencing.

29
0:01:24.308,000 --> 0:01:26,000
So instead, people who want to understand emotion,

30
0:01:26.689,000 --> 0:01:28,000
typically end up studying what's called motivated behavior,

31
0:01:29.49,000 --> 0:01:32,000
which is code for "what the mouse does when it really, really wants cheese."

32
0:01:33.839,000 --> 0:01:34,000
OK, I could go on and on.

33
0:01:35.538,000 --> 0:01:41,000
I mean, the point is, the NIH spends about 5.5 billion dollars a year

34
0:01:41.878,000 --> 0:01:42,000
on neuroscience research.

35
0:01:43.434,000 --> 0:01:46,000
And yet there have been almost no significant improvements in outcomes

36
0:01:47.061,000 --> 0:01:5,000
for patients with brain diseases in the past 40 years.

37
0:01:51.015,000 --> 0:01:53,000
And I think a lot of that is basically due to the fact

38
0:01:53.579,000 --> 0:01:57,000
that mice might be OK as a model for cancer or diabetes,

39
0:01:57.754,000 --> 0:01:59,000
but the mouse brain is just not sophisticated enough

40
0:02:00.465,000 --> 0:02:03,000
to reproduce human psychology or human brain disease.

41
0:02:04.379,000 --> 0:02:05,000
OK?

42
0:02:05.628,000 --> 0:02:08,000
So if the mouse models are so bad, why are we still using them?

43
0:02:10.143,000 --> 0:02:12,000
Well, it basically boils down to this:

44
0:02:12.27,000 --> 0:02:14,000
the brain is made up of neurons

45
0:02:14.85,000 --> 0:02:17,000
which are these little cells that send electrical signals to each other.

46
0:02:18.68,000 --> 0:02:2,000
If you want to understand how the brain works,

47
0:02:20.848,000 --> 0:02:23,000
you have to be able to measure the electrical activity of these neurons.

48
0:02:25.339,000 --> 0:02:27,000
But to do that, you have to get really close to the neurons

49
0:02:28.355,000 --> 0:02:3,000
with some kind of electrical recording device or a microscope.

50
0:02:31.563,000 --> 0:02:33,000
And so you can do that in mice and you can do it in monkeys,

51
0:02:34.397,000 --> 0:02:36,000
because you can physically put things into their brain

52
0:02:36.969,000 --> 0:02:39,000
but for some reason we still can't do that in humans, OK?

53
0:02:40.533,000 --> 0:02:43,000
So instead, we've invented all these proxies.

54
0:02:43.927,000 --> 0:02:45,000
So the most popular one is probably this,

55
0:02:46.466,000 --> 0:02:48,000
functional MRI, fMRI,

56
0:02:48.887,000 --> 0:02:5,000
which allows you to make these pretty pictures like this,

57
0:02:51.603,000 --> 0:02:53,000
that show which parts of your brain light up

58
0:02:53.683,000 --> 0:02:55,000
when you're engaged in different activities.

59
0:02:55.833,000 --> 0:02:56,000
But this is a proxy.

60
0:02:57.777,000 --> 0:03:,000
You're not actually measuring neural activity here.

61
0:03:01.093,000 --> 0:03:03,000
What you're doing is you're measuring, essentially,

62
0:03:03.959,000 --> 0:03:04,000
like, blood flow in the brain.

63
0:03:05.815,000 --> 0:03:06,000
Where there's more blood.

64
0:03:07.077,000 --> 0:03:1,000
It's actually where there's more oxygen, but you get the idea, OK?

65
0:03:10.204,000 --> 0:03:12,000
The other thing that you can do is you can do this --

66
0:03:12.747,000 --> 0:03:15,000
electroencephalography -- you can put these electrodes on your head, OK?

67
0:03:16.362,000 --> 0:03:18,000
And then you can measure your brain waves.

68
0:03:19.125,000 --> 0:03:22,000
And here, you're actually measuring electrical activity.

69
0:03:22.228,000 --> 0:03:24,000
But you're not measuring the activity of neurons.

70
0:03:24.911,000 --> 0:03:26,000
You're measuring these electrical currents,

71
0:03:27.379,000 --> 0:03:29,000
sloshing back and forth in your brain.

72
0:03:30.157,000 --> 0:03:32,000
So the point is just that these technologies that we have

73
0:03:32.855,000 --> 0:03:34,000
are really measuring the wrong thing.

74
0:03:35.315,000 --> 0:03:37,000
Because, for most of the diseases that we want to understand --

75
0:03:38.292,000 --> 0:03:4,000
like, Parkinson's is the classic example.

76
0:03:40.514,000 --> 0:03:43,000
In Parkinson's, there's one particular kind of neuron deep in your brain

77
0:03:44.092,000 --> 0:03:45,000
that is responsible for the disease,

78
0:03:45.847,000 --> 0:03:48,000
and these technologies just don't have the resolution that you need

79
0:03:49.053,000 --> 0:03:5,000
to get at that.

80
0:03:50.45,000 --> 0:03:53,000
And so that's why we're still stuck with the animals.

81
0:03:54.448,000 --> 0:03:56,000
Not that anyone wants to be studying depression

82
0:03:57.005,000 --> 0:03:59,000
by putting mice into jars, right?

83
0:03:59.291,000 --> 0:04:02,000
It's just that there's this pervasive sense that it's not possible

84
0:04:03.068,000 --> 0:04:06,000
to look at the activity of neurons in healthy humans.

85
0:04:08.18,000 --> 0:04:09,000
So here's what I want to do.

86
0:04:09.974,000 --> 0:04:11,000
I want to take you into the future.

87
0:04:12.519,000 --> 0:04:16,000
To have a look at one way in which I think it could potentially be possible.

88
0:04:17.526,000 --> 0:04:2,000
And I want to preface this by saying, I don't have all the details.

89
0:04:21.272,000 --> 0:04:23,000
So I'm just going to provide you with a kind of outline.

90
0:04:24.263,000 --> 0:04:26,000
But we're going to go the year 2100.

91
0:04:27.732,000 --> 0:04:29,000
Now what does the year 2100 look like?

92
0:04:30.055,000 --> 0:04:33,000
Well, to start with, the climate is a bit warmer that what you're used to.

93
0:04:33.597,000 --> 0:04:36,000
(Laughter)

94
0:04:37.204,000 --> 0:04:41,000
And that robotic vacuum cleaner that you know and love

95
0:04:42.18,000 --> 0:04:43,000
went through a few generations,

96
0:04:43.718,000 --> 0:04:45,000
and the improvements were not always so good.

97
0:04:46.585,000 --> 0:04:47,000
(Laughter)

98
0:04:48.53,000 --> 0:04:5,000
It was not always for the better.

99
0:04:52.221,000 --> 0:04:56,000
But actually, in the year 2100 most things are surprisingly recognizable.

100
0:04:57.458,000 --> 0:04:59,000
It's just the brain is totally different.

101
0:05:00.74,000 --> 0:05:02,000
For example, in the year 2100,

102
0:05:03.311,000 --> 0:05:05,000
we understand the root causes of Alzheimer's.

103
0:05:06.192,000 --> 0:05:09,000
So we can deliver targeted genetic therapies or drugs

104
0:05:09.93,000 --> 0:05:11,000
to stop the degenerative process before it begins.

105
0:05:13.629,000 --> 0:05:14,000
So how did we do it?

106
0:05:15.898,000 --> 0:05:17,000
Well, there were essentially three steps.

107
0:05:18.589,000 --> 0:05:2,000
The first step was that we had to figure out

108
0:05:21.427,000 --> 0:05:24,000
some way to get electrical connections through the skull

109
0:05:24.744,000 --> 0:05:27,000
so we could measure the electrical activity of neurons.

110
0:05:28.339,000 --> 0:05:32,000
And not only that, it had to be easy and risk-free.

111
0:05:32.712,000 --> 0:05:34,000
Something that basically anyone would be OK with,

112
0:05:35.114,000 --> 0:05:36,000
like getting a piercing.

113
0:05:37.156,000 --> 0:05:39,000
Because back in 2017,

114
0:05:39.927,000 --> 0:05:41,000
the only way that we knew of to get through the skull

115
0:05:42.864,000 --> 0:05:44,000
was to drill these holes the size of quarters.

116
0:05:46.015,000 --> 0:05:48,000
You would never let someone do that to you.

117
0:05:48.967,000 --> 0:05:5,000
So in the 2020s,

118
0:05:51.244,000 --> 0:05:54,000
people began to experiment -- rather than drilling these gigantic holes,

119
0:05:54.649,000 --> 0:05:57,000
drilling microscopic holes, no thicker than a piece of hair.

120
0:05:58.735,000 --> 0:06:,000
And the idea here was really for diagnosis --

121
0:06:00.855,000 --> 0:06:02,000
there are lots of times in the diagnosis of brain disorders

122
0:06:03.665,000 --> 0:06:07,000
when you would like to be able to look at the neural activity beneath the skull

123
0:06:08.561,000 --> 0:06:11,000
and being able to drill these microscopic holes

124
0:06:11.776,000 --> 0:06:13,000
would make that much easier for the patient.

125
0:06:13.942,000 --> 0:06:15,000
In the end, it would be like getting a shot.

126
0:06:16.315,000 --> 0:06:17,000
You just go in and you sit down

127
0:06:17.919,000 --> 0:06:19,000
and there's a thing that comes down on your head,

128
0:06:20.244,000 --> 0:06:21,000
and a momentary sting and then it's done,

129
0:06:22.221,000 --> 0:06:23,000
and you can go back about your day.

130
0:06:24.736,000 --> 0:06:28,000
So we're eventually able to do it

131
0:06:29.553,000 --> 0:06:31,000
using lasers to drill the holes.

132
0:06:32.244,000 --> 0:06:34,000
And with the lasers, it was fast and extremely reliable,

133
0:06:34.888,000 --> 0:06:36,000
you couldn't even tell the holes were there,

134
0:06:37.125,000 --> 0:06:4,000
any more than you could tell that one of your hairs was missing.

135
0:06:40.753,000 --> 0:06:44,000
And I know it might sound crazy, using lasers to drill holes in your skull,

136
0:06:45.515,000 --> 0:06:46,000
but back in 2017,

137
0:06:46.905,000 --> 0:06:5,000
people were OK with surgeons shooting lasers into their eyes

138
0:06:51.038,000 --> 0:06:52,000
for corrective surgery

139
0:06:52.276,000 --> 0:06:55,000
So when you're already here, it's not that big of a step.

140
0:06:57.561,000 --> 0:06:58,000
OK?

141
0:06:58.736,000 --> 0:07:01,000
So the next step, that happened in the 2030s,

142
0:07:02.331,000 --> 0:07:05,000
was that it's not just about getting through the skull.

143
0:07:05.441,000 --> 0:07:06,000
To measure the activity of neurons,

144
0:07:07.165,000 --> 0:07:1,000
you have to actually make it into the brain tissue itself.

145
0:07:11.344,000 --> 0:07:13,000
And the risk, whenever you put something into the brain tissue,

146
0:07:14.336,000 --> 0:07:15,000
is essentially that of stroke.

147
0:07:15.799,000 --> 0:07:17,000
That you would hit a blood vessel and burst it,

148
0:07:18.019,000 --> 0:07:19,000
and that causes a stroke.

149
0:07:19.916,000 --> 0:07:22,000
So, by the mid 2030s, we had invented these flexible probes

150
0:07:23.665,000 --> 0:07:25,000
that were capable of going around blood vessels,

151
0:07:25.967,000 --> 0:07:26,000
rather than through them.

152
0:07:27.467,000 --> 0:07:32,000
And thus, we could put huge batteries of these probes

153
0:07:33.188,000 --> 0:07:34,000
into the brains of patients

154
0:07:34.569,000 --> 0:07:37,000
and record from thousands of their neurons without any risk to them.

155
0:07:39.458,000 --> 0:07:43,000
And what we discovered, sort of to our surprise,

156
0:07:43.543,000 --> 0:07:45,000
is that the neurons that we could identify

157
0:07:45.757,000 --> 0:07:48,000
were not responding to things like ideas or emotion,

158
0:07:49.305,000 --> 0:07:5,000
which was what we had expected.

159
0:07:50.956,000 --> 0:07:53,000
They were mostly responding to things like Jennifer Aniston

160
0:07:54.776,000 --> 0:07:56,000
or Halle Berry

161
0:07:57.204,000 --> 0:07:58,000
or Justin Trudeau.

162
0:07:58.538,000 --> 0:07:59,000
I mean --

163
0:07:59.815,000 --> 0:08:01,000
(Laughter)

164
0:08:02.165,000 --> 0:08:04,000
In hindsight, we shouldn't have been that surprised.

165
0:08:04.626,000 --> 0:08:07,000
I mean, what do your neurons spend most of their time thinking about?

166
0:08:07.912,000 --> 0:08:08,000
(Laughter)

167
0:08:09.38,000 --> 0:08:11,000
But really, the point is that

168
0:08:11.444,000 --> 0:08:15,000
this technology enabled us to begin studying neuroscience in individuals.

169
0:08:15.898,000 --> 0:08:19,000
So much like the transition to genetics, at the single cell level,

170
0:08:20.152,000 --> 0:08:23,000
we started to study neuroscience, at the single human level.

171
0:08:23.89,000 --> 0:08:24,000
But we weren't quite there yet.

172
0:08:25.895,000 --> 0:08:26,000
Because these technologies

173
0:08:27.561,000 --> 0:08:3,000
were still restricted to medical applications,

174
0:08:30.641,000 --> 0:08:33,000
which meant that we were studying sick brains, not healthy brains.

175
0:08:35.235,000 --> 0:08:38,000
Because no matter how safe your technology is,

176
0:08:39.013,000 --> 0:08:41,000
you can't stick something into someone's brain

177
0:08:41.767,000 --> 0:08:42,000
for research purposes.

178
0:08:43.211,000 --> 0:08:44,000
They have to want it.

179
0:08:44.784,000 --> 0:08:45,000
And why would they want it?

180
0:08:46.268,000 --> 0:08:49,000
Because as soon as you have an electrical connection to the brain,

181
0:08:49.863,000 --> 0:08:51,000
you can use it to hook the brain up to a computer.

182
0:08:53.061,000 --> 0:08:56,000
Oh, well, you know, the general public was very skeptical at first.

183
0:08:56.514,000 --> 0:08:58,000
I mean, who wants to hook their brain up to their computers?

184
0:08:59.926,000 --> 0:09:03,000
Well just imagine being able to send an email with a thought.

185
0:09:04.186,000 --> 0:09:06,000
(Laughter)

186
0:09:06.463,000 --> 0:09:1,000
Imagine being able to take a picture with your eyes, OK?

187
0:09:10.987,000 --> 0:09:11,000
(Laughter)

188
0:09:12.241,000 --> 0:09:14,000
Imagine never forgetting anything anymore,

189
0:09:15.228,000 --> 0:09:17,000
because anything that you choose to remember

190
0:09:17.411,000 --> 0:09:19,000
will be stored permanently on a hard drive somewhere,

191
0:09:19.912,000 --> 0:09:21,000
able to be recalled at will.

192
0:09:21.965,000 --> 0:09:24,000
(Laughter)

193
0:09:25.355,000 --> 0:09:28,000
The line here between crazy and visionary

194
0:09:28.76,000 --> 0:09:29,000
was never quite clear.

195
0:09:30.72,000 --> 0:09:31,000
But the systems were safe.

196
0:09:32.879,000 --> 0:09:37,000
So when the FDA decided to deregulate these laser-drilling systems, in 2043,

197
0:09:37.919,000 --> 0:09:39,000
commercial demand just exploded.

198
0:09:40.3,000 --> 0:09:41,000
People started signing their emails,

199
0:09:42.212,000 --> 0:09:43,000
"Please excuse any typos.

200
0:09:43.577,000 --> 0:09:44,000
Sent from my brain."

201
0:09:44.934,000 --> 0:09:45,000
(Laughter)

202
0:09:45.959,000 --> 0:09:47,000
Commercial systems popped up left and right,

203
0:09:48.055,000 --> 0:09:51,000
offering the latest and greatest in neural interfacing technology.

204
0:09:51.792,000 --> 0:09:52,000
There were 100 electrodes.

205
0:09:53.569,000 --> 0:09:54,000
A thousand electrodes.

206
0:09:55.504,000 --> 0:09:57,000
High bandwidth for only 99.99 a month.

207
0:09:58.004,000 --> 0:09:59,000
(Laughter)

208
0:09:59.567,000 --> 0:10:,000
Soon, everyone had them.

209
0:10:01.694,000 --> 0:10:02,000
And that was the key.

210
0:10:03.289,000 --> 0:10:05,000
Because, in the 2050s, if you were a neuroscientist,

211
0:10:06.236,000 --> 0:10:09,000
you could have someone come into your lab essentially from off the street.

212
0:10:10.792,000 --> 0:10:12,000
And you could have them engaged in some emotional task

213
0:10:13.68,000 --> 0:10:15,000
or social behavior or abstract reasoning,

214
0:10:16.141,000 --> 0:10:18,000
things you could never study in mice.

215
0:10:18.696,000 --> 0:10:21,000
And you could record the activity of their neurons

216
0:10:21.831,000 --> 0:10:24,000
using the interfaces that they already had.

217
0:10:25.046,000 --> 0:10:28,000
And then you could also ask them about what they were experiencing.

218
0:10:28.259,000 --> 0:10:31,000
So this link between psychology and neuroscience

219
0:10:31.632,000 --> 0:10:34,000
that you could never make in the animals, was suddenly there.

220
0:10:35.695,000 --> 0:10:37,000
So perhaps the classic example of this

221
0:10:37.903,000 --> 0:10:4,000
was the discovery of the neural basis for insight.

222
0:10:41.45,000 --> 0:10:44,000
That "Aha!" moment, the moment it all comes together, it clicks.

223
0:10:45.593,000 --> 0:10:49,000
And this was discovered by two scientists in 2055,

224
0:10:49.673,000 --> 0:10:5,000
Barry and Late,

225
0:10:51.069,000 --> 0:10:54,000
who observed, in the dorsal prefrontal cortex,

226
0:10:54.756,000 --> 0:10:59,000
how in the brain of someone trying to understand an idea,

227
0:11:00.002,000 --> 0:11:03,000
how different populations of neurons would reorganize themselves --

228
0:11:03.395,000 --> 0:11:05,000
you're looking at neural activity here in orange --

229
0:11:05.855,000 --> 0:11:08,000
until finally their activity aligns in a way that leads to positive feedback.

230
0:11:10.339,000 --> 0:11:11,000
Right there.

231
0:11:12.723,000 --> 0:11:13,000
That is understanding.

232
0:11:15.413,000 --> 0:11:19,000
So finally, we were able to get at the things that make us human.

233
0:11:21.871,000 --> 0:11:25,000
And that's what really opened the way to major insights from medicine.

234
0:11:27.465,000 --> 0:11:29,000
Because, starting in the 2060s,

235
0:11:30.244,000 --> 0:11:32,000
with the ability to record the neural activity

236
0:11:32.752,000 --> 0:11:35,000
in the brains of patients with these different mental diseases,

237
0:11:36.363,000 --> 0:11:4,000
rather than defining the diseases on the basis of their symptoms,

238
0:11:41.077,000 --> 0:11:43,000
as we had at the beginning of the century,

239
0:11:43.141,000 --> 0:11:44,000
we started to define them

240
0:11:44.387,000 --> 0:11:47,000
on the basis of the actual pathology that we observed at the neural level.

241
0:11:48.768,000 --> 0:11:51,000
So for example, in the case of ADHD,

242
0:11:52.617,000 --> 0:11:55,000
we discovered that there are dozens of different diseases,

243
0:11:55.815,000 --> 0:11:58,000
all of which had been called ADHD at the start of the century,

244
0:11:58.848,000 --> 0:12:,000
that actually had nothing to do with each other,

245
0:12:01.173,000 --> 0:12:03,000
except that they had similar symptoms.

246
0:12:03.625,000 --> 0:12:05,000
And they needed to be treated in different ways.

247
0:12:06.307,000 --> 0:12:08,000
So it was kind of incredible, in retrospect,

248
0:12:08.578,000 --> 0:12:09,000
that at the beginning of the century,

249
0:12:10.379,000 --> 0:12:12,000
we had been treating all those different diseases

250
0:12:12.72,000 --> 0:12:13,000
with the same drug,

251
0:12:13.927,000 --> 0:12:16,000
just by giving people amphetamine, basically is what we were doing.

252
0:12:17.165,000 --> 0:12:19,000
And schizophrenia and depression are the same way.

253
0:12:19.677,000 --> 0:12:23,000
So rather than prescribing drugs to people essentially at random,

254
0:12:23.733,000 --> 0:12:24,000
as we had,

255
0:12:24.907,000 --> 0:12:27,000
we learned how to predict which drugs would be most effective

256
0:12:28.442,000 --> 0:12:29,000
in which patients,

257
0:12:29.649,000 --> 0:12:31,000
and that just led to this huge improvement in outcomes.

258
0:12:33.498,000 --> 0:12:36,000
OK, I want to bring you back now to the year 2017.

259
0:12:38.117,000 --> 0:12:41,000
Some of this may sound satirical or even far fetched.

260
0:12:41.514,000 --> 0:12:42,000
And some of it is.

261
0:12:43.291,000 --> 0:12:45,000
I mean, I can't actually see into the future.

262
0:12:45.966,000 --> 0:12:46,000
I don't actually know

263
0:12:47.356,000 --> 0:12:5,000
if we're going to be drilling hundreds or thousands of microscopic holes

264
0:12:51.047,000 --> 0:12:52,000
in our heads in 30 years.

265
0:12:53.762,000 --> 0:12:54,000
But what I can tell you

266
0:12:55.492,000 --> 0:12:57,000
is that we're not going to make any progress

267
0:12:57.691,000 --> 0:13:,000
towards understanding the human brain or human diseases

268
0:13:01.442,000 --> 0:13:05,000
until we figure out how to get at the electrical activity of neurons

269
0:13:05.982,000 --> 0:13:06,000
in healthy humans.

270
0:13:07.918,000 --> 0:13:1,000
And almost no one is working on figuring out how to do that today.

271
0:13:12.077,000 --> 0:13:14,000
That is the future of neuroscience.

272
0:13:14.752,000 --> 0:13:18,000
And I think it's time for neuroscientists to put down the mouse brain

273
0:13:19.169,000 --> 0:13:21,000
and to dedicate the thought and investment necessary

274
0:13:21.947,000 --> 0:13:24,000
to understand the human brain and human disease.

275
0:13:27.629,000 --> 0:13:28,000
Thank you.

276
0:13:28.804,000 --> 0:13:29,000
(Applause)

