1
0:00:13.548,000 --> 0:00:15,000
No matter who you are or where you live,

2
0:00:16.088,000 --> 0:00:18,000
I'm guessing that you have at least one relative

3
0:00:18.468,000 --> 0:00:2,000
that likes to forward those emails.

4
0:00:21.206,000 --> 0:00:22,000
You know the ones I'm talking about --

5
0:00:23.13,000 --> 0:00:25,000
the ones with dubious claims or conspiracy videos.

6
0:00:26.315,000 --> 0:00:28,000
And you've probably already muted them on Facebook

7
0:00:29.007,000 --> 0:00:31,000
for sharing social posts like this one.

8
0:00:31.379,000 --> 0:00:32,000
It's an image of a banana

9
0:00:32.807,000 --> 0:00:34,000
with a strange red cross running through the center.

10
0:00:35.498,000 --> 0:00:37,000
And the text around it is warning people

11
0:00:37.661,000 --> 0:00:39,000
not to eat fruits that look like this,

12
0:00:39.843,000 --> 0:00:41,000
suggesting they've been injected with blood

13
0:00:41.895,000 --> 0:00:43,000
contaminated with the HIV virus.

14
0:00:44.049,000 --> 0:00:46,000
And the social share message above it simply says,

15
0:00:46.676,000 --> 0:00:48,000
"Please forward to save lives."

16
0:00:49.672,000 --> 0:00:52,000
Now, fact-checkers have been debunking this one for years,

17
0:00:52.99,000 --> 0:00:54,000
but it's one of those rumors that just won't die.

18
0:00:55.823,000 --> 0:00:56,000
A zombie rumor.

19
0:00:57.513,000 --> 0:00:59,000
And, of course, it's entirely false.

20
0:01:00.18,000 --> 0:01:02,000
It might be tempting to laugh at an example like this, to say,

21
0:01:03.163,000 --> 0:01:04,000
"Well, who would believe this, anyway?"

22
0:01:05.419,000 --> 0:01:06,000
But the reason it's a zombie rumor

23
0:01:07.069,000 --> 0:01:1,000
is because it taps into people's deepest fears about their own safety

24
0:01:10.982,000 --> 0:01:12,000
and that of the people they love.

25
0:01:13.783,000 --> 0:01:16,000
And if you spend as enough time as I have looking at misinformation,

26
0:01:17.08,000 --> 0:01:19,000
you know that this is just one example of many

27
0:01:19.524,000 --> 0:01:22,000
that taps into people's deepest fears and vulnerabilities.

28
0:01:23.214,000 --> 0:01:27,000
Every day, across the world, we see scores of new memes on Instagram

29
0:01:27.607,000 --> 0:01:3,000
encouraging parents not to vaccinate their children.

30
0:01:30.67,000 --> 0:01:34,000
We see new videos on YouTube explaining that climate change is a hoax.

31
0:01:35.226,000 --> 0:01:39,000
And across all platforms, we see endless posts designed to demonize others

32
0:01:39.552,000 --> 0:01:42,000
on the basis of their race, religion or sexuality.

33
0:01:44.314,000 --> 0:01:47,000
Welcome to one of the central challenges of our time.

34
0:01:47.647,000 --> 0:01:51,000
How can we maintain an internet with freedom of expression at the core,

35
0:01:51.696,000 --> 0:01:54,000
while also ensuring that the content that's being disseminated

36
0:01:55.023,000 --> 0:01:58,000
doesn't cause irreparable harms to our democracies, our communities

37
0:01:58.933,000 --> 0:02:,000
and to our physical and mental well-being?

38
0:02:01.998,000 --> 0:02:03,000
Because we live in the information age,

39
0:02:04.109,000 --> 0:02:07,000
yet the central currency upon which we all depend -- information --

40
0:02:07.68,000 --> 0:02:09,000
is no longer deemed entirely trustworthy

41
0:02:10.061,000 --> 0:02:12,000
and, at times, can appear downright dangerous.

42
0:02:12.811,000 --> 0:02:15,000
This is thanks in part to the runaway growth of social sharing platforms

43
0:02:16.772,000 --> 0:02:17,000
that allow us to scroll through,

44
0:02:18.438,000 --> 0:02:2,000
where lies and facts sit side by side,

45
0:02:20.684,000 --> 0:02:23,000
but with none of the traditional signals of trustworthiness.

46
0:02:24.268,000 --> 0:02:27,000
And goodness -- our language around this is horribly muddled.

47
0:02:27.911,000 --> 0:02:3,000
People are still obsessed with the phrase "fake news,"

48
0:02:31.038,000 --> 0:02:33,000
despite the fact that it's extraordinarily unhelpful

49
0:02:33.593,000 --> 0:02:36,000
and used to describe a number of things that are actually very different:

50
0:02:37.077,000 --> 0:02:4,000
lies, rumors, hoaxes, conspiracies, propaganda.

51
0:02:40.911,000 --> 0:02:42,000
And I really wish we could stop using a phrase

52
0:02:43.847,000 --> 0:02:45,000
that's been co-opted by politicians right around the world,

53
0:02:46.733,000 --> 0:02:47,000
from the left and the right,

54
0:02:48.228,000 --> 0:02:51,000
used as a weapon to attack a free and independent press.

55
0:02:52.307,000 --> 0:02:56,000
(Applause)

56
0:02:57.033,000 --> 0:03:,000
Because we need our professional news media now more than ever.

57
0:03:00.882,000 --> 0:03:03,000
And besides, most of this content doesn't even masquerade as news.

58
0:03:04.279,000 --> 0:03:06,000
It's memes, videos, social posts.

59
0:03:06.945,000 --> 0:03:09,000
And most of it is not fake; it's misleading.

60
0:03:10.422,000 --> 0:03:13,000
We tend to fixate on what's true or false.

61
0:03:13.461,000 --> 0:03:17,000
But the biggest concern is actually the weaponization of context.

62
0:03:18.855,000 --> 0:03:19,000
Because the most effective disinformation

63
0:03:20.847,000 --> 0:03:23,000
has always been that which has a kernel of truth to it.

64
0:03:23.919,000 --> 0:03:26,000
Let's take this example from London, from March 2017,

65
0:03:27.419,000 --> 0:03:28,000
a tweet that circulated widely

66
0:03:28.983,000 --> 0:03:31,000
in the aftermath of a terrorist incident on Westminster Bridge.

67
0:03:32.594,000 --> 0:03:34,000
This is a genuine image, not fake.

68
0:03:35.046,000 --> 0:03:38,000
The woman who appears in the photograph was interviewed afterwards,

69
0:03:38.239,000 --> 0:03:4,000
and she explained that she was utterly traumatized.

70
0:03:40.672,000 --> 0:03:41,000
She was on the phone to a loved one,

71
0:03:42.434,000 --> 0:03:44,000
and she wasn't looking at the victim out of respect.

72
0:03:45.076,000 --> 0:03:48,000
But it still was circulated widely with this Islamophobic framing,

73
0:03:49.06,000 --> 0:03:52,000
with multiple hashtags, including: #BanIslam.

74
0:03:52.425,000 --> 0:03:54,000
Now, if you worked at Twitter, what would you do?

75
0:03:54.847,000 --> 0:03:56,000
Would you take that down, or would you leave it up?

76
0:03:58.553,000 --> 0:04:01,000
My gut reaction, my emotional reaction, is to take this down.

77
0:04:02.006,000 --> 0:04:04,000
I hate the framing of this image.

78
0:04:04.585,000 --> 0:04:06,000
But freedom of expression is a human right,

79
0:04:06.997,000 --> 0:04:09,000
and if we start taking down speech that makes us feel uncomfortable,

80
0:04:10.246,000 --> 0:04:11,000
we're in trouble.

81
0:04:11.5,000 --> 0:04:13,000
And this might look like a clear-cut case,

82
0:04:13.818,000 --> 0:04:14,000
but, actually, most speech isn't.

83
0:04:15.54,000 --> 0:04:17,000
These lines are incredibly difficult to draw.

84
0:04:18,000 --> 0:04:2,000
What's a well-meaning decision by one person

85
0:04:20.305,000 --> 0:04:22,000
is outright censorship to the next.

86
0:04:22.759,000 --> 0:04:24,000
What we now know is that this account, Texas Lone Star,

87
0:04:25.712,000 --> 0:04:28,000
was part of a wider Russian disinformation campaign,

88
0:04:28.966,000 --> 0:04:3,000
one that has since been taken down.

89
0:04:31.141,000 --> 0:04:32,000
Would that change your view?

90
0:04:33.322,000 --> 0:04:34,000
It would mine,

91
0:04:34.505,000 --> 0:04:36,000
because now it's a case of a coordinated campaign

92
0:04:36.83,000 --> 0:04:37,000
to sow discord.

93
0:04:38.069,000 --> 0:04:39,000
And for those of you who'd like to think

94
0:04:40.054,000 --> 0:04:42,000
that artificial intelligence will solve all of our problems,

95
0:04:42.909,000 --> 0:04:44,000
I think we can agree that we're a long way away

96
0:04:45.158,000 --> 0:04:47,000
from AI that's able to make sense of posts like this.

97
0:04:48.856,000 --> 0:04:5,000
So I'd like to explain three interlocking issues

98
0:04:51.387,000 --> 0:04:53,000
that make this so complex

99
0:04:53.784,000 --> 0:04:56,000
and then think about some ways we can consider these challenges.

100
0:04:57.348,000 --> 0:05:,000
First, we just don't have a rational relationship to information,

101
0:05:01.262,000 --> 0:05:02,000
we have an emotional one.

102
0:05:02.754,000 --> 0:05:05,000
It's just not true that more facts will make everything OK,

103
0:05:06.572,000 --> 0:05:09,000
because the algorithms that determine what content we see,

104
0:05:09.696,000 --> 0:05:12,000
well, they're designed to reward our emotional responses.

105
0:05:12.847,000 --> 0:05:13,000
And when we're fearful,

106
0:05:14.252,000 --> 0:05:17,000
oversimplified narratives, conspiratorial explanations

107
0:05:17.45,000 --> 0:05:2,000
and language that demonizes others is far more effective.

108
0:05:21.538,000 --> 0:05:22,000
And besides, many of these companies,

109
0:05:23.436,000 --> 0:05:25,000
their business model is attached to attention,

110
0:05:26.006,000 --> 0:05:29,000
which means these algorithms will always be skewed towards emotion.

111
0:05:30.371,000 --> 0:05:34,000
Second, most of the speech I'm talking about here is legal.

112
0:05:35.081,000 --> 0:05:36,000
It would be a different matter

113
0:05:36.551,000 --> 0:05:38,000
if I was talking about child sexual abuse imagery

114
0:05:38.916,000 --> 0:05:39,000
or content that incites violence.

115
0:05:40.867,000 --> 0:05:43,000
It can be perfectly legal to post an outright lie.

116
0:05:45.13,000 --> 0:05:49,000
But people keep talking about taking down "problematic" or "harmful" content,

117
0:05:49.188,000 --> 0:05:51,000
but with no clear definition of what they mean by that,

118
0:05:51.821,000 --> 0:05:52,000
including Mark Zuckerberg,

119
0:05:53.109,000 --> 0:05:56,000
who recently called for global regulation to moderate speech.

120
0:05:56.87,000 --> 0:05:58,000
And my concern is that we're seeing governments

121
0:05:59.109,000 --> 0:06:,000
right around the world

122
0:06:00.425,000 --> 0:06:02,000
rolling out hasty policy decisions

123
0:06:03.125,000 --> 0:06:05,000
that might actually trigger much more serious consequences

124
0:06:05.895,000 --> 0:06:06,000
when it comes to our speech.

125
0:06:08.006,000 --> 0:06:11,000
And even if we could decide which speech to take up or take down,

126
0:06:11.736,000 --> 0:06:13,000
we've never had so much speech.

127
0:06:13.934,000 --> 0:06:15,000
Every second, millions of pieces of content

128
0:06:16.089,000 --> 0:06:18,000
are uploaded by people right around the world

129
0:06:18.22,000 --> 0:06:19,000
in different languages,

130
0:06:19.412,000 --> 0:06:21,000
drawing on thousands of different cultural contexts.

131
0:06:22.204,000 --> 0:06:24,000
We've simply never had effective mechanisms

132
0:06:24.76,000 --> 0:06:25,000
to moderate speech at this scale,

133
0:06:26.522,000 --> 0:06:28,000
whether powered by humans or by technology.

134
0:06:30.284,000 --> 0:06:33,000
And third, these companies -- Google, Twitter, Facebook, WhatsApp --

135
0:06:34.252,000 --> 0:06:36,000
they're part of a wider information ecosystem.

136
0:06:37.117,000 --> 0:06:4,000
We like to lay all the blame at their feet, but the truth is,

137
0:06:40.493,000 --> 0:06:43,000
the mass media and elected officials can also play an equal role

138
0:06:44.347,000 --> 0:06:46,000
in amplifying rumors and conspiracies when they want to.

139
0:06:47.8,000 --> 0:06:51,000
As can we, when we mindlessly forward divisive or misleading content

140
0:06:52.768,000 --> 0:06:53,000
without trying.

141
0:06:54.077,000 --> 0:06:55,000
We're adding to the pollution.

142
0:06:57.236,000 --> 0:06:59,000
I know we're all looking for an easy fix.

143
0:06:59.878,000 --> 0:07:,000
But there just isn't one.

144
0:07:01.95,000 --> 0:07:05,000
Any solution will have to be rolled out at a massive scale, internet scale,

145
0:07:06.419,000 --> 0:07:09,000
and yes, the platforms, they're used to operating at that level.

146
0:07:09.704,000 --> 0:07:12,000
But can and should we allow them to fix these problems?

147
0:07:13.668,000 --> 0:07:14,000
They're certainly trying.

148
0:07:14.924,000 --> 0:07:18,000
But most of us would agree that, actually, we don't want global corporations

149
0:07:19.034,000 --> 0:07:21,000
to be the guardians of truth and fairness online.

150
0:07:21.39,000 --> 0:07:23,000
And I also think the platforms would agree with that.

151
0:07:24.257,000 --> 0:07:26,000
And at the moment, they're marking their own homework.

152
0:07:27.162,000 --> 0:07:28,000
They like to tell us

153
0:07:28.384,000 --> 0:07:3,000
that the interventions they're rolling out are working,

154
0:07:30.987,000 --> 0:07:32,000
but because they write their own transparency reports,

155
0:07:33.551,000 --> 0:07:36,000
there's no way for us to independently verify what's actually happening.

156
0:07:38.431,000 --> 0:07:41,000
(Applause)

157
0:07:41.797,000 --> 0:07:43,000
And let's also be clear that most of the changes we see

158
0:07:44.773,000 --> 0:07:46,000
only happen after journalists undertake an investigation

159
0:07:47.791,000 --> 0:07:48,000
and find evidence of bias

160
0:07:49.426,000 --> 0:07:51,000
or content that breaks their community guidelines.

161
0:07:52.815,000 --> 0:07:56,000
So yes, these companies have to play a really important role in this process,

162
0:07:57.434,000 --> 0:07:58,000
but they can't control it.

163
0:07:59.855,000 --> 0:08:,000
So what about governments?

164
0:08:01.863,000 --> 0:08:04,000
Many people believe that global regulation is our last hope

165
0:08:04.983,000 --> 0:08:06,000
in terms of cleaning up our information ecosystem.

166
0:08:07.887,000 --> 0:08:1,000
But what I see are lawmakers who are struggling to keep up to date

167
0:08:11.077,000 --> 0:08:13,000
with the rapid changes in technology.

168
0:08:13.442,000 --> 0:08:14,000
And worse, they're working in the dark,

169
0:08:15.37,000 --> 0:08:16,000
because they don't have access to data

170
0:08:17.215,000 --> 0:08:19,000
to understand what's happening on these platforms.

171
0:08:20.26,000 --> 0:08:23,000
And anyway, which governments would we trust to do this?

172
0:08:23.355,000 --> 0:08:25,000
We need a global response, not a national one.

173
0:08:27.419,000 --> 0:08:29,000
So the missing link is us.

174
0:08:29.72,000 --> 0:08:32,000
It's those people who use these technologies every day.

175
0:08:33.26,000 --> 0:08:37,000
Can we design a new infrastructure to support quality information?

176
0:08:38.371,000 --> 0:08:39,000
Well, I believe we can,

177
0:08:39.625,000 --> 0:08:42,000
and I've got a few ideas about what we might be able to actually do.

178
0:08:43.006,000 --> 0:08:46,000
So firstly, if we're serious about bringing the public into this,

179
0:08:46.133,000 --> 0:08:48,000
can we take some inspiration from Wikipedia?

180
0:08:48.538,000 --> 0:08:49,000
They've shown us what's possible.

181
0:08:50.386,000 --> 0:08:51,000
Yes, it's not perfect,

182
0:08:51.561,000 --> 0:08:53,000
but they've demonstrated that with the right structures,

183
0:08:54.219,000 --> 0:08:56,000
with a global outlook and lots and lots of transparency,

184
0:08:56.878,000 --> 0:08:59,000
you can build something that will earn the trust of most people.

185
0:08:59.998,000 --> 0:09:02,000
Because we have to find a way to tap into the collective wisdom

186
0:09:03.184,000 --> 0:09:05,000
and experience of all users.

187
0:09:05.517,000 --> 0:09:07,000
This is particularly the case for women, people of color

188
0:09:08.187,000 --> 0:09:09,000
and underrepresented groups.

189
0:09:09.557,000 --> 0:09:1,000
Because guess what?

190
0:09:10.747,000 --> 0:09:12,000
They are experts when it comes to hate and disinformation,

191
0:09:13.506,000 --> 0:09:16,000
because they have been the targets of these campaigns for so long.

192
0:09:17.046,000 --> 0:09:19,000
And over the years, they've been raising flags,

193
0:09:19.42,000 --> 0:09:2,000
and they haven't been listened to.

194
0:09:21.109,000 --> 0:09:22,000
This has got to change.

195
0:09:22.807,000 --> 0:09:26,000
So could we build a Wikipedia for trust?

196
0:09:27.157,000 --> 0:09:31,000
Could we find a way that users can actually provide insights?

197
0:09:31.37,000 --> 0:09:34,000
They could offer insights around difficult content-moderation decisions.

198
0:09:35.091,000 --> 0:09:36,000
They could provide feedback

199
0:09:36.578,000 --> 0:09:39,000
when platforms decide they want to roll out new changes.

200
0:09:40.241,000 --> 0:09:44,000
Second, people's experiences with the information is personalized.

201
0:09:44.427,000 --> 0:09:46,000
My Facebook news feed is very different to yours.

202
0:09:47.094,000 --> 0:09:49,000
Your YouTube recommendations are very different to mine.

203
0:09:49.863,000 --> 0:09:51,000
That makes it impossible for us to actually examine

204
0:09:52.379,000 --> 0:09:54,000
what information people are seeing.

205
0:09:54.815,000 --> 0:09:55,000
So could we imagine

206
0:09:56.228,000 --> 0:10:,000
developing some kind of centralized open repository for anonymized data,

207
0:10:01.03,000 --> 0:10:03,000
with privacy and ethical concerns built in?

208
0:10:04.22,000 --> 0:10:05,000
Because imagine what we would learn

209
0:10:06.022,000 --> 0:10:09,000
if we built out a global network of concerned citizens

210
0:10:09.307,000 --> 0:10:12,000
who wanted to donate their social data to science.

211
0:10:13.141,000 --> 0:10:14,000
Because we actually know very little

212
0:10:14.887,000 --> 0:10:16,000
about the long-term consequences of hate and disinformation

213
0:10:17.792,000 --> 0:10:18,000
on people's attitudes and behaviors.

214
0:10:20.236,000 --> 0:10:21,000
And what we do know,

215
0:10:21.427,000 --> 0:10:23,000
most of that has been carried out in the US,

216
0:10:23.644,000 --> 0:10:25,000
despite the fact that this is a global problem.

217
0:10:26.049,000 --> 0:10:27,000
We need to work on that, too.

218
0:10:28.192,000 --> 0:10:29,000
And third,

219
0:10:29.366,000 --> 0:10:31,000
can we find a way to connect the dots?

220
0:10:31.7,000 --> 0:10:34,000
No one sector, let alone nonprofit, start-up or government,

221
0:10:35.162,000 --> 0:10:36,000
is going to solve this.

222
0:10:36.608,000 --> 0:10:38,000
But there are very smart people right around the world

223
0:10:39.196,000 --> 0:10:4,000
working on these challenges,

224
0:10:40.601,000 --> 0:10:43,000
from newsrooms, civil society, academia, activist groups.

225
0:10:44.201,000 --> 0:10:45,000
And you can see some of them here.

226
0:10:46.123,000 --> 0:10:48,000
Some are building out indicators of content credibility.

227
0:10:49.074,000 --> 0:10:5,000
Others are fact-checking,

228
0:10:50.344,000 --> 0:10:53,000
so that false claims, videos and images can be down-ranked by the platforms.

229
0:10:53.959,000 --> 0:10:55,000
A nonprofit I helped to found, First Draft,

230
0:10:56.196,000 --> 0:10:58,000
is working with normally competitive newsrooms around the world

231
0:10:59.188,000 --> 0:11:02,000
to help them build out investigative, collaborative programs.

232
0:11:03.231,000 --> 0:11:05,000
And Danny Hillis, a software architect,

233
0:11:05.564,000 --> 0:11:07,000
is designing a new system called The Underlay,

234
0:11:07.969,000 --> 0:11:09,000
which will be a record of all public statements of fact

235
0:11:10.768,000 --> 0:11:11,000
connected to their sources,

236
0:11:12.121,000 --> 0:11:15,000
so that people and algorithms can better judge what is credible.

237
0:11:16.8,000 --> 0:11:19,000
And educators around the world are testing different techniques

238
0:11:20.18,000 --> 0:11:23,000
for finding ways to make people critical of the content they consume.

239
0:11:24.633,000 --> 0:11:27,000
All of these efforts are wonderful, but they're working in silos,

240
0:11:27.798,000 --> 0:11:29,000
and many of them are woefully underfunded.

241
0:11:30.502,000 --> 0:11:32,000
There are also hundreds of very smart people

242
0:11:32.579,000 --> 0:11:33,000
working inside these companies,

243
0:11:34.255,000 --> 0:11:36,000
but again, these efforts can feel disjointed,

244
0:11:36.604,000 --> 0:11:39,000
because they're actually developing different solutions to the same problems.

245
0:11:41.205,000 --> 0:11:43,000
How can we find a way to bring people together

246
0:11:43.498,000 --> 0:11:46,000
in one physical location for days or weeks at a time,

247
0:11:46.8,000 --> 0:11:48,000
so they can actually tackle these problems together

248
0:11:49.22,000 --> 0:11:5,000
but from their different perspectives?

249
0:11:51.062,000 --> 0:11:52,000
So can we do this?

250
0:11:52.426,000 --> 0:11:55,000
Can we build out a coordinated, ambitious response,

251
0:11:55.689,000 --> 0:11:58,000
one that matches the scale and the complexity of the problem?

252
0:11:59.819,000 --> 0:12:,000
I really think we can.

253
0:12:01.216,000 --> 0:12:03,000
Together, let's rebuild our information commons.

254
0:12:04.819,000 --> 0:12:05,000
Thank you.

255
0:12:06.033,000 --> 0:12:09,000
(Applause)

