1
0:00:,000 --> 0:00:07,000
Translator: Joseph Geni Reviewer: Morton Bast

2
0:00:12.588,000 --> 0:00:15,000
I'm a computer science professor,

3
0:00:15.619,000 --> 0:00:17,000
and my area of expertise is

4
0:00:17.932,000 --> 0:00:19,000
computer and information security.

5
0:00:20.131,000 --> 0:00:22,000
When I was in graduate school,

6
0:00:22.451,000 --> 0:00:24,000
I had the opportunity to overhear my grandmother

7
0:00:25.052,000 --> 0:00:29,000
describing to one of her fellow senior citizens

8
0:00:29.186,000 --> 0:00:31,000
what I did for a living.

9
0:00:31.555,000 --> 0:00:34,000
Apparently, I was in charge of making sure that

10
0:00:35.117,000 --> 0:00:38,000
no one stole the computers from the university. (Laughter)

11
0:00:39.017,000 --> 0:00:41,000
And, you know, that's a perfectly reasonable thing

12
0:00:41.761,000 --> 0:00:42,000
for her to think, because I told her I was working

13
0:00:43.681,000 --> 0:00:44,000
in computer security,

14
0:00:45.188,000 --> 0:00:48,000
and it was interesting to get her perspective.

15
0:00:48.785,000 --> 0:00:5,000
But that's not the most ridiculous thing I've ever heard

16
0:00:51.402,000 --> 0:00:53,000
anyone say about my work.

17
0:00:53.419,000 --> 0:00:55,000
The most ridiculous thing I ever heard is,

18
0:00:55.703,000 --> 0:00:58,000
I was at a dinner party, and a woman heard

19
0:00:58.837,000 --> 0:00:59,000
that I work in computer security,

20
0:01:00.62,000 --> 0:01:03,000
and she asked me if -- she said her computer had been

21
0:01:04.137,000 --> 0:01:07,000
infected by a virus, and she was very concerned that she

22
0:01:07.573,000 --> 0:01:1,000
might get sick from it, that she could get this virus. (Laughter)

23
0:01:11.524,000 --> 0:01:13,000
And I'm not a doctor, but I reassured her

24
0:01:14.467,000 --> 0:01:17,000
that it was very, very unlikely that this would happen,

25
0:01:17.611,000 --> 0:01:19,000
but if she felt more comfortable, she could be free to use

26
0:01:20.412,000 --> 0:01:21,000
latex gloves when she was on the computer,

27
0:01:22.26,000 --> 0:01:25,000
and there would be no harm whatsoever in that.

28
0:01:25.652,000 --> 0:01:27,000
I'm going to get back to this notion of being able to get

29
0:01:28.159,000 --> 0:01:31,000
a virus from your computer, in a serious way.

30
0:01:31.667,000 --> 0:01:32,000
What I'm going to talk to you about today

31
0:01:33.307,000 --> 0:01:37,000
are some hacks, some real world cyberattacks that people

32
0:01:38.153,000 --> 0:01:4,000
in my community, the academic research community,

33
0:01:40.707,000 --> 0:01:42,000
have performed, which I don't think

34
0:01:43.501,000 --> 0:01:44,000
most people know about,

35
0:01:44.709,000 --> 0:01:47,000
and I think they're very interesting and scary,

36
0:01:47.737,000 --> 0:01:49,000
and this talk is kind of a greatest hits

37
0:01:50.178,000 --> 0:01:52,000
of the academic security community's hacks.

38
0:01:53.169,000 --> 0:01:54,000
None of the work is my work. It's all work

39
0:01:55.156,000 --> 0:01:57,000
that my colleagues have done, and I actually asked them

40
0:01:57.33,000 --> 0:01:59,000
for their slides and incorporated them into this talk.

41
0:01:59.887,000 --> 0:02:,000
So the first one I'm going to talk about

42
0:02:01.629,000 --> 0:02:03,000
are implanted medical devices.

43
0:02:04.303,000 --> 0:02:07,000
Now medical devices have come a long way technologically.

44
0:02:07.343,000 --> 0:02:1,000
You can see in 1926 the first pacemaker was invented.

45
0:02:11.199,000 --> 0:02:14,000
1960, the first internal pacemaker was implanted,

46
0:02:14.751,000 --> 0:02:16,000
hopefully a little smaller than that one that you see there,

47
0:02:17.303,000 --> 0:02:19,000
and the technology has continued to move forward.

48
0:02:20.271,000 --> 0:02:24,000
In 2006, we hit an important milestone from the perspective

49
0:02:24.904,000 --> 0:02:27,000
of computer security.

50
0:02:28.071,000 --> 0:02:29,000
And why do I say that?

51
0:02:29.412,000 --> 0:02:31,000
Because that's when implanted devices inside of people

52
0:02:32.302,000 --> 0:02:34,000
started to have networking capabilities.

53
0:02:35.047,000 --> 0:02:36,000
One thing that brings us close to home is we look

54
0:02:36.927,000 --> 0:02:38,000
at Dick Cheney's device, he had a device that

55
0:02:39.632,000 --> 0:02:42,000
pumped blood from an aorta to another part of the heart,

56
0:02:43.501,000 --> 0:02:44,000
and as you can see at the bottom there,

57
0:02:44.684,000 --> 0:02:47,000
it was controlled by a computer controller,

58
0:02:47.693,000 --> 0:02:49,000
and if you ever thought that software liability

59
0:02:50.21,000 --> 0:02:53,000
was very important, get one of these inside of you.

60
0:02:53.799,000 --> 0:02:56,000
Now what a research team did was they got their hands

61
0:02:57.494,000 --> 0:02:58,000
on what's called an ICD.

62
0:02:58.914,000 --> 0:03:,000
This is a defibrillator, and this is a device

63
0:03:00.984,000 --> 0:03:04,000
that goes into a person to control their heart rhythm,

64
0:03:05.32,000 --> 0:03:07,000
and these have saved many lives.

65
0:03:07.658,000 --> 0:03:09,000
Well, in order to not have to open up the person

66
0:03:10.13,000 --> 0:03:12,000
every time you want to reprogram their device

67
0:03:12.324,000 --> 0:03:14,000
or do some diagnostics on it, they made the thing be able

68
0:03:14.779,000 --> 0:03:17,000
to communicate wirelessly, and what this research team did

69
0:03:17.881,000 --> 0:03:19,000
is they reverse engineered the wireless protocol,

70
0:03:20.491,000 --> 0:03:21,000
and they built the device you see pictured here,

71
0:03:22.363,000 --> 0:03:24,000
with a little antenna, that could talk the protocol

72
0:03:25.123,000 --> 0:03:29,000
to the device, and thus control it.

73
0:03:29.598,000 --> 0:03:31,000
In order to make their experience real -- they were unable

74
0:03:32.287,000 --> 0:03:34,000
to find any volunteers, and so they went

75
0:03:34.759,000 --> 0:03:36,000
and they got some ground beef and some bacon

76
0:03:36.903,000 --> 0:03:37,000
and they wrapped it all up to about the size

77
0:03:38.691,000 --> 0:03:4,000
of a human being's area where the device would go,

78
0:03:41.489,000 --> 0:03:42,000
and they stuck the device inside it

79
0:03:42.943,000 --> 0:03:45,000
to perform their experiment somewhat realistically.

80
0:03:46.075,000 --> 0:03:49,000
They launched many, many successful attacks.

81
0:03:49.095,000 --> 0:03:52,000
One that I'll highlight here is changing the patient's name.

82
0:03:52.151,000 --> 0:03:52,000
I don't know why you would want to do that,

83
0:03:53.144,000 --> 0:03:55,000
but I sure wouldn't want that done to me.

84
0:03:55.248,000 --> 0:03:57,000
And they were able to change therapies,

85
0:03:57.579,000 --> 0:03:59,000
including disabling the device -- and this is with a real,

86
0:04:00.074,000 --> 0:04:01,000
commercial, off-the-shelf device --

87
0:04:01.97,000 --> 0:04:03,000
simply by performing reverse engineering and sending

88
0:04:04.016,000 --> 0:04:06,000
wireless signals to it.

89
0:04:07.005,000 --> 0:04:1,000
There was a piece on NPR that some of these ICDs

90
0:04:10.585,000 --> 0:04:12,000
could actually have their performance disrupted

91
0:04:13.007,000 --> 0:04:16,000
simply by holding a pair of headphones onto them.

92
0:04:16.658,000 --> 0:04:17,000
Now, wireless and the Internet

93
0:04:18.067,000 --> 0:04:19,000
can improve health care greatly.

94
0:04:19.719,000 --> 0:04:21,000
There's several examples up on the screen

95
0:04:21.806,000 --> 0:04:24,000
of situations where doctors are looking to implant devices

96
0:04:24.913,000 --> 0:04:26,000
inside of people, and all of these devices now,

97
0:04:27.778,000 --> 0:04:3,000
it's standard that they communicate wirelessly,

98
0:04:30.903,000 --> 0:04:31,000
and I think this is great,

99
0:04:32.315,000 --> 0:04:35,000
but without a full understanding of trustworthy computing,

100
0:04:35.42,000 --> 0:04:37,000
and without understanding what attackers can do

101
0:04:37.827,000 --> 0:04:39,000
and the security risks from the beginning,

102
0:04:39.974,000 --> 0:04:41,000
there's a lot of danger in this.

103
0:04:42.364,000 --> 0:04:43,000
Okay, let me shift gears and show you another target.

104
0:04:43.841,000 --> 0:04:45,000
I'm going to show you a few different targets like this,

105
0:04:45.929,000 --> 0:04:47,000
and that's my talk. So we'll look at automobiles.

106
0:04:48.846,000 --> 0:04:5,000
This is a car, and it has a lot of components,

107
0:04:51.742,000 --> 0:04:52,000
a lot of electronics in it today.

108
0:04:53.362,000 --> 0:04:57,000
In fact, it's got many, many different computers inside of it,

109
0:04:57.739,000 --> 0:05:,000
more Pentiums than my lab did when I was in college,

110
0:05:00.894,000 --> 0:05:03,000
and they're connected by a wired network.

111
0:05:04.533,000 --> 0:05:07,000
There's also a wireless network in the car,

112
0:05:07.964,000 --> 0:05:1,000
which can be reached from many different ways.

113
0:05:11.197,000 --> 0:05:14,000
So there's Bluetooth, there's the FM and XM radio,

114
0:05:14.898,000 --> 0:05:16,000
there's actually wi-fi, there's sensors in the wheels

115
0:05:17.718,000 --> 0:05:19,000
that wirelessly communicate the tire pressure

116
0:05:19.871,000 --> 0:05:2,000
to a controller on board.

117
0:05:21.677,000 --> 0:05:25,000
The modern car is a sophisticated multi-computer device.

118
0:05:26.595,000 --> 0:05:29,000
And what happens if somebody wanted to attack this?

119
0:05:29.917,000 --> 0:05:3,000
Well, that's what the researchers

120
0:05:31.234,000 --> 0:05:32,000
that I'm going to talk about today did.

121
0:05:33.105,000 --> 0:05:35,000
They basically stuck an attacker on the wired network

122
0:05:36.082,000 --> 0:05:38,000
and on the wireless network.

123
0:05:38.404,000 --> 0:05:4,000
Now, they have two areas they can attack.

124
0:05:41.103,000 --> 0:05:43,000
One is short-range wireless, where you can actually

125
0:05:43.141,000 --> 0:05:44,000
communicate with the device from nearby,

126
0:05:44.922,000 --> 0:05:46,000
either through Bluetooth or wi-fi,

127
0:05:47.059,000 --> 0:05:49,000
and the other is long-range, where you can communicate

128
0:05:49.233,000 --> 0:05:5,000
with the car through the cellular network,

129
0:05:51.015,000 --> 0:05:52,000
or through one of the radio stations.

130
0:05:52.975,000 --> 0:05:55,000
Think about it. When a car receives a radio signal,

131
0:05:56.024,000 --> 0:05:58,000
it's processed by software.

132
0:05:58.225,000 --> 0:06:01,000
That software has to receive and decode the radio signal,

133
0:06:01.286,000 --> 0:06:02,000
and then figure out what to do with it,

134
0:06:02.405,000 --> 0:06:05,000
even if it's just music that it needs to play on the radio,

135
0:06:05.429,000 --> 0:06:07,000
and that software that does that decoding,

136
0:06:07.697,000 --> 0:06:1,000
if it has any bugs in it, could create a vulnerability

137
0:06:10.79,000 --> 0:06:13,000
for somebody to hack the car.

138
0:06:13.825,000 --> 0:06:15,000
The way that the researchers did this work is,

139
0:06:16.777,000 --> 0:06:2,000
they read the software in the computer chips

140
0:06:21,000 --> 0:06:24,000
that were in the car, and then they used sophisticated

141
0:06:24.193,000 --> 0:06:25,000
reverse engineering tools

142
0:06:25.607,000 --> 0:06:27,000
to figure out what that software did,

143
0:06:27.662,000 --> 0:06:3,000
and then they found vulnerabilities in that software,

144
0:06:30.703,000 --> 0:06:33,000
and then they built exploits to exploit those.

145
0:06:34.049,000 --> 0:06:36,000
They actually carried out their attack in real life.

146
0:06:36.431,000 --> 0:06:37,000
They bought two cars, and I guess

147
0:06:37.781,000 --> 0:06:39,000
they have better budgets than I do.

148
0:06:40.699,000 --> 0:06:42,000
The first threat model was to see what someone could do

149
0:06:43.289,000 --> 0:06:45,000
if an attacker actually got access

150
0:06:45.433,000 --> 0:06:47,000
to the internal network on the car.

151
0:06:47.486,000 --> 0:06:49,000
Okay, so think of that as, someone gets to go to your car,

152
0:06:50.089,000 --> 0:06:52,000
they get to mess around with it, and then they leave,

153
0:06:52.993,000 --> 0:06:54,000
and now, what kind of trouble are you in?

154
0:06:55.361,000 --> 0:06:57,000
The other threat model is that they contact you

155
0:06:58.153,000 --> 0:07:,000
in real time over one of the wireless networks

156
0:07:00.61,000 --> 0:07:02,000
like the cellular, or something like that,

157
0:07:02.665,000 --> 0:07:06,000
never having actually gotten physical access to your car.

158
0:07:06.665,000 --> 0:07:08,000
This is what their setup looks like for the first model,

159
0:07:09.489,000 --> 0:07:1,000
where you get to have access to the car.

160
0:07:11.172,000 --> 0:07:14,000
They put a laptop, and they connected to the diagnostic unit

161
0:07:14.559,000 --> 0:07:16,000
on the in-car network, and they did all kinds of silly things,

162
0:07:17.498,000 --> 0:07:19,000
like here's a picture of the speedometer

163
0:07:20.281,000 --> 0:07:22,000
showing 140 miles an hour when the car's in park.

164
0:07:23.097,000 --> 0:07:25,000
Once you have control of the car's computers,

165
0:07:25.47,000 --> 0:07:25,000
you can do anything.

166
0:07:26.389,000 --> 0:07:27,000
Now you might say, "Okay, that's silly."

167
0:07:28.005,000 --> 0:07:29,000
Well, what if you make the car always say

168
0:07:29.664,000 --> 0:07:31,000
it's going 20 miles an hour slower than it's actually going?

169
0:07:32.405,000 --> 0:07:34,000
You might produce a lot of speeding tickets.

170
0:07:34.947,000 --> 0:07:37,000
Then they went out to an abandoned airstrip with two cars,

171
0:07:38.803,000 --> 0:07:4,000
the target victim car and the chase car,

172
0:07:41.548,000 --> 0:07:43,000
and they launched a bunch of other attacks.

173
0:07:44.294,000 --> 0:07:46,000
One of the things they were able to do from the chase car

174
0:07:47.06,000 --> 0:07:48,000
is apply the brakes on the other car,

175
0:07:49.034,000 --> 0:07:5,000
simply by hacking the computer.

176
0:07:50.594,000 --> 0:07:52,000
They were able to disable the brakes.

177
0:07:53.025,000 --> 0:07:56,000
They also were able to install malware that wouldn't kick in

178
0:07:56.203,000 --> 0:07:58,000
and wouldn't trigger until the car was doing something like

179
0:07:58.628,000 --> 0:08:01,000
going over 20 miles an hour, or something like that.

180
0:08:02.374,000 --> 0:08:04,000
The results are astonishing, and when they gave this talk,

181
0:08:05.132,000 --> 0:08:06,000
even though they gave this talk at a conference

182
0:08:06.848,000 --> 0:08:07,000
to a bunch of computer security researchers,

183
0:08:08.574,000 --> 0:08:09,000
everybody was gasping.

184
0:08:10.274,000 --> 0:08:13,000
They were able to take over a bunch of critical computers

185
0:08:13.973,000 --> 0:08:16,000
inside the car: the brakes computer, the lighting computer,

186
0:08:17.734,000 --> 0:08:19,000
the engine, the dash, the radio, etc.,

187
0:08:20.561,000 --> 0:08:22,000
and they were able to perform these on real commercial

188
0:08:22.854,000 --> 0:08:25,000
cars that they purchased using the radio network.

189
0:08:25.881,000 --> 0:08:28,000
They were able to compromise every single one of the

190
0:08:28.884,000 --> 0:08:3,000
pieces of software that controlled every single one

191
0:08:31.35,000 --> 0:08:34,000
of the wireless capabilities of the car.

192
0:08:34.365,000 --> 0:08:36,000
All of these were implemented successfully.

193
0:08:36.878,000 --> 0:08:38,000
How would you steal a car in this model?

194
0:08:39.23,000 --> 0:08:42,000
Well, you compromise the car by a buffer overflow

195
0:08:42.91,000 --> 0:08:44,000
of vulnerability in the software, something like that.

196
0:08:45.437,000 --> 0:08:47,000
You use the GPS in the car to locate it.

197
0:08:47.64,000 --> 0:08:49,000
You remotely unlock the doors through the computer

198
0:08:49.835,000 --> 0:08:52,000
that controls that, start the engine, bypass anti-theft,

199
0:08:52.973,000 --> 0:08:53,000
and you've got yourself a car.

200
0:08:54.641,000 --> 0:08:56,000
Surveillance was really interesting.

201
0:08:57.128,000 --> 0:09:,000
The authors of the study have a video where they show

202
0:09:00.337,000 --> 0:09:02,000
themselves taking over a car and then turning on

203
0:09:02.886,000 --> 0:09:04,000
the microphone in the car, and listening in on the car

204
0:09:05.647,000 --> 0:09:08,000
while tracking it via GPS on a map,

205
0:09:08.998,000 --> 0:09:09,000
and so that's something that the drivers of the car

206
0:09:10.711,000 --> 0:09:12,000
would never know was happening.

207
0:09:12.879,000 --> 0:09:14,000
Am I scaring you yet?

208
0:09:15.013,000 --> 0:09:16,000
I've got a few more of these interesting ones.

209
0:09:16.956,000 --> 0:09:17,000
These are ones where I went to a conference,

210
0:09:18.789,000 --> 0:09:19,000
and my mind was just blown, and I said,

211
0:09:20.722,000 --> 0:09:21,000
"I have to share this with other people."

212
0:09:22.548,000 --> 0:09:23,000
This was Fabian Monrose's lab

213
0:09:24.171,000 --> 0:09:27,000
at the University of North Carolina, and what they did was

214
0:09:27.627,000 --> 0:09:29,000
something intuitive once you see it,

215
0:09:29.702,000 --> 0:09:3,000
but kind of surprising.

216
0:09:31.416,000 --> 0:09:33,000
They videotaped people on a bus,

217
0:09:33.675,000 --> 0:09:35,000
and then they post-processed the video.

218
0:09:36.515,000 --> 0:09:38,000
What you see here in number one is a

219
0:09:38.978,000 --> 0:09:42,000
reflection in somebody's glasses of the smartphone

220
0:09:43.361,000 --> 0:09:44,000
that they're typing in.

221
0:09:44.786,000 --> 0:09:45,000
They wrote software to stabilize --

222
0:09:46.761,000 --> 0:09:47,000
even though they were on a bus

223
0:09:48.126,000 --> 0:09:51,000
and maybe someone's holding their phone at an angle --

224
0:09:51.337,000 --> 0:09:53,000
to stabilize the phone, process it, and

225
0:09:53.707,000 --> 0:09:54,000
you may know on your smartphone, when you type

226
0:09:55.592,000 --> 0:09:57,000
a password, the keys pop out a little bit, and they were able

227
0:09:58.531,000 --> 0:10:,000
to use that to reconstruct what the person was typing,

228
0:10:01.371,000 --> 0:10:05,000
and had a language model for detecting typing.

229
0:10:05.692,000 --> 0:10:07,000
What was interesting is, by videotaping on a bus,

230
0:10:08.027,000 --> 0:10:1,000
they were able to produce exactly what people

231
0:10:10.156,000 --> 0:10:12,000
on their smartphones were typing,

232
0:10:12.307,000 --> 0:10:14,000
and then they had a surprising result, which is that

233
0:10:14.567,000 --> 0:10:16,000
their software had not only done it for their target,

234
0:10:17.331,000 --> 0:10:18,000
but other people who accidentally happened

235
0:10:18.734,000 --> 0:10:2,000
to be in the picture, they were able to produce

236
0:10:20.82,000 --> 0:10:22,000
what those people had been typing, and that was kind of

237
0:10:23.547,000 --> 0:10:26,000
an accidental artifact of what their software was doing.

238
0:10:27.164,000 --> 0:10:31,000
I'll show you two more. One is P25 radios.

239
0:10:31.467,000 --> 0:10:33,000
P25 radios are used by law enforcement

240
0:10:34.267,000 --> 0:10:37,000
and all kinds of government agencies

241
0:10:37.674,000 --> 0:10:38,000
and people in combat to communicate,

242
0:10:39.41,000 --> 0:10:41,000
and there's an encryption option on these phones.

243
0:10:42.243,000 --> 0:10:44,000
This is what the phone looks like. It's not really a phone.

244
0:10:44.971,000 --> 0:10:45,000
It's more of a two-way radio.

245
0:10:46.177,000 --> 0:10:49,000
Motorola makes the most widely used one, and you can see

246
0:10:49.499,000 --> 0:10:51,000
that they're used by Secret Service, they're used in combat,

247
0:10:52.148,000 --> 0:10:55,000
it's a very, very common standard in the U.S. and elsewhere.

248
0:10:55.25,000 --> 0:10:57,000
So one question the researchers asked themselves is,

249
0:10:57.555,000 --> 0:10:59,000
could you block this thing, right?

250
0:11:00.259,000 --> 0:11:01,000
Could you run a denial-of-service,

251
0:11:01.842,000 --> 0:11:02,000
because these are first responders?

252
0:11:03.666,000 --> 0:11:04,000
So, would a terrorist organization want to black out the

253
0:11:05.467,000 --> 0:11:09,000
ability of police and fire to communicate at an emergency?

254
0:11:09.955,000 --> 0:11:12,000
They found that there's this GirlTech device used for texting

255
0:11:13.027,000 --> 0:11:15,000
that happens to operate at the same exact frequency

256
0:11:15.745,000 --> 0:11:17,000
as the P25, and they built what they called

257
0:11:18.016,000 --> 0:11:22,000
My First Jammer. (Laughter)

258
0:11:22.35,000 --> 0:11:24,000
If you look closely at this device,

259
0:11:24.728,000 --> 0:11:27,000
it's got a switch for encryption or cleartext.

260
0:11:28.358,000 --> 0:11:31,000
Let me advance the slide, and now I'll go back.

261
0:11:31.408,000 --> 0:11:33,000
You see the difference?

262
0:11:33.955,000 --> 0:11:35,000
This is plain text. This is encrypted.

263
0:11:36.512,000 --> 0:11:38,000
There's one little dot that shows up on the screen,

264
0:11:39.069,000 --> 0:11:41,000
and one little tiny turn of the switch.

265
0:11:41.154,000 --> 0:11:42,000
And so the researchers asked themselves, "I wonder how

266
0:11:43.058,000 --> 0:11:47,000
many times very secure, important, sensitive conversations

267
0:11:47.315,000 --> 0:11:48,000
are happening on these two-way radios where they forget

268
0:11:48.938,000 --> 0:11:5,000
to encrypt and they don't notice that they didn't encrypt?"

269
0:11:51.848,000 --> 0:11:54,000
So they bought a scanner. These are perfectly legal

270
0:11:55.187,000 --> 0:11:58,000
and they run at the frequency of the P25,

271
0:11:58.645,000 --> 0:11:59,000
and what they did is they hopped around frequencies

272
0:12:00.412,000 --> 0:12:02,000
and they wrote software to listen in.

273
0:12:02.922,000 --> 0:12:04,000
If they found encrypted communication, they stayed

274
0:12:05.556,000 --> 0:12:06,000
on that channel and they wrote down, that's a channel

275
0:12:07.242,000 --> 0:12:08,000
that these people communicate in,

276
0:12:09.03,000 --> 0:12:1,000
these law enforcement agencies,

277
0:12:10.652,000 --> 0:12:13,000
and they went to 20 metropolitan areas and listened in

278
0:12:14.043,000 --> 0:12:17,000
on conversations that were happening at those frequencies.

279
0:12:17.518,000 --> 0:12:2,000
They found that in every metropolitan area,

280
0:12:20.757,000 --> 0:12:22,000
they would capture over 20 minutes a day

281
0:12:22.911,000 --> 0:12:24,000
of cleartext communication.

282
0:12:25.286,000 --> 0:12:27,000
And what kind of things were people talking about?

283
0:12:27.286,000 --> 0:12:28,000
Well, they found the names and information

284
0:12:28.77,000 --> 0:12:3,000
about confidential informants. They found information

285
0:12:31.622,000 --> 0:12:33,000
that was being recorded in wiretaps,

286
0:12:33.824,000 --> 0:12:35,000
a bunch of crimes that were being discussed,

287
0:12:36.534,000 --> 0:12:37,000
sensitive information.

288
0:12:37.696,000 --> 0:12:4,000
It was mostly law enforcement and criminal.

289
0:12:41.059,000 --> 0:12:42,000
They went and reported this to the law enforcement

290
0:12:42.893,000 --> 0:12:44,000
agencies, after anonymizing it,

291
0:12:44.916,000 --> 0:12:47,000
and the vulnerability here is simply the user interface

292
0:12:47.916,000 --> 0:12:48,000
wasn't good enough. If you're talking

293
0:12:49.31,000 --> 0:12:51,000
about something really secure and sensitive, it should

294
0:12:52.126,000 --> 0:12:55,000
be really clear to you that this conversation is encrypted.

295
0:12:55.419,000 --> 0:12:56,000
That one's pretty easy to fix.

296
0:12:57.305,000 --> 0:12:58,000
The last one I thought was really, really cool,

297
0:12:58.974,000 --> 0:13:,000
and I just had to show it to you, it's probably not something

298
0:13:01.787,000 --> 0:13:02,000
that you're going to lose sleep over

299
0:13:02.792,000 --> 0:13:03,000
like the cars or the defibrillators,

300
0:13:04.583,000 --> 0:13:07,000
but it's stealing keystrokes.

301
0:13:07.606,000 --> 0:13:09,000
Now, we've all looked at smartphones upside down.

302
0:13:10.353,000 --> 0:13:12,000
Every security expert wants to hack a smartphone,

303
0:13:12.543,000 --> 0:13:16,000
and we tend to look at the USB port, the GPS for tracking,

304
0:13:17.155,000 --> 0:13:2,000
the camera, the microphone, but no one up till this point

305
0:13:20.363,000 --> 0:13:21,000
had looked at the accelerometer.

306
0:13:21.943,000 --> 0:13:22,000
The accelerometer is the thing that determines

307
0:13:23.59,000 --> 0:13:26,000
the vertical orientation of the smartphone.

308
0:13:27.084,000 --> 0:13:28,000
And so they had a simple setup.

309
0:13:28.501,000 --> 0:13:3,000
They put a smartphone next to a keyboard,

310
0:13:31.259,000 --> 0:13:33,000
and they had people type, and then their goal was

311
0:13:33.971,000 --> 0:13:35,000
to use the vibrations that were created by typing

312
0:13:36.827,000 --> 0:13:4,000
to measure the change in the accelerometer reading

313
0:13:41.067,000 --> 0:13:44,000
to determine what the person had been typing.

314
0:13:44.243,000 --> 0:13:46,000
Now, when they tried this on an iPhone 3GS,

315
0:13:46.819,000 --> 0:13:48,000
this is a graph of the perturbations that were created

316
0:13:49.588,000 --> 0:13:52,000
by the typing, and you can see that it's very difficult

317
0:13:52.829,000 --> 0:13:55,000
to tell when somebody was typing or what they were typing,

318
0:13:55.907,000 --> 0:13:58,000
but the iPhone 4 greatly improved the accelerometer,

319
0:13:58.997,000 --> 0:14:01,000
and so the same measurement

320
0:14:02.477,000 --> 0:14:03,000
produced this graph.

321
0:14:04.309,000 --> 0:14:06,000
Now that gave you a lot of information while someone

322
0:14:06.795,000 --> 0:14:09,000
was typing, and what they did then is used advanced

323
0:14:10.036,000 --> 0:14:13,000
artificial intelligence techniques called machine learning

324
0:14:13.043,000 --> 0:14:14,000
to have a training phase,

325
0:14:14.474,000 --> 0:14:16,000
and so they got most likely grad students

326
0:14:16.71,000 --> 0:14:19,000
to type in a whole lot of things, and to learn,

327
0:14:20.499,000 --> 0:14:22,000
to have the system use the machine learning tools that

328
0:14:23.267,000 --> 0:14:25,000
were available to learn what it is that the people were typing

329
0:14:26.13,000 --> 0:14:28,000
and to match that up

330
0:14:28.957,000 --> 0:14:3,000
with the measurements in the accelerometer.

331
0:14:31.434,000 --> 0:14:32,000
And then there's the attack phase, where you get

332
0:14:33.069,000 --> 0:14:35,000
somebody to type something in, you don't know what it was,

333
0:14:35.88,000 --> 0:14:36,000
but you use your model that you created

334
0:14:37.177,000 --> 0:14:4,000
in the training phase to figure out what they were typing.

335
0:14:40.619,000 --> 0:14:43,000
They had pretty good success. This is an article from the USA Today.

336
0:14:44.103,000 --> 0:14:46,000
They typed in, "The Illinois Supreme Court has ruled

337
0:14:46.712,000 --> 0:14:48,000
that Rahm Emanuel is eligible to run for Mayor of Chicago"

338
0:14:49.674,000 --> 0:14:5,000
— see, I tied it in to the last talk —

339
0:14:51.028,000 --> 0:14:53,000
"and ordered him to stay on the ballot."

340
0:14:53.146,000 --> 0:14:55,000
Now, the system is interesting, because it produced

341
0:14:55.917,000 --> 0:14:57,000
"Illinois Supreme" and then it wasn't sure.

342
0:14:58.803,000 --> 0:14:59,000
The model produced a bunch of options,

343
0:15:00.753,000 --> 0:15:02,000
and this is the beauty of some of the A.I. techniques,

344
0:15:03.462,000 --> 0:15:05,000
is that computers are good at some things,

345
0:15:05.712,000 --> 0:15:06,000
humans are good at other things,

346
0:15:07.246,000 --> 0:15:08,000
take the best of both and let the humans solve this one.

347
0:15:09.177,000 --> 0:15:1,000
Don't waste computer cycles.

348
0:15:10.559,000 --> 0:15:12,000
A human's not going to think it's the Supreme might.

349
0:15:12.695,000 --> 0:15:13,000
It's the Supreme Court, right?

350
0:15:14.435,000 --> 0:15:16,000
And so, together we're able to reproduce typing

351
0:15:16.965,000 --> 0:15:18,000
simply by measuring the accelerometer.

352
0:15:19.914,000 --> 0:15:22,000
Why does this matter? Well, in the Android platform,

353
0:15:23.416,000 --> 0:15:27,000
for example, the developers have a manifest

354
0:15:27.564,000 --> 0:15:29,000
where every device on there, the microphone, etc.,

355
0:15:30.148,000 --> 0:15:31,000
has to register if you're going to use it

356
0:15:32.104,000 --> 0:15:34,000
so that hackers can't take over it,

357
0:15:34.42,000 --> 0:15:37,000
but nobody controls the accelerometer.

358
0:15:37.528,000 --> 0:15:39,000
So what's the point? You can leave your iPhone next to

359
0:15:39.744,000 --> 0:15:41,000
someone's keyboard, and just leave the room,

360
0:15:41.85,000 --> 0:15:42,000
and then later recover what they did,

361
0:15:43.489,000 --> 0:15:44,000
even without using the microphone.

362
0:15:45.2,000 --> 0:15:47,000
If someone is able to put malware on your iPhone,

363
0:15:47.374,000 --> 0:15:49,000
they could then maybe get the typing that you do

364
0:15:50.222,000 --> 0:15:52,000
whenever you put your iPhone next to your keyboard.

365
0:15:52.543,000 --> 0:15:54,000
There's several other notable attacks that unfortunately

366
0:15:54.814,000 --> 0:15:56,000
I don't have time to go into, but the one that I wanted

367
0:15:56.945,000 --> 0:15:58,000
to point out was a group from the University of Michigan

368
0:15:59.222,000 --> 0:16:01,000
which was able to take voting machines,

369
0:16:01.663,000 --> 0:16:03,000
the Sequoia AVC Edge DREs that

370
0:16:04.161,000 --> 0:16:05,000
were going to be used in New Jersey in the election

371
0:16:05.716,000 --> 0:16:07,000
that were left in a hallway, and put Pac-Man on it.

372
0:16:07.877,000 --> 0:16:1,000
So they ran the Pac-Man game.

373
0:16:11.5,000 --> 0:16:12,000
What does this all mean?

374
0:16:13.247,000 --> 0:16:16,000
Well, I think that society tends to adopt technology

375
0:16:16.894,000 --> 0:16:18,000
really quickly. I love the next coolest gadget.

376
0:16:19.718,000 --> 0:16:21,000
But it's very important, and these researchers are showing,

377
0:16:22.332,000 --> 0:16:23,000
that the developers of these things

378
0:16:23.692,000 --> 0:16:25,000
need to take security into account from the very beginning,

379
0:16:26.557,000 --> 0:16:28,000
and need to realize that they may have a threat model,

380
0:16:29.342,000 --> 0:16:31,000
but the attackers may not be nice enough

381
0:16:31.804,000 --> 0:16:32,000
to limit themselves to that threat model,

382
0:16:33.581,000 --> 0:16:35,000
and so you need to think outside of the box.

383
0:16:36.118,000 --> 0:16:37,000
What we can do is be aware

384
0:16:37.696,000 --> 0:16:39,000
that devices can be compromised,

385
0:16:40.175,000 --> 0:16:41,000
and anything that has software in it

386
0:16:41.874,000 --> 0:16:43,000
is going to be vulnerable. It's going to have bugs.

387
0:16:44.523,000 --> 0:16:47,000
Thank you very much. (Applause)

