1
0:00:,000 --> 0:00:07,000
Traducteur: Mariana Ciric Relecteur: eric vautier

2
0:00:12.82,000 --> 0:00:16,000
Aujourd'hui, je vais parler de technologie et de société.

3
0:00:18.86,000 --> 0:00:21,000
Le Département du Transport a estimé que, l'année dernière,

4
0:00:22.58,000 --> 0:00:26,000
le nombre de morts dû aux accidents de la circulation à 35 000, seulement aux USA.

5
0:00:27.86,000 --> 0:00:31,000
Au niveau mondial, c'est 1,2 million de gens qui meurent chaque année.

6
0:00:33.58,000 --> 0:00:37,000
S'il y avait une possibilité d'éliminer 90% de ces accidents,

7
0:00:37.7,000 --> 0:00:38,000
la soutiendriez-vous ?

8
0:00:39.54,000 --> 0:00:4,000
Bien sûr que oui.

9
0:00:40.86,000 --> 0:00:43,000
La technologie des voitures sans conducteur promet d'y arriver

10
0:00:44.54,000 --> 0:00:46,000
en éliminant la principale source d'accidents--

11
0:00:47.38,000 --> 0:00:48,000
l'erreur humaine.

12
0:00:49.74,000 --> 0:00:54,000
Imaginez vous, en 2030, dans une voiture sans conducteur,

13
0:00:55.18,000 --> 0:00:58,000
regardant cette ancienne vidéo TED.

14
0:00:58.66,000 --> 0:01:,000
(Rires)

15
0:01:01.34,000 --> 0:01:02,000
Quand tout à coup,

16
0:01:02.58,000 --> 0:01:05,000
une panne mécanique l'empêche de s'arrêter.

17
0:01:07.18,000 --> 0:01:08,000
Si la voiture roule toujours,

18
0:01:09.54,000 --> 0:01:13,000
elle peut écraser les piétons qui sont en train de traverser;

19
0:01:14.9,000 --> 0:01:16,000
mais elle peut aussi faire une embardée,

20
0:01:17.059,000 --> 0:01:18,000
touchant un passant en le tuant

21
0:01:18.94,000 --> 0:01:2,000
pour sauver les piétons.

22
0:01:21.86,000 --> 0:01:23,000
Que devrait faire la voiture et qui devrait décider ?

23
0:01:25.34,000 --> 0:01:28,000
Et si la voiture fonçait dans un mur,

24
0:01:28.9,000 --> 0:01:31,000
vous tuant vous, le passager

25
0:01:32.22,000 --> 0:01:34,000
pour sauver ces piétons ?

26
0:01:35.06,000 --> 0:01:38,000
Ce scénario est inspiré du problème venant des trolleybus,

27
0:01:38.78,000 --> 0:01:41,000
inventé par des philosophes il y a quelques décennies

28
0:01:42.58,000 --> 0:01:43,000
pour penser à l'éthique.

29
0:01:45.94,000 --> 0:01:47,000
Il est important de savoir ce que nous en pensons.

30
0:01:48.46,000 --> 0:01:5,000
Peut être que nous n'y pensons pas du tout.

31
0:01:51.1,000 --> 0:01:54,000
On peut trouver ce scénario irréel,

32
0:01:54.5,000 --> 0:01:56,000
extrêmement peu probable ou juste stupide.

33
0:01:57.58,000 --> 0:01:59,000
Je pense que cette critique manque d'un point important

34
0:02:00.34,000 --> 0:02:02,000
car elle suit ce scénario à la lettre.

35
0:02:03.74,000 --> 0:02:05,000
Il est évident qu'aucun accident ne ressemblera à ça,

36
0:02:06.5,000 --> 0:02:09,000
aucun accident n'a deux ou trois options

37
0:02:09.86,000 --> 0:02:11,000
où tout le monde finit par mourir.

38
0:02:13.3,000 --> 0:02:15,000
Pour contrer tout cela, la voiture pourrait calculer

39
0:02:15.9,000 --> 0:02:19,000
la probabilité d'heurter un groupe de personnes,

40
0:02:20.82,000 --> 0:02:23,000
si on dévie d'un côté ou d'un autre,

41
0:02:24.18,000 --> 0:02:27,000
on augmenterait le risque des passagers ou des conducteurs

42
0:02:27.66,000 --> 0:02:28,000
versus les piétons.

43
0:02:29.22,000 --> 0:02:31,000
Il s'agit d'un calcul un peu plus complexe,

44
0:02:32.3,000 --> 0:02:34,000
mais cela impliquerait quand même des compromis,

45
0:02:35.66,000 --> 0:02:37,000
et les compromis demandent souvent de l'éthique.

46
0:02:39.66,000 --> 0:02:41,000
On pourrait donc dire : « Ne nous en préoccupons pas.

47
0:02:42.42,000 --> 0:02:46,000
Attendons à ce que la technologie soit prête et sûre à 100%. »

48
0:02:48.34,000 --> 0:02:51,000
Imaginez qu'on puisse éliminer 90% de ces accidents,

49
0:02:52.9,000 --> 0:02:54,000
ou même 99% dans les dix prochaines années.

50
0:02:56.74,000 --> 0:02:59,000
Et si l'élimination du dernier 1% des accidents

51
0:02:59.94,000 --> 0:03:02,000
demandait plus de 50 ans de recherches ?

52
0:03:04.22,000 --> 0:03:06,000
Ne devrions-nous pas adopter cette technologie ?

53
0:03:06.556,000 --> 0:03:1,000
Ce sont 60 millions de morts à cause des accidents de voiture

54
0:03:11.34,000 --> 0:03:12,000
si nous maintenons les taux actuels.

55
0:03:14.58,000 --> 0:03:15,000
Le fait est que,

56
0:03:15.82,000 --> 0:03:18,000
attendre pour une sécurité optimale est aussi un choix

57
0:03:19.46,000 --> 0:03:21,000
mais cela implique aussi des compromis.

58
0:03:23.38,000 --> 0:03:27,000
Les gens ont trouvé plusieurs manières, sur les réseaux sociaux,

59
0:03:27.74,000 --> 0:03:29,000
pour éviter de penser à ce problème.

60
0:03:29.78,000 --> 0:03:32,000
Une personne a suggéré que la voiture devrait pouvoir dévier

61
0:03:33.02,000 --> 0:03:35,000
entre les piétons--

62
0:03:35.18,000 --> 0:03:36,000
(Rires)

63
0:03:36.22,000 --> 0:03:37,000
et les passants.

64
0:03:37.5,000 --> 0:03:4,000
Si la voiture avait les capacités de le faire, elle le ferait sûrement.

65
0:03:41.74,000 --> 0:03:43,000
Nous sommes intéressés par d'autres scénarios.

66
0:03:45.1,000 --> 0:03:5,000
Mon préféré est une suggestion de la part d'un blogueur,

67
0:03:50.54,000 --> 0:03:53,000
qui est d'avoir un bouton pour siège éjectable--

68
0:03:53.58,000 --> 0:03:54,000
(Rires)

69
0:03:54.82,000 --> 0:03:55,000
juste avant l'autodestruction.

70
0:03:56.511,000 --> 0:03:57,000
(Rires)

71
0:03:59.66,000 --> 0:04:04,000
Si nous considérons que les voitures devront aussi faire des compromis,

72
0:04:06.02,000 --> 0:04:07,000
comment y penser,

73
0:04:09.14,000 --> 0:04:1,000
mais surtout comment se décider ?

74
0:04:10.74,000 --> 0:04:13,000
Peut être devrions-nous faire un sondage auprès de la société,

75
0:04:13.9,000 --> 0:04:14,000
car finalement,

76
0:04:15.38,000 --> 0:04:18,000
les règlements et la loi sont les reflets des valeurs sociétales.

77
0:04:19.86,000 --> 0:04:2,000
Nous avons donc fait ceci.

78
0:04:21.7,000 --> 0:04:22,000
En compagnie de mes collaborateurs,

79
0:04:23.363,000 --> 0:04:25,000
Jean-François Bonnefon et Azim Shariff,

80
0:04:25.7,000 --> 0:04:26,000
nous avons mené une enquête

81
0:04:27.34,000 --> 0:04:29,000
où nous avons présenté ces scénarios aux gens.

82
0:04:30.219,000 --> 0:04:33,000
Nous leur avons donné deux options, inspirées par deux philosophes :

83
0:04:34.02,000 --> 0:04:36,000
Jeremy Bentham et Emmanuel Kant.

84
0:04:37.42,000 --> 0:04:4,000
Selon Bentham, la voiture devrait suivre l'éthique utilitariste :

85
0:04:40.54,000 --> 0:04:43,000
son action devrait être celle qui réduit tous les dommages --

86
0:04:43.98,000 --> 0:04:45,000
même si cette action finira par tuer un passant

87
0:04:46.82,000 --> 0:04:48,000
ou même si elle finira par tuer le passager.

88
0:04:49.94,000 --> 0:04:53,000
Selon Kant, la voiture devrait agir selon certains commandements,

89
0:04:54.94,000 --> 0:04:55,000
comme « Tu ne tueras point. »

90
0:04:57.3,000 --> 0:05:01,000
Ton action ne devrait donc pas nuire à un être humain de manière explicite,

91
0:05:01.78,000 --> 0:05:03,000
et tu devrais laisser la voiture faire sa route

92
0:05:04.26,000 --> 0:05:05,000
même si plus de gens finiront blessés.

93
0:05:07.46,000 --> 0:05:08,000
Qu'en pensez-vous ?

94
0:05:09.18,000 --> 0:05:1,000
Bentham ou bien Kant ?

95
0:05:11.58,000 --> 0:05:12,000
Voici notre solution.

96
0:05:12.86,000 --> 0:05:13,000
La plupart étaient pour Bentham.

97
0:05:15.98,000 --> 0:05:18,000
Les gens veulent donc que la voiture soit utilitariste,

98
0:05:19.78,000 --> 0:05:2,000
réduire les dommages,

99
0:05:21.22,000 --> 0:05:22,000
ce que nous devrions tous faire.

100
0:05:22.82,000 --> 0:05:23,000
Problème résolu.

101
0:05:25.06,000 --> 0:05:26,000
Mais il y a un petit piège.

102
0:05:27.74,000 --> 0:05:3,000
Lorsque nous leur avons demandé s'ils achèteraient ces voitures,

103
0:05:31.5,000 --> 0:05:32,000
ils ont répondu : « Absolument pas. »

104
0:05:33.348,000 --> 0:05:35,000
(Rires)

105
0:05:35.46,000 --> 0:05:38,000
Ils aimeraient acheter des voitures qui les protègent à tout prix,

106
0:05:39.38,000 --> 0:05:42,000
mais ils veulent que tous les autres les achètent pour réduire les dommages.

107
0:05:43.02,000 --> 0:05:45,000
(Rires)

108
0:05:46.54,000 --> 0:05:47,000
Nous connaissons ce type de problème.

109
0:05:48.42,000 --> 0:05:49,000
C'est un dilemme social.

110
0:05:50.98,000 --> 0:05:51,000
Pour comprendre ce dilemme social,

111
0:05:52.82,000 --> 0:05:54,000
nous devons remonter un peu dans le temps.

112
0:05:55.82,000 --> 0:05:57,000
Dans les années 1800,

113
0:05:58.42,000 --> 0:06:01,000
l'économiste anglais William Forster Lloyd a publié un pamphlet

114
0:06:02.18,000 --> 0:06:04,000
décrivant le scénario suivant.

115
0:06:04.42,000 --> 0:06:05,000
Vous avez un groupe de fermiers--

116
0:06:06.1,000 --> 0:06:07,000
fermiers anglais--

117
0:06:07.46,000 --> 0:06:09,000
qui se partagent des terres pour leurs moutons.

118
0:06:11.34,000 --> 0:06:13,000
Si chaque fermier ramène un certain nombre de moutons--

119
0:06:13.94,000 --> 0:06:14,000
disons trois moutons--

120
0:06:15.46,000 --> 0:06:17,000
la terre sera régénérée,

121
0:06:17.58,000 --> 0:06:18,000
les fermiers heureux,

122
0:06:18.82,000 --> 0:06:19,000
les moutons heureux,

123
0:06:20.46,000 --> 0:06:21,000
tout est en ordre.

124
0:06:22.26,000 --> 0:06:24,000
Maintenant si un fermier en ramène un de plus,

125
0:06:25.62,000 --> 0:06:29,000
il s'en sortira un peu mieux, au détriment de personne.

126
0:06:30.98,000 --> 0:06:33,000
Mais si chaque fermier faisait de même,

127
0:06:35.66,000 --> 0:06:37,000
la terre serait surexploitée et appauvrie

128
0:06:39.18,000 --> 0:06:41,000
au détriment de tous les fermiers

129
0:06:41.38,000 --> 0:06:43,000
et bien sûr à celui des moutons.

130
0:06:44.54,000 --> 0:06:47,000
Nous rencontrons ce problème à plusieurs endroits :

131
0:06:48.9,000 --> 0:06:51,000
dans la difficulté à gérer la surpêche,

132
0:06:52.1,000 --> 0:06:56,000
ou à réduire les émissions de carbone pour atténuer le changement climatique.

133
0:06:58.98,000 --> 0:07:,000
Quand il s'agit du règlement des voitures sans conducteur,

134
0:07:02.9,000 --> 0:07:06,000
la terre commune représente la sécurité publique --

135
0:07:07.26,000 --> 0:07:08,000
c'est le bien commun --

136
0:07:09.22,000 --> 0:07:1,000
et les fermiers sont les passagers

137
0:07:11.22,000 --> 0:07:14,000
ou les propriétaires de ces voitures qui décident de les conduire.

138
0:07:16.78,000 --> 0:07:18,000
En faisant ce choix rationnel

139
0:07:19.42,000 --> 0:07:21,000
de prioriser leur propre sécurité,

140
0:07:22.26,000 --> 0:07:25,000
ils sont peut être en train d'affaiblir le bien commun,

141
0:07:25.42,000 --> 0:07:27,000
de réduire donc les dommages.

142
0:07:30.14,000 --> 0:07:32,000
C'est la Tragédie des biens communs,

143
0:07:32.3,000 --> 0:07:33,000
typiquement,

144
0:07:33.62,000 --> 0:07:36,000
mais je pense que dans le cas des voitures sans conducteur,

145
0:07:36.74,000 --> 0:07:38,000
le problème est peut-être un peu plus insidieux

146
0:07:39.62,000 --> 0:07:42,000
car ce n'est pas forcément un seul être humain

147
0:07:43.14,000 --> 0:07:44,000
qui prend ces décisions.

148
0:07:44.86,000 --> 0:07:47,000
Les fabricants de voitures pourraient simplement les programmer

149
0:07:48.18,000 --> 0:07:5,000
pour maximiser la sécurité des clients,

150
0:07:51.9,000 --> 0:07:53,000
et elles pourraient apprendre par elles-mêmes

151
0:07:54.9,000 --> 0:07:57,000
qu'en agissant de cette manière, elles augmenteraient le risque des piétons.

152
0:07:59.34,000 --> 0:08:,000
Revenons à notre métaphore,

153
0:08:00.78,000 --> 0:08:03,000
c'est comme si maintenant nous avions des moutons électriques conscients.

154
0:08:04.42,000 --> 0:08:05,000
(Rires)

155
0:08:05.9,000 --> 0:08:08,000
Ils peuvent brouter sans que le fermier le sache.

156
0:08:10.46,000 --> 0:08:13,000
C'est ce qu'on pourrait appeler la Tragédie des communs algorithmiques,

157
0:08:14.46,000 --> 0:08:16,000
qui offre de nouveaux types de défis.

158
0:08:22.34,000 --> 0:08:23,000
De manière générale, typiquement,

159
0:08:24.26,000 --> 0:08:27,000
nous résolvons ces types de dilemmes sociaux grâce aux lois :

160
0:08:27.62,000 --> 0:08:29,000
soit les gouvernements soit les communautés se réunissent

161
0:08:30.38,000 --> 0:08:33,000
pour décider ensemble quel est le résultat voulu

162
0:08:34.14,000 --> 0:08:36,000
et quelles sortes de contraintes liées au comportement

163
0:08:36.82,000 --> 0:08:37,000
doivent être appliquées.

164
0:08:39.42,000 --> 0:08:41,000
Ensuite grâce à la surveillance et au contrôle,

165
0:08:42.06,000 --> 0:08:44,000
ils peuvent s'assurer que le bien civil soit préservé.

166
0:08:45.26,000 --> 0:08:46,000
Pourquoi ne pourrions-nous pas,

167
0:08:46.859,000 --> 0:08:47,000
en tant que régulateurs,

168
0:08:48.379,000 --> 0:08:5,000
exiger à ce que toutes les voitures réduisent les dommages ?

169
0:08:51.3,000 --> 0:08:53,000
Après tout, c'est ce que veulent les gens.

170
0:08:55.02,000 --> 0:08:56,000
Plus important encore,

171
0:08:56.46,000 --> 0:08:59,000
je peux être sûr qu'en tant qu'individu,

172
0:08:59.58,000 --> 0:09:02,000
si j'achète une voiture pouvant me sacrifier dans un cas très rare,

173
0:09:03.46,000 --> 0:09:04,000
je ne vais pas être le seul pigeon

174
0:09:05.14,000 --> 0:09:07,000
à le faire alors que tous les autres sont protégés.

175
0:09:08.94,000 --> 0:09:11,000
Dans notre enquête, nous avons demandé s'ils valideraient un telle loi

176
0:09:12.3,000 --> 0:09:13,000
et voici le résultat.

177
0:09:14.18,000 --> 0:09:17,000
Tout d'abord, les gens ont répondu non à la loi ;

178
0:09:19.1,000 --> 0:09:2,000
ensuite, ils ont dit :

179
0:09:20.38,000 --> 0:09:23,000
« Si vous légiférer pour que les voitures réduisent les dommages,

180
0:09:24.34,000 --> 0:09:25,000
je ne les achèterais pas. »

181
0:09:27.22,000 --> 0:09:28,000
Donc ironiquement,

182
0:09:28.62,000 --> 0:09:31,000
en réglant les voitures pour réduire les dommages,

183
0:09:32.14,000 --> 0:09:33,000
ça pourrait être pire

184
0:09:34.86,000 --> 0:09:37,000
car les gens n'opteraient pas pour la technologie plus sécurisée

185
0:09:38.54,000 --> 0:09:4,000
même si c'est plus sûr qu'un conducteur.

186
0:09:42.18,000 --> 0:09:45,000
Je n'ai pas la réponse finale de cette énigme,

187
0:09:45.62,000 --> 0:09:46,000
mais pour commencer,

188
0:09:47.22,000 --> 0:09:5,000
notre société devrait s'unir

189
0:09:50.54,000 --> 0:09:52,000
pour décider quels compromis nous correspondent le plus

190
0:09:54.18,000 --> 0:09:57,000
et trouver des moyens pour les appliquer.

191
0:09:58.34,000 --> 0:10:,000
Pour commencer, mes brillants étudiants,

192
0:10:00.9,000 --> 0:10:02,000
Edmond Awad et Sohan De Souza

193
0:10:03.38,000 --> 0:10:04,000
ont créé le site Moral Machine;

194
0:10:06.02,000 --> 0:10:08,000
celui-ci génère des scénarios aléatoires--

195
0:10:09.9,000 --> 0:10:11,000
une série de dilemmes à la chaîne

196
0:10:12.38,000 --> 0:10:15,000
où vous devez choisir ce que ferait la voiture dans ces cas.

197
0:10:16.86,000 --> 0:10:2,000
Nous varions les âges et la nature des différentes victimes.

198
0:10:22.86,000 --> 0:10:25,000
Jusqu'à présent, nous avons récolté plus de 5 millions de décisions

199
0:10:26.58,000 --> 0:10:28,000
prises par plus d'un million de gens

200
0:10:30.22,000 --> 0:10:31,000
depuis le site internet.

201
0:10:32.18,000 --> 0:10:34,000
Ceci nous aide à avoir une idée

202
0:10:34.62,000 --> 0:10:36,000
de quels compromis les gens sont prêts à accepter

203
0:10:37.26,000 --> 0:10:38,000
et ce qui compte pour eux--

204
0:10:39.18,000 --> 0:10:4,000
même à travers les cultures.

205
0:10:42.06,000 --> 0:10:43,000
Mais surtout,

206
0:10:43.58,000 --> 0:10:46,000
ces exercices aident les gens à reconnaître

207
0:10:46.98,000 --> 0:10:48,000
la difficulté que représente le fait de faire un choix

208
0:10:49.82,000 --> 0:10:52,000
et que les lesgilateurs sont confrontés à des choix impossibles.

209
0:10:55.18,000 --> 0:10:58,000
Peut-être que ceci nous aidera à comprendre, en tant que société,

210
0:10:58.78,000 --> 0:11:01,000
quels compromis seront mis en place dans la loi.

211
0:11:01.86,000 --> 0:11:02,000
En effet, j'ai été très heureux

212
0:11:03.62,000 --> 0:11:05,000
d'entendre que la première série

213
0:11:05.66,000 --> 0:11:07,000
venue du Département des Transports--

214
0:11:07.82,000 --> 0:11:08,000
annoncée la semaine dernière

215
0:11:09.22,000 --> 0:11:15,000
a inclus une liste de 15 points à remplir par tous les fabricants de voitures.

216
0:11:15.82,000 --> 0:11:18,000
Le numéro 14 était la consideration éthique --

217
0:11:19.1,000 --> 0:11:2,000
comment allez-vous gérer ça ?

218
0:11:23.62,000 --> 0:11:25,000
Ces personnes peuvent aussi réfléchir à leurs décisions

219
0:11:26.3,000 --> 0:11:29,000
en recevant des résumés à propos de leurs choix.

220
0:11:30.26,000 --> 0:11:31,000
Je vais vous donner un exemple --

221
0:11:31.94,000 --> 0:11:34,000
je vous avertis juste que ceci n'est pas un exemple typique,

222
0:11:35.5,000 --> 0:11:36,000
ni un utilisateur typique.

223
0:11:36.9,000 --> 0:11:39,000
Voici ce que la personne a sauvé et a tué le plus.

224
0:11:40.54,000 --> 0:11:45,000
(Rires)

225
0:11:46.5,000 --> 0:11:47,000
Certains d'entre vous allez être

226
0:11:48.42,000 --> 0:11:49,000
d'accord avec lui ou avec elle.

227
0:11:52.3,000 --> 0:11:58,000
Cette personne a aussi l'air de préférer les passagers aux piétons

228
0:11:58.46,000 --> 0:12:,000
dans ses choix

229
0:12:00.58,000 --> 0:12:03,000
et est très heureuse de punir ceux qui traversent en dehors des clous.

230
0:12:03.996,000 --> 0:12:05,000
(Rires)

231
0:12:09.14,000 --> 0:12:1,000
Résumons.

232
0:12:10.379,000 --> 0:12:13,000
Nous avons commencé avec une question -- un dilemme éthique --

233
0:12:13.82,000 --> 0:12:16,000
de ce que devrait faire la voiture dans un scénario précis :

234
0:12:16.9,000 --> 0:12:17,000
dévier ou rester ?

235
0:12:19.06,000 --> 0:12:21,000
Mais nous avons réalisé que le problème était autre.

236
0:12:21.82,000 --> 0:12:25,000
Le problème était de comment faire pour que la société accepte et applique

237
0:12:26.38,000 --> 0:12:27,000
les compromis qui lui conviennent.

238
0:12:28.34,000 --> 0:12:29,000
C'est un dilemme social.

239
0:12:29.62,000 --> 0:12:34,000
Dans les années 1940, Isaac Asimov a écrit sa célèbre loi de la robotique --

240
0:12:34.66,000 --> 0:12:35,000
les trois règles de la robotique.

241
0:12:37.06,000 --> 0:12:39,000
Un robot ne nuira pas à un être humain,

242
0:12:39.54,000 --> 0:12:41,000
un robot ne désobéira pas

243
0:12:42.1,000 --> 0:12:45,000
et il ne s'autorisera pas à faire au mal --

244
0:12:45.38,000 --> 0:12:46,000
dans cet ordre d'importance.

245
0:12:48.18,000 --> 0:12:5,000
Mais environ 40 ans plus tard

246
0:12:50.34,000 --> 0:12:53,000
et après tellement de scénarios qui ont testé ces règles,

247
0:12:54.1,000 --> 0:12:57,000
Asimov a présenté la loi Zéro

248
0:12:57.82,000 --> 0:12:59,000
qui a préséance avant tout.

249
0:13:00.1,000 --> 0:13:03,000
Un robot ne devrait pas nuire à l'Humanité.

250
0:13:04.3,000 --> 0:13:08,000
Je ne sais pas ce que cela représente dans le cas des voitures sans conducteur

251
0:13:08.7,000 --> 0:13:1,000
ou dans une situation spécifique,

252
0:13:11.46,000 --> 0:13:13,000
et je ne sais pas comment la mettre en uvre ;

253
0:13:13.7,000 --> 0:13:14,000
mais en admettant que

254
0:13:15.26,000 --> 0:13:21,000
la législation de ces voitures n'est pas uniquement un problème technique

255
0:13:21.42,000 --> 0:13:24,000
mais aussi un problème de coopération sociétale, j'espère que

256
0:13:25.62,000 --> 0:13:27,000
nous puissions au moins nous poser les bonnes questions.

257
0:13:29.02,000 --> 0:13:3,000
Merci.

258
0:13:30.26,000 --> 0:13:32,000
(Applaudissements)

