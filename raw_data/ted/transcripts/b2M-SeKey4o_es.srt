1
0:00:,000 --> 0:00:07,000
Traductor: Michael Wilson Revisor: Sebastian Betti

2
0:00:12.495,000 --> 0:00:14,000
En dos semanas será el noveno aniversario

3
0:00:15.27,000 --> 0:00:19,000
del día en que salí a aquel escenario de Jeopardy.

4
0:00:19.37,000 --> 0:00:2,000
O sea, nueve años es mucho tiempo.

5
0:00:21.22,000 --> 0:00:22,000
Y dada la demografía promedio de Jeopardy,

6
0:00:23.138,000 --> 0:00:24,000
creo que eso significa

7
0:00:24.43,000 --> 0:00:27,000
que la mayoría de la gente que me vio en ese programa ha muerto.

8
0:00:27.621,000 --> 0:00:29,000
(Risas)

9
0:00:29.804,000 --> 0:00:3,000
Pero no todos, algunos siguen con vida.

10
0:00:31.243,000 --> 0:00:33,000
Ocasionalmente me reconoce alguien en el centro comercial o donde sea.

11
0:00:33.756,000 --> 0:00:35,000
Y cuando ocurre, es por ser un poco sabelotodo.

12
0:00:36.458,000 --> 0:00:38,000
Creo que esa nave ya ha partido, y es demasiado tarde para mí.

13
0:00:38.912,000 --> 0:00:4,000
Para bien o para mal, así se me recordará,

14
0:00:41.103,000 --> 0:00:44,000
como el tipo que sabía muchas cosas raras.

15
0:00:44.337,000 --> 0:00:46,000
Y no puedo quejarme de esto.

16
0:00:46.625,000 --> 0:00:48,000
Siento que siempre fue como mi destino,

17
0:00:49.038,000 --> 0:00:52,000
aunque había pasado muchos años en el armario de datos triviales.

18
0:00:52.505,000 --> 0:00:54,000
Por lo menos, se da uno cuenta rápidamente en la adolescencia

19
0:00:55.271,000 --> 0:00:58,000
que saber el segundo nombre del Capitán Kirk no es exitoso con las chicas.

20
0:00:58.459,000 --> 0:00:59,000
(Risas)

21
0:00:59.624,000 --> 0:01:03,000
Y, en consecuencia, fui un sabihondo profundamente oculto durante muchos años.

22
0:01:03.855,000 --> 0:01:06,000
Pero si vemos más atrás, todo está ahí.

23
0:01:07.079,000 --> 0:01:1,000
Yo era el tipo de niño que siempre estaba molestando a mamá y papá

24
0:01:10.208,000 --> 0:01:12,000
con cualquier gran dato que acabara de leer;

25
0:01:13.14,000 --> 0:01:14,000
el cometa Halley o los calamares gigantes

26
0:01:14.841,000 --> 0:01:17,000
o el tamaño del pastel de calabaza más grande del mundo, o lo que fuera.

27
0:01:18.77,000 --> 0:01:21,000
Ahora tengo un hijo de 10 años que es exactamente igual.

28
0:01:21.987,000 --> 0:01:25,000
Y sé lo profundamente molesto que es eso, así que el karma sí funciona.

29
0:01:26.088,000 --> 0:01:27,000
(Risas)

30
0:01:27.588,000 --> 0:01:29,000
Yo amaba los programas de juegos, estaba fascinado con ellos.

31
0:01:30.421,000 --> 0:01:33,000
Recuerdo haber llorado en mi primer día de preescolar en 1979

32
0:01:33.827,000 --> 0:01:35,000
porque me di cuenta de que aunque tenía tantas ganas de ir a la escuela,

33
0:01:36.352,000 --> 0:01:39,000
también me iba a perder Hollywood Squares y Family Feud.

34
0:01:39.685,000 --> 0:01:43,000
Iba a perderme mis programas de concursos.

35
0:01:43.912,000 --> 0:01:44,000
Después, a mediados de la década de los 80,

36
0:01:45.602,000 --> 0:01:46,000
cuando volvió a transmitirse Jeopardy,

37
0:01:47.213,000 --> 0:01:51,000
recuerdo haber corrido a casa después del colegio cada día para ver el programa.

38
0:01:51.286,000 --> 0:01:56,000
Era mi programa favorito, aún antes de que me comprara una casa.

39
0:01:56.788,000 --> 0:01:58,000
Y vivíamos en el extranjero, en Corea del Sur, donde trabajaba mi papá,

40
0:01:59.202,000 --> 0:02:01,000
donde solamente había un canal de TV en lengua inglesa.

41
0:02:01.619,000 --> 0:02:02,000
Estaba la TV de las Fuerzas Armadas,

42
0:02:02.869,000 --> 0:02:04,000
y si no hablabas coreano, eso veías.

43
0:02:05.035,000 --> 0:02:08,000
De manera que todos mis amigos y yo corríamos a casa cada día para ver Jeopardy.

44
0:02:08.351,000 --> 0:02:11,000
Siempre fui ese tipo de niño obsesionado con la trivia.

45
0:02:11.352,000 --> 0:02:16,000
Recuerdo haber podido jugar Maratón [Trivial Pursuit] contra mis padres en los 80

46
0:02:16.535,000 --> 0:02:18,000
y me defendía, cuando eso era la moda.

47
0:02:19.369,000 --> 0:02:21,000
Hay un sentido raro de maestría que se obtiene

48
0:02:21.417,000 --> 0:02:24,000
cuando sabes algo que mamá y papá no conocen sobre su propia generación.

49
0:02:24.662,000 --> 0:02:27,000
Sabes algún detalle sobre los Beatles que papá no conocía.

50
0:02:28.218,000 --> 0:02:3,000
Y piensas, ajá, el conocimiento realmente es poder;

51
0:02:31.202,000 --> 0:02:38,000
el dato correcto aplicado justamente en el lugar correcto.

52
0:02:38.386,000 --> 0:02:39,000
Nunca tuve un consejero académico

53
0:02:40.202,000 --> 0:02:42,000
que pensara que esto fuera un camino legítimo de carrera,

54
0:02:42.586,000 --> 0:02:44,000
que pensara que pudiera estudiar trivia en el nivel superior

55
0:02:45.035,000 --> 0:02:48,000
o ser un ex concursante profesional de programa de concurso.

56
0:02:48.452,000 --> 0:02:5,000
Y así, vendí mis ideales demasiado joven.

57
0:02:50.852,000 --> 0:02:51,000
No intenté descifrar qué hacer con eso.

58
0:02:52.286,000 --> 0:02:54,000
Estudié computación porque oí decir que eso era la onda.

59
0:02:54.751,000 --> 0:02:55,000
Y me convertí en programador de cómputo

60
0:02:56.612,000 --> 0:02:57,000
--no muy bueno--

61
0:02:58.411,000 --> 0:03:03,000
ni especialmente feliz cuando aparecí por primera vez en Jeopardy en 2004.

62
0:03:03.42,000 --> 0:03:04,000
Pero eso era lo que estaba haciendo.

63
0:03:04.745,000 --> 0:03:06,000
Y por eso fue doblemente irónico, con mis antecedentes en computación,

64
0:03:07.585,000 --> 0:03:11,000
unos años después, creo que por el 2009,

65
0:03:11.801,000 --> 0:03:13,000
cuando recibí otra llamada de Jeopardy diciendo:

66
0:03:14.339,000 --> 0:03:16,000
"Es pronto aún, pero IBM nos dice

67
0:03:16.662,000 --> 0:03:21,000
que quiere construir una supercomputadora que te gane en Jeopardy.

68
0:03:21.746,000 --> 0:03:22,000
¿Te interesa?"

69
0:03:23.244,000 --> 0:03:24,000
Eso fue lo primero que supe al respecto.

70
0:03:24.834,000 --> 0:03:26,000
Y por supuesto que dije que sí, por varias razones.

71
0:03:27.502,000 --> 0:03:29,000
Una, porque jugar Jeopardy es divertidísimo.

72
0:03:29.754,000 --> 0:03:32,000
Es diversión. Es la mayor diversión que puede uno tener con los pantalones puestos.

73
0:03:33.426,000 --> 0:03:35,000
(Risas)

74
0:03:35.43,000 --> 0:03:36,000
Y lo hubiera hecho gratis.

75
0:03:36.835,000 --> 0:03:37,000
Por suerte creo que ellos no saben eso,

76
0:03:38.568,000 --> 0:03:41,000
pero yo hubiera vuelto a jugar por unos cupones de comida rápida.

77
0:03:41.786,000 --> 0:03:43,000
Me encanta Jeopardy, y siempre lo he hecho.

78
0:03:43.918,000 --> 0:03:47,000
En segundo lugar, porque soy un tipo nerd y esto parecía el futuro.

79
0:03:48.152,000 --> 0:03:49,000
Competir contra computadoras en concursos

80
0:03:50.018,000 --> 0:03:53,000
siempre imaginé que sucedería en el futuro,

81
0:03:53.07,000 --> 0:03:54,000
y ahora yo podría estar en ese escenario.

82
0:03:54.618,000 --> 0:03:55,000
No iba a decir que no.

83
0:03:55.702,000 --> 0:03:56,000
La tercera razón por la que acepté

84
0:03:56.835,000 --> 0:03:58,000
es porque tenía bastante confianza en que ganaría.

85
0:03:59.435,000 --> 0:04:01,000
Había tomado algunas clases de inteligencia artificial.

86
0:04:01.571,000 --> 0:04:05,000
Sabía que no existían computadoras que pudieran hacer lo que se requiere para ganar en Jeopardy.

87
0:04:05.901,000 --> 0:04:08,000
La gente no se percata de lo difícil que es crear ese tipo de programa,

88
0:04:09.101,000 --> 0:04:12,000
capaz de leer una pista de Jeopardy en un lenguaje natural como el inglés

89
0:04:12.661,000 --> 0:04:15,000
y comprender todos los dobles sentidos, los juegos de palabras y las pistas falsas,

90
0:04:16.286,000 --> 0:04:18,000
interpretar el significado de la pista.

91
0:04:18.853,000 --> 0:04:22,000
El tipo de cosa que un humano de 3 o 4 años de edad podría hacer,

92
0:04:23.494,000 --> 0:04:25,000
era muy difícil para una computadora.

93
0:04:25.719,000 --> 0:04:28,000
Y pensé, bueno, esto será un juego de niños.

94
0:04:28.828,000 --> 0:04:32,000
Sí, iré a destruir a la computadora en defensa de mi especie.

95
0:04:33.752,000 --> 0:04:34,000
(Risas)

96
0:04:35.603,000 --> 0:04:36,000
Pero conforme pasaron los años,

97
0:04:37.203,000 --> 0:04:4,000
IBM empezó a invertir dinero, mano de obra y velocidad de procesador en esto,

98
0:04:40.994,000 --> 0:04:42,000
empecé a recibir actualizaciones ocasionales de ellos,

99
0:04:43.085,000 --> 0:04:44,000
y empecé a preocuparme un poco.

100
0:04:44.87,000 --> 0:04:5,000
Recuerdo un artículo sobre este nuevo software para responder a preguntas, que incluía una gráfica.

101
0:04:50.953,000 --> 0:04:53,000
Era una gráfica de dispersión que mostraba el rendimiento en Jeopardy

102
0:04:54.87,000 --> 0:04:57,000
decenas de miles de puntos que representaban a los campeones de Jeopardy en la parte superior

103
0:04:58.521,000 --> 0:05:,000
con su rendimiento mostrado como el número de...

104
0:05:00.803,000 --> 0:05:04,000
iba a decir preguntas respondidas, pero más bien sería respuestas preguntadas, supongo,

105
0:05:04.869,000 --> 0:05:05,000
pistas respondidas...

106
0:05:06.828,000 --> 0:05:08,000
contra la precisión de tales respuestas.

107
0:05:09.219,000 --> 0:05:12,000
Así que hay un cierto nivel de rendimiento que tendría que alcanzar la computadora.

108
0:05:12.578,000 --> 0:05:13,000
Y al principio, era muy bajo.

109
0:05:14.369,000 --> 0:05:17,000
No existía software que pudiera competir en este tipo de escenario.

110
0:05:17.736,000 --> 0:05:19,000
Pero entonces se ve que la línea que comienza a ascender,

111
0:05:19.92,000 --> 0:05:21,000
y está llegando muy cerca a lo que llaman nube de ganadores,

112
0:05:22.403,000 --> 0:05:23,000
y noté que en la esquina superior derecha de la gráfica

113
0:05:24.077,000 --> 0:05:3,000
algunos puntos más oscuros, algunos negros, eran de otro color.

114
0:05:30.202,000 --> 0:05:31,000
Y me pregunté, ¿que serán estos?

115
0:05:31.77,000 --> 0:05:34,000
"Los puntos negros en la esquina superior representan al 74 veces campeón de Jeopardy, Ken Jennings".

116
0:05:35.662,000 --> 0:05:37,000
Y vi que esta línea venía por mí.

117
0:05:37.854,000 --> 0:05:38,000
Y me di cuenta, esto es.

118
0:05:38.954,000 --> 0:05:4,000
Así se ve cuando el futuro viene por uno.

119
0:05:40.994,000 --> 0:05:41,000
(Risas)

120
0:05:42.937,000 --> 0:05:44,000
No es la mira del arma de Terminator;

121
0:05:45.119,000 --> 0:05:49,000
es una pequeña línea que se acerca y se acerca a lo que uno hace,

122
0:05:49.286,000 --> 0:05:52,000
lo único que nos hace especiales, lo que mejor hacemos.

123
0:05:52.421,000 --> 0:05:56,000
Y cuando eventualmente ocurrió el juego como un año después,

124
0:05:57.173,000 --> 0:05:59,000
fue muy diferente a los juegos de Jeopardy que acostumbraba.

125
0:06:00.036,000 --> 0:06:02,000
No estábamos jugando en Los Ángeles en el escenario normal de Jeopardy.

126
0:06:02.537,000 --> 0:06:04,000
Watson no viaja.

127
0:06:04.803,000 --> 0:06:05,000
Watson es bastante grande.

128
0:06:05.903,000 --> 0:06:09,000
Sus miles de procesadores, un terabyte de memoria,

129
0:06:10.436,000 --> 0:06:11,000
trillones de bytes de memoria.

130
0:06:11.854,000 --> 0:06:13,000
Nos permitieron caminar por su cuarto de servidor climatizado.

131
0:06:14.287,000 --> 0:06:17,000
El único otro concursante de Jeopardy en cuyo interior me he encontrado.

132
0:06:17.912,000 --> 0:06:2,000
Así que Watson no viaja.

133
0:06:21.236,000 --> 0:06:25,000
Tienes que venir a él; debes hacer la peregrinación.

134
0:06:25.27,000 --> 0:06:27,000
De manera que el otro jugador humano y yo

135
0:06:27.327,000 --> 0:06:3,000
acabamos en un laboratorio secreto de investigación de IBM

136
0:06:30.535,000 --> 0:06:32,000
en medio de un bosque nevado en el Condado Westchester

137
0:06:33.083,000 --> 0:06:34,000
para concursar contra la computadora.

138
0:06:34.665,000 --> 0:06:36,000
Y nos percatamos de inmediato

139
0:06:36.715,000 --> 0:06:38,000
de que la computadora tenía una gran ventaja de cancha local.

140
0:06:39.614,000 --> 0:06:41,000
Había un enorme logotipo de Watson al centro de la cancha.

141
0:06:41.965,000 --> 0:06:43,000
Es como si fuera uno a jugar contra los Toros de Chicago,

142
0:06:44.632,000 --> 0:06:46,000
y está esa cosa a la mitad de su cancha.

143
0:06:46.932,000 --> 0:06:49,000
Y el público estaba repleto de personalidades y programadores de IBM

144
0:06:50.168,000 --> 0:06:51,000
echando porras a su preciosura,

145
0:06:52.065,000 --> 0:06:54,000
luego de vaciar millones de dólares en ello

146
0:06:54.431,000 --> 0:06:56,000
esperando contra esperanza que los humanos se equivocaran,

147
0:06:57.369,000 --> 0:06:58,000
y exhibiendo carteles de "Vamos Watson"

148
0:06:59.166,000 --> 0:07:03,000
y aplaudiendo como mamás en certamen cada vez que su crío acertaba.

149
0:07:03.814,000 --> 0:07:08,000
Creo que había algunos que tenían "W-A-T-S-O-N" escrito en sus barrigas con pintura de grasa.

150
0:07:09.681,000 --> 0:07:14,000
Si no pueden imaginar a programadores de cómputo con las letras "W-A-T-S-O-N" escritas en su panza,

151
0:07:15.085,000 --> 0:07:16,000
créanme que es desagradable.

152
0:07:16.752,000 --> 0:07:19,000
Pero tenían razón. Estaban exactamente en lo cierto.

153
0:07:19.799,000 --> 0:07:21,000
Yo no quería echarlo a perder, si aún conservan esto grabado en su DVR,

154
0:07:22.215,000 --> 0:07:25,000
pero Watson ganó fácilmente.

155
0:07:25.382,000 --> 0:07:27,000
Recuerdo estar parado ahí, detrás del estrado

156
0:07:28.249,000 --> 0:07:3,000
escuchando ese pequeño sonido insectoide

157
0:07:31.014,000 --> 0:07:34,000
tenía un pulgar robótico para oprimir el zumbador.

158
0:07:34.383,000 --> 0:07:38,000
Y se podía oír un pequeño tic, tic, tic, tic.

159
0:07:38.498,000 --> 0:07:4,000
Y recuerdo haber pensado, esto es.

160
0:07:41.251,000 --> 0:07:42,000
Me sentí obsoleto.

161
0:07:42.848,000 --> 0:07:45,000
Me sentí como obrero de fábrica de Detroit en los 80

162
0:07:45.899,000 --> 0:07:47,000
viendo a un robot que ahora podía realizar su trabajo en la línea de ensamblaje.

163
0:07:48.75,000 --> 0:07:53,000
Sentí que el empleo de concursante de programa de preguntas era ahora el primero en volverse obsoleto

164
0:07:53.841,000 --> 0:07:56,000
bajo este nuevo régimen de computadoras pensantes.

165
0:07:57.62,000 --> 0:07:58,000
Y no ha sido el último.

166
0:07:59.577,000 --> 0:08:01,000
Si ven las noticias, de vez en cuando verán

167
0:08:02.219,000 --> 0:08:03,000
--y yo veo esto constantemente--

168
0:08:03.911,000 --> 0:08:07,000
que en las farmacias ahora hay una máquina que puede surtir las recetas automáticamente

169
0:08:08.669,000 --> 0:08:1,000
sin necesidad de un farmacólogo humano.

170
0:08:11.078,000 --> 0:08:13,000
Y muchos despachos legales están eliminando a sus asistentes

171
0:08:13.269,000 --> 0:08:17,000
debido a la existencia de software que resume leyes y decisiones relevantes a un caso.

172
0:08:17.661,000 --> 0:08:19,000
Ya no se necesitan asistentes humanos para eso.

173
0:08:20.169,000 --> 0:08:23,000
Leí el otro día sobre un programa donde se ingresa un puntaje

174
0:08:23.302,000 --> 0:08:24,000
de un partido de beisbol o de futbol

175
0:08:25.086,000 --> 0:08:27,000
y produce un artículo deportivo como si un humano hubiese visto el partido

176
0:08:27.662,000 --> 0:08:28,000
y lo estuviera comentando.

177
0:08:29.087,000 --> 0:08:32,000
Y obviamente, estas nuevas tecnologías no pueden hacer un trabajo tan listo o creativo

178
0:08:33.086,000 --> 0:08:34,000
como los humanos a quienes reemplazan,

179
0:08:34.521,000 --> 0:08:37,000
pero son más rápidos, y lo crucial, mucho, mucho más baratos.

180
0:08:38.338,000 --> 0:08:43,000
así que me pregunto cuales serán los efectos económicos de esto.

181
0:08:43.355,000 --> 0:08:46,000
He leído a economistas que dicen que como consecuencia de estas nuevas tecnologías

182
0:08:46.701,000 --> 0:08:48,000
entraremos a una nueva era dorada del tiempo libre

183
0:08:48.826,000 --> 0:08:5,000
donde todos tendremos tiempo las cosas que realmente amamos

184
0:08:51.119,000 --> 0:08:57,000
porque todas estas tareas onerosas serán atendidas por Watson y sus hermanos digitales.

185
0:08:58.091,000 --> 0:08:59,000
He escuchado a otras personas decir lo contrario,

186
0:08:59.805,000 --> 0:09:01,000
que esta es otro sector más de la clase media

187
0:09:02.021,000 --> 0:09:07,000
a la que una nueva tecnología le priva de lo que pueden hacer

188
0:09:07.055,000 --> 0:09:08,000
y que esto es, de hecho, algo amenazante,

189
0:09:08.619,000 --> 0:09:09,000
algo de lo cual debiéramos preocuparnos.

190
0:09:10.124,000 --> 0:09:11,000
Yo no soy economista.

191
0:09:11.973,000 --> 0:09:14,000
Lo único que sé es como me sentí como el tipo que perdió su empleo.

192
0:09:15.245,000 --> 0:09:18,000
Y fue malditamente desmoralizador. Fue terrible.

193
0:09:18.305,000 --> 0:09:2,000
Aquí estaba la única cosa en la que he destacado,

194
0:09:20.536,000 --> 0:09:24,000
y solamente le tomó a IBM unas decenas de millones de dólares y su gente más lista

195
0:09:24.689,000 --> 0:09:26,000
y miles de procesadores funcionando en paralelo

196
0:09:27.222,000 --> 0:09:29,000
hacer la misma cosa.

197
0:09:29.306,000 --> 0:09:32,000
Lo podían hacer un poco más rápido y un poco mejor en la TV nacional,

198
0:09:32.537,000 --> 0:09:34,000
y "lo siento, Ken. Ya no te necesitamos".

199
0:09:35.272,000 --> 0:09:38,000
Y me hizo pensar, esto qué significa,

200
0:09:39.005,000 --> 0:09:4,000
si vamos a poder empezar a subcontratar,

201
0:09:40.744,000 --> 0:09:43,000
no solamente las funciones cerebrales inferiores sin importancia.

202
0:09:44.369,000 --> 0:09:46,000
Estoy seguro de que muchos de Uds. recuerdan un tiempo remoto

203
0:09:46.844,000 --> 0:09:48,000
en el que teníamos que saber los números telefónicos, cuando sabíamos los números de nuestros amigos.

204
0:09:49.671,000 --> 0:09:5,000
Y repentinamente hubo una máquina que hacía eso,

205
0:09:51.662,000 --> 0:09:53,000
y ahora ya no necesitamos recordarlos.

206
0:09:54.055,000 --> 0:09:55,000
He leído que de hecho ya existe evidencia

207
0:09:55.938,000 --> 0:09:59,000
de que el hipocampo, la parte de nuestro cerebro que maneja las relaciones espaciales,

208
0:10:00.382,000 --> 0:10:01,000
se atrofia y encoge físicamente

209
0:10:02.036,000 --> 0:10:04,000
en personas que usan herramientas como GPS,

210
0:10:04.638,000 --> 0:10:06,000
debido a que ya no ejercitan su sentido de orientación.

211
0:10:07.202,000 --> 0:10:09,000
Solamente estamos obedeciendo a una vocecita que nos habla desde el tablero.

212
0:10:09.536,000 --> 0:10:11,000
Y en consecuencia, una parte de nuestro cerebro que debiera encargarse de estas cosas

213
0:10:12.14,000 --> 0:10:13,000
se hace más pequeña y tonta.

214
0:10:13.953,000 --> 0:10:16,000
Y me hizo pensar, ¿qué pasa cuando las computadoras ahora son mejores

215
0:10:17.744,000 --> 0:10:2,000
para saber y recordar cosas que nosotros mismos?

216
0:10:21.244,000 --> 0:10:25,000
¿Va a encogerse y atrofiarse todo nuestro cerebro de esa manera?

217
0:10:25.32,000 --> 0:10:28,000
¿Vamos a empezar a valorar menos el conocimiento culturalmente?

218
0:10:29.087,000 --> 0:10:32,000
Como alguien quien siempre ha creído la importancia de lo que sabemos,

219
0:10:32.959,000 --> 0:10:39,000
esta idea me resultó aterradora.

220
0:10:40.183,000 --> 0:10:44,000
Entre más lo pensé, me di cuenta de que no, aún es importante.

221
0:10:45.16,000 --> 0:10:47,000
Las cosas que sabemos siguen importando.

222
0:10:47.369,000 --> 0:10:5,000
Llegué a creer que habían dos ventajas

223
0:10:50.371,000 --> 0:10:53,000
para aquellos que tenemos estas cosas en nuestras cabezas

224
0:10:54.042,000 --> 0:10:58,000
sobre alguien que dice: "Si, claro. Lo puedo buscar con Google. Espera un segundo".

225
0:10:58.868,000 --> 0:11:,000
Hay una ventaja de volumen, y hay una ventaja de tiempo.

226
0:11:01.536,000 --> 0:11:02,000
Primero, la ventaja de volumen,

227
0:11:03.092,000 --> 0:11:05,000
simplemente tiene que ver con la complejidad del mundo actual.

228
0:11:05.724,000 --> 0:11:06,000
Hay tanta información.

229
0:11:07.141,000 --> 0:11:08,000
Ser un hombre o mujer del Renacimiento,

230
0:11:08.425,000 --> 0:11:1,000
eso solamente era posible durante el Renacimiento.

231
0:11:10.591,000 --> 0:11:11,000
Ahora bien, no es realmente posible

232
0:11:11.857,000 --> 0:11:14,000
estar razonablemente educado en todos los campos del quehacer humano.

233
0:11:14.941,000 --> 0:11:15,000
Simplemente hay demasiado.

234
0:11:16.545,000 --> 0:11:18,000
Se dice que el monto de la información humana

235
0:11:19.042,000 --> 0:11:21,000
ahora se duplica aproximadamente cada 18 meses,

236
0:11:21.091,000 --> 0:11:23,000
la suma total de la información humana.

237
0:11:23.627,000 --> 0:11:26,000
Eso significa que entre ahora y fines del 2014,

238
0:11:27.209,000 --> 0:11:3,000
generaremos tanta información, en gigabytes,

239
0:11:30.676,000 --> 0:11:34,000
como acumuló toda la humanidad en los milenios anteriores.

240
0:11:34.756,000 --> 0:11:36,000
Se está duplicando cada 18 meses ahora.

241
0:11:36.827,000 --> 0:11:39,000
Esto es aterrador porque muchas de las grandes decisiones que tomamos

242
0:11:40.186,000 --> 0:11:43,000
requieren el dominio de muchos tipos diferentes de datos.

243
0:11:43.462,000 --> 0:11:48,000
Una decisión como ¿a dónde iré a la escuela? ¿Qué carrera debo estudiar?

244
0:11:48.494,000 --> 0:11:49,000
¿Por quién votaré?

245
0:11:49.928,000 --> 0:11:5,000
¿Tomo este o aquel empleo?

246
0:11:51.694,000 --> 0:11:54,000
Estas son decisiones que requieren juicios correctos

247
0:11:55.328,000 --> 0:11:57,000
sobre muchos tipos diferentes de datos.

248
0:11:57.448,000 --> 0:11:58,000
Si tenemos esos datos en nuestra mente,

249
0:11:59.185,000 --> 0:12:01,000
podremos tomar decisiones informadas.

250
0:12:01.885,000 --> 0:12:03,000
Por otra parte, si necesitamos buscarlos,

251
0:12:04.552,000 --> 0:12:05,000
podríamos estar en apuros.

252
0:12:06.412,000 --> 0:12:07,000
Según una encuesta de National Geographic que acabo de ver,

253
0:12:08.35,000 --> 0:12:1,000
algo como el 80 %

254
0:12:10.67,000 --> 0:12:14,000
de la gente que vota en las elecciones presidenciales de EE.UU., sobre asuntos como política exterior

255
0:12:14.685,000 --> 0:12:16,000
no puede ubicar a Iraq o a Afganistán en el mapa.

256
0:12:17.652,000 --> 0:12:19,000
Si no puedes realizar ese primer paso,

257
0:12:20.218,000 --> 0:12:23,000
¿Realmente vas a buscar los otros mil datos que requerirás saber

258
0:12:23.253,000 --> 0:12:25,000
para dominar el conocimiento sobre política externa de EE.UU.?

259
0:12:26.219,000 --> 0:12:27,000
Muy probablemente no.

260
0:12:27.484,000 --> 0:12:28,000
En algún momento solo dirás,

261
0:12:28.801,000 --> 0:12:29,000
"¿Sabes qué? Hay demasiado que saber. Al diablo".

262
0:12:30.668,000 --> 0:12:31,000
Y tomarás una decisión menos informada.

263
0:12:32.468,000 --> 0:12:35,000
El otro asunto es la ventaja de tiempo que tienes

264
0:12:35.952,000 --> 0:12:36,000
si tienes todos estos datos en mano.

265
0:12:37.886,000 --> 0:12:4,000
Siempre pienso en la historia de una niña pequeña llamada Tilly Smith.

266
0:12:41.302,000 --> 0:12:43,000
Ella era una niña de 10 años de Surrey, Inglaterra,

267
0:12:43.985,000 --> 0:12:46,000
de vacaciones con sus padres hace unos años en Phuket, Tailandia.

268
0:12:47.318,000 --> 0:12:48,000
Ella fue corriendo hacia ellos en la playa una mañana

269
0:12:48.968,000 --> 0:12:5,000
y les dijo: "Mamá, papá, tenemos que irnos de la playa".

270
0:12:51.553,000 --> 0:12:53,000
Y ellos le dijeron: "¿Qué quieres decir? Acabamos de llegar".

271
0:12:54.1,000 --> 0:12:57,000
Y ella les dijo: "En clase de geografía con el Sr. Kearney el mes pasado

272
0:12:57.2,000 --> 0:12:59,000
él nos dijo que cuando la marea sale súbitamente al mar

273
0:13:00.135,000 --> 0:13:01,000
y ves olas batiéndose a lo lejos,

274
0:13:01.827,000 --> 0:13:05,000
es la señal de un tsunami, y necesitas alejarte de la playa".

275
0:13:06.025,000 --> 0:13:08,000
¿Qué harías si tu hija de 10 años viniera a decirte esto?

276
0:13:08.22,000 --> 0:13:09,000
Sus padres lo pensaron,

277
0:13:09.564,000 --> 0:13:11,000
y finalmente, para su bien, decidieron creerle.

278
0:13:11.698,000 --> 0:13:13,000
Le dijeron al salvavidas, fueron de vuelta a su hotel,

279
0:13:13.713,000 --> 0:13:16,000
y el salvavidas despejó a más de 100 personas de la playa, por suerte,

280
0:13:17.597,000 --> 0:13:19,000
porque ese fue el día del tsunami del Día del Box,

281
0:13:20.502,000 --> 0:13:21,000
el día después de la Navidad 2004,

282
0:13:22.097,000 --> 0:13:25,000
que mató a miles de personas en el Sureste de Asia y alrededor del Océano Índico.

283
0:13:26.063,000 --> 0:13:28,000
Pero no en esa playa, no en la Playa Mai Khao,

284
0:13:28.537,000 --> 0:13:33,000
porque esta niñita había recordado un dato de su maestro de geografía el mes anterior.

285
0:13:33.751,000 --> 0:13:35,000
Ahora, cuando los datos se vuelven así de útiles...

286
0:13:35.848,000 --> 0:13:38,000
me encanta ese relato porque demuestra el poder de un dato,

287
0:13:39.463,000 --> 0:13:43,000
un dato recordado en exactamente el lugar y momento correctos...

288
0:13:43.735,000 --> 0:13:45,000
normalmente algo que es más fácil ver en programas de concurso que en la vida real.

289
0:13:46.48,000 --> 0:13:48,000
Pero en este caso, sucedió en la vida real.

290
0:13:48.583,000 --> 0:13:49,000
Y sucede en la vida real constantemente.

291
0:13:50.202,000 --> 0:13:52,000
No siempre es un tsunami, frecuentemente es una situación social.

292
0:13:52.416,000 --> 0:13:56,000
Es una reunión de trabajo, o una entrevista de empleo, o una primera cita

293
0:13:57.129,000 --> 0:13:58,000
o alguna relación que se lubrica

294
0:13:58.803,000 --> 0:14:01,000
porque dos personas se percatan de que comparten alguna pieza de conocimiento.

295
0:14:02.183,000 --> 0:14:04,000
Dices de dónde eres, y yo digo: "Ah, claro".

296
0:14:04.734,000 --> 0:14:05,000
O tu alma máter o tu empleo,

297
0:14:06.367,000 --> 0:14:07,000
y solamente sé alguna pequeñez al respecto,

298
0:14:08.284,000 --> 0:14:09,000
lo suficiente para echar a andar las cosas.

299
0:14:09.833,000 --> 0:14:1,000
La gente ama ese vínculo que se crea

300
0:14:11.702,000 --> 0:14:13,000
cuando alguien sabe algo sobre ti.

301
0:14:13.994,000 --> 0:14:17,000
Es como si se hubieran ocupado de conocerte antes de encontrarte.

302
0:14:18.004,000 --> 0:14:19,000
Esa es frecuentemente la ventaja de tiempo.

303
0:14:19.416,000 --> 0:14:2,000
Y no es efectivo si dices: "Bueno, espera.

304
0:14:20.901,000 --> 0:14:25,000
Eres de Fargo, Dakota del Norte. Déjame ver que sale.

305
0:14:26.465,000 --> 0:14:27,000
Ah, sí. Roger Maris era de Fargo".

306
0:14:28.367,000 --> 0:14:31,000
Eso no funciona. Eso es simplemente molesto.

307
0:14:31.4,000 --> 0:14:33,000
(Risas)

308
0:14:33.847,000 --> 0:14:38,000
El gran teólogo y pensador británico del siglo XVIII, amigo del Dr. Johnson,

309
0:14:39.549,000 --> 0:14:45,000
Samuel Parr una vez declaró: "Siempre es mejor saber algo que no saberlo".

310
0:14:45.917,000 --> 0:14:49,000
Y si hubiera vivido mi vida según un credo, probablemente sería ese.

311
0:14:50.45,000 --> 0:14:55,000
Siempre he creído que las cosas que sabemos --que el conocimiento es un bien absoluto,

312
0:14:55.983,000 --> 0:14:57,000
que las cosas que hemos aprendido y que cargamos con nosotros en nuestras cabezas

313
0:14:58.783,000 --> 0:15:,000
son lo que nos hace quienes somos,

314
0:15:00.801,000 --> 0:15:02,000
como individuos y como especie.

315
0:15:03.318,000 --> 0:15:07,000
No sé si quiero vivir en un mundo donde el conocimiento sea obsoleto.

316
0:15:07.415,000 --> 0:15:1,000
No deseo vivir en un mundo donde el alfabetismo cultural ha sido reemplazado

317
0:15:10.751,000 --> 0:15:11,000
por estas burbujitas de especialidad,

318
0:15:12.661,000 --> 0:15:15,000
de manera que ninguno de nosotros sepa de las asociaciones comunes

319
0:15:16.167,000 --> 0:15:18,000
que unían a nuestras civilizaciones.

320
0:15:18.683,000 --> 0:15:19,000
No quiero ser el último sabelotodo de trivialidades

321
0:15:20.617,000 --> 0:15:21,000
sentado en una montaña en alguna parte,

322
0:15:22.035,000 --> 0:15:26,000
recitando solo las capitales de los estados y los nombres de los episodios de "Los Simpson"

323
0:15:26.4,000 --> 0:15:29,000
y las letras de canciones de Abba.

324
0:15:30.066,000 --> 0:15:34,000
Siento que nuestra civilización funciona cuando esta es una herencia cultural vasta que todos compartimos

325
0:15:34.549,000 --> 0:15:37,000
y que sabemos sin tener que externalizar a nuestros aparatos,

326
0:15:37.735,000 --> 0:15:39,000
a nuestros buscadores y teléfonos inteligentes.

327
0:15:40.34,000 --> 0:15:44,000
En el cine, cuando las computadoras como Watson comienzan a pensar,

328
0:15:44.967,000 --> 0:15:46,000
las cosas no siempre terminan bien.

329
0:15:47.55,000 --> 0:15:5,000
Esas películas nunca son sobre utopías hermosas.

330
0:15:51.348,000 --> 0:15:56,000
Siempre es un Terminator o una Matrix o un astronauta que es expulsado por una escotilla en "2001".

331
0:15:57.083,000 --> 0:16:,000
Las cosas siempre salen terriblemente mal.

332
0:16:00.318,000 --> 0:16:01,000
Y siento que estamos en un punto ahora

333
0:16:02.25,000 --> 0:16:06,000
donde necesitamos decidir el tipo de futuro en el que deseamos vivir.

334
0:16:06.333,000 --> 0:16:07,000
Es una cuestión de liderazgo,

335
0:16:07.85,000 --> 0:16:1,000
porque se vuelve una cuestión de quién conduce al futuro.

336
0:16:11.517,000 --> 0:16:17,000
Por una parte, podemos elegir entre una nueva era de oro

337
0:16:17.7,000 --> 0:16:2,000
donde la información está disponible más universalmente

338
0:16:21.002,000 --> 0:16:22,000
que nunca antes en la historia de la humanidad,

339
0:16:22.663,000 --> 0:16:24,000
donde tenemos todas las respuestas a nuestras preguntas en la punta de nuestros dedos.

340
0:16:25.034,000 --> 0:16:26,000
Y, por otro lado,

341
0:16:26.577,000 --> 0:16:28,000
tenemos el potencial de vivir en una distopía lúgubre

342
0:16:29.35,000 --> 0:16:31,000
donde las máquinas han dominado

343
0:16:31.369,000 --> 0:16:34,000
y hemos decidido que ya no es importante lo que sabemos,

344
0:16:34.418,000 --> 0:16:36,000
que el conocimiento ya no es valioso porque todo está ahí en la nube,

345
0:16:37.033,000 --> 0:16:43,000
y por qué habríamos de molestarnos con aprender algo nuevo.

346
0:16:43.144,000 --> 0:16:46,000
Esas son las dos opciones que se nos presentan. Yo sé cual futuro es el que preferiría para vivir en él.

347
0:16:46.933,000 --> 0:16:48,000
Y todos podemos tomar esa decisión.

348
0:16:49.166,000 --> 0:16:52,000
Tomamos esa decisión siendo personas curiosas e inquisitivas a quienes nos gusta aprender,

349
0:16:52.501,000 --> 0:16:55,000
que no solamente decimos: "Bueno, en cuanto suene la campana y haya terminado la clase,

350
0:16:55.637,000 --> 0:16:56,000
ya no tengo que aprender más",

351
0:16:56.96,000 --> 0:16:59,000
o "Agradezco tener mi diploma. He concluido mi aprendizaje para mi vida.

352
0:17:00.037,000 --> 0:17:02,000
Ya no tengo que aprender cosas nuevas".

353
0:17:02.396,000 --> 0:17:05,000
No, cada día debiéramos esforzarnos por aprender algo nuevo.

354
0:17:05.703,000 --> 0:17:09,000
Debiéramos tener esta curiosidad insaciable por el mundo que nos rodea.

355
0:17:09.895,000 --> 0:17:11,000
De ahí viene la gente que vemos en Jeopardy.

356
0:17:12.62,000 --> 0:17:15,000
Estos sabelotodos, son eruditos al estilo Rainman,

357
0:17:15.701,000 --> 0:17:17,000
sentados en casa memorizando el libro telefónico.

358
0:17:17.803,000 --> 0:17:18,000
He conocido a muchos de ellos.

359
0:17:19.119,000 --> 0:17:2,000
La mayoría son solamente gente normal

360
0:17:20.669,000 --> 0:17:24,000
que tiene curiosidad universal, interesados en el mundo que los rodea, curiosos sobre todo,

361
0:17:24.767,000 --> 0:17:27,000
sedientos de este conocimiento sobre cualquier tema.

362
0:17:28.086,000 --> 0:17:3,000
Podemos vivir en uno de estos dos mundos.

363
0:17:30.619,000 --> 0:17:33,000
Podemos vivir en un mundo donde nuestros cerebros, las cosas que sabemos,

364
0:17:33.92,000 --> 0:17:34,000
siguen siendo lo que nos hacen especiales,

365
0:17:35.75,000 --> 0:17:41,000
o en un mundo en el cual hemos externalizado todo eso a supercomputadoras malvadas del futuro como Watson.

366
0:17:42.12,000 --> 0:17:44,000
Damas y caballeros, la elección es suya.

367
0:17:44.768,000 --> 0:17:46,000
Muchas gracias.

