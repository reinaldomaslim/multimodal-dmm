1
0:00:,000 --> 0:00:07,000
Traducteur: Jules Daunay Relecteur: eric vautier

2
0:00:12.76,000 --> 0:00:14,000
On a une seule chance de faire une première impression,

3
0:00:15.44,000 --> 0:00:18,000
c'est aussi vrai pour un robot que pour une personne.

4
0:00:18.64,000 --> 0:00:21,000
La première fois que j'ai rencontré un de ces robots,

5
0:00:21.68,000 --> 0:00:23,000
c'était au Willow Garage, en 2008.

6
0:00:24.6,000 --> 0:00:27,000
Quand je l'ai visité, mon hôte entrait dans le bâtiment

7
0:00:27.64,000 --> 0:00:28,000
et on a rencontré ce petit gars.

8
0:00:29.24,000 --> 0:00:3,000
Il roulait dans le hall,

9
0:00:30.92,000 --> 0:00:31,000
est arrivé vers moi, s'est assis,

10
0:00:32.76,000 --> 0:00:34,000
m'a fixée d'un regard vide,

11
0:00:35.04,000 --> 0:00:36,000
n'a rien fait pendant un moment,

12
0:00:36.72,000 --> 0:00:37,000
a rapidement tourné sa tête à 180 degrés

13
0:00:38.68,000 --> 0:00:39,000
et puis s'est enfui.

14
0:00:40.24,000 --> 0:00:42,000
Ce n'était pas une super première impression.

15
0:00:42.44,000 --> 0:00:44,000
Ce que j'ai appris sur les robots ce jour-là,

16
0:00:44.64,000 --> 0:00:46,000
c'est qu'ils font un peu leur vie

17
0:00:46.84,000 --> 0:00:48,000
et ne sont pas totalement conscients de nous.

18
0:00:49,000 --> 0:00:52,000
Je pense qu'en expérimentant ces futurs robotiques possibles,

19
0:00:52.263,000 --> 0:00:54,000
on finit par en apprendre plus sur nous-mêmes

20
0:00:54.96,000 --> 0:00:55,000
que juste sur les robots.

21
0:00:56.64,000 --> 0:00:57,000
J'ai appris, ce jour-là,

22
0:00:58,000 --> 0:01:01,000
que j'avais des attentes plutôt élevées pour ce petit gars.

23
0:01:01.44,000 --> 0:01:04,000
Il ne devait pas seulement pouvoir évoluer dans l'espace physique,

24
0:01:04.64,000 --> 0:01:06,000
mais aussi dans mon espace social –

25
0:01:07.32,000 --> 0:01:09,000
il en fait partie : c'est un robot personnel.

26
0:01:09.52,000 --> 0:01:11,000
Pourquoi ne me comprend-il pas?

27
0:01:11.56,000 --> 0:01:12,000
Mon hôte m'a expliqué :

28
0:01:12.84,000 --> 0:01:15,000
« Un robot essaie d'aller d'un point A vers un point B,

29
0:01:16.04,000 --> 0:01:17,000
tu étais un obstacle sur son chemin,

30
0:01:17.84,000 --> 0:01:19,000
il a donc dû reprogrammer son parcours,

31
0:01:19.88,000 --> 0:01:2,000
réfléchir où aller

32
0:01:21.16,000 --> 0:01:22,000
et ensuite s'y rendre autrement »,

33
0:01:22.88,000 --> 0:01:24,000
ce qui n'était pas une façon de faire très efficace.

34
0:01:25.36,000 --> 0:01:28,000
Si ce robot avait réalisé que j'étais une personne, pas une chaise,

35
0:01:28.64,000 --> 0:01:3,000
que j'étais prête à m'écarter de son chemin,

36
0:01:30.76,000 --> 0:01:31,000
où qu'il ait cherché à aller,

37
0:01:32.44,000 --> 0:01:34,000
il aurait plus efficacement

38
0:01:34.68,000 --> 0:01:35,000
rempli sa mission,

39
0:01:35.96,000 --> 0:01:37,000
s'il avait détecté que j'étais humaine

40
0:01:38.2,000 --> 0:01:41,000
et que mes potentialités sont différentes des chaises et des murs.

41
0:01:41.8,000 --> 0:01:44,000
On pense souvent que ces robots viennent d'une autre planète,

42
0:01:45.04,000 --> 0:01:47,000
du futur, de la science-fiction.

43
0:01:47.2,000 --> 0:01:48,000
Et même si c'était vrai,

44
0:01:48.64,000 --> 0:01:5,000
j'aimerais soutenir qu'aujourd'hui, les robots existent,

45
0:01:51.32,000 --> 0:01:53,000
ils vivent et travaillent parmi nous.

46
0:01:54.12,000 --> 0:01:56,000
Voici deux robots qui vivent chez moi.

47
0:01:57,000 --> 0:01:59,000
Ils passent l'aspirateur et coupent l'herbe,

48
0:01:59.52,000 --> 0:02:,000
tous les jours,

49
0:02:00.76,000 --> 0:02:03,000
c'est plus que ce que je ferais, si j'avais le temps.

50
0:02:04.16,000 --> 0:02:06,000
Ils le font aussi probablement mieux que moi.

51
0:02:06.52,000 --> 0:02:08,000
Celui-ci s'occupe de mon chat.

52
0:02:09.04,000 --> 0:02:11,000
Chaque fois qu'il utilise son bac, il le nettoie,

53
0:02:11.64,000 --> 0:02:12,000
ce que je ne suis pas prête à faire,

54
0:02:13.64,000 --> 0:02:15,000
même si ça améliore beaucoup sa vie et la mienne.

55
0:02:16.24,000 --> 0:02:18,000
Comme on appelle ces robots des produits –

56
0:02:18.68,000 --> 0:02:2,000
« robot aspirateur », « robot tondeuse à gazon »,

57
0:02:21.4,000 --> 0:02:22,000
« robot bac à litière » -

58
0:02:22.919,000 --> 0:02:26,000
je pense qu'il y a beaucoup d'autres robots cachés sous nos yeux,

59
0:02:27.08,000 --> 0:02:28,000
ils sont juste devenus tellement utiles

60
0:02:28.96,000 --> 0:02:29,000
et banals

61
0:02:30.44,000 --> 0:02:32,000
qu'on les appelle par exemple « lave-vaisselle ».

62
0:02:32.96,000 --> 0:02:33,000
Ils ont de nouveaux noms.

63
0:02:34.2,000 --> 0:02:35,000
On ne les appelle plus « robots »

64
0:02:35.92,000 --> 0:02:37,000
parce qu'ils ont une raison d'être dans nos vies.

65
0:02:38.36,000 --> 0:02:39,000
De même, un thermostat –

66
0:02:39.88,000 --> 0:02:4,000
je sais que mes amis roboticiens

67
0:02:41.68,000 --> 0:02:43,000
seront sans doute gênés que j'y voie un robot,

68
0:02:44.04,000 --> 0:02:45,000
mais il a un but.

69
0:02:45.32,000 --> 0:02:47,000
Son but est de faire en sorte que ma maison soit à 19 degrés,

70
0:02:48.24,000 --> 0:02:49,000
il sent ce qui l'entoure.

71
0:02:49.52,000 --> 0:02:5,000
Quand il fait un peu froid,

72
0:02:51.12,000 --> 0:02:53,000
il crée un programme et agit sur l'univers physique.

73
0:02:53.76,000 --> 0:02:54,000
C'est de la robotique.

74
0:02:55.04,000 --> 0:02:57,000
Même s'il ne ressemble pas à Rosie, le robot des Jetson,

75
0:02:57.656,000 --> 0:02:59,000
il accomplit quelque chose qui est très utile dans ma vie.

76
0:03:00.6,000 --> 0:03:01,000
Donc je n'ai plus besoin

77
0:03:02,000 --> 0:03:04,000
d'augmenter ou de baisser la température moi-même.

78
0:03:04.6,000 --> 0:03:07,000
Les robots vivent et travaillent parmi nous aujourd'hui,

79
0:03:08.44,000 --> 0:03:1,000
mais ils ne font pas que vivre parmi nous :

80
0:03:10.8,000 --> 0:03:12,000
vous êtes probablement vous aussi un opérateur de robot.

81
0:03:13.48,000 --> 0:03:14,000
Quand vous conduisez,

82
0:03:14.76,000 --> 0:03:16,000
c'est comme si vous pilotiez un engin.

83
0:03:17,000 --> 0:03:19,000
Vous allez vous aussi d'un point A à un point B,

84
0:03:19.84,000 --> 0:03:21,000
mais vous avez sûrement une direction assistée,

85
0:03:22.08,000 --> 0:03:24,000
un système automatique de freins,

86
0:03:24.8,000 --> 0:03:27,000
peut-être une boîte automatique et même un régulateur de vitesse.

87
0:03:28.56,000 --> 0:03:3,000
Bien que votre voiture ne soit pas complètement autonome,

88
0:03:31.51,000 --> 0:03:32,000
elle l'est quand même un peu

89
0:03:32.846,000 --> 0:03:33,000
et c'est tellement utile,

90
0:03:34.2,000 --> 0:03:35,000
on peut conduire plus sûrement,

91
0:03:36.04,000 --> 0:03:39,000
on a juste l'impression que ça disparaît à l'usage, pas vrai ?

92
0:03:39.72,000 --> 0:03:4,000
Donc, en conduisant sa voiture,

93
0:03:41.32,000 --> 0:03:44,000
c'est juste comme aller d'un endroit à un autre.

94
0:03:44.44,000 --> 0:03:47,000
On n'a pas l'impression que c'est si compliqué à utiliser,

95
0:03:48.2,000 --> 0:03:49,000
toutes ces commandes.

96
0:03:49.48,000 --> 0:03:51,000
C'était si long d'apprendre à conduire

97
0:03:51.68,000 --> 0:03:53,000
qu'elles deviennent des extensions de nous-mêmes.

98
0:03:54.4,000 --> 0:03:56,000
Quand on gare sa voiture dans une place étroite,

99
0:03:57.12,000 --> 0:03:58,000
on sait où sont les côtés.

100
0:03:58.72,000 --> 0:04:01,000
Mais quand on loue une voiture, qu'on n'a peut-être jamais conduite,

101
0:04:02,000 --> 0:04:05,000
on prend du temps à s'habituer à notre nouveau corps robotique.

102
0:04:05.08,000 --> 0:04:08,000
C'est aussi vrai pour les gens qui utilisent d'autres types de robots

103
0:04:09.08,000 --> 0:04:11,000
et je voudrais vous partager quelques expériences.

104
0:04:12.24,000 --> 0:04:14,000
À propos du travail à distance.

105
0:04:14.6,000 --> 0:04:16,000
A Willow Garage, mon collègue Dallas

106
0:04:17.2,000 --> 0:04:18,000
ressemblait à ça.

107
0:04:18.8,000 --> 0:04:22,000
Il travaillait pour notre entreprise californienne, de chez lui dans l'Indiana.

108
0:04:22.88,000 --> 0:04:24,000
C'était une voix au micro pendant la plupart de nos réunions,

109
0:04:25.84,000 --> 0:04:27,000
et ça marchait plutôt bien, sauf

110
0:04:28.079,000 --> 0:04:31,000
quand on avait un débat animé, qu'on n'était pas d'accord avec lui,

111
0:04:31.48,000 --> 0:04:32,000
on pouvait juste raccrocher.

112
0:04:32.92,000 --> 0:04:33,000
(Rires)

113
0:04:33.959,000 --> 0:04:35,000
Ensuite on pouvait avoir une autre réunion

114
0:04:36.2,000 --> 0:04:38,000
et prendre les décisions dans le couloir,

115
0:04:38.92,000 --> 0:04:39,000
quand il n'était plus là.

116
0:04:40.36,000 --> 0:04:41,000
Ce n'était pas super pour lui.

117
0:04:41.96,000 --> 0:04:42,000
Comme on est fabricants de robots,

118
0:04:43.72,000 --> 0:04:45,000
on avait quelques morceaux en trop qui traînaient,

119
0:04:46.08,000 --> 0:04:48,000
alors Dallas et son ami Curt ont fabriqué ça :

120
0:04:48.6,000 --> 0:04:5,000
une sorte de Skype fixé sur un bâton équipé de roues,

121
0:04:51.56,000 --> 0:04:52,000
qui ressemble à un jouet geek idiot,

122
0:04:53.3,000 --> 0:04:55,000
mais, en vrai, c'est un des outils les plus puissants

123
0:04:56.12,000 --> 0:04:58,000
que j'aie jamais vus pour travailler à distance.

124
0:04:59.16,000 --> 0:05:02,000
Maintenant, si je ne réponds pas à la question de Dallas par mail,

125
0:05:02.68,000 --> 0:05:04,000
il peut littéralement rouler vers mon bureau,

126
0:05:04.92,000 --> 0:05:06,000
bloquer ma porte et me reposer la question –

127
0:05:07.52,000 --> 0:05:08,000
(Rires)

128
0:05:08.52,000 --> 0:05:09,000
jusqu'à ce que je réponde.

129
0:05:09.856,000 --> 0:05:11,000
Je ne vais pas le désactiver, ce serait impoli.

130
0:05:12.8,000 --> 0:05:14,000
Ce robot ne lui sert pas que pour les discussions à deux,

131
0:05:15.52,000 --> 0:05:17,000
mais aussi à être visible à la réunion des employés.

132
0:05:18.48,000 --> 0:05:19,000
Se caler sur une chaise,

133
0:05:20.2,000 --> 0:05:23,000
montrer aux gens que vous êtes là, engagé dans votre projet,

134
0:05:23.44,000 --> 0:05:24,000
c'est important

135
0:05:24.72,000 --> 0:05:26,000
et ça peut booster le travail à distance.

136
0:05:26.92,000 --> 0:05:28,000
On a pu le voir pendant des mois, puis des années,

137
0:05:29.8,000 --> 0:05:31,000
pas que dans notre entreprise, ailleurs aussi.

138
0:05:32.72,000 --> 0:05:34,000
Ce qui peut arriver de mieux avec un robot,

139
0:05:35.08,000 --> 0:05:37,000
c'est de sentir ne faire qu'un avec lui.

140
0:05:37.44,000 --> 0:05:38,000
C'est juste vous, votre corps

141
0:05:39.16,000 --> 0:05:42,000
et les gens commencent à lui reconnaître une existence propre.

142
0:05:42.28,000 --> 0:05:43,000
Donc, quand il y aura une réunion debout,

143
0:05:44.28,000 --> 0:05:45,000
les gens se placeront autour de lui

144
0:05:45.96,000 --> 0:05:47,000
comme ils l'auraient fait si vous étiez là.

145
0:05:48.2,000 --> 0:05:5,000
C'est génial, jusqu'à ce qu'il y ait des bugs.

146
0:05:50.8,000 --> 0:05:52,000
La première fois qu'ils voient ces robots,

147
0:05:52.806,000 --> 0:05:55,000
les gens cherchent les composants : « Il doit y avoir une caméra par ici »,

148
0:05:56.4,000 --> 0:05:57,000
ils tapotent votre tête :

149
0:05:58,000 --> 0:06:,000
« Tu parles trop bas, je vais augmenter ton volume. »

150
0:06:00.96,000 --> 0:06:02,000
C'est comme si un collègue venait chez vous et disait :

151
0:06:03.6,000 --> 0:06:05,000
« Tu parles trop bas, je vais monter le son de ta tête. »

152
0:06:06.52,000 --> 0:06:07,000
C'est gênant et incorrect,

153
0:06:07.8,000 --> 0:06:09,000
on a donc dû créer de nouvelles normes sociales

154
0:06:10.44,000 --> 0:06:12,000
pour utiliser ces systèmes.

155
0:06:12.68,000 --> 0:06:15,000
De même, dès qu'on commence à s'y sentir comme dans son propre corps,

156
0:06:16.12,000 --> 0:06:19,000
on se met à remarquer des choses comme : « Mon robot est trop petit. »

157
0:06:19.84,000 --> 0:06:21,000
Si Dallas me parlait – il fait 1 mètre 80 –

158
0:06:22.52,000 --> 0:06:25,000
qu'on emmenait son robot à des cocktails et à des soirées,

159
0:06:26,000 --> 0:06:27,000
où on va,

160
0:06:27.24,000 --> 0:06:3,000
et que le robot fait 1 mètre 50, ce qui est proche de ma taille,

161
0:06:30.6,000 --> 0:06:31,000
Dallas me dirait :

162
0:06:31.84,000 --> 0:06:33,000
« Les gens ne me regardent pas vraiment.

163
0:06:34.4,000 --> 0:06:36,000
J'ai l'impression de ne voir qu'une mer d'épaules –

164
0:06:37.12,000 --> 0:06:38,000
il me faut un robot plus grand. »

165
0:06:39.12,000 --> 0:06:4,000
Je lui ai répondu :

166
0:06:40.4,000 --> 0:06:41,000
« Euh, non.

167
0:06:41.72,000 --> 0:06:42,000
Tu vas te sentir à ma place aujourd'hui.

168
0:06:43.68,000 --> 0:06:46,000
Tu vas voir ce que ça fait d'être au niveau de l'épaule des gens. »

169
0:06:47.24,000 --> 0:06:5,000
Cette expérience lui a fait ressentir beaucoup d'empathie,

170
0:06:50.64,000 --> 0:06:51,000
c'était plutôt formidable.

171
0:06:51.92,000 --> 0:06:52,000
Quand il venait lui-même me voir,

172
0:06:53.6,000 --> 0:06:55,000
il ne restait plus debout, devant moi, à me parler,

173
0:06:56.04,000 --> 0:06:58,000
il s'asseyait et me parlait à mon niveau,

174
0:06:58.16,000 --> 0:06:59,000
c'était très touchant.

175
0:06:59.92,000 --> 0:07:01,000
On a donc décidé d'étudier ce phénomène en laboratoire,

176
0:07:02.6,000 --> 0:07:05,000
de voir ce que des différences, comme la taille d'un robot, changent.

177
0:07:06.286,000 --> 0:07:08,000
La moitié de notre échantillon a utilisé un petit robot,

178
0:07:09.16,000 --> 0:07:11,000
l'autre moitié, un grand robot.

179
0:07:11.6,000 --> 0:07:13,000
On a découvert que la même personne,

180
0:07:13.88,000 --> 0:07:16,000
avec exactement le même physique et qui dit exactement la même chose,

181
0:07:17.24,000 --> 0:07:19,000
est plus convaincante et paraît plus crédible

182
0:07:19.88,000 --> 0:07:2,000
avec la grande version du robot.

183
0:07:21.56,000 --> 0:07:22,000
Ce n'est pas rationnel,

184
0:07:23.4,000 --> 0:07:24,000
d'où le recours à la psychologie.

185
0:07:25.12,000 --> 0:07:27,000
En fait, Cliff Nass dirait

186
0:07:28,000 --> 0:07:31,000
qu'il nous faut interagir avec ces nouvelles technologies,

187
0:07:31.04,000 --> 0:07:33,000
malgré le fait que nous ayons de très vieux cerveaux.

188
0:07:33.8,000 --> 0:07:35,000
La psychologie humaine n'évolue pas aussi vite que la tech,

189
0:07:36.796,000 --> 0:07:37,000
on se met tout le temps à jour,

190
0:07:38.636,000 --> 0:07:39,000
pour chercher du sens dans un monde

191
0:07:40.32,000 --> 0:07:42,000
où les machines autonomes nous entourent.

192
0:07:42.68,000 --> 0:07:44,000
Normalement ce sont les gens qui parlent, pas les machines.

193
0:07:45.44,000 --> 0:07:49,000
Et donc on associe beaucoup de choses comme la taille d'une machine –

194
0:07:50.04,000 --> 0:07:51,000
pas de la personne –

195
0:07:51.32,000 --> 0:07:53,000
à celui qui utilise le système.

196
0:07:55.12,000 --> 0:07:57,000
Je pense que c'est très important

197
0:07:57.36,000 --> 0:07:58,000
quand on réfléchit à la robotique.

198
0:07:59.12,000 --> 0:08:01,000
Ce n'est pas tant réinventer l'humain,

199
0:08:01.24,000 --> 0:08:04,000
mais plutôt découvrir comment étendre ses possibilités.

200
0:08:04.4,000 --> 0:08:06,000
On en vient à utiliser les robots d'une façon très surprenante.

201
0:08:07.4,000 --> 0:08:11,000
Ces robots ne peuvent pas jouer au billard car ils n'ont pas de bras,

202
0:08:11.68,000 --> 0:08:13,000
mais ils peuvent interpeller les joueurs,

203
0:08:14.04,000 --> 0:08:17,000
ce qui peut beaucoup contribuer à créer un esprit d'équipe,

204
0:08:17.24,000 --> 0:08:18,000
c'est assez génial.

205
0:08:18.56,000 --> 0:08:2,000
Les gens qui arrivent très bien à utiliser les robots

206
0:08:21.08,000 --> 0:08:23,000
inventent même de nouveaux jeux,

207
0:08:23.16,000 --> 0:08:25,000
comme le football robotique en pleine nuit,

208
0:08:25.32,000 --> 0:08:26,000
renversant les poubelles.

209
0:08:26.8,000 --> 0:08:27,000
Tout le monde n'y arrive pas.

210
0:08:28.4,000 --> 0:08:3,000
Beaucoup de gens ont du mal à utiliser les robots.

211
0:08:30.92,000 --> 0:08:32,000
Cet homme s'est connecté à un robot,

212
0:08:33.2,000 --> 0:08:35,000
dont l'œil était tourné à 90 degrés sur la gauche.

213
0:08:35.6,000 --> 0:08:36,000
Il ne le savait pas

214
0:08:36.88,000 --> 0:08:38,000
et il s'est mis à se cogner partout au bureau,

215
0:08:39.08,000 --> 0:08:41,000
à foncer dans les gens, qui devenaient très gênés.

216
0:08:41.72,000 --> 0:08:43,000
Il en riait – son volume était bien trop fort.

217
0:08:44.08,000 --> 0:08:46,000
Cet homme me disait sur l'image :

218
0:08:46.24,000 --> 0:08:48,000
« Il faut un bouton pour couper le son. »

219
0:08:48.36,000 --> 0:08:51,000
En fait, il voulait dire qu'on ne veut pas de robots aussi turbulents.

220
0:08:51.88,000 --> 0:08:52,000
En tant que fabricants de robots,

221
0:08:53.52,000 --> 0:08:55,000
on a intégré au système des obstacles à éviter.

222
0:08:56,000 --> 0:08:59,000
Il a reçu un petit laser pour détecter les obstacles.

223
0:08:59.08,000 --> 0:09:02,000
Si l'opérateur du robot essaie de le faire aller dans une chaise,

224
0:09:02.24,000 --> 0:09:04,000
ça ne marchera pas : le robot en fera juste le tour,

225
0:09:04.76,000 --> 0:09:05,000
ce qui est une meilleure idée.

226
0:09:06.64,000 --> 0:09:09,000
Grâce à ce programme, les gens heurtaient moins d'obstacles,

227
0:09:09.84,000 --> 0:09:11,000
mais quelques personnes

228
0:09:11.96,000 --> 0:09:13,000
mettaient plus de temps à finir la classe d'obstacles,

229
0:09:14.84,000 --> 0:09:15,000
on a voulu savoir pourquoi.

230
0:09:17.08,000 --> 0:09:2,000
Il s'est avéré qu'il y a un élément important chez l'humain :

231
0:09:20.16,000 --> 0:09:22,000
le locus de contrôle, rattaché à la personnalité.

232
0:09:22.48,000 --> 0:09:25,000
Les gens dotés d'un fort locus interne de contrôle

233
0:09:25.64,000 --> 0:09:28,000
ont besoin de maîtriser leur propre destin –

234
0:09:28.72,000 --> 0:09:31,000
ils détestent laisser le contrôle à un système autonome.

235
0:09:31.84,000 --> 0:09:33,000
Ils se battent contre ce système autonome :

236
0:09:34,000 --> 0:09:37,000
« Si je veux heurter cette chaise, je le ferai. »

237
0:09:37.12,000 --> 0:09:4,000
Ils n'apprécient pas d'avoir une aide programmée,

238
0:09:40.76,000 --> 0:09:42,000
c'est une chose importante à savoir

239
0:09:43.36,000 --> 0:09:46,000
alors qu'on construit des voitures de plus en plus autonomes, non ?

240
0:09:46.68,000 --> 0:09:49,000
Comment réagiront des gens différents face à cette perte de contrôle ?

241
0:09:50.88,000 --> 0:09:52,000
Ce sera différent selon les caractéristiques de chacun.

242
0:09:53.6,000 --> 0:09:56,000
On ne peut pas traiter les humains comme s'ils étaient semblables.

243
0:09:57.12,000 --> 0:09:59,000
On diffère par notre personnalité, notre culture,

244
0:09:59.56,000 --> 0:10:01,000
parfois même, en fonction de notre état émotionnel.

245
0:10:02.04,000 --> 0:10:03,000
Pour concevoir des systèmes

246
0:10:04.04,000 --> 0:10:06,000
d'interactions entre humains et robots,

247
0:10:06.36,000 --> 0:10:08,000
on doit prendre en compte les caractéristiques humaines,

248
0:10:09.12,000 --> 0:10:1,000
pas que celles de la technologie.

249
0:10:11.64,000 --> 0:10:15,000
Avec le sens du contrôle vient le sens des responsabilités.

250
0:10:15.96,000 --> 0:10:17,000
Si vous pilotiez un robot utilisant un de ces systèmes,

251
0:10:18.84,000 --> 0:10:2,000
voilà ce à quoi l'interface ressemblerait.

252
0:10:20.92,000 --> 0:10:21,000
C'est un peu comme dans un jeu vidéo :

253
0:10:22.88,000 --> 0:10:24,000
c'est bien, car c'est très familier pour les gens.

254
0:10:25.88,000 --> 0:10:26,000
Mais le mauvais côté,

255
0:10:27.12,000 --> 0:10:29,000
c'est que les gens se croient dans un jeu vidéo.

256
0:10:29.6,000 --> 0:10:31,000
Des enfants sont venus jouer avec le système à Stanford,

257
0:10:32.48,000 --> 0:10:34,000
conduire un robot dans nos bureaux à Menlo Park

258
0:10:34.96,000 --> 0:10:35,000
et ils se sont mis à dire, par exemple :

259
0:10:36.92,000 --> 0:10:39,000
« 10 points si tu cognes le type là-bas, 20 points pour l'autre ».

260
0:10:40.12,000 --> 0:10:42,000
Ils poursuivaient les gens dans le hall.

261
0:10:42.16,000 --> 0:10:43,000
(Rires)

262
0:10:43.2,000 --> 0:10:44,000
Je leur ai dit : « Ce sont de vrais gens

263
0:10:45.16,000 --> 0:10:48,000
qui vont vraiment saigner et avoir mal si vous les cognez. »

264
0:10:48.48,000 --> 0:10:49,000
Ils répondaient : « OK, compris. »

265
0:10:50.12,000 --> 0:10:52,000
Mais cinq minutes plus tard, c'était :

266
0:10:52.2,000 --> 0:10:55,000
« 20 points pour le type là-bas, il a l'air d'avoir besoin d'être cogné. »

267
0:10:55.84,000 --> 0:10:57,000
Un peu comme dans La Stratégie Ender.

268
0:10:58,000 --> 0:10:59,000
Il y a un monde réel de l'autre côté,

269
0:10:59.96,000 --> 0:11:02,000
je pense que c'est notre responsabilité, en créant ces interfaces,

270
0:11:03.4,000 --> 0:11:04,000
d'aider à se souvenir

271
0:11:04.68,000 --> 0:11:06,000
que nos actions ont des conséquences réelles,

272
0:11:06.96,000 --> 0:11:08,000
à avoir le sens des responsabilités

273
0:11:09.28,000 --> 0:11:12,000
quand on utilise ces systèmes de plus en plus autonomes.

274
0:11:13.84,000 --> 0:11:15,000
Voici de très bons exemples

275
0:11:16.16,000 --> 0:11:19,000
d'expérimentations d'un futur robotique possible.

276
0:11:19.44,000 --> 0:11:22,000
Je pense que c'est formidable de pouvoir étendre ses possibilités,

277
0:11:23.32,000 --> 0:11:25,000
d'apprendre de notre façon de le faire,

278
0:11:25.68,000 --> 0:11:26,000
dans ces machines,

279
0:11:26.92,000 --> 0:11:28,000
tout en étant capable d'exprimer son humanité

280
0:11:29.64,000 --> 0:11:3,000
et sa personnalité.

281
0:11:30.88,000 --> 0:11:31,000
Et on a de l'empathie pour autrui

282
0:11:32.48,000 --> 0:11:35,000
quelle que soit sa taille, sa vitesse

283
0:11:35.72,000 --> 0:11:36,000
et parfois même son handicap,

284
0:11:37.16,000 --> 0:11:38,000
c'est génial.

285
0:11:38.52,000 --> 0:11:4,000
On a aussi de l'empathie pour les robots eux-mêmes.

286
0:11:41.08,000 --> 0:11:42,000
Voici un de mes robots préférés.

287
0:11:42.76,000 --> 0:11:43,000
C'est le Tweenbot.

288
0:11:44.24,000 --> 0:11:45,000
Ce petit gars a un fanion qui dit :

289
0:11:46.24,000 --> 0:11:48,000
« J'essaie d'aller à ce croisement à Manhattan »,

290
0:11:48.84,000 --> 0:11:5,000
il est mignon, il roule, et c'est tout.

291
0:11:51.64,000 --> 0:11:54,000
Il ne sait pas comment créer une carte, ni voir le monde :

292
0:11:55.12,000 --> 0:11:56,000
il demande de l'aide.

293
0:11:56.4,000 --> 0:11:57,000
Ce qui est bien chez les gens,

294
0:11:57.836,000 --> 0:12:,000
c'est qu'il a pu compter sur la gentillesse d'étrangers.

295
0:12:00.88,000 --> 0:12:03,000
Il a bien traversé le parc jusqu'à l'autre côté de Manhattan –

296
0:12:04.8,000 --> 0:12:05,000
ce qui est génial –

297
0:12:06.08,000 --> 0:12:09,000
juste parce que les gens le mettaient dans la bonne direction.

298
0:12:09.56,000 --> 0:12:09,000
(Rires)

299
0:12:10.52,000 --> 0:12:11,000
Et c'est génial, non ?

300
0:12:11.8,000 --> 0:12:13,000
On cherche à construire ce monde mixte humain et robotique

301
0:12:14.516,000 --> 0:12:17,000
dans lequel on peut coexister et travailler, tous ensemble.

302
0:12:17.96,000 --> 0:12:2,000
Pas besoin d'être complètement autonome et de tout faire tout seul.

303
0:12:21.36,000 --> 0:12:22,000
On fait les choses ensemble.

304
0:12:22.88,000 --> 0:12:23,000
Et pour y arriver,

305
0:12:24.16,000 --> 0:12:27,000
on a besoin de l'aide des artistes et designers,

306
0:12:27.44,000 --> 0:12:28,000
dirigeants politiques, juristes,

307
0:12:29.32,000 --> 0:12:31,000
psychologues, sociologues, anthropologues, etc.

308
0:12:31.56,000 --> 0:12:32,000
On a besoin de plus de points de vue

309
0:12:33.4,000 --> 0:12:35,000
si on veut faire ce que Stuart Card nous conseille :

310
0:12:36.4,000 --> 0:12:39,000
inventer le futur dans lequel on veut vraiment vivre.

311
0:12:40.36,000 --> 0:12:42,000
Je pense qu'on peut continuer à expérimenter

312
0:12:43.04,000 --> 0:12:45,000
différents futurs robotiques ensemble,

313
0:12:45.24,000 --> 0:12:49,000
et, ce faisant, on finira par en apprendre beaucoup plus sur nous-mêmes.

314
0:12:50.72,000 --> 0:12:51,000
Merci.

315
0:12:51.96,000 --> 0:12:53,000
(Applaudissements)

