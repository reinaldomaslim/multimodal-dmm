1
0:00:,000 --> 0:00:07,000
Traducteur: Emmanuel Cohen Relecteur: Elisabeth Buffard

2
0:00:25,000 --> 0:00:27,000
Comme d'autres conférenciers l'ont dit, c'est plutôt intimidant --

3
0:00:27,000 --> 0:00:3,000
particulièrement intimidant -- de parler devant vous.

4
0:00:3,000 --> 0:00:33,000
Mais contrairement à d'autres, je ne vous parlerai pas

5
0:00:33,000 --> 0:00:35,000
des mystères de l'univers, des merveilles de l'évolution,

6
0:00:35,000 --> 0:00:39,000
ou des manières intelligentes et innovantes de s'attaquer

7
0:00:39,000 --> 0:00:41,000
aux grandes inégalités de notre monde.

8
0:00:41,000 --> 0:00:46,000
Ni même des défis des états-nations dans l'économie mondiale moderne.

9
0:00:46,000 --> 0:00:5,000
Mon exposé, comme on vous l'a annoncé, parle de statistiques --

10
0:00:5,000 --> 0:00:53,000
et, pour être plus précis, d'aspects vraiment passionnants desstatistiques .

11
0:00:53,000 --> 0:00:54,000
Et c'est --

12
0:00:54,000 --> 0:00:55,000
(Rires)

13
0:00:55,000 --> 0:00:57,000
-- c'est plus un défi pour moi

14
0:00:57,000 --> 0:00:59,000
que pour tous les autres conférenciers avant et après moi.

15
0:00:59,000 --> 0:01:,000
(Rires)

16
0:01:01,000 --> 0:01:06,000
Un de mes aînés m'a dit, quand je débutais dans ce métier,

17
0:01:06,000 --> 0:01:1,000
plutôt fièrement, que les statisticiens étaient des gens qui aimaient les chiffres

18
0:01:1,000 --> 0:01:13,000
mais n'avaient pas la personnalité requise pour devenir comptables.

19
0:01:13,000 --> 0:01:15,000
(Rires)

20
0:01:15,000 --> 0:01:18,000
Il y a une autre blague que les statisticiens font à propos d'eux-mêmes,

21
0:01:18,000 --> 0:01:21,000
"Quelle est la différence entre un statisticien introverti et un statisticien extraverti ?"

22
0:01:21,000 --> 0:01:23,000
Et la réponse est,

23
0:01:23,000 --> 0:01:28,000
"L'extraverti est celui qui regarde les chaussures de l'autre."

24
0:01:28,000 --> 0:01:31,000
(Rires)

25
0:01:31,000 --> 0:01:36,000
Mais je veux vous parler de quelque chose d'utile -- concentrez-vous.

26
0:01:36,000 --> 0:01:39,000
Ce soir, il y a une réception au musée d'histoire naturelle de l'université.

27
0:01:39,000 --> 0:01:41,000
Le cadre est magnifique, j'espère que vous en conviendrez,

28
0:01:41,000 --> 0:01:46,000
et il est très représentatif du meilleur de la tradition victorienne.

29
0:01:46,000 --> 0:01:51,000
Dans ce décor magnifique, avec tous ces gens réunis -- c'est peu probable

30
0:01:51,000 --> 0:01:54,000
mais vous pourriez vous vous retrouver dans une conversation et vouloir en sortir.

31
0:01:54,000 --> 0:01:56,000
Dans ce cas voici ce que vous faites.

32
0:01:56,000 --> 0:02:,000
Quand on vous demande, "Que faites-vous dans la vie ?" -- vous dites, "Je suis statisticien."

33
0:02:,000 --> 0:02:01,000
(Rires)

34
0:02:01,000 --> 0:02:05,000
Sauf que maintenant ils sont prévenus, et qu'ils sauront que vous mentez.

35
0:02:05,000 --> 0:02:07,000
Alors il peut se passer deux choses.

36
0:02:07,000 --> 0:02:09,000
Soit ils vont se trouver un lointain cousin à l'autre bout de la salle

37
0:02:09,000 --> 0:02:11,000
et courir pour aller lui parler.

38
0:02:11,000 --> 0:02:14,000
Soit ils vont soudain avoir très soif ou faim -- et souvent les deux --

39
0:02:14,000 --> 0:02:16,000
et filer se servir à boire à ou manger.

40
0:02:16,000 --> 0:02:2,000
Et vous pourrez tranquillement parler avec qui vous voulez.

41
0:02:2,000 --> 0:02:23,000
C'est un des défis de notre profession d'essayer d'expliquer ce que nous faisons.

42
0:02:23,000 --> 0:02:28,000
Nous ne sommes pas les meilleurs convives dans les dîners en ville et les conversations mondaines...

43
0:02:28,000 --> 0:02:3,000
Et je ne suis pas vraiment doué pour cela.

44
0:02:3,000 --> 0:02:33,000
Mais ma femme -- ma petite amie alors --

45
0:02:33,000 --> 0:02:36,000
le faisait beaucoup mieux que moi.

46
0:02:36,000 --> 0:02:39,000
Il y a longtemps, quand nous nous sommes rencontrés, elle travaillait pour la BBC en Grande Bretagne,

47
0:02:39,000 --> 0:02:41,000
et je travaillais, à ce moment-là, en Amérique.

48
0:02:41,000 --> 0:02:43,000
J'étais de retour pour la voir.

49
0:02:43,000 --> 0:02:49,000
Elle l'a dit à une de ses collèges, qui a demandé "Et que fait-il dans la vie ?"

50
0:02:49,000 --> 0:02:51,000
Sarah s'est mise à réfléchir à ce que je lui avais raconté --

51
0:02:51,000 --> 0:02:55,000
et elle faisait l'effort d'écouter, à cette époque.

52
0:02:55,000 --> 0:02:57,000
(Rires)

53
0:02:58,000 --> 0:03:,000
Ne lui dites pas que j'ai dit ça.

54
0:03:,000 --> 0:03:04,000
Et elle pensait à mon travail sur la conception de modèles mathématiques

55
0:03:04,000 --> 0:03:07,000
pour comprendre l'évolution et la génétique moderne.

56
0:03:07,000 --> 0:03:1,000
Alors quand son collègue lui a dit "Que fait-il?"

57
0:03:1,000 --> 0:03:14,000
Elle a hésité et a dit, "Il fait du design"

58
0:03:14,000 --> 0:03:15,000
(Rires)

59
0:03:15,000 --> 0:03:19,000
Alors sa collègue s'est sentie vivement intéressée, étrangement,

60
0:03:19,000 --> 0:03:22,000
et elle a dit, "Et que fait-il comme design ?"

61
0:03:22,000 --> 0:03:25,000
Alors Sarah a réfléchi un peu plus et a dit, "des jeans" (ndt gènes)

62
0:03:25,000 --> 0:03:29,000
(Rires)

63
0:03:29,000 --> 0:03:31,000
"Il design des jeans." (des gènes)

64
0:03:31,000 --> 0:03:35,000
C'est mon premier amour, je n'en dirai pas plus.

65
0:03:35,000 --> 0:03:39,000
Plus généralement je voudrais que vous pensiez

66
0:03:39,000 --> 0:03:42,000
au rôle de l'incertain, de l'aléatoire et du hasard dans notre monde,

67
0:03:42,000 --> 0:03:47,000
comment nous y réagissons, et comment nous voyons les choses.

68
0:03:47,000 --> 0:03:49,000
Bon. Ca a été plutôt facile pour vous jusqu'ici --

69
0:03:49,000 --> 0:03:51,000
vous avez ri, etc -- dans les conférences jusqu'à celle-ci.

70
0:03:51,000 --> 0:03:54,000
Vous allez devoir réfléchir, et je vais vous poser des questions.

71
0:03:54,000 --> 0:03:56,000
Pour ma première question, imaginez la situation suivante.

72
0:03:56,000 --> 0:03:59,000
Si vous lancez une pièce plusieurs fois d'affilée,

73
0:03:59,000 --> 0:04:02,000
Pour une raison donnée -- que je n'expliquerai pas pour le moment --

74
0:04:02,000 --> 0:04:04,000
nous nous intéressons à un séquence particulière.

75
0:04:04,000 --> 0:04:07,000
En voici une -- face, suivi de pile, suivi de pile.

76
0:04:07,000 --> 0:04:1,000
Supposez donc que nous lancions une pièce plusieurs fois de suite.

77
0:04:1,000 --> 0:04:15,000
La séquence, face-pile-pile, qui nous tient à coeur, se produit.

78
0:04:15,000 --> 0:04:19,000
Si l'on compte : un, deux, trois, quatre, cinq, six, sept, huit, neuf, dix --

79
0:04:19,000 --> 0:04:21,000
elle se produit après le 10e lancer.

80
0:04:21,000 --> 0:04:24,000
Vous pouvez penser qu'il y a des choses plus intéressantes à faire, mais laissez moi poursuivre.

81
0:04:24,000 --> 0:04:28,000
Imaginez que cette moitié d'entre vous a des pièces, et les lance

82
0:04:28,000 --> 0:04:31,000
jusqu'à voir la séquence face-pile-pile.

83
0:04:31,000 --> 0:04:33,000
La première fois, ça se passera peut-être après le 10e lancer, comme ici.

84
0:04:33,000 --> 0:04:35,000
La seconde fois, ce sera peut-être après le 4e lancer.

85
0:04:35,000 --> 0:04:37,000
La fois suivante, après le 15e.

86
0:04:37,000 --> 0:04:4,000
Vous relancez de nombreuses fois, et vous calculez la moyenne de ces chiffres.

87
0:04:4,000 --> 0:04:43,000
Je veux que ce côté de la salle y réfléchisse.

88
0:04:43,000 --> 0:04:45,000
L'autre moitié de l'assistance, elle, n'aime pas face-pile-pile --

89
0:04:45,000 --> 0:04:48,000
ils trouvent ça ennuyeux, pour d'obscures raisons culturelles --

90
0:04:48,000 --> 0:04:51,000
et ils sont plus intéressés par une séquence différente -- face-pile-face.

91
0:04:51,000 --> 0:04:54,000
Donc, de ce côté; vous prenez vos pièces, et vous les lancez et relancez.

92
0:04:54,000 --> 0:04:57,000
Et vous comptez le nombre de lancers jusqu'à la séquence face-pile-face

93
0:04:57,000 --> 0:05:,000
et vous faites la moyenne. OK ?

94
0:05:,000 --> 0:05:02,000
De ce côté-ci, vous obtenez un nombre --

95
0:05:02,000 --> 0:05:04,000
Vous l'avez beaucoup fait, donc vous obtenez un nombre exact --

96
0:05:04,000 --> 0:05:07,000
le nombre moyen de lancers pour obtenir face-pile-pile.

97
0:05:07,000 --> 0:05:11,000
De ce côté-là -- le nombre moyen de lancers jusqu'à face-pile-face.

98
0:05:11,000 --> 0:05:13,000
C'est une règle mathématique fondamentale --

99
0:05:13,000 --> 0:05:16,000
si vous avez deux nombres, une des 3 choses suivantes est vraie.

100
0:05:16,000 --> 0:05:19,000
Ou bien ils sont égaux, ou bien l'un est plus grand que l'autre,

101
0:05:19,000 --> 0:05:2,000
ou l'autre plus grand que l'un.

102
0:05:2,000 --> 0:05:23,000
Que se passe-t-il dans notre cas ?

103
0:05:23,000 --> 0:05:25,000
Vous devez tous y penser, et vous allez devoir voter --

104
0:05:25,000 --> 0:05:26,000
nous en restons là pour le moment.

105
0:05:26,000 --> 0:05:28,000
Je ne veux pas vous laisser trop de temps

106
0:05:28,000 --> 0:05:32,000
pour y réfléchir, jusqu'à ce que tout le monde ait voté. OK.

107
0:05:32,000 --> 0:05:36,000
Vous comparez le nombre de lancers moyen jusqu'à la première apparition

108
0:05:36,000 --> 0:05:4,000
de face-pile-face avec le nombre moyen de lancers jusqu'à face-pile-pile.

109
0:05:41,000 --> 0:05:43,000
Qui pense que A est vrai --

110
0:05:43,000 --> 0:05:47,000
qu'en moyenne, ce sera plus long pour face-pile-face que pour face-pile-pile ?

111
0:05:47,000 --> 0:05:5,000
Qui pense que B est vrai -- qu'en moyenne, ils sont égaux ?

112
0:05:51,000 --> 0:05:53,000
Qui pense que C est vrai -- qu'en moyenne, il faudra moins de temps

113
0:05:53,000 --> 0:05:56,000
pour face-pile-face que pour face-pile-pile ?

114
0:05:57,000 --> 0:06:,000
OK, qui n'a pas voté ? Parce que c'est mal -- on a dit que vous deviez le faire.

115
0:06:,000 --> 0:06:01,000
(Rires)

116
0:06:02,000 --> 0:06:05,000
OK. Donc la majorité pense que B est vrai.

117
0:06:05,000 --> 0:06:08,000
Et vous serez rassurés de savoir que même de distingués mathématiciens le pensent aussi.

118
0:06:08,000 --> 0:06:12,000
C'est faux. C'est A qui est vrai.

119
0:06:12,000 --> 0:06:14,000
C'est plus long, en moyenne.

120
0:06:14,000 --> 0:06:16,000
En fait, la moyenne du nombre de lancers nécessaires pour face-pile-face est 10

121
0:06:16,000 --> 0:06:21,000
et la moyenne pour face-pile-pile est 8.

122
0:06:21,000 --> 0:06:23,000
Comment est-ce possible ?

123
0:06:24,000 --> 0:06:27,000
Les deux séquences sont-elles différentes ?

124
0:06:3,000 --> 0:06:35,000
Elles le sont. Face-pile-face se "mord la queue".

125
0:06:35,000 --> 0:06:39,000
Si vous avez face-pile-face-pile-face, vous obtenez d'un coup deux occurrences

126
0:06:39,000 --> 0:06:42,000
de la séquence en seulement cinq lancers.

127
0:06:42,000 --> 0:06:44,000
Ce n'est pas possible avec face-pile-pile.

128
0:06:44,000 --> 0:06:46,000
Ceci s'avère important.

129
0:06:46,000 --> 0:06:48,000
Il y a deux manières de voir.

130
0:06:48,000 --> 0:06:5,000
Je vais vous en donner une.

131
0:06:5,000 --> 0:06:52,000
Supposons que nous sommes en train de lancer.

132
0:06:52,000 --> 0:06:54,000
De ce côté -- rappelez-vous, vous êtes enthousiastes pour face-pile-pile,

133
0:06:54,000 --> 0:06:56,000
et vous, pour face-pile-face.

134
0:06:56,000 --> 0:06:59,000
On lancer, et on obtient face --

135
0:06:59,000 --> 0:07:,000
vous commencez à vous tortiller sur votre fauteuil

136
0:07:,000 --> 0:07:05,000
car quelque chose de grand et de merveilleux, est sur le point de se produire.

137
0:07:05,000 --> 0:07:07,000
Le second lancer donne un pile -- et vous vous emballez.

138
0:07:07,000 --> 0:07:11,000
Le champagne glacé est juste à côté de vous, vous avez refroidi les flûtes et vous vous préparez fêter ça.

139
0:07:11,000 --> 0:07:13,000
Vous attendez, haletant, le lancer final.

140
0:07:13,000 --> 0:07:15,000
Et s'il sort un face, c'est super.

141
0:07:15,000 --> 0:07:17,000
Vous êtes contents, et vous fêtez ça.

142
0:07:17,000 --> 0:07:19,000
Si c'est un pile -- bon, un peu déçus, vous rangez les flûtes

143
0:07:19,000 --> 0:07:21,000
et le champagne.

144
0:07:21,000 --> 0:07:24,000
Et vous continuez vos lancers, vous attendez la sortie du prochain face pour vous emballer de nouveau.

145
0:07:25,000 --> 0:07:27,000
De ce côté-là, vous vivez une expérience différente.

146
0:07:27,000 --> 0:07:3,000
C'est la même chose pour les deux premières parties de la séquence.

147
0:07:3,000 --> 0:07:32,000
Vous êtes un peu excités par le premier face --

148
0:07:32,000 --> 0:07:34,000
vous êtes encore plus excités avec le pile suivant.

149
0:07:34,000 --> 0:07:36,000
Et vous relancez.

150
0:07:36,000 --> 0:07:39,000
Si c'est un pile, vous sabrez le champagne.

151
0:07:39,000 --> 0:07:41,000
Si c'est un face vous êtes déçus,

152
0:07:41,000 --> 0:07:44,000
mais vous êtes de nouveau au tiers de votre motif.

153
0:07:44,000 --> 0:07:48,000
C'est une manière informelle de présenter les choses -- mais c'est pour cela qu'il y a une différence.

154
0:07:48,000 --> 0:07:5,000
Une autre manière de voir --

155
0:07:5,000 --> 0:07:52,000
si vous avez lancé huit millions de fois,

156
0:07:52,000 --> 0:07:54,000
alors nous devrions avoir un million de face-pile-face,

157
0:07:54,000 --> 0:08:01,000
et un million de face-pile-pile -- mais face-pile-face pourrait apparaître en grappes.

158
0:08:01,000 --> 0:08:03,000
Donc sur huit millions de positions, si vous voulez en placer un million

159
0:08:03,000 --> 0:08:08,000
et que certains d'entre eux se recouvrent, les paquets seront plus séparés.

160
0:08:08,000 --> 0:08:1,000
C'est une autre façon de rendre les choses intuitives.

161
0:08:1,000 --> 0:08:12,000
Qu'est-ce que je veux dire par là ?

162
0:08:12,000 --> 0:08:16,000
Il s'agit d'un exemple très, très simple, une question de probabilité facile,

163
0:08:16,000 --> 0:08:19,000
sur laquelle tout le monde se trompe -- et nous sommes en bonne compagnie.

164
0:08:19,000 --> 0:08:23,000
Ceci est une petite digression pour en venir à ma véritable passion, la génétique.

165
0:08:23,000 --> 0:08:26,000
Il y a un lien entre face-pile-face et face-pile-pile en génétique,

166
0:08:26,000 --> 0:08:29,000
et c'est le suivant.

167
0:08:29,000 --> 0:08:32,000
Quand vous lancez une pièce, vous obtenez une séquence de faces et de piles.

168
0:08:32,000 --> 0:08:35,000
Dans l'ADN il y a une séquence de non pas deux choses -- face et pile --

169
0:08:35,000 --> 0:08:38,000
mais de quatre lettres -- A, G, C et T.

170
0:08:38,000 --> 0:08:41,000
Il y a de petits ciseaux chimiques, appelés enzymes de restriction,

171
0:08:41,000 --> 0:08:43,000
qui coupent l'ADN à chaque fois qu'ils tombent sur une séquence particulière.

172
0:08:43,000 --> 0:08:47,000
C'est un outil extrêmement utile en biologie moléculaire moderne.

173
0:08:48,000 --> 0:08:51,000
Au lieu de demander, "Au bout de combien de temps obtient-on un face-pile-face ?" --

174
0:08:51,000 --> 0:08:54,000
vous pouvez demander, "De quelle taille sont les paquets si j'utilise une enzyme de restriction

175
0:08:54,000 --> 0:08:58,000
qui coupe quand elle voit G-A-A-G, par exemple,

176
0:08:58,000 --> 0:09:,000
De quelle taille sont ces paquets ?"

177
0:09:,000 --> 0:09:05,000
C'est un lien des plus courants entre probabilité et génétique.

178
0:09:05,000 --> 0:09:08,000
Il y a un lien plus profond encore, dont je n'ai pas eu le temps de parler

179
0:09:08,000 --> 0:09:11,000
et qui fait de la génétique moderne une science vraiment passionnante.

180
0:09:11,000 --> 0:09:15,000
Nous entendrons d'autres conférences à ce sujet.

181
0:09:15,000 --> 0:09:19,000
Il s'avère que, pour comprendre les secrets que recèle les données générées par les technologies

182
0:09:19,000 --> 0:09:24,000
expérimentales modernes, une des clés provient de techniques plutôt sophistiquées --

183
0:09:24,000 --> 0:09:27,000
rassurez-vous, je fais quelque chose d'utile dans mon travail,

184
0:09:27,000 --> 0:09:29,000
beaucoup plus sophistiquées que l'histoire de face-pile-face --

185
0:09:29,000 --> 0:09:33,000
la modélisation informatique et mathématique

186
0:09:33,000 --> 0:09:35,000
et des techniques statistiques modernes.

187
0:09:35,000 --> 0:09:38,000
Je vais vous donner deux aperçus-- deux exemples --

188
0:09:38,000 --> 0:09:41,000
de projets dont mon groupe à Oxford fait partie,

189
0:09:41,000 --> 0:09:43,000
qui sont tous les deux assez passionnants.

190
0:09:43,000 --> 0:09:45,000
Vous connaissez le projet Génome Humain.

191
0:09:45,000 --> 0:09:49,000
Ce projet avait pour but de séquencer un génome humain complet.

192
0:09:51,000 --> 0:09:53,000
Ce qui vient naturellement après ça --

193
0:09:53,000 --> 0:09:55,000
c'est le projet international HapMap,

194
0:09:55,000 --> 0:10:,000
qui est une collaboration de laboratoires de cinq ou six pays différents.

195
0:10:,000 --> 0:10:04,000
Pensez au projet Génome Humain comme une manière d'apprendre ce que nous avons en commun

196
0:10:04,000 --> 0:10:06,000
et au projet HapMap comme une façon de comprendre

197
0:10:06,000 --> 0:10:08,000
les différences entre des gens différents.

198
0:10:08,000 --> 0:10:1,000
Pourquoi se préoccuper de cela ?

199
0:10:1,000 --> 0:10:12,000
Et bien, il y a de nombreuses raisons.

200
0:10:12,000 --> 0:10:16,000
La plus pressante est de comprendre comment ces différences

201
0:10:16,000 --> 0:10:2,000
prédisposent certaines personnes à une maladie donnée -- disons, un diabète de type 2 --

202
0:10:2,000 --> 0:10:25,000
et que d'autres différences prédisposent d'autres à des maladies du coeur,

203
0:10:25,000 --> 0:10:27,000
ou à des accidents vasculaires cérébraux, ou à l'autisme etc.

204
0:10:27,000 --> 0:10:29,000
C'est un grand projet.

205
0:10:29,000 --> 0:10:31,000
Il y a un second grand projet.

206
0:10:31,000 --> 0:10:33,000
récemment financé par le Wellcome Trust dans ce pays,

207
0:10:33,000 --> 0:10:35,000
qui implique de vastes études --

208
0:10:35,000 --> 0:10:38,000
des milliers d'individus, et huit maladies différentes,

209
0:10:38,000 --> 0:10:42,000
des maladies courantes comme les diabètes de type 1 et 2, l'infarctus du myocarde,

210
0:10:42,000 --> 0:10:46,000
les maladies bipolaires etc -- pour essayer de comprendre leur génétique.

211
0:10:46,000 --> 0:10:49,000
Essayer de comprendre ce qui, dans les différences génétiques cause ces maladies.

212
0:10:49,000 --> 0:10:51,000
Pourquoi voulons-nous faire cela ?

213
0:10:51,000 --> 0:10:54,000
Parce que nous connaissons très peu la plupart des maladies humaines.

214
0:10:54,000 --> 0:10:56,000
Nous ne connaissons pas leurs causes.

215
0:10:56,000 --> 0:10:58,000
Et si nous pouvons approfondir et comprendre leur génétique,

216
0:10:58,000 --> 0:11:01,000
nous aurons une accès à leur manière de fonctionner.

217
0:11:01,000 --> 0:11:03,000
Une toute nouvelle façon de concevoir les thérapies

218
0:11:03,000 --> 0:11:06,000
et les traitements préventifs etc.

219
0:11:06,000 --> 0:11:09,000
C'était, comme je l'ai dit, une petite digression à ma passion principale.

220
0:11:09,000 --> 0:11:14,000
Revenons à des problèmes plus prosaïques sur la manière de penser l'incertain.

221
0:11:14,000 --> 0:11:16,000
Voici un autre quizz --

222
0:11:16,000 --> 0:11:18,000
supposez que nous ayons un test pour une maladie

223
0:11:18,000 --> 0:11:2,000
qui n'est pas infaillible, mais plutôt bon.

224
0:11:2,000 --> 0:11:23,000
Il est juste 99 pour cent du temps.

225
0:11:23,000 --> 0:11:26,000
Je choisis l'un de vous, ou quelqu'un dans la rue,

226
0:11:26,000 --> 0:11:28,000
et je fais le test pour cette maladie.

227
0:11:28,000 --> 0:11:32,000
Supposons qu'il existe un test pour le VIH -- le virus responsable du SIDA --

228
0:11:32,000 --> 0:11:35,000
et que le test soit positif pour cette personne.

229
0:11:35,000 --> 0:11:38,000
Quel est le pourcentage de chance que la personne soit effectivement malade ?

230
0:11:38,000 --> 0:11:4,000
Le test est bon 99 pour cent des fois.

231
0:11:4,000 --> 0:11:44,000
Donc la réponse naturelle est 99 pour cent.

232
0:11:44,000 --> 0:11:46,000
Qui aime cette réponse ?

233
0:11:46,000 --> 0:11:47,000
Allez -- tout le monde est concerné.

234
0:11:47,000 --> 0:11:49,000
Oubliez que vous ne me faites plus confiance.

235
0:11:49,000 --> 0:11:5,000
(Rires)

236
0:11:5,000 --> 0:11:53,000
Bien, vous avez raison d'être un peu sceptiques, parce que ce n'est pas la bonne réponse.

237
0:11:53,000 --> 0:11:55,000
C'est ce que vous pourriez croire.

238
0:11:55,000 --> 0:11:58,000
Ce n'est pas la réponse, parce qu'il manque des éléments.

239
0:11:58,000 --> 0:12:01,000
La réponse dépend en fait de la rareté de la maladie.

240
0:12:01,000 --> 0:12:03,000
Je vais vous donner un exemple.

241
0:12:03,000 --> 0:12:07,000
Voici une représentation d'un million d'individus.

242
0:12:07,000 --> 0:12:1,000
Prenons une maladie qui affecte --

243
0:12:1,000 --> 0:12:12,000
c'est une maladie rare, elle affecte une personne sur 10 000.

244
0:12:12,000 --> 0:12:15,000
Parmi ce million d'individus, la plupart sont en bonne santé

245
0:12:15,000 --> 0:12:17,000
et certains ont la maladie.

246
0:12:17,000 --> 0:12:2,000
Et, si ceci est la prévalence de la maladie,

247
0:12:2,000 --> 0:12:23,000
100 l'auront et le reste non.

248
0:12:23,000 --> 0:12:25,000
Maintenant, supposez qu'on teste tout le monde.

249
0:12:25,000 --> 0:12:27,000
Que se passe-t-il ?

250
0:12:27,000 --> 0:12:29,000
Et bien, sur les 100 qui ont la maladie,

251
0:12:29,000 --> 0:12:34,000
le test aura la bonne réponse 99 fois sur cent, et 99 seront testés positifs.

252
0:12:34,000 --> 0:12:36,000
Sur tous les autres, qui sont en bonne santé,

253
0:12:36,000 --> 0:12:39,000
le test répondra juste à 99 pour cent.

254
0:12:39,000 --> 0:12:41,000
Il sera faux seulement une fois sur cent.

255
0:12:41,000 --> 0:12:45,000
Mais il y a tellement de gens qu'il y aura un grand nombre de faux positifs.

256
0:12:45,000 --> 0:12:47,000
Dit autrement --

257
0:12:47,000 --> 0:12:52,000
sur tous ceux qui sont testés positifs -- voici les individus concernés --

258
0:12:52,000 --> 0:12:57,000
moins d'un sur 100 a vraiment la maladie.

259
0:12:57,000 --> 0:13:01,000
Donc même si nous considérons que le test est rigoureux, un élément important est

260
0:13:01,000 --> 0:13:04,000
qu'une partie de l'information est manquante.

261
0:13:04,000 --> 0:13:06,000
Voici l'intuition essentielle.

262
0:13:07,000 --> 0:13:1,000
Ce que nous devons faire, lorsque le test est positif

263
0:13:1,000 --> 0:13:16,000
est de peser la plausibilité, ou la vraisemblance, de deux explications opposées.

264
0:13:16,000 --> 0:13:19,000
Chacune de ces explications est en partie vraisemblable et en partie peu vraisemblable.

265
0:13:19,000 --> 0:13:22,000
Une explication est que la personne n'a pas la maladie --

266
0:13:22,000 --> 0:13:25,000
ce qui est extrêmement probable, si vous prenez quelqu'un au hasard --

267
0:13:25,000 --> 0:13:28,000
et que le test est faux, ce qui est peu probable.

268
0:13:29,000 --> 0:13:32,000
L'autre explication est que la personne a la maladie -- c'est peu probable --

269
0:13:32,000 --> 0:13:35,000
et que le test est juste, ce qui est probable.

270
0:13:35,000 --> 0:13:37,000
Et le nombre que nous obtenons --

271
0:13:37,000 --> 0:13:4,000
ce nombre est légèrement inférieur à 100 --

272
0:13:4,000 --> 0:13:46,000
dépend de la probabilité qu'une de ces explications est liée à l'autre.

273
0:13:46,000 --> 0:13:48,000
Chacune des deux prises ensemble est improbable.

274
0:13:49,000 --> 0:13:52,000
Voici un autre exemple pris dans l'actualité.

275
0:13:52,000 --> 0:13:56,000
Les britanniques ont entendu parler de ce cas devenu célèbre

276
0:13:56,000 --> 0:14:01,000
d'une femme nommée Sally Clark, dont les deux bébés sont morts soudainement.

277
0:14:01,000 --> 0:14:05,000
Initialement, on a pensé qu'ils étaient morts de ce que l'on appelle la mort subite du nourrisson

278
0:14:05,000 --> 0:14:08,000
(familièrement en anglais : "mort du berceau")

279
0:14:08,000 --> 0:14:1,000
Pour différentes raisons, on l'a plus tard accusée de meurtre.

280
0:14:1,000 --> 0:14:14,000
Et au procès, son procès, un pédiatre distingué a déclaré dans son témoignage

281
0:14:14,000 --> 0:14:19,000
que le pourcentage de chance d'avoir deux nourrissons morts du même syndrome, dans une famille comme la sienne --

282
0:14:19,000 --> 0:14:25,000
en activité et non fumeur -- était de 1 sur 73 millions.

283
0:14:26,000 --> 0:14:29,000
Pour faire court, elle a été condamnée à l'époque.

284
0:14:29,000 --> 0:14:34,000
Plus tard, assez récemment, elle a été acquittée en appel -- en fait, au second appel.

285
0:14:34,000 --> 0:14:38,000
Pour replacer les choses dans leur contexte, imaginez l'épreuve pour quelqu'un

286
0:14:38,000 --> 0:14:41,000
de perdre un enfant, et un deuxième, et tout en étant innocent,

287
0:14:41,000 --> 0:14:43,000
d'être reconnu coupable de meurtre.

288
0:14:43,000 --> 0:14:45,000
De subir le stress du procès, d'être condamné pour meurtre --

289
0:14:45,000 --> 0:14:48,000
et d'être détenue dans une prison pour femmes, où toutes les autres prisonnières

290
0:14:48,000 --> 0:14:53,000
pensent que vous avez tué vos enfants -- c'est vraiment horrible.

291
0:14:53,000 --> 0:14:58,000
Et c'est arrivé en grande partie parce que l'expert s'est totalement fourvoyé

292
0:14:58,000 --> 0:15:01,000
sur ses statistiques, et ce sur deux points.

293
0:15:01,000 --> 0:15:05,000
Alors d'où a-t-il sorti son chiffre de 73 millions ?

294
0:15:05,000 --> 0:15:08,000
Il a lu quelques publications de recherche, qui disaient que la chance d'une mort subite du nourrisson dans une famille

295
0:15:08,000 --> 0:15:13,000
comme celle de Sally Clark est à peu près d'une sur huit mille cinq cent.

296
0:15:13,000 --> 0:15:17,000
Et il a dit, "Je suppose que si on a un nourrisson mort du syndrome dans une famille,

297
0:15:17,000 --> 0:15:21,000
le pourcentage de chance d'avoir une second enfant mort sont inchangées."

298
0:15:21,000 --> 0:15:24,000
C'est ce que les statisticiens appelleraient une hypothèse d'indépendance.

299
0:15:24,000 --> 0:15:26,000
C'est la même chose de dire, "Si vous lancez une pièce et obtenez face la première fois,

300
0:15:26,000 --> 0:15:29,000
cela n'aura pas d'incidence sur vos chances d'avoir face la seconde fois."

301
0:15:29,000 --> 0:15:34,000
Alors si vous lancez une pièce deux fois de suite, la probabilité d'obtenir deux faces est un demi --

302
0:15:34,000 --> 0:15:37,000
soit la probabilité du premier lancer -- multiplié par un demi -- la probabilité du second lancer.

303
0:15:37,000 --> 0:15:39,000
Alors il a dit, "Supposons --

304
0:15:39,000 --> 0:15:43,000
Je suppose que ces évènements sont indépendants.

305
0:15:43,000 --> 0:15:45,000
Quand vous multipliez huit mille cinq cents par huit mille cinq cents,

306
0:15:45,000 --> 0:15:47,000
vous obtenez à peu près 73 millions."

307
0:15:47,000 --> 0:15:49,000
Et aucune de ces hypothèses n'a été exposée au tribunal

308
0:15:49,000 --> 0:15:51,000
ou présentée au jury de cette façon.

309
0:15:52,000 --> 0:15:55,000
Malheureusement -- et vraiment, de manière regrettable --

310
0:15:55,000 --> 0:15:59,000
d'abord, dans une telle situation il faudrait effectuer des vérifications empiriques.

311
0:15:59,000 --> 0:16:01,000
Et ensuite, c'est évidemment faux.

312
0:16:02,000 --> 0:16:07,000
Il y a énormément de choses que nous ne savons pas sur la mort subite du nourrisson.

313
0:16:07,000 --> 0:16:1,000
Il se peut que certains facteurs environnementaux inconnus soient à l'oeuvre,

314
0:16:1,000 --> 0:16:12,000
et il y a de fortes chances qu'il y ait

315
0:16:12,000 --> 0:16:14,000
des facteurs génétiques que nous ne connaissons pas.

316
0:16:14,000 --> 0:16:17,000
Donc si une famille est victime d'une mort d'un nourrisson, elle devient de fait partie d'un groupe à haut risque.

317
0:16:17,000 --> 0:16:19,000
Ìls sont probablement exposés à des facteurs de risque environnemental

318
0:16:19,000 --> 0:16:22,000
et/ou à ces facteurs génétiques que nous ne connaissons pas.

319
0:16:22,000 --> 0:16:25,000
Et de prétendre, alors, connaître les chances d'avoir un second décès comme si on ignorait ces informations

320
0:16:25,000 --> 0:16:28,000
est vraiment idiot.

321
0:16:28,000 --> 0:16:32,000
C'est pire qu'idiot -- c'est vraiment de la très mauvaise science.

322
0:16:32,000 --> 0:16:37,000
Néanmoins, c'est ainsi que ça a été présenté, et au procès personne n'a essayé de contester.

323
0:16:37,000 --> 0:16:39,000
C'est le premier problème.

324
0:16:39,000 --> 0:16:43,000
Le second problème est, que veut dire le nombre de un sur 73 millions ?

325
0:16:43,000 --> 0:16:45,000
Après que Sally Clark a été reconnue coupable --

326
0:16:45,000 --> 0:16:49,000
imaginez, ça a vraiment fait les gros titres de la presse --

327
0:16:49,000 --> 0:16:56,000
un des journalistes de l'un des journaux les plus respectables de Grande Bretagne a écrit

328
0:16:56,000 --> 0:16:58,000
que selon l'expert,

329
0:16:58,000 --> 0:17:03,000
"Les chances qu'elle soit innocente était d'une sur 73 millions."

330
0:17:03,000 --> 0:17:05,000
Ça, c'est une erreur de logique.

331
0:17:05,000 --> 0:17:08,000
C'est exactement la même erreur de logique que de penser

332
0:17:08,000 --> 0:17:1,000
après le test de la maladie, qui est 99 fois sur cent juste,

333
0:17:1,000 --> 0:17:14,000
que la probabilité d'avoir la maladie est de 99 pour cent.

334
0:17:14,000 --> 0:17:18,000
Dans l'exemple du test, il fallait avoir deux choses en tête,

335
0:17:18,000 --> 0:17:22,000
l'une était la possibilité que le test se trompe ou pas.

336
0:17:22,000 --> 0:17:26,000
L'autre était la probabilité, a priori, qu'une personne soit malade.

337
0:17:26,000 --> 0:17:29,000
C'est exactement la même chose ici.

338
0:17:29,000 --> 0:17:33,000
Il y a deux phénomènes à l'oeuvre -- deux faces à l'explication.

339
0:17:33,000 --> 0:17:37,000
Nous voulons savoir la probabilité, ou la probabilité relative, de deux explications différentes.

340
0:17:37,000 --> 0:17:4,000
L'une des deux est que Sally Clark était innocente --

341
0:17:4,000 --> 0:17:42,000
ce qui est, a priori, extrêmement probable --

342
0:17:42,000 --> 0:17:45,000
la plupart des mères ne tuent pas leur enfants.

343
0:17:45,000 --> 0:17:47,000
Et la seconde partie de l'explication

344
0:17:47,000 --> 0:17:5,000
est qu'elle a subi un évènement incroyablement improbable.

345
0:17:5,000 --> 0:17:54,000
Pas aussi improbable que le suggère le un sur 73 millions, mais néanmoins assez improblable.

346
0:17:54,000 --> 0:17:56,000
L'autre explication est qu'elle est coupable,

347
0:17:56,000 --> 0:17:58,000
Nous pouvons penser a priori que c'est improbable.

348
0:17:58,000 --> 0:18:01,000
Et certes, nous devrions penser dans le contexte d'un procès criminel

349
0:18:01,000 --> 0:18:04,000
que c'est improbable, en raison de la présomption d'innocence.

350
0:18:04,000 --> 0:18:08,000
Mais si elle a cherché à tuer les enfants, elle a réussi.

351
0:18:08,000 --> 0:18:12,000
Donc la probabilité qu'elle soit innocente n'est pas d'un sur 73 millions.

352
0:18:12,000 --> 0:18:14,000
Nous ne savons pas ce qu'elle vaut.

353
0:18:14,000 --> 0:18:18,000
Elle est liée à la pondération des preuves contre l'accusée

354
0:18:18,000 --> 0:18:2,000
et des résultats statistiques.

355
0:18:2,000 --> 0:18:22,000
Nous savons que les enfants sont morts.

356
0:18:22,000 --> 0:18:26,000
Ce qui compte, c'est la probabilité ou l'improbabilité relative

357
0:18:26,000 --> 0:18:28,000
des deux explications.

358
0:18:28,000 --> 0:18:3,000
Qui sont toutes deux improbables.

359
0:18:31,000 --> 0:18:35,000
Il y a des situations dans lesquelles les erreurs de statistiques ont des conséquences

360
0:18:35,000 --> 0:18:38,000
profondes et vraiment funestes.

361
0:18:38,000 --> 0:18:4,000
En fait, deux autres femmes ont aussi été condamnées sur la base

362
0:18:4,000 --> 0:18:44,000
du témoignage du pédiatre, et ont plus tard été libérées en appel.

363
0:18:44,000 --> 0:18:46,000
De nombreux cas ont été revus.

364
0:18:46,000 --> 0:18:5,000
C'est un sujet d'actualité car ce pédiatre subit actuellement un procès en disgrâce

365
0:18:5,000 --> 0:18:53,000
de la part de l'ordre des médecins de Grande Bretagne.

366
0:18:53,000 --> 0:18:57,000
Pour conclure -- quels sont les messages que vous devez retenir de tout ça ?

367
0:18:57,000 --> 0:19:01,000
Bien, nous savons que le hasard, l'incertain, et la probabilité

368
0:19:01,000 --> 0:19:04,000
font partie de notre vie de tous les jours.

369
0:19:04,000 --> 0:19:09,000
Collectivement, bien que spéciaux à bien des égards,

370
0:19:09,000 --> 0:19:13,000
vous êtes passés comme tout le monde à côté des exemples que j'ai donnés.

371
0:19:13,000 --> 0:19:16,000
C'est un fait que les gens ne comprennent pas ces choses-là.

372
0:19:16,000 --> 0:19:19,000
Ils font des erreurs de logique lorsqu'ils raisonnent dans l'incertain.

373
0:19:2,000 --> 0:19:22,000
Nous pouvons nous débrouiller brillamment avec les subtilités du langage --

374
0:19:22,000 --> 0:19:25,000
et il y a des travaux intéressants sur l'évolution pour expliquer pourquoi nous en sommes là.

375
0:19:25,000 --> 0:19:28,000
Nous ne sommes pas doués pour raisonner dans l'incertain.

376
0:19:28,000 --> 0:19:3,000
C'est un problème dans nos vies de tous les jours.

377
0:19:3,000 --> 0:19:33,000
Comme vous l'avez entendu dans les conférences, les statistiques sous-tendent un grand nombre

378
0:19:33,000 --> 0:19:36,000
de recherches en science -- en science sociale, en médecine

379
0:19:36,000 --> 0:19:38,000
et aussi, bien sûr, dans l'industrie.

380
0:19:38,000 --> 0:19:42,000
Tout le contrôle de la qualité, qui a un impact majeur sur les processus industriels

381
0:19:42,000 --> 0:19:44,000
est sous-tendu par les statistiques.

382
0:19:44,000 --> 0:19:46,000
C'est quelque chose que nous ne savons pas faire.

383
0:19:46,000 --> 0:19:49,000
Tout au moins, nous devrions l'admettre, mais nous avons tendance à ne pas le faire.

384
0:19:49,000 --> 0:19:53,000
Pour en revenir à la justice, au procès de Sally Clark

385
0:19:53,000 --> 0:19:57,000
les avocats ont tous accepté ce que l'expert a dit.

386
0:19:57,000 --> 0:19:59,000
Mais si un pédiatre avait dit au jury,

387
0:19:59,000 --> 0:20:02,000
"Je sais construire des ponts. J'en ai construit un en bas de la rue.

388
0:20:02,000 --> 0:20:04,000
Vous pouvez passer dessus en voiture"

389
0:20:04,000 --> 0:20:06,000
ils auraient dit, "Les pédiatres ne savent pas construire de ponts.

390
0:20:06,000 --> 0:20:08,000
C'est le travail des ingénieurs."

391
0:20:08,000 --> 0:20:11,000
Mais il a dit, effectivement ou implicitement,

392
0:20:11,000 --> 0:20:14,000
"Je sais raisonner dans l'incertain. Je connais les statistiques."

393
0:20:14,000 --> 0:20:17,000
Et tout le monde a dit, "C'est OK. Il est expert."

394
0:20:17,000 --> 0:20:2,000
Nous devons savoir quelles sont nos compétences.

395
0:20:2,000 --> 0:20:24,000
Les mêmes problèmes ont été soulevés au début des tests ADN,

396
0:20:24,000 --> 0:20:28,000
quand les scientifiques, les avocats et certains juges,

397
0:20:28,000 --> 0:20:31,000
déformaient systématiquement la vérité.

398
0:20:32,000 --> 0:20:35,000
En général -- on peut l'espérer -- innocemment, mais ils la déformaient quand même.

399
0:20:35,000 --> 0:20:4,000
Les légistes disaient, "La probabilité que ce type soit innocent est d'un sur trois millions.

400
0:20:4,000 --> 0:20:42,000
Même si vous croyez en ce nombre, comme un sur 73 millions,

401
0:20:42,000 --> 0:20:44,000
il ne veut pas dire ce que vous croyez.

402
0:20:44,000 --> 0:20:46,000
Et il y a eu des cas célèbres d'appels

403
0:20:46,000 --> 0:20:48,000
en Grande Bretagne et ailleurs à cause de cela.

404
0:20:48,000 --> 0:20:51,000
Et juste pour finir sur le thème de la justice,

405
0:20:51,000 --> 0:20:55,000
c'est très bien de dire, "Faisons de notre mieux pour présenter les témoignages."

406
0:20:55,000 --> 0:20:58,000
Mais de plus en plus, dans les cas de tests ADN -- et c'en est un --

407
0:20:58,000 --> 0:21:01,000
nous attendons des jurés, qui sont des gens ordinaires --

408
0:21:01,000 --> 0:21:03,000
et il est connu qu'ils ne savent pas le faire --

409
0:21:03,000 --> 0:21:07,000
nous attendons des jurés qu'ils soient capables de ce genre de raisonnements.

410
0:21:07,000 --> 0:21:12,000
Dans d'autres domaines de la vie, si les gens, en débattant -- disons, sauf en politique.

411
0:21:12,000 --> 0:21:14,000
Mais dans d'autres domaines, si les gens utilisent des arguments illogiques,

412
0:21:14,000 --> 0:21:16,000
nous dirions que ce n'est pas une bonne chose.

413
0:21:16,000 --> 0:21:2,000
C'est ce que nous demandons aux politiciens et nous n'en demandons pas beaucoup plus.

414
0:21:2,000 --> 0:21:23,000
Dans le cas des probabilités, nous avons toujours faux --

415
0:21:23,000 --> 0:21:25,000
mais au moins, nous devrions en être conscients.

416
0:21:25,000 --> 0:21:27,000
Et idéalement, nous pourrions peut-être y faire quelque chose.

417
0:21:27,000 --> 0:21:28,000
Merci beaucoup.

