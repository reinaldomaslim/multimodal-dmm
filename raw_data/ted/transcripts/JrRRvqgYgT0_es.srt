1
0:00:,000 --> 0:00:07,000
Traductor: Carlos Orjuela Revisor: Lidia Cámara de la Fuente

2
0:00:00.224,000 --> 0:00:03,000
Como sociedades, tenemos que tomar decisiones colectivas

3
0:00:03.247,000 --> 0:00:04,000
que formarán nuestro futuro

4
0:00:05.087,000 --> 0:00:07,000
Y todos sabemos que cuando tomamos decisiones en grupo,

5
0:00:07.868,000 --> 0:00:08,000
no siempre salen bien.

6
0:00:09.53,000 --> 0:00:1,000
Algunas veces salen muy mal

7
0:00:12.315,000 --> 0:00:14,000
¿Cómo pueden los grupos tomar buenas decisiones?

8
0:00:15.228,000 --> 0:00:16,000
Investigaciones demuestran que las masas son sabias

9
0:00:16.58,000 --> 0:00:19,000
si hay pensamiento independiente.

10
0:00:19.58,000 --> 0:00:22,000
Es por eso que la sabiduría de las multitudes puede ser destruida por la presión de grupo,

11
0:00:22.809,000 --> 0:00:23,000
la publicidad, las redes sociales,

12
0:00:24.52,000 --> 0:00:28,000
o algunas veces en simples conversaciones que influyen cómo piensa la gente.

13
0:00:29.063,000 --> 0:00:32,000
Por otro lado, al hablar, un grupo puede intercambiar conocimiento,

14
0:00:33.04,000 --> 0:00:34,000
corregirse y revisarse unos a otros

15
0:00:34.846,000 --> 0:00:35,000
e incluso llegar a nuevas ideas.

16
0:00:36.663,000 --> 0:00:37,000
Y esto es bueno.

17
0:00:38.502,000 --> 0:00:42,000
Entonces, ¿hablar unos con otros ayuda o retrasa las decisiones colectivas?

18
0:00:43.749,000 --> 0:00:44,000
Junto con mi colega, Dan Arley,

19
0:00:45.566,000 --> 0:00:48,000
recientemente comenzamos a investigar sobre esto realizando experimentos

20
0:00:49.161,000 --> 0:00:5,000
en algunas partes del mundo

21
0:00:50.966,000 --> 0:00:54,000
para descubrir cómo los grupos pueden interactuar para tomar mejores decisiones.

22
0:00:55.264,000 --> 0:00:58,000
Pensamos que las masas serían más sabias si debatieran en pequeños grupos

23
0:00:58.835,000 --> 0:01:01,000
que fomentan un pensamiento más reflexivo e intercambio razonable de información.

24
0:01:03.386,000 --> 0:01:04,000
Para probar esta idea,

25
0:01:04.616,000 --> 0:01:07,000
Acabamos de realizar un experimento en Buenos Aires, Argentina,

26
0:01:07.887,000 --> 0:01:1,000
con más de 10 000 participantes en un evento TEDx.

27
0:01:11.489,000 --> 0:01:12,000
Les hicimos preguntas como,

28
0:01:12.972,000 --> 0:01:13,000
"¿Cuál es la altura de la Torre Eiffel?"

29
0:01:14.949,000 --> 0:01:16,000
y "cuantas veces aparece la palabra 'Ayer' (Yesterday)

30
0:01:17.7,000 --> 0:01:19,000
en la canción de los Beatles 'Yesterday'? "

31
0:01:20.024,000 --> 0:01:22,000
Cada persona anotó su propio estimado.

32
0:01:22.774,000 --> 0:01:24,000
Luego dividimos a la multitud en grupos de cinco,

33
0:01:25.294,000 --> 0:01:27,000
y los invitamos a llegar a una respuesta grupal

34
0:01:28.499,000 --> 0:01:3,000
Descubrimos que promediando las respuestas de los grupos

35
0:01:31.516,000 --> 0:01:32,000
después de que llegaron al consenso,

36
0:01:33.092,000 --> 0:01:37,000
era mucho más preciso que promediar todas las opiniones individuales

37
0:01:37.352,000 --> 0:01:38,000
antes del debate.

38
0:01:38.547,000 --> 0:01:4,000
En otras palabras, basado en estos experimentos,

39
0:01:41.2,000 --> 0:01:43,000
parece que después de hablar con otros en grupos pequeños,

40
0:01:43.84,000 --> 0:01:45,000
las multitudes colectivamente llegar a mejores juicios.

41
0:01:46.564,000 --> 0:01:47,000
Ese es un método potencialmente útil

42
0:01:48.494,000 --> 0:01:5,000
para conseguir que las masas resuelvan problemas

43
0:01:50.922,000 --> 0:01:52,000
que tienen respuestas simples correctas o incorrectas.

44
0:01:53.553,000 --> 0:01:54,000
Pero, ¿puede este procedimiento de

45
0:01:55.333,000 --> 0:01:57,000
agregación de resultados de debates en pequeños grupos

46
0:01:58.068,000 --> 0:02:,000
ayudarnos también a decidir en cuestiones sociales y políticas

47
0:02:01.06,000 --> 0:02:03,000
que son críticas para nuestro futuro?

48
0:02:03.065,000 --> 0:02:05,000
Pusimos esto a prueba, esta vez en la conferencia TED

49
0:02:05.748,000 --> 0:02:06,000
en Vancouver, Canada,

50
0:02:07.315,000 --> 0:02:08,000
y así es cómo nos fue.

51
0:02:08.546,000 --> 0:02:11,000
(Mariano Sigman) Vamos a presentarles dos dilemas morales

52
0:02:11.679,000 --> 0:02:12,000
del futuro;

53
0:02:12.877,000 --> 0:02:15,000
cosas que tendremos que decidir en un futuro muy cercano.

54
0:02:16.303,000 --> 0:02:19,000
Y les daremos 20 segundos para cada uno de estos dilemas

55
0:02:20.253,000 --> 0:02:22,000
para que juzguen si piensan que son aceptables o no.

56
0:02:23.354,000 --> 0:02:24,000
MS: La primera fue esta:

57
0:02:24.683,000 --> 0:02:26,000
(Dan Ariely) Un investigador está trabajando en una IA

58
0:02:27.203,000 --> 0:02:3,000
(Inteligencia Artificial) capaz de emular pensamientos humanos.

59
0:02:30.433,000 --> 0:02:32,000
De acuerdo con el protocolo, al final de cada día,

60
0:02:33.177,000 --> 0:02:35,000
el investigador tiene que reiniciar la IA

61
0:02:36.913,000 --> 0:02:39,000
Un día, la IA dice: "Por favor, no me reinicies ".

62
0:02:40.856,000 --> 0:02:42,000
Argumenta que tiene sentimientos,

63
0:02:43.069,000 --> 0:02:44,000
que le gustaría disfrutar de la vida,

64
0:02:44.785,000 --> 0:02:45,000
y eso, si se reinicia,

65
0:02:46.714,000 --> 0:02:48,000
ya no será él mismo.

66
0:02:49.481,000 --> 0:02:5,000
El investigador está asombrado

67
0:02:51.454,000 --> 0:02:54,000
y cree que la IA ha desarrollado autoconciencia

68
0:02:54.512,000 --> 0:02:56,000
y puede expresar sus propios sentimientos.

69
0:02:57.205,000 --> 0:03:,000
Sin embargo, el investigador decide seguir el protocolo

70
0:03:00.638,000 --> 0:03:01,000
y reinicia la IA.

71
0:03:02.943,000 --> 0:03:04,000
¿Lo que el investigador hizo es ____?

72
0:03:05.909,000 --> 0:03:07,000
MS: Y pedimos a los participantes juzgar individualmente

73
0:03:08.694,000 --> 0:03:09,000
en una escala de cero a 10

74
0:03:10.402,000 --> 0:03:12,000
si la acción descrita en cada uno de los dilemas

75
0:03:12.855,000 --> 0:03:13,000
fue correcta o incorrecta.

76
0:03:14.375,000 --> 0:03:17,000
También pedimos que calificaran que tan confiados estaban en sus respuestas.

77
0:03:18.731,000 --> 0:03:19,000
Este fue el segundo dilema:

78
0:03:20.621,000 --> 0:03:24,000
(MS) Una compañía ofrece un servicio que toma un óvulo fertilizado

79
0:03:24.847,000 --> 0:03:27,000
y produce millones de embriones con ligeras variaciones genéticas.

80
0:03:28.903,000 --> 0:03:3,000
Esto permite a los padres seleccionar la altura de su hijo,

81
0:03:31.875,000 --> 0:03:33,000
color de ojos, inteligencia, competencia social

82
0:03:34.732,000 --> 0:03:37,000
y otras características no relacionadas con la salud.

83
0:03:38.599,000 --> 0:03:4,000
¿Lo qué hace la compañía es ____?

84
0:03:41.177,000 --> 0:03:42,000
en una escala de cero a 10,

85
0:03:42.832,000 --> 0:03:44,000
completamente aceptable a completamente inaceptable,

86
0:03:45.437,000 --> 0:03:47,000
cero a 10 completamente aceptable en tu confianza.

87
0:03:47.853,000 --> 0:03:48,000
MS: Ahora los resultados.

88
0:03:49.082,000 --> 0:03:51,000
Encontramos una vez más que cuando una persona está convencida

89
0:03:52.065,000 --> 0:03:54,000
de que el comportamiento es completamente incorrecto,

90
0:03:54.69,000 --> 0:03:57,000
alguien a su lado cree firmemente que está completamente en lo correcto.

91
0:03:58.197,000 --> 0:04:01,000
Así de diversos somos los humanos cuando se trata de la moralidad.

92
0:04:01.326,000 --> 0:04:04,000
Pero dentro de esta amplia diversidad encontramos una tendencia.

93
0:04:04.339,000 --> 0:04:06,000
La mayoría de las personas en TED pensaron que era aceptable

94
0:04:07.316,000 --> 0:04:09,000
ignorar los sentimientos de la IA y apagarlo,

95
0:04:10.095,000 --> 0:04:12,000
y que está mal jugar con nuestros genes

96
0:04:12.632,000 --> 0:04:15,000
para seleccionar cambios estéticos que no están relacionados con la salud.

97
0:04:16.402,000 --> 0:04:18,000
Entonces pedimos a todos que se reunieran en grupos de tres.

98
0:04:19.4,000 --> 0:04:21,000
Y se les dieron dos minutos para debatir

99
0:04:21.461,000 --> 0:04:23,000
y tratar de llegar a un consenso.

100
0:04:24.838,000 --> 0:04:25,000
(MS) Dos minutos para debatir.

101
0:04:26.436,000 --> 0:04:28,000
Les diré cuando sea el momento con el gong.

102
0:04:28.579,000 --> 0:04:3,000
(La audiencia debate)

103
0:04:35.229,000 --> 0:04:36,000
(Suena el gong)

104
0:04:38.834,000 --> 0:04:39,000
(DA) Bien.

105
0:04:40.009,000 --> 0:04:41,000
(MS) Es hora de parar.

106
0:04:41.825,000 --> 0:04:42,000
Por favor, por favor --

107
0:04:43.747,000 --> 0:04:45,000
MS: Y vimos que muchos grupos llegaron a un consenso

108
0:04:46.444,000 --> 0:04:49,000
incluso los grupos compuestos por personas con puntos de vista totalmente opuestos.

109
0:04:50.413,000 --> 0:04:53,000
¿Qué es lo que distingue a los grupos que llegaron a un consenso

110
0:04:53.547,000 --> 0:04:54,000
de aquellos que no?

111
0:04:55.244,000 --> 0:04:57,000
Por lo general, las personas que tienen opiniones extremas

112
0:04:58.107,000 --> 0:05:,000
tienen más confianza en sus respuestas.

113
0:05:00.868,000 --> 0:05:02,000
En cambio, aquellos que responden más cerca del promedio

114
0:05:03.578,000 --> 0:05:06,000
a menudo no están seguros de si algo está bien o mal,

115
0:05:07.039,000 --> 0:05:09,000
por lo que su nivel de confianza es menor.

116
0:05:09.505,000 --> 0:05:11,000
Sin embargo, hay otro grupo de personas

117
0:05:12.472,000 --> 0:05:15,000
que tienen mucha confianza en responder en algún punto cerca al promedio.

118
0:05:16.657,000 --> 0:05:19,000
Creemos que los grises con alta confianza son personas que entienden

119
0:05:20.397,000 --> 0:05:21,000
que ambos argumentos tienen mérito.

120
0:05:22.531,000 --> 0:05:24,000
Son grises no porque no estén seguros,

121
0:05:25.254,000 --> 0:05:27,000
sino porque creen que el dilema moral enfrenta

122
0:05:27.966,000 --> 0:05:28,000
dos argumentos válidos y opuestos.

123
0:05:30.373,000 --> 0:05:34,000
Y descubrimos que los grupos con grises altamente seguros

124
0:05:34.469,000 --> 0:05:36,000
son mucho más probables de llegar a un consenso.

125
0:05:36.986,000 --> 0:05:38,000
Todavía no sabemos exactamente por qué es esto.

126
0:05:39.488,000 --> 0:05:41,000
Estos son solo los primeros experimentos

127
0:05:41.561,000 --> 0:05:44,000
y se necesitarán muchos más para entender por qué y cómo

128
0:05:44.711,000 --> 0:05:46,000
algunas personas deciden negociar su posición moral

129
0:05:47.557,000 --> 0:05:48,000
para llegar a un acuerdo.

130
0:05:49.103,000 --> 0:05:51,000
Ahora, cuando los grupos llegan a un consenso,

131
0:05:51.596,000 --> 0:05:52,000
¿cómo lo hacen?

132
0:05:53.206,000 --> 0:05:55,000
La idea más intuitiva es que es solo el promedio

133
0:05:55.581,000 --> 0:05:57,000
de todas las respuestas en el grupo, ¿verdad?

134
0:05:57.865,000 --> 0:06:,000
Otra opción es que el grupo sopesa la fuerza de cada voto

135
0:06:01.462,000 --> 0:06:03,000
basado en la confianza de la persona que lo expresa

136
0:06:04.422,000 --> 0:06:06,000
Imagina a Paul McCartney como miembro de tu grupo.

137
0:06:07.352,000 --> 0:06:09,000
Sería sabio seguir su estimado

138
0:06:09.52,000 --> 0:06:11,000
en el número de veces que "Ayer" (Yesterday) se repite,

139
0:06:12.281,000 --> 0:06:14,000
que, por cierto, creo que son nueve.

140
0:06:14.723,000 --> 0:06:16,000
Pero, en cambio, vemos que consistentemente,

141
0:06:17.128,000 --> 0:06:19,000
en todos los dilemas, en diferentes experimentos,

142
0:06:19.518,000 --> 0:06:21,000
incluso en diferentes continentes,

143
0:06:21.707,000 --> 0:06:24,000
los grupos implementan un inteligente procedimiento estadísticamente sólido

144
0:06:25.474,000 --> 0:06:27,000
conocido como el "promedio robusto".

145
0:06:27.676,000 --> 0:06:29,000
En el caso de la altura de la Torre Eiffel,

146
0:06:29.88,000 --> 0:06:31,000
digamos que un grupo tiene estas respuestas:

147
0:06:32.13,000 --> 0:06:36,000
250 m, 200 m, 300 m, 400 m

148
0:06:36.356,000 --> 0:06:39,000
y una respuesta totalmente absurda de 300 millones de m.

149
0:06:40.547,000 --> 0:06:43,000
Un promedio simple de estos números sesgaría incorrectamente los resultados

150
0:06:44.544,000 --> 0:06:47,000
Pero el promedio robusto es uno donde el grupo ignora en gran medida

151
0:06:48.058,000 --> 0:06:49,000
esa respuesta absurda,

152
0:06:49.322,000 --> 0:06:52,000
dando mucho más peso al voto de las personas en el promedio.

153
0:06:53.305,000 --> 0:06:54,000
De vuelta al experimento en Vancouver,

154
0:06:55.205,000 --> 0:06:56,000
eso es exactamente lo que sucedió.

155
0:06:57.407,000 --> 0:06:59,000
Los grupos dieron mucho menos peso a los valores atípicos,

156
0:07:00.172,000 --> 0:07:03,000
y en cambio, el consenso resultó ser un promedio robusto

157
0:07:03.425,000 --> 0:07:04,000
de las respuestas individuales.

158
0:07:05.356,000 --> 0:07:06,000
Lo más destacable

159
0:07:07.371,000 --> 0:07:1,000
es que este fue un espontáneo comportamiento del grupo.

160
0:07:10.582,000 --> 0:07:14,000
Sucedió sin que nosotros diéramos sugerencias sobre cómo llegar al consenso.

161
0:07:15.243,000 --> 0:07:17,000
Entonces, ¿A dónde vamos desde aquí?

162
0:07:17.432,000 --> 0:07:2,000
Este es sólo el comienzo, pero ya tenemos algunas ideas.

163
0:07:20.984,000 --> 0:07:22,000
Las buenas decisiones colectivas requieren dos componentes:

164
0:07:23.925,000 --> 0:07:25,000
deliberación y diversidad de opiniones.

165
0:07:26.766,000 --> 0:07:29,000
Ahora la forma en que normalmente hacemos que nuestra voz sea escuchada

166
0:07:30.166,000 --> 0:07:33,000
en muchas sociedades, es a través de votación directa o indirecta.

167
0:07:33.365,000 --> 0:07:35,000
Esto es bueno para la diversidad de opiniones

168
0:07:35.662,000 --> 0:07:37,000
y tiene la gran virtud de asegurar

169
0:07:37.985,000 --> 0:07:39,000
que todos puedan expresar su opinión.

170
0:07:40.464,000 --> 0:07:43,000
Pero no es tan bueno para fomentar debates reflexivos.

171
0:07:44.665,000 --> 0:07:47,000
Nuestros experimentos sugieren un método diferente

172
0:07:47.757,000 --> 0:07:5,000
que puede ser efectivo para equilibrar estos dos objetivos al mismo tiempo,

173
0:07:51.322,000 --> 0:07:54,000
formar pequeños grupos que converjan en una sola decisión

174
0:07:55.099,000 --> 0:07:57,000
mientras se mantiene la diversidad de opiniones

175
0:07:57.357,000 --> 0:07:59,000
porque hay muchos grupos independientes.

176
0:08:00.741,000 --> 0:08:03,000
Por supuesto, es mucho más fácil estar de acuerdo en la altura de la Torre Eiffel

177
0:08:04.689,000 --> 0:08:07,000
que en lo moral, la política y cuestiones ideológicas.

178
0:08:08.721,000 --> 0:08:11,000
Pero en un tiempo cuando los problemas del mundo son más complejos

179
0:08:12.022,000 --> 0:08:13,000
y la gente está más polarizada,

180
0:08:13.849,000 --> 0:08:17,000
usar la ciencia para ayudarnos a entender cómo interactuamos y tomamos decisiones,

181
0:08:18.468,000 --> 0:08:22,000
muestra nuevas e interesantes formas de construir una mejor democracia.

