1
0:00:,000 --> 0:00:07,000
Traductor: Sebastian Betti Revisor: Carlos Arturo Morales

2
0:00:12,000 --> 0:00:14,000
Quiero contarles una historia

3
0:00:15.702,000 --> 0:00:17,000
que conecta el célebre incidente de privacidad

4
0:00:18.171,000 --> 0:00:2,000
que involucró a Adán y Eva,

5
0:00:20.94,000 --> 0:00:23,000
y el cambio notorio de los límites

6
0:00:24.386,000 --> 0:00:26,000
entre lo público y lo privado

7
0:00:27.072,000 --> 0:00:28,000
ocurrido en los últimos 10 años.

8
0:00:28.842,000 --> 0:00:29,000
Ya conocen el incidente.

9
0:00:30.14,000 --> 0:00:33,000
Adán y Eva, un día en el Jardín del Edén,

10
0:00:33.47,000 --> 0:00:34,000
se dan cuenta de que están desnudos.

11
0:00:35.313,000 --> 0:00:36,000
Se asustan.

12
0:00:36.813,000 --> 0:00:38,000
Y el resto es historia.

13
0:00:39.57,000 --> 0:00:41,000
Hoy en día, Adán y Eva

14
0:00:41.758,000 --> 0:00:43,000
probablemente actuarían diferente.

15
0:00:44.119,000 --> 0:00:46,000
[@Adan ¡Lo de anoche fue genial! Me encantó la manzana LOL] (Risas)

16
0:00:46.387,000 --> 0:00:47,000
(Risas) [@Eva Sip... bebé, ¿sabes qué le pasó a mis pantalones?]

17
0:00:48.26,000 --> 0:00:5,000
Revelamos mucha más información

18
0:00:50.896,000 --> 0:00:53,000
personal en línea que nunca antes,

19
0:00:54.23,000 --> 0:00:55,000
y las empresas recolectan mucha más

20
0:00:55.934,000 --> 0:00:57,000
información sobre nosotros.

21
0:00:58.158,000 --> 0:01:01,000
Podemos ganar mucho y beneficiarnos

22
0:01:01.44,000 --> 0:01:03,000
del análisis masivo de esta información personal,

23
0:01:03.886,000 --> 0:01:04,000
o "big data",

24
0:01:05.832,000 --> 0:01:06,000
pero también hay un precio grande que pagar en términos de privacidad,

25
0:01:07.744,000 --> 0:01:08,000
[Salí por una pizza] [Mis amigos y la NSA]

26
0:01:09.656,000 --> 0:01:1,000
en contraprestación.

27
0:01:11.568,000 --> 0:01:15,000
Mi historia trata de esas contraprestaciones.

28
0:01:15.591,000 --> 0:01:17,000
Empecemos con un verdad que, en mi opinión,

29
0:01:18.175,000 --> 0:01:21,000
se ha hecho cada vez más evidente en los últimos años:

30
0:01:21.502,000 --> 0:01:23,000
toda información personal

31
0:01:23.599,000 --> 0:01:25,000
puede volverse sensible.

32
0:01:25.884,000 --> 0:01:29,000
En el 2000, se tomaron unos 100 000 millones

33
0:01:30.009,000 --> 0:01:31,000
de fotos en el mundo,

34
0:01:31.921,000 --> 0:01:34,000
pero solo una ínfima parte de ellas

35
0:01:34.986,000 --> 0:01:35,000
se subíeron a la Web.

36
0:01:36.869,000 --> 0:01:39,000
En el 2010, solo en Facebook, en un solo mes,

37
0:01:40.23,000 --> 0:01:43,000
se subieron 2500 millones de fotos,

38
0:01:43.5,000 --> 0:01:44,000
la mayoría identificadas.

39
0:01:45.382,000 --> 0:01:46,000
En el mismo lapso de tiempo,

40
0:01:47.262,000 --> 0:01:51,000
la capacidad de las computadoras para reconocer personas en las fotos

41
0:01:52.132,000 --> 0:01:55,000
mejoró en tres órdenes de magnitud.

42
0:01:55.74,000 --> 0:01:56,000
¿Qué ocurre cuando uno combina

43
0:01:57.622,000 --> 0:01:58,000
estas tecnologías:

44
0:01:59.123,000 --> 0:02:01,000
aumento de la disponibilidad de datos faciales;

45
0:02:01.781,000 --> 0:02:04,000
mejora en el reconocimiento facial informático;

46
0:02:05.429,000 --> 0:02:07,000
y también la computación en la nube,

47
0:02:07.611,000 --> 0:02:08,000
que nos da a todos los presentes en la sala

48
0:02:09.499,000 --> 0:02:1,000
el poder computacional

49
0:02:11.059,000 --> 0:02:12,000
que hace unos años estaba disponible

50
0:02:12.945,000 --> 0:02:13,000
solo para agencias gubernamentales;

51
0:02:14.727,000 --> 0:02:15,000
y la computación ubicua,

52
0:02:16.105,000 --> 0:02:18,000
que le permite a mi móvil, que no es una supercomputadora,

53
0:02:18.997,000 --> 0:02:19,000
conectarse a Internet

54
0:02:20.668,000 --> 0:02:22,000
y tomar allí cientos de miles

55
0:02:23.002,000 --> 0:02:25,000
de medidas faciales en unos pocos segundos?

56
0:02:25.641,000 --> 0:02:27,000
Bueno, pensamos que el resultado

57
0:02:28.269,000 --> 0:02:3,000
de esta combinación de tecnologías

58
0:02:30.333,000 --> 0:02:32,000
será un cambio radical en nuestras nociones

59
0:02:33.221,000 --> 0:02:35,000
de privacidad y anonimato.

60
0:02:35.478,000 --> 0:02:36,000
Para comprobalo hicimos un experimento

61
0:02:37.471,000 --> 0:02:39,000
en el campus del Carnegie Mellon.

62
0:02:39.592,000 --> 0:02:41,000
Le pedimos a los estudiantes que pasaban por allí

63
0:02:41.691,000 --> 0:02:42,000
que participaran en un estudio,

64
0:02:43.47,000 --> 0:02:45,000
les tomamos una foto con una cámara web

65
0:02:46.032,000 --> 0:02:48,000
y les pedimos que completaran una encuesta en el portátil.

66
0:02:48.814,000 --> 0:02:49,000
Mientras completaban la encuesta,

67
0:02:50.793,000 --> 0:02:52,000
subimos la foto a un clúster de computación en la nube,

68
0:02:53.59,000 --> 0:02:54,000
y usamos un reconocedor facial

69
0:02:55.317,000 --> 0:02:57,000
para cotejar esa foto con una base de datos

70
0:02:57.722,000 --> 0:02:59,000
de cientos de miles de imágenes

71
0:03:00.115,000 --> 0:03:03,000
que habíamos bajado de perfiles de Facebook.

72
0:03:03.711,000 --> 0:03:06,000
Para cuando el sujeto llegaba al final

73
0:03:06.97,000 --> 0:03:09,000
de la encuesta, la página se había actualizado en forma dinámica

74
0:03:10.317,000 --> 0:03:12,000
con las 10 fotos encontradas por el reconocedor

75
0:03:12.63,000 --> 0:03:14,000
que mejor concordaban,

76
0:03:14.915,000 --> 0:03:15,000
y le pedímos al sujeto que indicara

77
0:03:16.653,000 --> 0:03:2,000
si se encontraba en la foto.

78
0:03:20.773,000 --> 0:03:23,000
¿Ven al sujeto?

79
0:03:24.472,000 --> 0:03:26,000
Bueno, la computadora sí, y de hecho,

80
0:03:27.317,000 --> 0:03:29,000
reconoció 1 de cada 3 sujetos.

81
0:03:29.466,000 --> 0:03:32,000
En esencia, podemos partir de un rostro anónimo,

82
0:03:32.65,000 --> 0:03:35,000
en disco o en la web, y usar el reconocimiento facial

83
0:03:36.134,000 --> 0:03:38,000
para ponerle nombre a ese rostro anónimo

84
0:03:38.494,000 --> 0:03:4,000
gracias a las redes sociales.

85
0:03:40.602,000 --> 0:03:41,000
Pero hace unos años hicimos algo más.

86
0:03:42.474,000 --> 0:03:43,000
Partimos de datos de redes sociales,

87
0:03:44.297,000 --> 0:03:47,000
los combinamos estadísticamente con datos

88
0:03:47.348,000 --> 0:03:49,000
de la seguridad social del gobierno de EE.UU.

89
0:03:49.45,000 --> 0:03:52,000
y terminamos prediciendo los números de la seguridad social,

90
0:03:52.774,000 --> 0:03:53,000
que en Estados Unidos

91
0:03:54.286,000 --> 0:03:56,000
son una información extremadamente sensible.

92
0:03:56.326,000 --> 0:03:58,000
¿Ven a dónde quiero llegar con esto?

93
0:03:58.419,000 --> 0:04:,000
Si combinan los dos estudios,

94
0:04:01.341,000 --> 0:04:02,000
entonces la pregunta pasa a ser:

95
0:04:02.853,000 --> 0:04:04,000
¿Podemos partir de un rostro

96
0:04:05.573,000 --> 0:04:07,000
y mediante reconocimiento facial, hallar un nombre

97
0:04:07.884,000 --> 0:04:09,000
e información pública

98
0:04:10.553,000 --> 0:04:11,000
sobre ese nombre y esa persona,

99
0:04:12.485,000 --> 0:04:14,000
y a partir de esa información pública

100
0:04:14.733,000 --> 0:04:16,000
inferir información no pública,

101
0:04:16.775,000 --> 0:04:17,000
mucho más sensible,

102
0:04:18.381,000 --> 0:04:19,000
para luego asociarla a aquel rostro?

103
0:04:19.873,000 --> 0:04:2,000
La respuesta es sí se puede, y lo hicimos.

104
0:04:21.789,000 --> 0:04:23,000
Por supuesto, la precisión va desmejorando a cada paso.

105
0:04:24.357,000 --> 0:04:24,000
[Se identificó el 27% de los 5 primeros dígitos del SSN de los sujetos (con 4 intentos)]

106
0:04:25.301,000 --> 0:04:28,000
Decidimos incluso, desarrollar una app para iPhone,

107
0:04:29.128,000 --> 0:04:31,000
que usa la cámara interna del móvil

108
0:04:31.843,000 --> 0:04:32,000
para tomar una foto del sujeto

109
0:04:33.443,000 --> 0:04:34,000
y subirla a la nube,

110
0:04:34.93,000 --> 0:04:36,000
y luego hacer lo que les describí en tiempo real:

111
0:04:37.592,000 --> 0:04:39,000
cotejarla, encontrar información pública,

112
0:04:39.68,000 --> 0:04:4,000
tratar de inferir información sensible,

113
0:04:41.41,000 --> 0:04:43,000
y luego enviarla nuevamente al móvil

114
0:04:44.001,000 --> 0:04:47,000
para ser superpuesta en el rostro del sujeto,

115
0:04:47.61,000 --> 0:04:48,000
un ejemplo de realidad aumentada,

116
0:04:49.511,000 --> 0:04:51,000
quizá un ejemplo escalofriante de realidad aumentada.

117
0:04:51.962,000 --> 0:04:54,000
De hecho, no desarrollamos la app para que estuviera disponible,

118
0:04:55.301,000 --> 0:04:56,000
sino como una prueba de concepto.

119
0:04:57.223,000 --> 0:04:59,000
Incluso tomamos estas tecnologías

120
0:04:59.536,000 --> 0:05:,000
y las llevamos al extremo lógico.

121
0:05:01.373,000 --> 0:05:03,000
Imaginen un futuro en el que los extraños que los rodeen

122
0:05:04.092,000 --> 0:05:06,000
los miren con sus gafas Google

123
0:05:06.403,000 --> 0:05:08,000
o, algún día, con sus lentes de contacto,

124
0:05:08.71,000 --> 0:05:12,000
y usen 7 o 8 datos de ustedes

125
0:05:12.73,000 --> 0:05:14,000
para inferir todo lo demás

126
0:05:15.312,000 --> 0:05:17,000
que pueda saberse.

127
0:05:17.915,000 --> 0:05:21,000
¿Cómo será ese futuro sin secretos?

128
0:05:22.709,000 --> 0:05:23,000
¿Debería importarnos?

129
0:05:24.673,000 --> 0:05:25,000
Nos gustaría creer

130
0:05:26.564,000 --> 0:05:29,000
que un futuro con tanta riqueza de datos

131
0:05:29.604,000 --> 0:05:31,000
sería un futuro sin más prejuicios,

132
0:05:32.118,000 --> 0:05:35,000
pero, de hecho, contar con tanta información

133
0:05:35.701,000 --> 0:05:37,000
no significa que tomaremos decisiones

134
0:05:37.892,000 --> 0:05:38,000
más objetivas.

135
0:05:39.598,000 --> 0:05:41,000
En otro experimento, presentamos a nuestros sujetos

136
0:05:42.158,000 --> 0:05:44,000
información sobre un potencial candidato laboral.

137
0:05:44.404,000 --> 0:05:47,000
incluimos algunas referencias

138
0:05:47.582,000 --> 0:05:49,000
a cierta información totalmente legal,

139
0:05:50.228,000 --> 0:05:52,000
divertida pero un poco embarazosa,

140
0:05:52.693,000 --> 0:05:54,000
que el candidato había publicado en línea.

141
0:05:54.713,000 --> 0:05:56,000
Curiosamente, entre nuestros sujetos

142
0:05:57.079,000 --> 0:06:,000
algunos habían publicado información similar

143
0:06:00.162,000 --> 0:06:02,000
y otros no.

144
0:06:02.524,000 --> 0:06:03,000
¿Qué grupo ceeen

145
0:06:04.473,000 --> 0:06:08,000
que mostró propensión a juzgar con más severidad a nuestro sujeto?

146
0:06:09.025,000 --> 0:06:1,000
Paradójicamente, fue el grupo

147
0:06:10.982,000 --> 0:06:11,000
que había publicado información similar,

148
0:06:12.715,000 --> 0:06:14,000
un ejemplo de disonancia moral.

149
0:06:15.657,000 --> 0:06:16,000
Quizá estén pensando

150
0:06:17.407,000 --> 0:06:18,000
que eso no los afecta a Uds.

151
0:06:19.109,000 --> 0:06:21,000
porque no tienen nada que ocultar.

152
0:06:21.271,000 --> 0:06:23,000
Pero, de hecho, la privacidad no tiene que ver

153
0:06:23.753,000 --> 0:06:26,000
con tener algo negativo que ocultar.

154
0:06:27.429,000 --> 0:06:29,000
Imaginen que son el director de RR.HH.

155
0:06:29.783,000 --> 0:06:31,000
de cierta empresa, que reciben unas hojas de vida

156
0:06:32.73,000 --> 0:06:34,000
y deciden buscar más información sobre los candidatos.

157
0:06:35.203,000 --> 0:06:37,000
Entonces, googlean sus nombres

158
0:06:37.663,000 --> 0:06:39,000
y en determinado universo

159
0:06:39.903,000 --> 0:06:41,000
encuentran esta información--

160
0:06:41.911,000 --> 0:06:45,000
O en un universo paralelo, encuentran esta información.

161
0:06:46.348,000 --> 0:06:48,000
¿Creen que todos los candidatos tendrían con Uds.

162
0:06:49.065,000 --> 0:06:51,000
la misma oportunidad de ser llamados para una entrevista?

163
0:06:51.868,000 --> 0:06:53,000
Si piensan que sí, entonces no son

164
0:06:54.15,000 --> 0:06:56,000
como los empleadores de EE.UU. ya que, de hecho,

165
0:06:56.732,000 --> 0:06:59,000
en algún punto de nuestro experimento hicimos exactamente eso.

166
0:07:00.039,000 --> 0:07:03,000
Creamos perfiles de Facebook, manipulamos los rasgos,

167
0:07:03.221,000 --> 0:07:05,000
y luego empezamos a enviar hojas de vida a empresas en EE.UU.,

168
0:07:06.072,000 --> 0:07:07,000
y detectamos, monitoreamos,

169
0:07:07.98,000 --> 0:07:09,000
si estaban buscando información de nuestros candidatos,

170
0:07:10.373,000 --> 0:07:11,000
y si actuaban con base en la información

171
0:07:12.205,000 --> 0:07:13,000
que encontraban en los medios sociales. Y lo hicieron.

172
0:07:14.143,000 --> 0:07:16,000
Se discriminó con base en los medios sociales

173
0:07:16.244,000 --> 0:07:19,000
a candidatos con iguales habilidades.

174
0:07:19.317,000 --> 0:07:23,000
Los vendedores quieren que creamos

175
0:07:23.892,000 --> 0:07:25,000
que toda la información sobre nosotros siempre

176
0:07:26.161,000 --> 0:07:29,000
será usada a nuestro favor.

177
0:07:29.434,000 --> 0:07:32,000
Pero piensen de nuevo. ¿Por qué habría de ser siempre así?

178
0:07:33.149,000 --> 0:07:35,000
En una película de hace unos años,

179
0:07:35.813,000 --> 0:07:37,000
"Minority Report", en una escena famosa

180
0:07:38.366,000 --> 0:07:4,000
Tom Cruise camina por un centro comercial

181
0:07:40.942,000 --> 0:07:43,000
rodeado de publicidad

182
0:07:44.718,000 --> 0:07:45,000
holográfica personalizada.

183
0:07:46.553,000 --> 0:07:49,000
La película transcurre en el 2054,

184
0:07:49.78,000 --> 0:07:5,000
dentro de unos 40 años,

185
0:07:51.422,000 --> 0:07:53,000
y aunque la tecnología luce emocionante,

186
0:07:54.33,000 --> 0:07:56,000
subestima en mucho

187
0:07:56.976,000 --> 0:07:58,000
la cantidad de información que las organizaciones

188
0:07:59.116,000 --> 0:08:01,000
pueden recolectar sobre nosotros, y la forma de usarla

189
0:08:01.599,000 --> 0:08:04,000
para influir en nosotros de maneras imperceptibles.

190
0:08:04.997,000 --> 0:08:06,000
Como ejemplo, está este otro experimento

191
0:08:07.1,000 --> 0:08:09,000
que estamos haciendo y que todavía no terminamos.

192
0:08:09.373,000 --> 0:08:11,000
Imaginemos que una organización tiene acceso

193
0:08:11.692,000 --> 0:08:13,000
a tu lista de amigos de Facebook,

194
0:08:13.748,000 --> 0:08:14,000
y mediante algún tipo de algoritmo

195
0:08:15.52,000 --> 0:08:18,000
puede identificar sus dos mejores amigos.

196
0:08:19.254,000 --> 0:08:21,000
Que luego crean, en tiempo real,

197
0:08:21.534,000 --> 0:08:23,000
un rostro compuesto de estos dos amigos.

198
0:08:24.376,000 --> 0:08:27,000
Estudios previos al nuestro han demostrado que las personas

199
0:08:27.445,000 --> 0:08:29,000
no se reconocen ni a sí mismas

200
0:08:30.33,000 --> 0:08:32,000
en rostros compuestos, pero reaccionan

201
0:08:32.792,000 --> 0:08:34,000
a esas composiciones de manera positiva.

202
0:08:34.909,000 --> 0:08:37,000
Y entonces, la próxima vez que busquen algún producto

203
0:08:38.324,000 --> 0:08:4,000
y que haya una publicidad sugiriéndoles que lo compren,

204
0:08:40.883,000 --> 0:08:42,000
no será un vendedor común.

205
0:08:43.79,000 --> 0:08:45,000
Será uno de sus amigos,

206
0:08:46.103,000 --> 0:08:49,000
y ni siquiera sabrán lo que está pasando.

207
0:08:49.406,000 --> 0:08:51,000
El problema es que

208
0:08:51.819,000 --> 0:08:53,000
los mecanismos que tenemos en la política acutal

209
0:08:54.338,000 --> 0:08:57,000
para la protección contra los abusos de la información personal

210
0:08:57.776,000 --> 0:08:59,000
son como llevar un cuchillo a un tiroteo.

211
0:09:00.76,000 --> 0:09:02,000
Uno de estos mecanismos es la transparencia,

212
0:09:03.673,000 --> 0:09:06,000
decirle a las personas lo que uno va a hacer con sus datos.

213
0:09:06.873,000 --> 0:09:08,000
En principio, eso es algo muy bueno.

214
0:09:08.979,000 --> 0:09:11,000
Es necesario, pero no es suficiente.

215
0:09:12.646,000 --> 0:09:15,000
La transparencia puede estar mal dirigida.

216
0:09:16.344,000 --> 0:09:18,000
Uno puede contarle a la gente lo que hará,

217
0:09:18.448,000 --> 0:09:2,000
y luego empujarlos a revelar

218
0:09:20.68,000 --> 0:09:22,000
cantidades arbitrarias de información personal.

219
0:09:23.303,000 --> 0:09:25,000
Por eso en un experimento más, este con estudiantes,

220
0:09:26.189,000 --> 0:09:29,000
les pedimos que nos dieran información

221
0:09:29.247,000 --> 0:09:3,000
sobre su comportamiento en el campus,

222
0:09:31.06,000 --> 0:09:33,000
formulándoles preguntas tan sensibles como estas.

223
0:09:34,000 --> 0:09:34,000
[¿Alguna vez te copiaste en un examen?]

224
0:09:34.621,000 --> 0:09:36,000
A un grupo de sujetos les dijimos:

225
0:09:36.921,000 --> 0:09:38,000
"Solo otro grupo de estudiantes verá sus respuestas".

226
0:09:39.762,000 --> 0:09:4,000
A otro grupo de sujetos les dijimos:

227
0:09:41.341,000 --> 0:09:44,000
"Sus respuestas serán vistas por estudiantes y profesores".

228
0:09:44.902,000 --> 0:09:46,000
Transparencia. Notificación. Y por supuesto, esto funcionó,

229
0:09:47.493,000 --> 0:09:48,000
en el sentido de que el primer grupo de sujetos

230
0:09:48.9,000 --> 0:09:5,000
fue mucho más propenso a revelar información que el segundo.

231
0:09:51.468,000 --> 0:09:52,000
Tiene sentido, ¿no?

232
0:09:52.988,000 --> 0:09:53,000
Pero luego añadimos un distractor.

233
0:09:54.478,000 --> 0:09:56,000
Repetimos el experimento con los mismos dos grupos,

234
0:09:57.238,000 --> 0:09:59,000
esta vez añadiendo una demora

235
0:09:59.665,000 --> 0:10:01,000
entre el tiempo en que le dijimos a los sujetos

236
0:10:02.6,000 --> 0:10:04,000
cómo usaríamos sus datos

237
0:10:04.68,000 --> 0:10:08,000
y el tiempo en que empezamos a [formular] las preguntas.

238
0:10:09.068,000 --> 0:10:11,000
¿Cuánta demora creen que tuvimos que añadir

239
0:10:11.629,000 --> 0:10:15,000
para anular el efecto inhibidor

240
0:10:16.242,000 --> 0:10:19,000
de saber que los profesores verían sus respuestas?

241
0:10:19.653,000 --> 0:10:2,000
¿10 minutos?

242
0:10:21.433,000 --> 0:10:22,000
¿5 minutos?

243
0:10:23.224,000 --> 0:10:24,000
¿1 minuto?

244
0:10:25,000 --> 0:10:27,000
¿Qué tal 15 segundos?

245
0:10:27.049,000 --> 0:10:29,000
Quince segundos fueron suficientes para que ambos grupos

246
0:10:29.717,000 --> 0:10:3,000
revelaran la misma cantidad de información,

247
0:10:31.285,000 --> 0:10:33,000
como si al segundo grupo no le importara más

248
0:10:34.031,000 --> 0:10:36,000
que los profesores leyeran sus respuestas.

249
0:10:36.687,000 --> 0:10:39,000
Tengo que admitir que esta charla hasta ahora

250
0:10:40.023,000 --> 0:10:42,000
puede sonar en extremo negativa,

251
0:10:42.503,000 --> 0:10:43,000
pero esa no es mi intención.

252
0:10:44.224,000 --> 0:10:46,000
De hecho, quiero compartir con Uds.

253
0:10:46.923,000 --> 0:10:47,000
las alternativas que hay.

254
0:10:48.695,000 --> 0:10:5,000
La forma en que hacemos las cosas ahora no es la única

255
0:10:51.194,000 --> 0:10:54,000
forma de hacerlas, y ciertamente no es la mejor

256
0:10:54.231,000 --> 0:10:56,000
forma en que pueden ser hechas.

257
0:10:56.258,000 --> 0:11:,000
Cuando alguien les diga: "La gente no se preocupa por la privacidad",

258
0:11:00.429,000 --> 0:11:02,000
piensen si el juego no ha sido diseñado

259
0:11:03.071,000 --> 0:11:05,000
y manipulado para que no se preocupen por la privacidad,

260
0:11:05.795,000 --> 0:11:08,000
y cuando concluyamos que que estas manipulaciones ocurren,

261
0:11:09.057,000 --> 0:11:1,000
ya estaremos a mitad de camino

262
0:11:10.664,000 --> 0:11:12,000
de poder autoprotegernos.

263
0:11:12.922,000 --> 0:11:15,000
Si alguien les dice que la privacidad es incompatible

264
0:11:16.632,000 --> 0:11:17,000
con los beneficios del "big data",

265
0:11:18.481,000 --> 0:11:2,000
piensen que en los últimos 20 años,

266
0:11:20.954,000 --> 0:11:21,000
los investigadores han creado tecnologías

267
0:11:22.871,000 --> 0:11:25,000
que permiten que virtualmente cualquier transacción electrónica

268
0:11:26.189,000 --> 0:11:29,000
se realice en formas más preservadoras de la privacidad.

269
0:11:29.938,000 --> 0:11:31,000
Podemos navegar Internet en forma anónima.

270
0:11:32.493,000 --> 0:11:34,000
Podemos enviar correos electrónicos que solo pueda leer

271
0:11:35.171,000 --> 0:11:38,000
el destinatario, y no la NSA [agencia de seguridad].

272
0:11:38.88,000 --> 0:11:4,000
Podemos proteger incluso la privacidad de la minería de datos.

273
0:11:41.877,000 --> 0:11:44,000
En otras palabras, podemos tener los beneficios del "big data"

274
0:11:45.771,000 --> 0:11:47,000
y proteger la privacidad.

275
0:11:47.903,000 --> 0:11:5,000
Estas tecnologías, claro está, implican una inversión

276
0:11:51.694,000 --> 0:11:52,000
de la relacón costo- beneficio

277
0:11:53.24,000 --> 0:11:55,000
para los tenedores de los datos,

278
0:11:55.347,000 --> 0:11:58,000
razón por la cual, quizá, no escuchamos mucho de ellas.

279
0:11:58.8,000 --> 0:12:01,000
Eso me lleva de vuelta al Jardín del Edén.

280
0:12:02.506,000 --> 0:12:04,000
Hay una segunda interpretación de la privacidad

281
0:12:05.286,000 --> 0:12:06,000
en la historia del Jardín del Edén

282
0:12:07.095,000 --> 0:12:09,000
que no tiene que ver con el tema

283
0:12:09.191,000 --> 0:12:11,000
del desnudo de Adán y Eva

284
0:12:11.416,000 --> 0:12:13,000
ni con sentir vergüenza.

285
0:12:13.797,000 --> 0:12:15,000
Pueden encontrar ecos de esta interpretación

286
0:12:16.578,000 --> 0:12:18,000
en "El paraíso perdido" de John Milton.

287
0:12:19.36,000 --> 0:12:23,000
En el jardín, Adán y Eva están materialmente contentos.

288
0:12:23.557,000 --> 0:12:25,000
Están felices. Están satisfechos.

289
0:12:25.661,000 --> 0:12:27,000
No obstante, carecen de conocimiento

290
0:12:27.954,000 --> 0:12:28,000
y de autoconciencia.

291
0:12:29.594,000 --> 0:12:32,000
En el momento en que comen el bien llamado

292
0:12:32.913,000 --> 0:12:33,000
fruto del conocimiento,

293
0:12:34.206,000 --> 0:12:36,000
es cuando se descubren a sí mismos.

294
0:12:36.811,000 --> 0:12:4,000
Se hacen conscientes. Logran autonomía.

295
0:12:40.842,000 --> 0:12:43,000
El precio a pagar, sin embargo, es abandonar el jardín.

296
0:12:43.968,000 --> 0:12:46,000
La privacidad, en cierto modo, es tanto el medio

297
0:12:47.849,000 --> 0:12:49,000
como el precio a pagar por la libertad.

298
0:12:50.811,000 --> 0:12:52,000
Y los vendedores otra vez nos dicen

299
0:12:53.581,000 --> 0:12:56,000
que el "big data" y los medios sociales

300
0:12:56.6,000 --> 0:12:58,000
no son solo un paraíso de ganancias para ellos,

301
0:12:59.579,000 --> 0:13:01,000
sino el Jardín del Edén para el resto de nosotros.

302
0:13:02.036,000 --> 0:13:03,000
Recibimos contenido gratis.

303
0:13:03.274,000 --> 0:13:06,000
Podemos jugar a Angry Birds. Tenemos aplicaciones específicas.

304
0:13:06.397,000 --> 0:13:08,000
Pero, de hecho, en unos años, las organizaciones

305
0:13:09.294,000 --> 0:13:1,000
sabrán tanto de nosotros

306
0:13:10.903,000 --> 0:13:12,000
que podrán inferir nuestros deseos

307
0:13:13.613,000 --> 0:13:15,000
incluso antes de formularlos y quizá hasta

308
0:13:15.817,000 --> 0:13:17,000
compren productos en nuestro nombre

309
0:13:18.264,000 --> 0:13:2,000
antes de que sepamos que los necesitamos.

310
0:13:20.538,000 --> 0:13:23,000
Hay un escritor inglés

311
0:13:23.775,000 --> 0:13:26,000
que anticipó esta especie de futuro

312
0:13:26.82,000 --> 0:13:27,000
en el que cambiaríamos

313
0:13:28.225,000 --> 0:13:31,000
nuestra autonomía y libertad por comodidad--

314
0:13:31.773,000 --> 0:13:33,000
Incluso más que George Orwell.

315
0:13:33.934,000 --> 0:13:35,000
El autor es, por supuesto, Aldous Huxley.

316
0:13:36.695,000 --> 0:13:38,000
En "Un mundo feliz", él imagina una sociedad

317
0:13:39.549,000 --> 0:13:41,000
en la que las tecnologías que creamos

318
0:13:41.72,000 --> 0:13:42,000
en principio para la libertad

319
0:13:43.579,000 --> 0:13:45,000
terminan coaccionándonos.

320
0:13:46.146,000 --> 0:13:5,000
Sin embargo, en el libro, él también nos ofrece una salida

321
0:13:50.937,000 --> 0:13:53,000
de esa sociedad, similar al sendero

322
0:13:54.375,000 --> 0:13:57,000
que Adán y Eva tuvieron que seguir para salir del jardín.

323
0:13:58.33,000 --> 0:14:,000
En palabras de Savage,

324
0:14:00.477,000 --> 0:14:03,000
recuperar la autonomía y la libertad es posible,

325
0:14:03.546,000 --> 0:14:05,000
aunque el precio a pagar es elevado.

326
0:14:06.225,000 --> 0:14:11,000
Por eso creo que una de las peleas decisivas

327
0:14:11.94,000 --> 0:14:13,000
de nuestros tiempos será la pelea

328
0:14:14.503,000 --> 0:14:16,000
por el control de la información personal,

329
0:14:16.89,000 --> 0:14:19,000
la pelea porque el "big data" se vuelva una fuerza

330
0:14:20.397,000 --> 0:14:21,000
de libertad,

331
0:14:21.686,000 --> 0:14:25,000
en lugar de una fuerza que nos manipule desde las sombras.

332
0:14:26.432,000 --> 0:14:28,000
Muchos de nosotros

333
0:14:29.025,000 --> 0:14:31,000
ni siquiera sabemos que se está dando esta pelea,

334
0:14:31.778,000 --> 0:14:33,000
pero es así, nos guste o no.

335
0:14:34.45,000 --> 0:14:36,000
Y a riesgo de interpretar a la serpiente,

336
0:14:37.254,000 --> 0:14:39,000
les diré que las herramientas para pelear

337
0:14:40.151,000 --> 0:14:43,000
están aquí, la conciencia de lo que ocurre,

338
0:14:43.16,000 --> 0:14:44,000
y en sus manos,

339
0:14:44.515,000 --> 0:14:47,000
a unos pocos clics de distancia.

340
0:14:48.255,000 --> 0:14:49,000
Gracias.

341
0:14:49.737,000 --> 0:14:53,000
(Aplausos)

