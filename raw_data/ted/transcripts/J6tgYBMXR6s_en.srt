1
0:00:12.904,000 --> 0:00:14,000
Chris Anderson: Help us understand what machine learning is,

2
0:00:15.814,000 --> 0:00:17,000
because that seems to be the key driver

3
0:00:17.892,000 --> 0:00:19,000
of so much of the excitement and also of the concern

4
0:00:20.653,000 --> 0:00:21,000
around artificial intelligence.

5
0:00:22.171,000 --> 0:00:23,000
How does machine learning work?

6
0:00:23.838,000 --> 0:00:26,000
Sebastian Thrun: So, artificial intelligence and machine learning

7
0:00:27.758,000 --> 0:00:29,000
is about 60 years old

8
0:00:29.784,000 --> 0:00:33,000
and has not had a great day in its past until recently.

9
0:00:34.077,000 --> 0:00:36,000
And the reason is that today,

10
0:00:37.025,000 --> 0:00:4,000
we have reached a scale of computing and datasets

11
0:00:41.022,000 --> 0:00:43,000
that was necessary to make machines smart.

12
0:00:43.683,000 --> 0:00:44,000
So here's how it works.

13
0:00:45.458,000 --> 0:00:48,000
If you program a computer today, say, your phone,

14
0:00:48.979,000 --> 0:00:5,000
then you hire software engineers

15
0:00:51.338,000 --> 0:00:54,000
that write a very, very long kitchen recipe,

16
0:00:55.216,000 --> 0:00:58,000
like, "If the water is too hot, turn down the temperature.

17
0:00:58.372,000 --> 0:01:,000
If it's too cold, turn up the temperature."

18
0:01:00.675,000 --> 0:01:02,000
The recipes are not just 10 lines long.

19
0:01:03.548,000 --> 0:01:05,000
They are millions of lines long.

20
0:01:06.175,000 --> 0:01:1,000
A modern cell phone has 12 million lines of code.

21
0:01:10.283,000 --> 0:01:12,000
A browser has five million lines of code.

22
0:01:12.953,000 --> 0:01:16,000
And each bug in this recipe can cause your computer to crash.

23
0:01:17.946,000 --> 0:01:2,000
That's why a software engineer makes so much money.

24
0:01:21.953,000 --> 0:01:24,000
The new thing now is that computers can find their own rules.

25
0:01:25.637,000 --> 0:01:28,000
So instead of an expert deciphering, step by step,

26
0:01:29.267,000 --> 0:01:31,000
a rule for every contingency,

27
0:01:31.439,000 --> 0:01:34,000
what you do now is you give the computer examples

28
0:01:34.537,000 --> 0:01:35,000
and have it infer its own rules.

29
0:01:36.142,000 --> 0:01:4,000
A really good example is AlphaGo, which recently was won by Google.

30
0:01:40.472,000 --> 0:01:43,000
Normally, in game playing, you would really write down all the rules,

31
0:01:44.183,000 --> 0:01:45,000
but in AlphaGo's case,

32
0:01:45.992,000 --> 0:01:47,000
the system looked over a million games

33
0:01:48.082,000 --> 0:01:5,000
and was able to infer its own rules

34
0:01:50.298,000 --> 0:01:52,000
and then beat the world's residing Go champion.

35
0:01:53.853,000 --> 0:01:56,000
That is exciting, because it relieves the software engineer

36
0:01:57.386,000 --> 0:01:58,000
of the need of being super smart,

37
0:01:59.229,000 --> 0:02:01,000
and pushes the burden towards the data.

38
0:02:01.578,000 --> 0:02:05,000
As I said, the inflection point where this has become really possible --

39
0:02:06.136,000 --> 0:02:08,000
very embarrassing, my thesis was about machine learning.

40
0:02:08.906,000 --> 0:02:1,000
It was completely insignificant, don't read it,

41
0:02:11.135,000 --> 0:02:12,000
because it was 20 years ago

42
0:02:12.509,000 --> 0:02:14,000
and back then, the computers were as big as a cockroach brain.

43
0:02:15.44,000 --> 0:02:17,000
Now they are powerful enough to really emulate

44
0:02:17.795,000 --> 0:02:19,000
kind of specialized human thinking.

45
0:02:19.895,000 --> 0:02:21,000
And then the computers take advantage of the fact

46
0:02:22.232,000 --> 0:02:24,000
that they can look at much more data than people can.

47
0:02:24.756,000 --> 0:02:27,000
So I'd say AlphaGo looked at more than a million games.

48
0:02:27.86,000 --> 0:02:29,000
No human expert can ever study a million games.

49
0:02:30.723,000 --> 0:02:33,000
Google has looked at over a hundred billion web pages.

50
0:02:33.929,000 --> 0:02:35,000
No person can ever study a hundred billion web pages.

51
0:02:36.603,000 --> 0:02:38,000
So as a result, the computer can find rules

52
0:02:39.341,000 --> 0:02:4,000
that even people can't find.

53
0:02:41.12,000 --> 0:02:45,000
CA: So instead of looking ahead to, "If he does that, I will do that,"

54
0:02:45.456,000 --> 0:02:48,000
it's more saying, "Here is what looks like a winning pattern,

55
0:02:48.552,000 --> 0:02:5,000
here is what looks like a winning pattern."

56
0:02:50.655,000 --> 0:02:52,000
ST: Yeah. I mean, think about how you raise children.

57
0:02:53.196,000 --> 0:02:56,000
You don't spend the first 18 years giving kids a rule for every contingency

58
0:02:56.864,000 --> 0:02:58,000
and set them free and they have this big program.

59
0:02:59.235,000 --> 0:03:01,000
They stumble, fall, get up, they get slapped or spanked,

60
0:03:01.978,000 --> 0:03:03,000
and they have a positive experience, a good grade in school,

61
0:03:04.886,000 --> 0:03:05,000
and they figure it out on their own.

62
0:03:06.744,000 --> 0:03:07,000
That's happening with computers now,

63
0:03:08.505,000 --> 0:03:11,000
which makes computer programming so much easier all of a sudden.

64
0:03:11.558,000 --> 0:03:14,000
Now we don't have to think anymore. We just give them lots of data.

65
0:03:14.757,000 --> 0:03:17,000
CA: And so, this has been key to the spectacular improvement

66
0:03:18.203,000 --> 0:03:21,000
in power of self-driving cars.

67
0:03:21.291,000 --> 0:03:22,000
I think you gave me an example.

68
0:03:23.054,000 --> 0:03:25,000
Can you explain what's happening here?

69
0:03:25.763,000 --> 0:03:28,000
ST: This is a drive of a self-driving car

70
0:03:29.351,000 --> 0:03:3,000
that we happened to have at Udacity

71
0:03:31.332,000 --> 0:03:33,000
and recently made into a spin-off called Voyage.

72
0:03:33.754,000 --> 0:03:35,000
We have used this thing called deep learning

73
0:03:36.352,000 --> 0:03:37,000
to train a car to drive itself,

74
0:03:37.999,000 --> 0:03:39,000
and this is driving from Mountain View, California,

75
0:03:40.41,000 --> 0:03:41,000
to San Francisco

76
0:03:41.602,000 --> 0:03:43,000
on El Camino Real on a rainy day,

77
0:03:43.885,000 --> 0:03:46,000
with bicyclists and pedestrians and 133 traffic lights.

78
0:03:47.433,000 --> 0:03:49,000
And the novel thing here is,

79
0:03:50.093,000 --> 0:03:53,000
many, many moons ago, I started the Google self-driving car team.

80
0:03:53.237,000 --> 0:03:56,000
And back in the day, I hired the world's best software engineers

81
0:03:56.442,000 --> 0:03:57,000
to find the world's best rules.

82
0:03:58.073,000 --> 0:03:59,000
This is just trained.

83
0:03:59.851,000 --> 0:04:02,000
We drive this road 20 times,

84
0:04:03.211,000 --> 0:04:05,000
we put all this data into the computer brain,

85
0:04:05.682,000 --> 0:04:07,000
and after a few hours of processing,

86
0:04:07.788,000 --> 0:04:1,000
it comes up with behavior that often surpasses human agility.

87
0:04:11.738,000 --> 0:04:13,000
So it's become really easy to program it.

88
0:04:13.779,000 --> 0:04:16,000
This is 100 percent autonomous, about 33 miles, an hour and a half.

89
0:04:17.606,000 --> 0:04:2,000
CA: So, explain it -- on the big part of this program on the left,

90
0:04:21.26,000 --> 0:04:24,000
you're seeing basically what the computer sees as trucks and cars

91
0:04:24.541,000 --> 0:04:26,000
and those dots overtaking it and so forth.

92
0:04:27.451,000 --> 0:04:3,000
ST: On the right side, you see the camera image, which is the main input here,

93
0:04:31.237,000 --> 0:04:33,000
and it's used to find lanes, other cars, traffic lights.

94
0:04:33.937,000 --> 0:04:35,000
The vehicle has a radar to do distance estimation.

95
0:04:36.45,000 --> 0:04:38,000
This is very commonly used in these kind of systems.

96
0:04:39.095,000 --> 0:04:4,000
On the left side you see a laser diagram,

97
0:04:41.111,000 --> 0:04:44,000
where you see obstacles like trees and so on depicted by the laser.

98
0:04:44.335,000 --> 0:04:47,000
But almost all the interesting work is centering on the camera image now.

99
0:04:47.795,000 --> 0:04:5,000
We're really shifting over from precision sensors like radars and lasers

100
0:04:51.295,000 --> 0:04:52,000
into very cheap, commoditized sensors.

101
0:04:53.161,000 --> 0:04:54,000
A camera costs less than eight dollars.

102
0:04:55.172,000 --> 0:04:57,000
CA: And that green dot on the left thing, what is that?

103
0:04:57.989,000 --> 0:04:58,000
Is that anything meaningful?

104
0:04:59.384,000 --> 0:05:02,000
ST: This is a look-ahead point for your adaptive cruise control,

105
0:05:03.076,000 --> 0:05:05,000
so it helps us understand how to regulate velocity

106
0:05:05.577,000 --> 0:05:07,000
based on how far the cars in front of you are.

107
0:05:08.235,000 --> 0:05:1,000
CA: And so, you've also got an example, I think,

108
0:05:10.975,000 --> 0:05:12,000
of how the actual learning part takes place.

109
0:05:13.38,000 --> 0:05:15,000
Maybe we can see that. Talk about this.

110
0:05:15.862,000 --> 0:05:18,000
ST: This is an example where we posed a challenge to Udacity students

111
0:05:19.529,000 --> 0:05:22,000
to take what we call a self-driving car Nanodegree.

112
0:05:22.684,000 --> 0:05:23,000
We gave them this dataset

113
0:05:24.203,000 --> 0:05:27,000
and said "Hey, can you guys figure out how to steer this car?"

114
0:05:27.281,000 --> 0:05:28,000
And if you look at the images,

115
0:05:28.929,000 --> 0:05:32,000
it's, even for humans, quite impossible to get the steering right.

116
0:05:33.026,000 --> 0:05:36,000
And we ran a competition and said, "It's a deep learning competition,

117
0:05:36.641,000 --> 0:05:37,000
AI competition,"

118
0:05:37.838,000 --> 0:05:38,000
and we gave the students 48 hours.

119
0:05:39.749,000 --> 0:05:43,000
So if you are a software house like Google or Facebook,

120
0:05:43.945,000 --> 0:05:45,000
something like this costs you at least six months of work.

121
0:05:46.686,000 --> 0:05:48,000
So we figured 48 hours is great.

122
0:05:48.912,000 --> 0:05:51,000
And within 48 hours, we got about 100 submissions from students,

123
0:05:52.403,000 --> 0:05:55,000
and the top four got it perfectly right.

124
0:05:55.797,000 --> 0:05:57,000
It drives better than I could drive on this imagery,

125
0:05:58.461,000 --> 0:05:59,000
using deep learning.

126
0:05:59.674,000 --> 0:06:,000
And again, it's the same methodology.

127
0:06:01.497,000 --> 0:06:02,000
It's this magical thing.

128
0:06:02.685,000 --> 0:06:04,000
When you give enough data to a computer now,

129
0:06:04.794,000 --> 0:06:06,000
and give enough time to comprehend the data,

130
0:06:06.958,000 --> 0:06:07,000
it finds its own rules.

131
0:06:09.339,000 --> 0:06:13,000
CA: And so that has led to the development of powerful applications

132
0:06:14.208,000 --> 0:06:15,000
in all sorts of areas.

133
0:06:15.757,000 --> 0:06:17,000
You were talking to me the other day about cancer.

134
0:06:18.449,000 --> 0:06:19,000
Can I show this video?

135
0:06:19.662,000 --> 0:06:21,000
ST: Yeah, absolutely, please. CA: This is cool.

136
0:06:22.04,000 --> 0:06:25,000
ST: This is kind of an insight into what's happening

137
0:06:25.598,000 --> 0:06:27,000
in a completely different domain.

138
0:06:28.051,000 --> 0:06:31,000
This is augmenting, or competing --

139
0:06:31.827,000 --> 0:06:32,000
it's in the eye of the beholder --

140
0:06:33.6,000 --> 0:06:36,000
with people who are being paid 400,000 dollars a year,

141
0:06:37.078,000 --> 0:06:38,000
dermatologists,

142
0:06:38.339,000 --> 0:06:39,000
highly trained specialists.

143
0:06:40.346,000 --> 0:06:43,000
It takes more than a decade of training to be a good dermatologist.

144
0:06:43.931,000 --> 0:06:46,000
What you see here is the machine learning version of it.

145
0:06:47.151,000 --> 0:06:48,000
It's called a neural network.

146
0:06:49.016,000 --> 0:06:52,000
"Neural networks" is the technical term for these machine learning algorithms.

147
0:06:52.782,000 --> 0:06:53,000
They've been around since the 1980s.

148
0:06:54.595,000 --> 0:06:58,000
This one was invented in 1988 by a Facebook Fellow called Yann LeCun,

149
0:06:59.259,000 --> 0:07:02,000
and it propagates data stages

150
0:07:02.841,000 --> 0:07:04,000
through what you could think of as the human brain.

151
0:07:05.443,000 --> 0:07:07,000
It's not quite the same thing, but it emulates the same thing.

152
0:07:08.433,000 --> 0:07:09,000
It goes stage after stage.

153
0:07:09.759,000 --> 0:07:12,000
In the very first stage, it takes the visual input and extracts edges

154
0:07:13.42,000 --> 0:07:15,000
and rods and dots.

155
0:07:16.056,000 --> 0:07:19,000
And the next one becomes more complicated edges

156
0:07:19.117,000 --> 0:07:22,000
and shapes like little half-moons.

157
0:07:22.332,000 --> 0:07:26,000
And eventually, it's able to build really complicated concepts.

158
0:07:26.799,000 --> 0:07:28,000
Andrew Ng has been able to show

159
0:07:28.871,000 --> 0:07:31,000
that it's able to find cat faces and dog faces

160
0:07:32.375,000 --> 0:07:33,000
in vast amounts of images.

161
0:07:34.06,000 --> 0:07:36,000
What my student team at Stanford has shown is that

162
0:07:36.808,000 --> 0:07:42,000
if you train it on 129,000 images of skin conditions,

163
0:07:42.905,000 --> 0:07:44,000
including melanoma and carcinomas,

164
0:07:45.494,000 --> 0:07:48,000
you can do as good a job

165
0:07:48.819,000 --> 0:07:5,000
as the best human dermatologists.

166
0:07:51.04,000 --> 0:07:53,000
And to convince ourselves that this is the case,

167
0:07:53.613,000 --> 0:07:56,000
we captured an independent dataset that we presented to our network

168
0:07:57.627,000 --> 0:08:01,000
and to 25 board-certified Stanford-level dermatologists,

169
0:08:01.993,000 --> 0:08:02,000
and compared those.

170
0:08:03.689,000 --> 0:08:04,000
And in most cases,

171
0:08:05.217,000 --> 0:08:08,000
they were either on par or above the performance classification accuracy

172
0:08:09.116,000 --> 0:08:1,000
of human dermatologists.

173
0:08:10.607,000 --> 0:08:11,000
CA: You were telling me an anecdote.

174
0:08:12.377,000 --> 0:08:13,000
I think about this image right here.

175
0:08:14.358,000 --> 0:08:15,000
What happened here?

176
0:08:15.866,000 --> 0:08:19,000
ST: This was last Thursday. That's a moving piece.

177
0:08:19.898,000 --> 0:08:22,000
What we've shown before and we published in "Nature" earlier this year

178
0:08:23.522,000 --> 0:08:25,000
was this idea that we show dermatologists images

179
0:08:26.03,000 --> 0:08:27,000
and our computer program images,

180
0:08:27.593,000 --> 0:08:28,000
and count how often they're right.

181
0:08:29.244,000 --> 0:08:3,000
But all these images are past images.

182
0:08:31.046,000 --> 0:08:34,000
They've all been biopsied to make sure we had the correct classification.

183
0:08:34.53,000 --> 0:08:35,000
This one wasn't.

184
0:08:35.726,000 --> 0:08:38,000
This one was actually done at Stanford by one of our collaborators.

185
0:08:38.929,000 --> 0:08:4,000
The story goes that our collaborator,

186
0:08:41.267,000 --> 0:08:44,000
who is a world-famous dermatologist, one of the three best, apparently,

187
0:08:44.682,000 --> 0:08:46,000
looked at this mole and said, "This is not skin cancer."

188
0:08:47.641,000 --> 0:08:49,000
And then he had a second moment, where he said,

189
0:08:50.141,000 --> 0:08:51,000
"Well, let me just check with the app."

190
0:08:52.031,000 --> 0:08:54,000
So he took out his iPhone and ran our piece of software,

191
0:08:54.754,000 --> 0:08:56,000
our "pocket dermatologist," so to speak,

192
0:08:56.899,000 --> 0:08:58,000
and the iPhone said: cancer.

193
0:08:59.917,000 --> 0:09:,000
It said melanoma.

194
0:09:01.849,000 --> 0:09:02,000
And then he was confused.

195
0:09:03.106,000 --> 0:09:07,000
And he decided, "OK, maybe I trust the iPhone a little bit more than myself,"

196
0:09:07.681,000 --> 0:09:09,000
and he sent it out to the lab to get it biopsied.

197
0:09:10.44,000 --> 0:09:12,000
And it came up as an aggressive melanoma.

198
0:09:13.545,000 --> 0:09:16,000
So I think this might be the first time that we actually found,

199
0:09:16.636,000 --> 0:09:18,000
in the practice of using deep learning,

200
0:09:19.147,000 --> 0:09:22,000
an actual person whose melanoma would have gone unclassified,

201
0:09:22.543,000 --> 0:09:24,000
had it not been for deep learning.

202
0:09:24.682,000 --> 0:09:25,000
CA: I mean, that's incredible.

203
0:09:26.266,000 --> 0:09:27,000
(Applause)

204
0:09:28.059,000 --> 0:09:31,000
It feels like there'd be an instant demand for an app like this right now,

205
0:09:31.683,000 --> 0:09:32,000
that you might freak out a lot of people.

206
0:09:33.673,000 --> 0:09:36,000
Are you thinking of doing this, making an app that allows self-checking?

207
0:09:37.224,000 --> 0:09:41,000
ST: So my in-box is flooded about cancer apps,

208
0:09:42.221,000 --> 0:09:44,000
with heartbreaking stories of people.

209
0:09:44.548,000 --> 0:09:47,000
I mean, some people have had 10, 15, 20 melanomas removed,

210
0:09:47.776,000 --> 0:09:5,000
and are scared that one might be overlooked, like this one,

211
0:09:51.752,000 --> 0:09:52,000
and also, about, I don't know,

212
0:09:53.517,000 --> 0:09:55,000
flying cars and speaker inquiries these days, I guess.

213
0:09:56.273,000 --> 0:09:58,000
My take is, we need more testing.

214
0:09:59.449,000 --> 0:10:,000
I want to be very careful.

215
0:10:01.251,000 --> 0:10:04,000
It's very easy to give a flashy result and impress a TED audience.

216
0:10:04.941,000 --> 0:10:06,000
It's much harder to put something out that's ethical.

217
0:10:07.592,000 --> 0:10:09,000
And if people were to use the app

218
0:10:10.01,000 --> 0:10:12,000
and choose not to consult the assistance of a doctor

219
0:10:12.831,000 --> 0:10:13,000
because we get it wrong,

220
0:10:14.438,000 --> 0:10:15,000
I would feel really bad about it.

221
0:10:16.115,000 --> 0:10:17,000
So we're currently doing clinical tests,

222
0:10:18.064,000 --> 0:10:2,000
and if these clinical tests commence and our data holds up,

223
0:10:20.886,000 --> 0:10:22,000
we might be able at some point to take this kind of technology

224
0:10:23.9,000 --> 0:10:24,000
and take it out of the Stanford clinic

225
0:10:25.816,000 --> 0:10:26,000
and bring it to the entire world,

226
0:10:27.498,000 --> 0:10:29,000
places where Stanford doctors never, ever set foot.

227
0:10:30.617,000 --> 0:10:32,000
CA: And do I hear this right,

228
0:10:33.221,000 --> 0:10:34,000
that it seemed like what you were saying,

229
0:10:35.211,000 --> 0:10:39,000
because you are working with this army of Udacity students,

230
0:10:39.489,000 --> 0:10:42,000
that in a way, you're applying a different form of machine learning

231
0:10:42.734,000 --> 0:10:43,000
than might take place in a company,

232
0:10:44.493,000 --> 0:10:47,000
which is you're combining machine learning with a form of crowd wisdom.

233
0:10:48.001,000 --> 0:10:51,000
Are you saying that sometimes you think that could actually outperform

234
0:10:51.409,000 --> 0:10:53,000
what a company can do, even a vast company?

235
0:10:53.483,000 --> 0:10:55,000
ST: I believe there's now instances that blow my mind,

236
0:10:56.447,000 --> 0:10:57,000
and I'm still trying to understand.

237
0:10:58.229,000 --> 0:11:01,000
What Chris is referring to is these competitions that we run.

238
0:11:02.19,000 --> 0:11:04,000
We turn them around in 48 hours,

239
0:11:04.482,000 --> 0:11:06,000
and we've been able to build a self-driving car

240
0:11:06.758,000 --> 0:11:09,000
that can drive from Mountain View to San Francisco on surface streets.

241
0:11:10.169,000 --> 0:11:13,000
It's not quite on par with Google after seven years of Google work,

242
0:11:13.777,000 --> 0:11:15,000
but it's getting there.

243
0:11:16.329,000 --> 0:11:19,000
And it took us only two engineers and three months to do this.

244
0:11:19.437,000 --> 0:11:21,000
And the reason is, we have an army of students

245
0:11:22.317,000 --> 0:11:23,000
who participate in competitions.

246
0:11:24.191,000 --> 0:11:26,000
We're not the only ones who use crowdsourcing.

247
0:11:26.435,000 --> 0:11:28,000
Uber and Didi use crowdsource for driving.

248
0:11:28.682,000 --> 0:11:3,000
Airbnb uses crowdsourcing for hotels.

249
0:11:31.465,000 --> 0:11:35,000
There's now many examples where people do bug-finding crowdsourcing

250
0:11:35.496,000 --> 0:11:37,000
or protein folding, of all things, in crowdsourcing.

251
0:11:38.324,000 --> 0:11:4,000
But we've been able to build this car in three months,

252
0:11:41.263,000 --> 0:11:44,000
so I am actually rethinking

253
0:11:44.942,000 --> 0:11:46,000
how we organize corporations.

254
0:11:47.204,000 --> 0:11:51,000
We have a staff of 9,000 people who are never hired,

255
0:11:51.924,000 --> 0:11:52,000
that I never fire.

256
0:11:53.256,000 --> 0:11:55,000
They show up to work and I don't even know.

257
0:11:55.642,000 --> 0:11:58,000
Then they submit to me maybe 9,000 answers.

258
0:11:58.724,000 --> 0:12:,000
I'm not obliged to use any of those.

259
0:12:00.924,000 --> 0:12:01,000
I end up -- I pay only the winners,

260
0:12:02.939,000 --> 0:12:05,000
so I'm actually very cheapskate here, which is maybe not the best thing to do.

261
0:12:06.681,000 --> 0:12:09,000
But they consider it part of their education, too, which is nice.

262
0:12:09.89,000 --> 0:12:13,000
But these students have been able to produce amazing deep learning results.

263
0:12:14.115,000 --> 0:12:17,000
So yeah, the synthesis of great people and great machine learning is amazing.

264
0:12:18,000 --> 0:12:2,000
CA: I mean, Gary Kasparov said on the first day [of TED2017]

265
0:12:20.848,000 --> 0:12:25,000
that the winners of chess, surprisingly, turned out to be two amateur chess players

266
0:12:26.284,000 --> 0:12:31,000
with three mediocre-ish, mediocre-to-good, computer programs,

267
0:12:31.679,000 --> 0:12:34,000
that could outperform one grand master with one great chess player,

268
0:12:34.866,000 --> 0:12:35,000
like it was all part of the process.

269
0:12:36.633,000 --> 0:12:39,000
And it almost seems like you're talking about a much richer version

270
0:12:39.992,000 --> 0:12:4,000
of that same idea.

271
0:12:41.216,000 --> 0:12:44,000
ST: Yeah, I mean, as you followed the fantastic panels yesterday morning,

272
0:12:45.097,000 --> 0:12:46,000
two sessions about AI,

273
0:12:47.115,000 --> 0:12:49,000
robotic overlords and the human response,

274
0:12:49.306,000 --> 0:12:5,000
many, many great things were said.

275
0:12:51.312,000 --> 0:12:53,000
But one of the concerns is that we sometimes confuse

276
0:12:54.023,000 --> 0:12:58,000
what's actually been done with AI with this kind of overlord threat,

277
0:12:58.109,000 --> 0:13:01,000
where your AI develops consciousness, right?

278
0:13:01.557,000 --> 0:13:03,000
The last thing I want is for my AI to have consciousness.

279
0:13:04.552,000 --> 0:13:05,000
I don't want to come into my kitchen

280
0:13:06.292,000 --> 0:13:1,000
and have the refrigerator fall in love with the dishwasher

281
0:13:10.509,000 --> 0:13:12,000
and tell me, because I wasn't nice enough,

282
0:13:12.657,000 --> 0:13:13,000
my food is now warm.

283
0:13:14.518,000 --> 0:13:16,000
I wouldn't buy these products, and I don't want them.

284
0:13:17.825,000 --> 0:13:18,000
But the truth is, for me,

285
0:13:19.651,000 --> 0:13:21,000
AI has always been an augmentation of people.

286
0:13:22.893,000 --> 0:13:23,000
It's been an augmentation of us,

287
0:13:24.593,000 --> 0:13:25,000
to make us stronger.

288
0:13:26.074,000 --> 0:13:28,000
And I think Kasparov was exactly correct.

289
0:13:28.929,000 --> 0:13:31,000
It's been the combination of human smarts and machine smarts

290
0:13:32.802,000 --> 0:13:33,000
that make us stronger.

291
0:13:34.29,000 --> 0:13:38,000
The theme of machines making us stronger is as old as machines are.

292
0:13:39.567,000 --> 0:13:42,000
The agricultural revolution took place because it made steam engines

293
0:13:43.349,000 --> 0:13:45,000
and farming equipment that couldn't farm by itself,

294
0:13:46.039,000 --> 0:13:48,000
that never replaced us; it made us stronger.

295
0:13:48.185,000 --> 0:13:51,000
And I believe this new wave of AI will make us much, much stronger

296
0:13:51.947,000 --> 0:13:52,000
as a human race.

297
0:13:53.765,000 --> 0:13:54,000
CA: We'll come on to that a bit more,

298
0:13:55.602,000 --> 0:13:58,000
but just to continue with the scary part of this for some people,

299
0:13:59.297,000 --> 0:14:02,000
like, what feels like it gets scary for people is when you have

300
0:14:02.879,000 --> 0:14:06,000
a computer that can, one, rewrite its own code,

301
0:14:07.521,000 --> 0:14:1,000
so, it can create multiple copies of itself,

302
0:14:11.129,000 --> 0:14:12,000
try a bunch of different code versions,

303
0:14:13.05,000 --> 0:14:14,000
possibly even at random,

304
0:14:14.849,000 --> 0:14:17,000
and then check them out and see if a goal is achieved and improved.

305
0:14:18.505,000 --> 0:14:21,000
So, say the goal is to do better on an intelligence test.

306
0:14:22.17,000 --> 0:14:25,000
You know, a computer that's moderately good at that,

307
0:14:26.088,000 --> 0:14:28,000
you could try a million versions of that.

308
0:14:28.621,000 --> 0:14:3,000
You might find one that was better,

309
0:14:30.735,000 --> 0:14:32,000
and then, you know, repeat.

310
0:14:32.763,000 --> 0:14:35,000
And so the concern is that you get some sort of runaway effect

311
0:14:35.827,000 --> 0:14:38,000
where everything is fine on Thursday evening,

312
0:14:38.859,000 --> 0:14:4,000
and you come back into the lab on Friday morning,

313
0:14:41.219,000 --> 0:14:43,000
and because of the speed of computers and so forth,

314
0:14:43.692,000 --> 0:14:44,000
things have gone crazy, and suddenly --

315
0:14:45.619,000 --> 0:14:47,000
ST: I would say this is a possibility,

316
0:14:47.663,000 --> 0:14:48,000
but it's a very remote possibility.

317
0:14:49.603,000 --> 0:14:52,000
So let me just translate what I heard you say.

318
0:14:52.964,000 --> 0:14:54,000
In the AlphaGo case, we had exactly this thing:

319
0:14:55.692,000 --> 0:14:57,000
the computer would play the game against itself

320
0:14:58.031,000 --> 0:14:59,000
and then learn new rules.

321
0:14:59.305,000 --> 0:15:02,000
And what machine learning is is a rewriting of the rules.

322
0:15:02.564,000 --> 0:15:03,000
It's the rewriting of code.

323
0:15:04.357,000 --> 0:15:06,000
But I think there was absolutely no concern

324
0:15:07.226,000 --> 0:15:09,000
that AlphaGo would take over the world.

325
0:15:09.676,000 --> 0:15:1,000
It can't even play chess.

326
0:15:11.164,000 --> 0:15:16,000
CA: No, no, no, but now, these are all very single-domain things.

327
0:15:16.335,000 --> 0:15:18,000
But it's possible to imagine.

328
0:15:19.238,000 --> 0:15:22,000
I mean, we just saw a computer that seemed nearly capable

329
0:15:22.351,000 --> 0:15:24,000
of passing a university entrance test,

330
0:15:25.03,000 --> 0:15:28,000
that can kind of -- it can't read and understand in the sense that we can,

331
0:15:28.742,000 --> 0:15:29,000
but it can certainly absorb all the text

332
0:15:30.753,000 --> 0:15:32,000
and maybe see increased patterns of meaning.

333
0:15:33.676,000 --> 0:15:36,000
Isn't there a chance that, as this broadens out,

334
0:15:37.394,000 --> 0:15:39,000
there could be a different kind of runaway effect?

335
0:15:39.884,000 --> 0:15:41,000
ST: That's where I draw the line, honestly.

336
0:15:41.986,000 --> 0:15:43,000
And the chance exists -- I don't want to downplay it --

337
0:15:44.653,000 --> 0:15:47,000
but I think it's remote, and it's not the thing that's on my mind these days,

338
0:15:48.349,000 --> 0:15:5,000
because I think the big revolution is something else.

339
0:15:50.885,000 --> 0:15:52,000
Everything successful in AI to the present date

340
0:15:53.831,000 --> 0:15:55,000
has been extremely specialized,

341
0:15:56.069,000 --> 0:15:58,000
and it's been thriving on a single idea,

342
0:15:58.582,000 --> 0:16:,000
which is massive amounts of data.

343
0:16:01.345,000 --> 0:16:05,000
The reason AlphaGo works so well is because of massive numbers of Go plays,

344
0:16:05.516,000 --> 0:16:08,000
and AlphaGo can't drive a car or fly a plane.

345
0:16:08.795,000 --> 0:16:11,000
The Google self-driving car or the Udacity self-driving car

346
0:16:11.85,000 --> 0:16:14,000
thrives on massive amounts of data, and it can't do anything else.

347
0:16:15.114,000 --> 0:16:16,000
It can't even control a motorcycle.

348
0:16:16.865,000 --> 0:16:18,000
It's a very specific, domain-specific function,

349
0:16:19.651,000 --> 0:16:2,000
and the same is true for our cancer app.

350
0:16:21.582,000 --> 0:16:24,000
There has been almost no progress on this thing called "general AI,"

351
0:16:24.842,000 --> 0:16:28,000
where you go to an AI and say, "Hey, invent for me special relativity

352
0:16:28.866,000 --> 0:16:29,000
or string theory."

353
0:16:30.556,000 --> 0:16:31,000
It's totally in the infancy.

354
0:16:32.511,000 --> 0:16:34,000
The reason I want to emphasize this,

355
0:16:34.662,000 --> 0:16:37,000
I see the concerns, and I want to acknowledge them.

356
0:16:38.524,000 --> 0:16:4,000
But if I were to think about one thing,

357
0:16:41.434,000 --> 0:16:46,000
I would ask myself the question, "What if we can take anything repetitive

358
0:16:47.021,000 --> 0:16:5,000
and make ourselves 100 times as efficient?"

359
0:16:51.17,000 --> 0:16:55,000
It so turns out, 300 years ago, we all worked in agriculture

360
0:16:55.443,000 --> 0:16:57,000
and did farming and did repetitive things.

361
0:16:57.518,000 --> 0:16:59,000
Today, 75 percent of us work in offices

362
0:17:00.098,000 --> 0:17:02,000
and do repetitive things.

363
0:17:02.246,000 --> 0:17:04,000
We've become spreadsheet monkeys.

364
0:17:04.453,000 --> 0:17:06,000
And not just low-end labor.

365
0:17:06.531,000 --> 0:17:08,000
We've become dermatologists doing repetitive things,

366
0:17:09.309,000 --> 0:17:1,000
lawyers doing repetitive things.

367
0:17:11.082,000 --> 0:17:14,000
I think we are at the brink of being able to take an AI,

368
0:17:14.929,000 --> 0:17:15,000
look over our shoulders,

369
0:17:16.671,000 --> 0:17:2,000
and they make us maybe 10 or 50 times as effective in these repetitive things.

370
0:17:20.753,000 --> 0:17:21,000
That's what is on my mind.

371
0:17:22.052,000 --> 0:17:24,000
CA: That sounds super exciting.

372
0:17:24.526,000 --> 0:17:27,000
The process of getting there seems a little terrifying to some people,

373
0:17:28.08,000 --> 0:17:31,000
because once a computer can do this repetitive thing

374
0:17:31.284,000 --> 0:17:34,000
much better than the dermatologist

375
0:17:34.742,000 --> 0:17:37,000
or than the driver, especially, is the thing that's talked about

376
0:17:37.996,000 --> 0:17:38,000
so much now,

377
0:17:39.31,000 --> 0:17:4,000
suddenly millions of jobs go,

378
0:17:41.292,000 --> 0:17:43,000
and, you know, the country's in revolution

379
0:17:44.011,000 --> 0:17:48,000
before we ever get to the more glorious aspects of what's possible.

380
0:17:48.364,000 --> 0:17:5,000
ST: Yeah, and that's an issue, and it's a big issue,

381
0:17:50.905,000 --> 0:17:54,000
and it was pointed out yesterday morning by several guest speakers.

382
0:17:55.125,000 --> 0:17:57,000
Now, prior to me showing up onstage,

383
0:17:57.903,000 --> 0:18:,000
I confessed I'm a positive, optimistic person,

384
0:18:01.666,000 --> 0:18:03,000
so let me give you an optimistic pitch,

385
0:18:04.079,000 --> 0:18:08,000
which is, think of yourself back 300 years ago.

386
0:18:08.898,000 --> 0:18:11,000
Europe just survived 140 years of continuous war,

387
0:18:12.918,000 --> 0:18:13,000
none of you could read or write,

388
0:18:14.653,000 --> 0:18:16,000
there were no jobs that you hold today,

389
0:18:17.622,000 --> 0:18:21,000
like investment banker or software engineer or TV anchor.

390
0:18:21.742,000 --> 0:18:23,000
We would all be in the fields and farming.

391
0:18:24.18,000 --> 0:18:27,000
Now here comes little Sebastian with a little steam engine in his pocket,

392
0:18:27.777,000 --> 0:18:28,000
saying, "Hey guys, look at this.

393
0:18:29.349,000 --> 0:18:32,000
It's going to make you 100 times as strong, so you can do something else."

394
0:18:32.968,000 --> 0:18:34,000
And then back in the day, there was no real stage,

395
0:18:35.462,000 --> 0:18:37,000
but Chris and I hang out with the cows in the stable,

396
0:18:38.012,000 --> 0:18:4,000
and he says, "I'm really concerned about it,

397
0:18:40.136,000 --> 0:18:43,000
because I milk my cow every day, and what if the machine does this for me?"

398
0:18:43.812,000 --> 0:18:44,000
The reason why I mention this is,

399
0:18:46.36,000 --> 0:18:49,000
we're always good in acknowledging past progress and the benefit of it,

400
0:18:49.987,000 --> 0:18:52,000
like our iPhones or our planes or electricity or medical supply.

401
0:18:53.365,000 --> 0:18:57,000
We all love to live to 80, which was impossible 300 years ago.

402
0:18:57.634,000 --> 0:19:01,000
But we kind of don't apply the same rules to the future.

403
0:19:02.621,000 --> 0:19:05,000
So if I look at my own job as a CEO,

404
0:19:05.852,000 --> 0:19:08,000
I would say 90 percent of my work is repetitive,

405
0:19:09.016,000 --> 0:19:1,000
I don't enjoy it,

406
0:19:10.391,000 --> 0:19:13,000
I spend about four hours per day on stupid, repetitive email.

407
0:19:14.393,000 --> 0:19:17,000
And I'm burning to have something that helps me get rid of this.

408
0:19:18.058,000 --> 0:19:19,000
Why?

409
0:19:19.24,000 --> 0:19:22,000
Because I believe all of us are insanely creative;

410
0:19:22.731,000 --> 0:19:25,000
I think the TED community more than anybody else.

411
0:19:25.949,000 --> 0:19:28,000
But even blue-collar workers; I think you can go to your hotel maid

412
0:19:29.532,000 --> 0:19:31,000
and have a drink with him or her,

413
0:19:31.958,000 --> 0:19:33,000
and an hour later, you find a creative idea.

414
0:19:34.699,000 --> 0:19:38,000
What this will empower is to turn this creativity into action.

415
0:19:39.265,000 --> 0:19:42,000
Like, what if you could build Google in a day?

416
0:19:43.221,000 --> 0:19:46,000
What if you could sit over beer and invent the next Snapchat,

417
0:19:46.561,000 --> 0:19:47,000
whatever it is,

418
0:19:47.75,000 --> 0:19:49,000
and tomorrow morning it's up and running?

419
0:19:49.961,000 --> 0:19:5,000
And that is not science fiction.

420
0:19:51.758,000 --> 0:19:52,000
What's going to happen is,

421
0:19:53.036,000 --> 0:19:54,000
we are already in history.

422
0:19:54.927,000 --> 0:19:57,000
We've unleashed this amazing creativity

423
0:19:58.179,000 --> 0:19:59,000
by de-slaving us from farming

424
0:19:59.814,000 --> 0:20:02,000
and later, of course, from factory work

425
0:20:03.201,000 --> 0:20:06,000
and have invented so many things.

426
0:20:06.387,000 --> 0:20:08,000
It's going to be even better, in my opinion.

427
0:20:08.589,000 --> 0:20:1,000
And there's going to be great side effects.

428
0:20:10.685,000 --> 0:20:11,000
One of the side effects will be

429
0:20:12.198,000 --> 0:20:16,000
that things like food and medical supply and education and shelter

430
0:20:17.017,000 --> 0:20:18,000
and transportation

431
0:20:18.218,000 --> 0:20:2,000
will all become much more affordable to all of us,

432
0:20:20.683,000 --> 0:20:21,000
not just the rich people.

433
0:20:22.029,000 --> 0:20:23,000
CA: Hmm.

434
0:20:23.235,000 --> 0:20:27,000
So when Martin Ford argued, you know, that this time it's different

435
0:20:27.6,000 --> 0:20:3,000
because the intelligence that we've used in the past

436
0:20:31.077,000 --> 0:20:33,000
to find new ways to be

437
0:20:33.584,000 --> 0:20:35,000
will be matched at the same pace

438
0:20:35.887,000 --> 0:20:37,000
by computers taking over those things,

439
0:20:38.202,000 --> 0:20:41,000
what I hear you saying is that, not completely,

440
0:20:41.304,000 --> 0:20:43,000
because of human creativity.

441
0:20:44.279,000 --> 0:20:47,000
Do you think that that's fundamentally different from the kind of creativity

442
0:20:48.088,000 --> 0:20:5,000
that computers can do?

443
0:20:50.808,000 --> 0:20:54,000
ST: So, that's my firm belief as an AI person --

444
0:20:55.266,000 --> 0:20:58,000
that I haven't seen any real progress on creativity

445
0:20:59.949,000 --> 0:21:,000
and out-of-the-box thinking.

446
0:21:01.38,000 --> 0:21:04,000
What I see right now -- and this is really important for people to realize,

447
0:21:05.027,000 --> 0:21:07,000
because the word "artificial intelligence" is so threatening,

448
0:21:07.954,000 --> 0:21:09,000
and then we have Steve Spielberg tossing a movie in,

449
0:21:10.501,000 --> 0:21:12,000
where all of a sudden the computer is our overlord,

450
0:21:12.938,000 --> 0:21:13,000
but it's really a technology.

451
0:21:14.414,000 --> 0:21:16,000
It's a technology that helps us do repetitive things.

452
0:21:17.42,000 --> 0:21:19,000
And the progress has been entirely on the repetitive end.

453
0:21:20.357,000 --> 0:21:22,000
It's been in legal document discovery.

454
0:21:22.609,000 --> 0:21:23,000
It's been contract drafting.

455
0:21:24.313,000 --> 0:21:28,000
It's been screening X-rays of your chest.

456
0:21:28.56,000 --> 0:21:29,000
And these things are so specialized,

457
0:21:30.357,000 --> 0:21:32,000
I don't see the big threat of humanity.

458
0:21:32.772,000 --> 0:21:33,000
In fact, we as people --

459
0:21:34.59,000 --> 0:21:36,000
I mean, let's face it: we've become superhuman.

460
0:21:36.999,000 --> 0:21:37,000
We've made us superhuman.

461
0:21:38.787,000 --> 0:21:4,000
We can swim across the Atlantic in 11 hours.

462
0:21:41.443,000 --> 0:21:43,000
We can take a device out of our pocket

463
0:21:43.541,000 --> 0:21:45,000
and shout all the way to Australia,

464
0:21:45.712,000 --> 0:21:47,000
and in real time, have that person shouting back to us.

465
0:21:48.336,000 --> 0:21:51,000
That's physically not possible. We're breaking the rules of physics.

466
0:21:51.984,000 --> 0:21:53,000
When this is said and done, we're going to remember everything

467
0:21:54.951,000 --> 0:21:55,000
we've ever said and seen,

468
0:21:56.188,000 --> 0:21:57,000
you'll remember every person,

469
0:21:57.708,000 --> 0:21:59,000
which is good for me in my early stages of Alzheimer's.

470
0:22:00.358,000 --> 0:22:01,000
Sorry, what was I saying? I forgot.

471
0:22:02.059,000 --> 0:22:03,000
CA: (Laughs)

472
0:22:03.661,000 --> 0:22:06,000
ST: We will probably have an IQ of 1,000 or more.

473
0:22:06.762,000 --> 0:22:09,000
There will be no more spelling classes for our kids,

474
0:22:10.211,000 --> 0:22:12,000
because there's no spelling issue anymore.

475
0:22:12.321,000 --> 0:22:13,000
There's no math issue anymore.

476
0:22:14.177,000 --> 0:22:17,000
And I think what really will happen is that we can be super creative.

477
0:22:17.711,000 --> 0:22:18,000
And we are. We are creative.

478
0:22:19.592,000 --> 0:22:2,000
That's our secret weapon.

479
0:22:21.168,000 --> 0:22:23,000
CA: So the jobs that are getting lost,

480
0:22:23.345,000 --> 0:22:25,000
in a way, even though it's going to be painful,

481
0:22:25.863,000 --> 0:22:27,000
humans are capable of more than those jobs.

482
0:22:27.934,000 --> 0:22:28,000
This is the dream.

483
0:22:29.176,000 --> 0:22:33,000
The dream is that humans can rise to just a new level of empowerment

484
0:22:33.447,000 --> 0:22:34,000
and discovery.

485
0:22:35.128,000 --> 0:22:36,000
That's the dream.

486
0:22:36.604,000 --> 0:22:37,000
ST: And think about this:

487
0:22:38.271,000 --> 0:22:4,000
if you look at the history of humanity,

488
0:22:40.316,000 --> 0:22:43,000
that might be whatever -- 60-100,000 years old, give or take --

489
0:22:43.668,000 --> 0:22:46,000
almost everything that you cherish in terms of invention,

490
0:22:47.418,000 --> 0:22:49,000
of technology, of things we've built,

491
0:22:49.593,000 --> 0:22:52,000
has been invented in the last 150 years.

492
0:22:53.756,000 --> 0:22:56,000
If you toss in the book and the wheel, it's a little bit older.

493
0:22:56.828,000 --> 0:22:57,000
Or the axe.

494
0:22:58.021,000 --> 0:23:,000
But your phone, your sneakers,

495
0:23:00.835,000 --> 0:23:03,000
these chairs, modern manufacturing, penicillin --

496
0:23:04.41,000 --> 0:23:05,000
the things we cherish.

497
0:23:06.148,000 --> 0:23:09,000
Now, that to me means

498
0:23:09.83,000 --> 0:23:12,000
the next 150 years will find more things.

499
0:23:12.895,000 --> 0:23:16,000
In fact, the pace of invention has gone up, not gone down, in my opinion.

500
0:23:17.073,000 --> 0:23:21,000
I believe only one percent of interesting things have been invented yet. Right?

501
0:23:22.002,000 --> 0:23:23,000
We haven't cured cancer.

502
0:23:24.014,000 --> 0:23:27,000
We don't have flying cars -- yet. Hopefully, I'll change this.

503
0:23:27.756,000 --> 0:23:3,000
That used to be an example people laughed about. (Laughs)

504
0:23:31.037,000 --> 0:23:33,000
It's funny, isn't it? Working secretly on flying cars.

505
0:23:34.053,000 --> 0:23:36,000
We don't live twice as long yet. OK?

506
0:23:36.76,000 --> 0:23:38,000
We don't have this magic implant in our brain

507
0:23:39.569,000 --> 0:23:4,000
that gives us the information we want.

508
0:23:41.425,000 --> 0:23:42,000
And you might be appalled by it,

509
0:23:42.975,000 --> 0:23:44,000
but I promise you, once you have it, you'll love it.

510
0:23:45.443,000 --> 0:23:46,000
I hope you will.

511
0:23:46.633,000 --> 0:23:47,000
It's a bit scary, I know.

512
0:23:48.566,000 --> 0:23:5,000
There are so many things we haven't invented yet

513
0:23:50.844,000 --> 0:23:51,000
that I think we'll invent.

514
0:23:52.136,000 --> 0:23:53,000
We have no gravity shields.

515
0:23:53.466,000 --> 0:23:55,000
We can't beam ourselves from one location to another.

516
0:23:56.043,000 --> 0:23:57,000
That sounds ridiculous,

517
0:23:57.218,000 --> 0:23:58,000
but about 200 years ago,

518
0:23:58.53,000 --> 0:24:,000
experts were of the opinion that flight wouldn't exist,

519
0:24:01.221,000 --> 0:24:02,000
even 120 years ago,

520
0:24:02.569,000 --> 0:24:04,000
and if you moved faster than you could run,

521
0:24:05.175,000 --> 0:24:06,000
you would instantly die.

522
0:24:06.719,000 --> 0:24:09,000
So who says we are correct today that you can't beam a person

523
0:24:10.312,000 --> 0:24:12,000
from here to Mars?

524
0:24:12.585,000 --> 0:24:13,000
CA: Sebastian, thank you so much

525
0:24:14.178,000 --> 0:24:16,000
for your incredibly inspiring vision and your brilliance.

526
0:24:16.884,000 --> 0:24:17,000
Thank you, Sebastian Thrun.

527
0:24:18.231,000 --> 0:24:19,000
That was fantastic. (Applause)

