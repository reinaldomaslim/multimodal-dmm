1
0:00:,000 --> 0:00:07,000
Traducteur: Benjamin Tchassama-Lacroix Relecteur: Jules Daunay

2
0:00:13.381,000 --> 0:00:17,000
J'aide les ordinateurs à communiquer avec le monde qui nous entoure.

3
0:00:17.754,000 --> 0:00:18,000
Il y a plein de façons de le faire,

4
0:00:19.571,000 --> 0:00:21,000
mais j'aime me consacrer à aider les ordinateurs

5
0:00:22.187,000 --> 0:00:24,000
à parler de ce qu'ils voient et de ce qu'ils comprennent.

6
0:00:25.514,000 --> 0:00:26,000
Pour une scène comme celle-ci,

7
0:00:27.109,000 --> 0:00:28,000
un algorithme moderne de reconnaissance

8
0:00:29.038,000 --> 0:00:32,000
peut vous dire qu'il y a une femme et un chien.

9
0:00:32.157,000 --> 0:00:34,000
Il peut vous dire que la femme est souriante.

10
0:00:34.887,000 --> 0:00:37,000
Il pourrait même vous dire que le chien est incroyablement mignon.

11
0:00:38.784,000 --> 0:00:39,000
Je travaille sur ce problème

12
0:00:40.157,000 --> 0:00:44,000
en étudiant la façon dont les humains comprennent et traitent le monde.

13
0:00:45.577,000 --> 0:00:47,000
Les pensées, les souvenirs et les histoires

14
0:00:48.553,000 --> 0:00:5,000
qu'une scène comme celle-ci pourrait évoquer pour un humain.

15
0:00:51.395,000 --> 0:00:55,000
Toutes les interconnexions avec les situations proches.

16
0:00:55.704,000 --> 0:00:58,000
Peut-être avez-vous déjà vu un chien semblable auparavant,

17
0:00:58.854,000 --> 0:01:,000
ou que vous avez beaucoup couru sur une plage comme celle-ci

18
0:01:01.847,000 --> 0:01:05,000
et cela évoque d'autres pensées et des souvenirs de vacances passées,

19
0:01:06.649,000 --> 0:01:07,000
des moments passés à la plage,

20
0:01:08.593,000 --> 0:01:1,000
des moments passés à courir avec d'autres chiens.

21
0:01:11.688,000 --> 0:01:16,000
Un de mes principes est d'aider les ordinateurs à comprendre

22
0:01:16.919,000 --> 0:01:18,000
à quoi ressemblent de telles expériences,

23
0:01:19.839,000 --> 0:01:24,000
pour leur faire comprendre ce que nous partageons, croyons et ressentons.

24
0:01:26.094,000 --> 0:01:3,000
C'est ainsi qu'on pourra commencer à faire évoluer la technologie informatique

25
0:01:30.428,000 --> 0:01:34,000
pour qu'elle soit complémentaire de nos propres expériences.

26
0:01:35.539,000 --> 0:01:38,000
Donc, pour approfondir ce sujet,

27
0:01:38.95,000 --> 0:01:43,000
ces dernières années, j'ai aidé les ordinateurs à créer des histoires

28
0:01:44.879,000 --> 0:01:45,000
à partir de séquences d'images.

29
0:01:47.427,000 --> 0:01:48,000
Un jour,

30
0:01:49.355,000 --> 0:01:53,000
j'étais sur mon PC et je lui ai demandé son avis sur un voyage en Australie.

31
0:01:54.768,000 --> 0:01:56,000
Il a regardé les images, et il a vu un koala.

32
0:01:58.236,000 --> 0:01:59,000
Il ne savait pas ce que c'était,

33
0:01:59.903,000 --> 0:02:01,000
mais, selon lui, cette créature avait l'air intéressante.

34
0:02:04.053,000 --> 0:02:08,000
Puis, j'ai partagé avec lui une séquence d'images d'une maison qui brûlait.

35
0:02:09.704,000 --> 0:02:12,000
Il a regardé les images et il a dit :

36
0:02:13.013,000 --> 0:02:16,000
« C'est une vue incroyable ! C'est spectaculaire ! »

37
0:02:17.45,000 --> 0:02:19,000
Et ça m'a donné des frissons.

38
0:02:20.983,000 --> 0:02:24,000
Il a vu un événement horrible, qui peut bouleverser et détruire une vie,

39
0:02:25.579,000 --> 0:02:27,000
et il pensait que c'était une chose positive.

40
0:02:27.985,000 --> 0:02:3,000
J'ai compris qu'il discernait le contraste,

41
0:02:31.45,000 --> 0:02:33,000
les rouges, les jaunes,

42
0:02:34.173,000 --> 0:02:37,000
et il trouvait, sans mauvaise pensée, que ça méritait d'être vu.

43
0:02:37.928,000 --> 0:02:38,000
S'il disait ça, c'était en partie

44
0:02:39.577,000 --> 0:02:41,000
parce que la plupart des images que je lui avais apportées

45
0:02:42.546,000 --> 0:02:43,000
étaient des images positives.

46
0:02:44.903,000 --> 0:02:47,000
C'est parce que les gens ont tendance à partager des images positives

47
0:02:48.585,000 --> 0:02:5,000
quand ils parlent de leurs expériences.

48
0:02:51.267,000 --> 0:02:53,000
Se prend-on en portrait avec son portable à un enterrement ?

49
0:02:55.434,000 --> 0:02:58,000
J'ai compris que, pendant que j'améliorais l'IA,

50
0:02:58.553,000 --> 0:03:01,000
fonction par fonction, jeu de données par jeu de données,

51
0:03:02.291,000 --> 0:03:04,000
je créais des lacunes massives,

52
0:03:05.212,000 --> 0:03:08,000
des trous et des biais cognitifs dans ce que l'IA pouvait comprendre.

53
0:03:10.307,000 --> 0:03:11,000
Et, en faisant cela,

54
0:03:11.665,000 --> 0:03:13,000
je codais toute sorte de préjugés.

55
0:03:15.029,000 --> 0:03:18,000
Des préjugés qui reflètent un point de vue limité,

56
0:03:18.371,000 --> 0:03:2,000
limité à un seul jeu de données.

57
0:03:21.283,000 --> 0:03:24,000
Des partis pris qui reflètent des biais humains, retrouvés dans les données,

58
0:03:25.165,000 --> 0:03:28,000
comme les préjugés et les stéréotypes.

59
0:03:29.554,000 --> 0:03:32,000
Je repensais à l'évolution technologique

60
0:03:32.635,000 --> 0:03:34,000
qui m'a amenée là où j'en suis aujourd'hui.

61
0:03:35.966,000 --> 0:03:37,000
Que les premières images en couleur

62
0:03:38.223,000 --> 0:03:41,000
ont été calibrées sur la peau d'une femme blanche,

63
0:03:41.665,000 --> 0:03:45,000
les photographies en couleur étaient donc biaisées pour les visages noirs.

64
0:03:46.514,000 --> 0:03:48,000
Et ces mêmes préjugés, ces mêmes biais cognitifs

65
0:03:49.463,000 --> 0:03:5,000
se sont poursuivis dans les années 90.

66
0:03:51.701,000 --> 0:03:54,000
Ces biais cognitifs sont encore là aujourd'hui

67
0:03:54.879,000 --> 0:03:57,000
dans notre façon de reconnaître les visages de gens différents

68
0:03:58.601,000 --> 0:04:,000
grâce à la reconnaissance faciale.

69
0:04:01.323,000 --> 0:04:04,000
J'ai réfléchi à l'état de l'art de la recherche actuelle,

70
0:04:04.49,000 --> 0:04:08,000
qui a tendance à limiter la réflexion à un seul jeu de données et à un seul problème.

71
0:04:09.688,000 --> 0:04:13,000
Ce faisant, on a créé encore plus de biais cognitifs et de préjugés

72
0:04:14.593,000 --> 0:04:16,000
que l'IA pourrait encore amplifier.

73
0:04:17.712,000 --> 0:04:19,000
J'ai alors compris qu'il fallait imaginer

74
0:04:19.815,000 --> 0:04:24,000
le devenir de la technologie actuelle dans cinq ou dix ans.

75
0:04:25.99,000 --> 0:04:28,000
L'être humain évolue lentement, il prend le temps de corriger

76
0:04:29.156,000 --> 0:04:32,000
ses interactions avec son environnement.

77
0:04:33.276,000 --> 0:04:38,000
En revanche, l'intelligence artificielle évolue à un rythme incroyablement rapide.

78
0:04:39.013,000 --> 0:04:4,000
Par conséquent, c'est important

79
0:04:40.81,000 --> 0:04:42,000
de réfléchir soigneusement à ça dès maintenant –

80
0:04:44.18,000 --> 0:04:47,000
à nos propres biais cognitifs,

81
0:04:47.212,000 --> 0:04:49,000
nos préjugés,

82
0:04:49.553,000 --> 0:04:52,000
et comment ils influencent la technologie en création –

83
0:04:53.434,000 --> 0:04:56,000
et de discuter du sens de la technologie pour le futur.

84
0:04:58.593,000 --> 0:05:01,000
Des PDG et des scientifiques se sont prononcés

85
0:05:01.808,000 --> 0:05:04,000
sur le futur de l'intelligence artificielle de demain.

86
0:05:05.157,000 --> 0:05:06,000
Stephen Hawking nous alerte :

87
0:05:06.799,000 --> 0:05:09,000
« L'intelligence artificielle pourrait détruire l'humanité. »

88
0:05:10.307,000 --> 0:05:12,000
Elon Musk nous dit que c'est un risque existentiel,

89
0:05:13.014,000 --> 0:05:16,000
un des plus grands que notre civilisation devra affronter.

90
0:05:17.665,000 --> 0:05:18,000
Bill Gates a fait remarquer :

91
0:05:19.141,000 --> 0:05:22,000
« Je ne sais pas pourquoi les gens ne sont pas plus inquiets. »

92
0:05:23.412,000 --> 0:05:24,000
Mais ces points de vue

93
0:05:25.618,000 --> 0:05:26,000
font partie d'une histoire.

94
0:05:28.079,000 --> 0:05:3,000
Les maths, les modèles,

95
0:05:30.523,000 --> 0:05:33,000
les bases même de l'intelligence artificielle

96
0:05:33.617,000 --> 0:05:36,000
sont des choses que nous pouvons tous aborder et améliorer.

97
0:05:36.776,000 --> 0:05:39,000
Il existe des logiciels libres pour l'apprentissage automatique

98
0:05:40.585,000 --> 0:05:41,000
auxquels chacun peut contribuer.

99
0:05:42.919,000 --> 0:05:45,000
Et au-delà de ça, nous pouvons partager notre expérience,

100
0:05:46.76,000 --> 0:05:49,000
nos expériences avec la technologie, comment elle nous touche

101
0:05:50.252,000 --> 0:05:51,000
et nous passionne.

102
0:05:52.251,000 --> 0:05:53,000
On peut parler de nos passions.

103
0:05:55.244,000 --> 0:05:57,000
On peut parler de nos hypothèses

104
0:05:57.299,000 --> 0:06:01,000
sur les aspects de la technologie qui pourraient devenir plus utiles

105
0:06:02.18,000 --> 0:06:04,000
ou bien poser plus de problèmes au fil du temps.

106
0:06:05.799,000 --> 0:06:09,000
Si on se met tous à ouvrir une discussion sur l'IA,

107
0:06:09.966,000 --> 0:06:1,000
avec un horizon de long terme,

108
0:06:13.093,000 --> 0:06:17,000
on contribuera à créer un débat et une prise de conscience générale

109
0:06:17.387,000 --> 0:06:19,000
sur ce qu'est l'IA aujourd'hui,

110
0:06:21.212,000 --> 0:06:23,000
ce qu'elle peut devenir

111
0:06:23.237,000 --> 0:06:24,000
et tout ce qu'on doit faire

112
0:06:25.046,000 --> 0:06:28,000
pour arriver à atteindre le meilleur résultat pour nous.

113
0:06:29.49,000 --> 0:06:32,000
C'est déjà le cas avec la technologie qu'on utilise aujourd'hui.

114
0:06:33.767,000 --> 0:06:36,000
On utilise des téléphones, des assistants personnels et des robots aspirateurs.

115
0:06:38.457,000 --> 0:06:39,000
Sont-ils nuisibles ?

116
0:06:40.268,000 --> 0:06:41,000
Parfois, peut-être.

117
0:06:42.664,000 --> 0:06:43,000
Sont-ils utiles ?

118
0:06:45.005,000 --> 0:06:46,000
Oui, ils le sont aussi.

119
0:06:48.236,000 --> 0:06:49,000
Et ils ne sont pas tous pareils.

120
0:06:50.489,000 --> 0:06:53,000
Et c'est là qu'on voit déjà une lueur positive dans le futur.

121
0:06:54.942,000 --> 0:06:57,000
Le futur avance grâce à tout ce qu'on construit aujourd'hui.

122
0:06:59.165,000 --> 0:07:01,000
On a mis en mouvement l'effet domino

123
0:07:01.831,000 --> 0:07:03,000
qui construit l'évolution de l'IA.

124
0:07:05.173,000 --> 0:07:07,000
C'est aujourd'hui qu'on façonne l'IA de demain.

125
0:07:08.566,000 --> 0:07:11,000
Une technologie qui nous immerge dans les réalités augmentées,

126
0:07:12.289,000 --> 0:07:14,000
ramenant à la vie des mondes passés.

127
0:07:15.844,000 --> 0:07:19,000
Une technologie qui aide les gens à partager leurs expériences

128
0:07:20.18,000 --> 0:07:22,000
quand ils ont du mal à communiquer.

129
0:07:23.323,000 --> 0:07:27,000
Une technologie basée sur la compréhension des univers visuels en flux continu,

130
0:07:27.879,000 --> 0:07:3,000
qui fait fonctionner les véhicules autonomes.

131
0:07:32.49,000 --> 0:07:35,000
Une technologique d'analyse d'images et de création de langages,

132
0:07:35.927,000 --> 0:07:39,000
qui évolue en une technologie pour aider les troubles de la vue,

133
0:07:40.014,000 --> 0:07:42,000
afin de pouvoir mieux accéder au monde visuel.

134
0:07:42.838,000 --> 0:07:45,000
On voit aussi comment la technologie peut créer des problèmes.

135
0:07:46.885,000 --> 0:07:47,000
On est maintenant

136
0:07:48.337,000 --> 0:07:51,000
capables d’analyser les caractéristiques physiques innées,

137
0:07:52.196,000 --> 0:07:55,000
comme la couleur de notre peau ou l'apparence du visage,

138
0:07:55.492,000 --> 0:07:58,000
pour déterminer nos probabilités de devenir un criminel ou un terroriste.

139
0:07:59.688,000 --> 0:08:01,000
On a des logiciels capables de traiter toutes les données

140
0:08:02.617,000 --> 0:08:04,000
même celles concernant notre genre ou notre couleur de peau,

141
0:08:05.537,000 --> 0:08:07,000
afin de déterminer si on peut ou non contracter un crédit.

142
0:08:09.494,000 --> 0:08:1,000
Tout ce qu'on voit

143
0:08:11.097,000 --> 0:08:14,000
n'est qu'un aperçu de l'évolution de l'intelligence artificielle.

144
0:08:15.763,000 --> 0:08:16,000
Parce que nous sommes maintenant

145
0:08:17.565,000 --> 0:08:19,000
à un stade précis de cette évolution.

146
0:08:20.69,000 --> 0:08:23,000
C'est-à-dire que ce qu'on fait maintenant aura des répercussions par la suite

147
0:08:24.516,000 --> 0:08:25,000
et dans le futur.

148
0:08:26.063,000 --> 0:08:29,000
Si on veut que l'IA évolue pour aider l'humanité,

149
0:08:30.038,000 --> 0:08:32,000
on doit définir des objectifs et des stratégies

150
0:08:32.863,000 --> 0:08:33,000
pour tracer aujourd'hui cette voie.

151
0:08:35.68,000 --> 0:08:38,000
Je voudrais voir une IA qui cohabite avec nous, les humains,

152
0:08:39.442,000 --> 0:08:41,000
avec nos cultures et notre environnement.

153
0:08:43.435,000 --> 0:08:47,000
Une technologie qui vient en aide à ceux qui ont des problèmes neurologiques

154
0:08:47.943,000 --> 0:08:48,000
ou tout autre handicap

155
0:08:49.688,000 --> 0:08:52,000
afin de garantir l'égalité des chances dans la vie pour tous.

156
0:08:54.097,000 --> 0:08:55,000
Une technologie fonctionnant

157
0:08:55.542,000 --> 0:08:58,000
quel que soit votre statut démographique ou votre couleur de peau.

158
0:09:00.383,000 --> 0:09:04,000
Aujourd'hui, je me focalise donc sur la technologie de demain,

159
0:09:05.149,000 --> 0:09:06,000
pour les dix ans à venir.

160
0:09:08.53,000 --> 0:09:1,000
L'IA peut devenir plein de choses différentes.

161
0:09:11.688,000 --> 0:09:12,000
Mais dans ce cas,

162
0:09:12.937,000 --> 0:09:15,000
ce ne sera pas un véhicule autonome sans destination.

163
0:09:16.884,000 --> 0:09:18,000
Ce sera une voiture qu'on pilote.

164
0:09:19.953,000 --> 0:09:22,000
On choisira quand accélérer et quand ralentir.

165
0:09:23.572,000 --> 0:09:25,000
On décidera de faire un virage ou non.

166
0:09:26.868,000 --> 0:09:29,000
On sélectionnera ce à quoi ressemblera l'IA dans le futur.

167
0:09:31.186,000 --> 0:09:32,000
Il y a beaucoup de choses

168
0:09:32.547,000 --> 0:09:34,000
que l'intelligence artificielle peut devenir.

169
0:09:36.064,000 --> 0:09:37,000
Elle a un énorme potentiel.

170
0:09:39.694,000 --> 0:09:4,000
Et c'est à nous, aujourd'hui,

171
0:09:41.45,000 --> 0:09:44,000
de trouver ce qu'on doit mettre en place

172
0:09:44.535,000 --> 0:09:47,000
pour s'assurer que les effets de l'intelligence artificielle

173
0:09:48.366,000 --> 0:09:51,000
soient bénéfiques pour nous tous.

174
0:09:51.456,000 --> 0:09:52,000
Merci.

175
0:09:52.63,000 --> 0:09:54,000
(Applaudissements)

