1
0:00:12.82,000 --> 0:00:16,000
Today I'm going to talk about technology and society.

2
0:00:18.86,000 --> 0:00:21,000
The Department of Transport estimated that last year

3
0:00:22.58,000 --> 0:00:26,000
35,000 people died from traffic crashes in the US alone.

4
0:00:27.86,000 --> 0:00:31,000
Worldwide, 1.2 million people die every year in traffic accidents.

5
0:00:33.58,000 --> 0:00:37,000
If there was a way we could eliminate 90 percent of those accidents,

6
0:00:37.7,000 --> 0:00:38,000
would you support it?

7
0:00:39.54,000 --> 0:00:4,000
Of course you would.

8
0:00:40.86,000 --> 0:00:43,000
This is what driverless car technology promises to achieve

9
0:00:44.54,000 --> 0:00:46,000
by eliminating the main source of accidents --

10
0:00:47.38,000 --> 0:00:48,000
human error.

11
0:00:49.74,000 --> 0:00:54,000
Now picture yourself in a driverless car in the year 2030,

12
0:00:55.18,000 --> 0:00:58,000
sitting back and watching this vintage TEDxCambridge video.

13
0:00:58.66,000 --> 0:01:,000
(Laughter)

14
0:01:01.34,000 --> 0:01:02,000
All of a sudden,

15
0:01:02.58,000 --> 0:01:05,000
the car experiences mechanical failure and is unable to stop.

16
0:01:07.18,000 --> 0:01:08,000
If the car continues,

17
0:01:09.54,000 --> 0:01:13,000
it will crash into a bunch of pedestrians crossing the street,

18
0:01:14.9,000 --> 0:01:16,000
but the car may swerve,

19
0:01:17.059,000 --> 0:01:18,000
hitting one bystander,

20
0:01:18.94,000 --> 0:01:2,000
killing them to save the pedestrians.

21
0:01:21.86,000 --> 0:01:23,000
What should the car do, and who should decide?

22
0:01:25.34,000 --> 0:01:28,000
What if instead the car could swerve into a wall,

23
0:01:28.9,000 --> 0:01:31,000
crashing and killing you, the passenger,

24
0:01:32.22,000 --> 0:01:34,000
in order to save those pedestrians?

25
0:01:35.06,000 --> 0:01:38,000
This scenario is inspired by the trolley problem,

26
0:01:38.78,000 --> 0:01:41,000
which was invented by philosophers a few decades ago

27
0:01:42.58,000 --> 0:01:43,000
to think about ethics.

28
0:01:45.94,000 --> 0:01:47,000
Now, the way we think about this problem matters.

29
0:01:48.46,000 --> 0:01:5,000
We may for example not think about it at all.

30
0:01:51.1,000 --> 0:01:54,000
We may say this scenario is unrealistic,

31
0:01:54.5,000 --> 0:01:56,000
incredibly unlikely, or just silly.

32
0:01:57.58,000 --> 0:01:59,000
But I think this criticism misses the point

33
0:02:00.34,000 --> 0:02:02,000
because it takes the scenario too literally.

34
0:02:03.74,000 --> 0:02:05,000
Of course no accident is going to look like this;

35
0:02:06.5,000 --> 0:02:09,000
no accident has two or three options

36
0:02:09.86,000 --> 0:02:11,000
where everybody dies somehow.

37
0:02:13.3,000 --> 0:02:15,000
Instead, the car is going to calculate something

38
0:02:15.9,000 --> 0:02:19,000
like the probability of hitting a certain group of people,

39
0:02:20.82,000 --> 0:02:23,000
if you swerve one direction versus another direction,

40
0:02:24.18,000 --> 0:02:27,000
you might slightly increase the risk to passengers or other drivers

41
0:02:27.66,000 --> 0:02:28,000
versus pedestrians.

42
0:02:29.22,000 --> 0:02:31,000
It's going to be a more complex calculation,

43
0:02:32.3,000 --> 0:02:34,000
but it's still going to involve trade-offs,

44
0:02:35.66,000 --> 0:02:37,000
and trade-offs often require ethics.

45
0:02:39.66,000 --> 0:02:41,000
We might say then, "Well, let's not worry about this.

46
0:02:42.42,000 --> 0:02:46,000
Let's wait until technology is fully ready and 100 percent safe."

47
0:02:48.34,000 --> 0:02:51,000
Suppose that we can indeed eliminate 90 percent of those accidents,

48
0:02:52.9,000 --> 0:02:54,000
or even 99 percent in the next 10 years.

49
0:02:56.74,000 --> 0:02:59,000
What if eliminating the last one percent of accidents

50
0:02:59.94,000 --> 0:03:02,000
requires 50 more years of research?

51
0:03:04.22,000 --> 0:03:05,000
Should we not adopt the technology?

52
0:03:06.54,000 --> 0:03:1,000
That's 60 million people dead in car accidents

53
0:03:11.34,000 --> 0:03:12,000
if we maintain the current rate.

54
0:03:14.58,000 --> 0:03:15,000
So the point is,

55
0:03:15.82,000 --> 0:03:18,000
waiting for full safety is also a choice,

56
0:03:19.46,000 --> 0:03:21,000
and it also involves trade-offs.

57
0:03:23.38,000 --> 0:03:27,000
People online on social media have been coming up with all sorts of ways

58
0:03:27.74,000 --> 0:03:29,000
to not think about this problem.

59
0:03:29.78,000 --> 0:03:32,000
One person suggested the car should just swerve somehow

60
0:03:33.02,000 --> 0:03:35,000
in between the passengers --

61
0:03:35.18,000 --> 0:03:36,000
(Laughter)

62
0:03:36.22,000 --> 0:03:37,000
and the bystander.

63
0:03:37.5,000 --> 0:03:4,000
Of course if that's what the car can do, that's what the car should do.

64
0:03:41.74,000 --> 0:03:43,000
We're interested in scenarios in which this is not possible.

65
0:03:45.1,000 --> 0:03:5,000
And my personal favorite was a suggestion by a blogger

66
0:03:50.54,000 --> 0:03:53,000
to have an eject button in the car that you press --

67
0:03:53.58,000 --> 0:03:54,000
(Laughter)

68
0:03:54.82,000 --> 0:03:55,000
just before the car self-destructs.

69
0:03:56.511,000 --> 0:03:57,000
(Laughter)

70
0:03:59.66,000 --> 0:04:04,000
So if we acknowledge that cars will have to make trade-offs on the road,

71
0:04:06.02,000 --> 0:04:07,000
how do we think about those trade-offs,

72
0:04:09.14,000 --> 0:04:1,000
and how do we decide?

73
0:04:10.74,000 --> 0:04:13,000
Well, maybe we should run a survey to find out what society wants,

74
0:04:13.9,000 --> 0:04:14,000
because ultimately,

75
0:04:15.38,000 --> 0:04:18,000
regulations and the law are a reflection of societal values.

76
0:04:19.86,000 --> 0:04:2,000
So this is what we did.

77
0:04:21.7,000 --> 0:04:22,000
With my collaborators,

78
0:04:23.34,000 --> 0:04:25,000
Jean-François Bonnefon and Azim Shariff,

79
0:04:25.7,000 --> 0:04:26,000
we ran a survey

80
0:04:27.34,000 --> 0:04:29,000
in which we presented people with these types of scenarios.

81
0:04:30.219,000 --> 0:04:33,000
We gave them two options inspired by two philosophers:

82
0:04:34.02,000 --> 0:04:36,000
Jeremy Bentham and Immanuel Kant.

83
0:04:37.42,000 --> 0:04:4,000
Bentham says the car should follow utilitarian ethics:

84
0:04:40.54,000 --> 0:04:43,000
it should take the action that will minimize total harm --

85
0:04:43.98,000 --> 0:04:45,000
even if that action will kill a bystander

86
0:04:46.82,000 --> 0:04:48,000
and even if that action will kill the passenger.

87
0:04:49.94,000 --> 0:04:53,000
Immanuel Kant says the car should follow duty-bound principles,

88
0:04:54.94,000 --> 0:04:55,000
like "Thou shalt not kill."

89
0:04:57.3,000 --> 0:05:01,000
So you should not take an action that explicitly harms a human being,

90
0:05:01.78,000 --> 0:05:03,000
and you should let the car take its course

91
0:05:04.26,000 --> 0:05:05,000
even if that's going to harm more people.

92
0:05:07.46,000 --> 0:05:08,000
What do you think?

93
0:05:09.18,000 --> 0:05:1,000
Bentham or Kant?

94
0:05:11.58,000 --> 0:05:12,000
Here's what we found.

95
0:05:12.86,000 --> 0:05:13,000
Most people sided with Bentham.

96
0:05:15.98,000 --> 0:05:18,000
So it seems that people want cars to be utilitarian,

97
0:05:19.78,000 --> 0:05:2,000
minimize total harm,

98
0:05:21.22,000 --> 0:05:22,000
and that's what we should all do.

99
0:05:22.82,000 --> 0:05:23,000
Problem solved.

100
0:05:25.06,000 --> 0:05:26,000
But there is a little catch.

101
0:05:27.74,000 --> 0:05:3,000
When we asked people whether they would purchase such cars,

102
0:05:31.5,000 --> 0:05:32,000
they said, "Absolutely not."

103
0:05:33.14,000 --> 0:05:35,000
(Laughter)

104
0:05:35.46,000 --> 0:05:38,000
They would like to buy cars that protect them at all costs,

105
0:05:39.38,000 --> 0:05:42,000
but they want everybody else to buy cars that minimize harm.

106
0:05:43.02,000 --> 0:05:45,000
(Laughter)

107
0:05:46.54,000 --> 0:05:47,000
We've seen this problem before.

108
0:05:48.42,000 --> 0:05:49,000
It's called a social dilemma.

109
0:05:50.98,000 --> 0:05:51,000
And to understand the social dilemma,

110
0:05:52.82,000 --> 0:05:54,000
we have to go a little bit back in history.

111
0:05:55.82,000 --> 0:05:57,000
In the 1800s,

112
0:05:58.42,000 --> 0:06:01,000
English economist William Forster Lloyd published a pamphlet

113
0:06:02.18,000 --> 0:06:04,000
which describes the following scenario.

114
0:06:04.42,000 --> 0:06:05,000
You have a group of farmers --

115
0:06:06.1,000 --> 0:06:07,000
English farmers --

116
0:06:07.46,000 --> 0:06:09,000
who are sharing a common land for their sheep to graze.

117
0:06:11.34,000 --> 0:06:13,000
Now, if each farmer brings a certain number of sheep --

118
0:06:13.94,000 --> 0:06:14,000
let's say three sheep --

119
0:06:15.46,000 --> 0:06:17,000
the land will be rejuvenated,

120
0:06:17.58,000 --> 0:06:18,000
the farmers are happy,

121
0:06:18.82,000 --> 0:06:19,000
the sheep are happy,

122
0:06:20.46,000 --> 0:06:21,000
everything is good.

123
0:06:22.26,000 --> 0:06:24,000
Now, if one farmer brings one extra sheep,

124
0:06:25.62,000 --> 0:06:29,000
that farmer will do slightly better, and no one else will be harmed.

125
0:06:30.98,000 --> 0:06:33,000
But if every farmer made that individually rational decision,

126
0:06:35.66,000 --> 0:06:37,000
the land will be overrun, and it will be depleted

127
0:06:39.18,000 --> 0:06:41,000
to the detriment of all the farmers,

128
0:06:41.38,000 --> 0:06:43,000
and of course, to the detriment of the sheep.

129
0:06:44.54,000 --> 0:06:47,000
We see this problem in many places:

130
0:06:48.9,000 --> 0:06:51,000
in the difficulty of managing overfishing,

131
0:06:52.1,000 --> 0:06:56,000
or in reducing carbon emissions to mitigate climate change.

132
0:06:58.98,000 --> 0:07:,000
When it comes to the regulation of driverless cars,

133
0:07:02.9,000 --> 0:07:06,000
the common land now is basically public safety --

134
0:07:07.26,000 --> 0:07:08,000
that's the common good --

135
0:07:09.22,000 --> 0:07:1,000
and the farmers are the passengers

136
0:07:11.22,000 --> 0:07:14,000
or the car owners who are choosing to ride in those cars.

137
0:07:16.78,000 --> 0:07:18,000
And by making the individually rational choice

138
0:07:19.42,000 --> 0:07:21,000
of prioritizing their own safety,

139
0:07:22.26,000 --> 0:07:25,000
they may collectively be diminishing the common good,

140
0:07:25.42,000 --> 0:07:27,000
which is minimizing total harm.

141
0:07:30.14,000 --> 0:07:32,000
It's called the tragedy of the commons,

142
0:07:32.3,000 --> 0:07:33,000
traditionally,

143
0:07:33.62,000 --> 0:07:36,000
but I think in the case of driverless cars,

144
0:07:36.74,000 --> 0:07:38,000
the problem may be a little bit more insidious

145
0:07:39.62,000 --> 0:07:42,000
because there is not necessarily an individual human being

146
0:07:43.14,000 --> 0:07:44,000
making those decisions.

147
0:07:44.86,000 --> 0:07:47,000
So car manufacturers may simply program cars

148
0:07:48.18,000 --> 0:07:5,000
that will maximize safety for their clients,

149
0:07:51.9,000 --> 0:07:53,000
and those cars may learn automatically on their own

150
0:07:54.9,000 --> 0:07:57,000
that doing so requires slightly increasing risk for pedestrians.

151
0:07:59.34,000 --> 0:08:,000
So to use the sheep metaphor,

152
0:08:00.78,000 --> 0:08:03,000
it's like we now have electric sheep that have a mind of their own.

153
0:08:04.42,000 --> 0:08:05,000
(Laughter)

154
0:08:05.9,000 --> 0:08:08,000
And they may go and graze even if the farmer doesn't know it.

155
0:08:10.46,000 --> 0:08:13,000
So this is what we may call the tragedy of the algorithmic commons,

156
0:08:14.46,000 --> 0:08:16,000
and if offers new types of challenges.

157
0:08:22.34,000 --> 0:08:23,000
Typically, traditionally,

158
0:08:24.26,000 --> 0:08:27,000
we solve these types of social dilemmas using regulation,

159
0:08:27.62,000 --> 0:08:29,000
so either governments or communities get together,

160
0:08:30.38,000 --> 0:08:33,000
and they decide collectively what kind of outcome they want

161
0:08:34.14,000 --> 0:08:36,000
and what sort of constraints on individual behavior

162
0:08:36.82,000 --> 0:08:37,000
they need to implement.

163
0:08:39.42,000 --> 0:08:41,000
And then using monitoring and enforcement,

164
0:08:42.06,000 --> 0:08:44,000
they can make sure that the public good is preserved.

165
0:08:45.26,000 --> 0:08:46,000
So why don't we just,

166
0:08:46.859,000 --> 0:08:47,000
as regulators,

167
0:08:48.379,000 --> 0:08:5,000
require that all cars minimize harm?

168
0:08:51.3,000 --> 0:08:53,000
After all, this is what people say they want.

169
0:08:55.02,000 --> 0:08:56,000
And more importantly,

170
0:08:56.46,000 --> 0:08:59,000
I can be sure that as an individual,

171
0:08:59.58,000 --> 0:09:02,000
if I buy a car that may sacrifice me in a very rare case,

172
0:09:03.46,000 --> 0:09:04,000
I'm not the only sucker doing that

173
0:09:05.14,000 --> 0:09:07,000
while everybody else enjoys unconditional protection.

174
0:09:08.94,000 --> 0:09:11,000
In our survey, we did ask people whether they would support regulation

175
0:09:12.3,000 --> 0:09:13,000
and here's what we found.

176
0:09:14.18,000 --> 0:09:17,000
First of all, people said no to regulation;

177
0:09:19.1,000 --> 0:09:2,000
and second, they said,

178
0:09:20.38,000 --> 0:09:23,000
"Well if you regulate cars to do this and to minimize total harm,

179
0:09:24.34,000 --> 0:09:25,000
I will not buy those cars."

180
0:09:27.22,000 --> 0:09:28,000
So ironically,

181
0:09:28.62,000 --> 0:09:31,000
by regulating cars to minimize harm,

182
0:09:32.14,000 --> 0:09:33,000
we may actually end up with more harm

183
0:09:34.86,000 --> 0:09:37,000
because people may not opt into the safer technology

184
0:09:38.54,000 --> 0:09:4,000
even if it's much safer than human drivers.

185
0:09:42.18,000 --> 0:09:45,000
I don't have the final answer to this riddle,

186
0:09:45.62,000 --> 0:09:46,000
but I think as a starting point,

187
0:09:47.22,000 --> 0:09:5,000
we need society to come together

188
0:09:50.54,000 --> 0:09:52,000
to decide what trade-offs we are comfortable with

189
0:09:54.18,000 --> 0:09:57,000
and to come up with ways in which we can enforce those trade-offs.

190
0:09:58.34,000 --> 0:10:,000
As a starting point, my brilliant students,

191
0:10:00.9,000 --> 0:10:02,000
Edmond Awad and Sohan Dsouza,

192
0:10:03.38,000 --> 0:10:04,000
built the Moral Machine website,

193
0:10:06.02,000 --> 0:10:08,000
which generates random scenarios at you --

194
0:10:09.9,000 --> 0:10:11,000
basically a bunch of random dilemmas in a sequence

195
0:10:12.38,000 --> 0:10:15,000
where you have to choose what the car should do in a given scenario.

196
0:10:16.86,000 --> 0:10:2,000
And we vary the ages and even the species of the different victims.

197
0:10:22.86,000 --> 0:10:25,000
So far we've collected over five million decisions

198
0:10:26.58,000 --> 0:10:28,000
by over one million people worldwide

199
0:10:30.22,000 --> 0:10:31,000
from the website.

200
0:10:32.18,000 --> 0:10:34,000
And this is helping us form an early picture

201
0:10:34.62,000 --> 0:10:36,000
of what trade-offs people are comfortable with

202
0:10:37.26,000 --> 0:10:38,000
and what matters to them --

203
0:10:39.18,000 --> 0:10:4,000
even across cultures.

204
0:10:42.06,000 --> 0:10:43,000
But more importantly,

205
0:10:43.58,000 --> 0:10:46,000
doing this exercise is helping people recognize

206
0:10:46.98,000 --> 0:10:48,000
the difficulty of making those choices

207
0:10:49.82,000 --> 0:10:52,000
and that the regulators are tasked with impossible choices.

208
0:10:55.18,000 --> 0:10:58,000
And maybe this will help us as a society understand the kinds of trade-offs

209
0:10:58.78,000 --> 0:11:01,000
that will be implemented ultimately in regulation.

210
0:11:01.86,000 --> 0:11:02,000
And indeed, I was very happy to hear

211
0:11:03.62,000 --> 0:11:05,000
that the first set of regulations

212
0:11:05.66,000 --> 0:11:07,000
that came from the Department of Transport --

213
0:11:07.82,000 --> 0:11:08,000
announced last week --

214
0:11:09.22,000 --> 0:11:15,000
included a 15-point checklist for all carmakers to provide,

215
0:11:15.82,000 --> 0:11:18,000
and number 14 was ethical consideration --

216
0:11:19.1,000 --> 0:11:2,000
how are you going to deal with that.

217
0:11:23.62,000 --> 0:11:25,000
We also have people reflect on their own decisions

218
0:11:26.3,000 --> 0:11:29,000
by giving them summaries of what they chose.

219
0:11:30.26,000 --> 0:11:31,000
I'll give you one example --

220
0:11:31.94,000 --> 0:11:34,000
I'm just going to warn you that this is not your typical example,

221
0:11:35.5,000 --> 0:11:36,000
your typical user.

222
0:11:36.9,000 --> 0:11:39,000
This is the most sacrificed and the most saved character for this person.

223
0:11:40.54,000 --> 0:11:45,000
(Laughter)

224
0:11:46.5,000 --> 0:11:47,000
Some of you may agree with him,

225
0:11:48.42,000 --> 0:11:49,000
or her, we don't know.

226
0:11:52.3,000 --> 0:11:58,000
But this person also seems to slightly prefer passengers over pedestrians

227
0:11:58.46,000 --> 0:12:,000
in their choices

228
0:12:00.58,000 --> 0:12:02,000
and is very happy to punish jaywalking.

229
0:12:03.42,000 --> 0:12:06,000
(Laughter)

230
0:12:09.14,000 --> 0:12:1,000
So let's wrap up.

231
0:12:10.379,000 --> 0:12:13,000
We started with the question -- let's call it the ethical dilemma --

232
0:12:13.82,000 --> 0:12:16,000
of what the car should do in a specific scenario:

233
0:12:16.9,000 --> 0:12:17,000
swerve or stay?

234
0:12:19.06,000 --> 0:12:21,000
But then we realized that the problem was a different one.

235
0:12:21.82,000 --> 0:12:25,000
It was the problem of how to get society to agree on and enforce

236
0:12:26.38,000 --> 0:12:27,000
the trade-offs they're comfortable with.

237
0:12:28.34,000 --> 0:12:29,000
It's a social dilemma.

238
0:12:29.62,000 --> 0:12:34,000
In the 1940s, Isaac Asimov wrote his famous laws of robotics --

239
0:12:34.66,000 --> 0:12:35,000
the three laws of robotics.

240
0:12:37.06,000 --> 0:12:39,000
A robot may not harm a human being,

241
0:12:39.54,000 --> 0:12:41,000
a robot may not disobey a human being,

242
0:12:42.1,000 --> 0:12:45,000
and a robot may not allow itself to come to harm --

243
0:12:45.38,000 --> 0:12:46,000
in this order of importance.

244
0:12:48.18,000 --> 0:12:5,000
But after 40 years or so

245
0:12:50.34,000 --> 0:12:53,000
and after so many stories pushing these laws to the limit,

246
0:12:54.1,000 --> 0:12:57,000
Asimov introduced the zeroth law

247
0:12:57.82,000 --> 0:12:59,000
which takes precedence above all,

248
0:13:00.1,000 --> 0:13:03,000
and it's that a robot may not harm humanity as a whole.

249
0:13:04.3,000 --> 0:13:08,000
I don't know what this means in the context of driverless cars

250
0:13:08.7,000 --> 0:13:1,000
or any specific situation,

251
0:13:11.46,000 --> 0:13:13,000
and I don't know how we can implement it,

252
0:13:13.7,000 --> 0:13:14,000
but I think that by recognizing

253
0:13:15.26,000 --> 0:13:21,000
that the regulation of driverless cars is not only a technological problem

254
0:13:21.42,000 --> 0:13:24,000
but also a societal cooperation problem,

255
0:13:25.62,000 --> 0:13:27,000
I hope that we can at least begin to ask the right questions.

256
0:13:29.02,000 --> 0:13:3,000
Thank you.

257
0:13:30.26,000 --> 0:13:32,000
(Applause)

