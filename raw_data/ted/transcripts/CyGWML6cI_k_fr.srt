1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Mirjana Marinkovic

2
0:00:13.373,000 --> 0:00:14,000
Est-ce que c'est juste moi

3
0:00:15.141,000 --> 0:00:17,000
ou y a-t-il d'autres gens ici

4
0:00:17.497,000 --> 0:00:19,000
qui sont un peu déçus par la démocratie?

5
0:00:20.986,000 --> 0:00:22,000
(Applaudissements)

6
0:00:24.141,000 --> 0:00:26,000
Considérons quelques chiffres.

7
0:00:26.934,000 --> 0:00:28,000
Si nous regardons à travers le monde,

8
0:00:29.131,000 --> 0:00:32,000
le taux moyen de participation aux élections présidentielles

9
0:00:33.047,000 --> 0:00:34,000
au cours des 30 dernières années

10
0:00:34.698,000 --> 0:00:36,000
s'élève seulement à 67%.

11
0:00:38.329,000 --> 0:00:39,000
Si nous allons en Europe

12
0:00:40.326,000 --> 0:00:44,000
et considérons les gens ayant participé aux élections parlementaires de l'UE,

13
0:00:44.778,000 --> 0:00:46,000
le taux moyen de participation à ces élections

14
0:00:47.019,000 --> 0:00:48,000
est seulement de 42%.

15
0:00:50.125,000 --> 0:00:51,000
Allons à présent à New York

16
0:00:51.818,000 --> 0:00:55,000
et regardons combien de personnes ont voté dans les dernières élections municipales.

17
0:00:56.523,000 --> 0:00:59,000
Nous découvrons que seulement 24% sont allés voter.

18
0:01:01.063,000 --> 0:01:04,000
Cela veut dire que si la série « Friends » était encore diffusée,

19
0:01:04.182,000 --> 0:01:07,000
Joey et peut-être Phoebe seraient allés voter.

20
0:01:07.554,000 --> 0:01:08,000
(Rires)

21
0:01:09.434,000 --> 0:01:13,000
Vous ne pouvez pas leur en vouloir car les gens en ont marre des politiciens.

22
0:01:13.884,000 --> 0:01:16,000
Les gens en ont marre des autres gens utilisant les données qu'ils ont générées

23
0:01:17.795,000 --> 0:01:19,000
en communiquant avec leurs amis et leur famille,

24
0:01:20.033,000 --> 0:01:22,000
pour en faire des cibles de propagande politique.

25
0:01:22.519,000 --> 0:01:24,000
Le fait est que cela n'est pas nouveau.

26
0:01:25.271,000 --> 0:01:28,000
On utilise les « j'aime » pour vous cibler.

27
0:01:28.52,000 --> 0:01:31,000
Avant, c'était votre code postal, votre sexe ou votre âge,

28
0:01:31.917,000 --> 0:01:34,000
car l'idée de faire des gens des cibles de propagande à des fins politiques

29
0:01:35.511,000 --> 0:01:36,000
est aussi vieille que la politique.

30
0:01:37.53,000 --> 0:01:39,000
Cette idée existe bien, car

31
0:01:39.832,000 --> 0:01:42,000
la démocratie a une vulnérabilité fondamentale.

32
0:01:43.71,000 --> 0:01:44,000
Voici le schéma d'une élection.

33
0:01:46.049,000 --> 0:01:49,000
En principe, la démocratie est la capacité du peuple à exercer le pouvoir.

34
0:01:50.017,000 --> 0:01:53,000
Mais en pratique, nous devons déléguer ce pouvoir à un représentant

35
0:01:53.859,000 --> 0:01:55,000
qui peut exercer ce pouvoir pour nous.

36
0:01:56.561,000 --> 0:01:58,000
Ce représentant est le goulot d'étranglement

37
0:01:58.606,000 --> 0:01:59,000
ou un point faible.

38
0:01:59.731,000 --> 0:02:02,000
C'est lui que vous voulez cibler si vous voulez attaquer la démocratie,

39
0:02:03.704,000 --> 0:02:06,000
car vous pouvez vous emparer de la démocratie via ce représentant

40
0:02:07.217,000 --> 0:02:09,000
ou via la façon dont les gens le choisissent.

41
0:02:10.065,000 --> 0:02:11,000
La question est :

42
0:02:11.505,000 --> 0:02:12,000
Est-ce la fin de l'histoire ?

43
0:02:13.989,000 --> 0:02:16,000
Est-ce le mieux que nous puissions faire

44
0:02:17.878,000 --> 0:02:2,000
ou y a-t-il des alternatives ?

45
0:02:22.13,000 --> 0:02:24,000
Certains réfléchissent à des alternatives

46
0:02:24.508,000 --> 0:02:27,000
et l'une des idées énoncées est la démocratie directe.

47
0:02:28.79,000 --> 0:02:32,000
Elle consiste à shunter les politiciens et faire voter les gens directement

48
0:02:32.82,000 --> 0:02:35,000
sur des problèmes et des projets de loi.

49
0:02:36.415,000 --> 0:02:37,000
Mais cette idée est naïve,

50
0:02:37.775,000 --> 0:02:4,000
car nous serions obligés de choisir trop de choses.

51
0:02:40.97,000 --> 0:02:42,000
Si vous considérez le 114e congrès des États-Unis,

52
0:02:43.776,000 --> 0:02:45,000
vous verrez que la Chambre des représentants

53
0:02:46.287,000 --> 0:02:48,000
a examiné plus de 6 000 projets de loi,

54
0:02:49.2,000 --> 0:02:51,000
le Sénat a examiné plus de 3 000 projets de loi

55
0:02:51.88,000 --> 0:02:53,000
et ils ont approuvé plus de 300 lois.

56
0:02:54.712,000 --> 0:02:55,000
Cela obligerait chacun à prendre

57
0:02:56.285,000 --> 0:02:58,000
de nombreuses décisions chaque semaine

58
0:02:58.622,000 --> 0:03:,000
sur des sujets sur lesquels il a peu de connaissances.

59
0:03:01.248,000 --> 0:03:03,000
Il y a un problème de bande passante cognitive

60
0:03:03.534,000 --> 0:03:06,000
si nous essayons de voir la démocratie directe comme une alternative viable.

61
0:03:08.205,000 --> 0:03:12,000
Certains imaginent une démocratie liquide ou fluide,

62
0:03:12.664,000 --> 0:03:15,000
dans laquelle vous donnez votre pouvoir politique à quelqu'un,

63
0:03:16.404,000 --> 0:03:17,000
qui peut le donner à quelqu'un d'autre

64
0:03:18.214,000 --> 0:03:2,000
et, finalement, vous créez un grand réseau de suiveurs

65
0:03:20.753,000 --> 0:03:23,000
où seulement quelques personnes prennent les décisions

66
0:03:24.071,000 --> 0:03:27,000
au nom de tous leurs suiveurs et des suiveurs de leurs suiveurs.

67
0:03:28.326,000 --> 0:03:32,000
Cette idée ne résout pas le problème de la bande passante cognitive

68
0:03:32.479,000 --> 0:03:35,000
et, pour être honnête, est similaire au fait d'avoir un représentant.

69
0:03:36.795,000 --> 0:03:39,000
Aujourd'hui, je vais être un peu provocateur

70
0:03:40.277,000 --> 0:03:42,000
et je vais vous demander :

71
0:03:42.601,000 --> 0:03:48,000
« Et si, au lieu d'essayer de shunter les politiciens,

72
0:03:49.187,000 --> 0:03:51,000
nous essayions de les automatiser ? »

73
0:03:57.871,000 --> 0:03:59,000
L'idée de l'automatisation n'est pas nouvelle.

74
0:04:00.821,000 --> 0:04:02,000
Elle a commencé il y a 300 ans,

75
0:04:02.925,000 --> 0:04:05,000
quand les tisserands français ont décidé d'automatiser le métier à tisser.

76
0:04:06.82,000 --> 0:04:1,000
Le vainqueur de cette guerre industrielle fut Joseph-Marie Jacquard.

77
0:04:11.204,000 --> 0:04:12,000
C'était un tisserand et marchand français

78
0:04:13.165,000 --> 0:04:15,000
qui a marié le métier à tisser et la machine à vapeur

79
0:04:15.639,000 --> 0:04:17,000
pour créer des métiers à tisser autonomes.

80
0:04:17.657,000 --> 0:04:19,000
Avec ces métiers à tisser, il a gagné en contrôle.

81
0:04:20.434,000 --> 0:04:23,000
Il pouvait fabriquer des tissus plus complexes et sophistiqués

82
0:04:24.343,000 --> 0:04:26,000
que ceux qui étaient faits à la main.

83
0:04:27.193,000 --> 0:04:29,000
En gagnant cette guerre industrielle,

84
0:04:29.849,000 --> 0:04:32,000
il a installé ce qui est devenu le schéma directeur de l'automatisation.

85
0:04:34.135,000 --> 0:04:36,000
Notre façon d'automatiser les choses ces 300 dernières années

86
0:04:37.029,000 --> 0:04:38,000
n'a jamais changé :

87
0:04:39.006,000 --> 0:04:41,000
nous identifions d'abord un besoin,

88
0:04:41.539,000 --> 0:04:44,000
puis nous créons un outil pour satisfaire ce besoin,

89
0:04:44.747,000 --> 0:04:46,000
comme le métier à tisser,

90
0:04:46.811,000 --> 0:04:48,000
puis nous étudions comment les gens utilisent l'outil

91
0:04:49.312,000 --> 0:04:5,000
pour automatiser cet utilisateur.

92
0:04:51.242,000 --> 0:04:54,000
C'est ainsi que nous sommes passés du métier à tisser mécanique

93
0:04:54.327,000 --> 0:04:55,000
au métier à tisser autonome

94
0:04:56.271,000 --> 0:04:58,000
et cela nous a pris mille ans.

95
0:04:58.391,000 --> 0:05:,000
Alors qu'il ne nous a fallu que cent ans

96
0:05:00.486,000 --> 0:05:03,000
pour utiliser ce même schéma afin d'automatiser la voiture.

97
0:05:05.286,000 --> 0:05:07,000
Cependant, cette fois,

98
0:05:07.762,000 --> 0:05:09,000
l'automatisation est réelle.

99
0:05:09.915,000 --> 0:05:12,000
Voici une vidéo qu'un de mes collègues de Toshiba a partagée avec moi

100
0:05:13.26,000 --> 0:05:16,000
et qui montre l'usine qui produit des disques SSD.

101
0:05:16.543,000 --> 0:05:18,000
L'usine entière est un robot.

102
0:05:18.585,000 --> 0:05:19,000
Il n'y a pas d'humain dans cette usine.

103
0:05:21.033,000 --> 0:05:23,000
Les robots vont bientôt quitter les usines

104
0:05:23.278,000 --> 0:05:25,000
et faire partie de notre monde,

105
0:05:25.324,000 --> 0:05:26,000
faire partie de notre main d’œuvre.

106
0:05:27.183,000 --> 0:05:28,000
Ce que je fais au quotidien,

107
0:05:29.004,000 --> 0:05:32,000
c'est de créer des outils qui intègrent des données de pays entiers

108
0:05:32.996,000 --> 0:05:35,000
afin que nous disposions des fondations nécessaires

109
0:05:36.486,000 --> 0:05:39,000
pour un futur où nous aurons besoin de gérer ces machines.

110
0:05:41.195,000 --> 0:05:43,000
Mais je ne suis pas là pour vous parler de ces outils

111
0:05:44.101,000 --> 0:05:45,000
qui intègrent les données des pays.

112
0:05:46.463,000 --> 0:05:48,000
Je suis ici pour vous parler d'une autre idée

113
0:05:49.109,000 --> 0:05:51,000
qui pourrait nous aider à réfléchir à comment utiliser

114
0:05:51.698,000 --> 0:05:53,000
l'intelligence artificielle dans la démocratie.

115
0:05:53.998,000 --> 0:05:57,000
Car les outils que je développe sont conçus pour des décisions exécutives.

116
0:05:58.755,000 --> 0:06:01,000
Ce sont des décisions pouvant être prises avec une certaine objectivité --

117
0:06:02.621,000 --> 0:06:03,000
des décisions d'investissement public.

118
0:06:04.885,000 --> 0:06:06,000
Mais il y a des décisions législatives

119
0:06:07.54,000 --> 0:06:1,000
et ces décisions nécessitent une communication entre des gens

120
0:06:11.351,000 --> 0:06:12,000
qui ont des points de vue différents,

121
0:06:13.171,000 --> 0:06:15,000
requièrent une participation, un débat,

122
0:06:15.712,000 --> 0:06:16,000
une délibération.

123
0:06:18.241,000 --> 0:06:2,000
Pendant longtemps, nous avons pensé

124
0:06:21.069,000 --> 0:06:24,000
que pour améliorer la démocratie, il fallait plus de communication.

125
0:06:24.553,000 --> 0:06:27,000
Les technologies que nous avons élaborées dans le contexte de la démocratie,

126
0:06:28.286,000 --> 0:06:3,000
que ce soient les journaux ou les réseaux sociaux,

127
0:06:31.088,000 --> 0:06:33,000
ont essayé de nous offrir plus de communication.

128
0:06:34.079,000 --> 0:06:35,000
Nous nous sommes aventurés sur ce terrain

129
0:06:36.035,000 --> 0:06:38,000
et nous savons que cela ne résoudra pas le problème.

130
0:06:38.721,000 --> 0:06:39,000
Ce n'est pas un problème de communication,

131
0:06:40.741,000 --> 0:06:41,000
mais de bande passante cognitive.

132
0:06:42.513,000 --> 0:06:44,000
S'il s'agit d'un problème de bande passante cognitive,

133
0:06:45.049,000 --> 0:06:47,000
accentuer la communication

134
0:06:47.514,000 --> 0:06:49,000
ne va pas le résoudre.

135
0:06:50.282,000 --> 0:06:53,000
Nous allons avoir besoin de technologies supplémentaires

136
0:06:53.419,000 --> 0:06:56,000
qui nous aident à gérer une partie de la communication

137
0:06:56.489,000 --> 0:06:58,000
qui nous sature.

138
0:06:58.755,000 --> 0:06:59,000
Pensez à un petit avatar,

139
0:07:00.478,000 --> 0:07:01,000
un agent logiciel,

140
0:07:01.841,000 --> 0:07:02,000
un Jiminy Cricket numérique --

141
0:07:03.743,000 --> 0:07:04,000
(Rires)

142
0:07:05.005,000 --> 0:07:09,000
qui est capable de répondre à des choses en votre nom.

143
0:07:09.759,000 --> 0:07:1,000
Si nous avions cette technologie,

144
0:07:11.57,000 --> 0:07:13,000
nous pourrions nous délester d'un peu de communication

145
0:07:14.108,000 --> 0:07:18,000
et aider à prendre de meilleures décisions ou des décisions à plus grande échelle.

146
0:07:18.86,000 --> 0:07:21,000
L'idée des agents logiciels n'est pas nouvelle non plus.

147
0:07:22.603,000 --> 0:07:24,000
Nous les utilisons déjà constamment.

148
0:07:25.216,000 --> 0:07:26,000
Nous utilisons ces agents

149
0:07:26.761,000 --> 0:07:29,000
pour choisir comment nous rendre à un endroit donné,

150
0:07:31.07,000 --> 0:07:33,000
la musique que nous allons écouter

151
0:07:33.758,000 --> 0:07:36,000
ou pour obtenir des suggestions de livres.

152
0:07:37.994,000 --> 0:07:39,000
Au XXIe siècle, il y a une idée évidente,

153
0:07:40.592,000 --> 0:07:42,000
aussi évidente que celle

154
0:07:43.259,000 --> 0:07:48,000
de réunir une machine à vapeur et un métier à tisser

155
0:07:48.538,000 --> 0:07:49,000
au temps de Jacquard.

156
0:07:49.538,000 --> 0:07:53,000
Il s'agit d'associer la démocratie directe aux agents logiciels.

157
0:07:54.849,000 --> 0:07:56,000
Imaginez un instant un monde

158
0:07:56.994,000 --> 0:07:59,000
où, au lieu d'avoir un représentant pour vous représenter,

159
0:08:00.184,000 --> 0:08:01,000
vous et des millions d'autres,

160
0:08:01.782,000 --> 0:08:04,000
vous aviez un représentant qui ne représentait que vous,

161
0:08:05.504,000 --> 0:08:07,000
avec vos opinions politiques nuancées --

162
0:08:07.782,000 --> 0:08:1,000
cette étrange combinaison de libertaire, de libéral,

163
0:08:11.15,000 --> 0:08:13,000
peut-être un peu de conservateur sur certains sujets

164
0:08:13.612,000 --> 0:08:15,000
et très progressiste sur d'autres.

165
0:08:15.698,000 --> 0:08:18,000
Aujourd'hui, les politiciens sont des ensembles pleins de compromis.

166
0:08:18.989,000 --> 0:08:21,000
Mais vous pourriez avoir quelqu'un uniquement pour vous représenter,

167
0:08:22.647,000 --> 0:08:23,000
si vous êtes prêts à abandonner l'idée

168
0:08:24.523,000 --> 0:08:26,000
que ce représentant doit être un être humain.

169
0:08:27.229,000 --> 0:08:29,000
Si ce représentant est un agent logiciel,

170
0:08:29.335,000 --> 0:08:33,000
nous pourrions avoir un sénat avec autant de sénateurs que de citoyens.

171
0:08:33.529,000 --> 0:08:35,000
Ces sénateurs seront capables de lire tous les projets de loi

172
0:08:36.411,000 --> 0:08:38,000
et de voter pour chacun de ces projets.

173
0:08:39.822,000 --> 0:08:41,000
C'est une idée claire que nous voulons peut-être considérer.

174
0:08:42.802,000 --> 0:08:44,000
Mais je comprends que de nos jours,

175
0:08:45.248,000 --> 0:08:46,000
cette idée puisse être effrayante.

176
0:08:48.391,000 --> 0:08:51,000
En effet, imaginer un robot venu du futur

177
0:08:51.855,000 --> 0:08:52,000
pour nous aider à diriger nos pays

178
0:08:53.552,000 --> 0:08:54,000
paraît terrifiant.

179
0:08:56.223,000 --> 0:08:57,000
Mais nous l'avons déjà fait.

180
0:08:57.898,000 --> 0:08:58,000
(Rires)

181
0:08:59.195,000 --> 0:09:01,000
Et c'était un gars plutôt sympa.

182
0:09:03.677,000 --> 0:09:09,000
A quoi ressemblerait le métier à tisser de Jacquard pour cette idée ?

183
0:09:10.135,000 --> 0:09:11,000
Ce serait un système très simple.

184
0:09:12.06,000 --> 0:09:15,000
Imaginez un système où vous vous connectez, créez un avatar

185
0:09:15.542,000 --> 0:09:17,000
puis commencez à entraîner votre avatar.

186
0:09:18.022,000 --> 0:09:2,000
Vous pouvez fournir à votre avatar vos habitudes de lecture,

187
0:09:20.834,000 --> 0:09:21,000
le connecter à vos réseaux sociaux

188
0:09:22.613,000 --> 0:09:24,000
ou à d'autres données,

189
0:09:25.045,000 --> 0:09:27,000
par exemple en passant des tests de psychologie.

190
0:09:27.341,000 --> 0:09:29,000
Ce qui est agréable, c'est qu'il n'y a pas de supercherie.

191
0:09:30.333,000 --> 0:09:31,000
Vous ne donnez pas des données

192
0:09:31.776,000 --> 0:09:33,000
pour communiquer avec vos amis et votre famille

193
0:09:33.996,000 --> 0:09:35,000
qui sont ensuite utilisées dans un système politique.

194
0:09:36.871,000 --> 0:09:39,000
Vous procurez des données à un système conçu pour être utilisé

195
0:09:40.599,000 --> 0:09:42,000
pour prendre des décisions en votre nom.

196
0:09:43.264,000 --> 0:09:46,000
Vous prenez ensuite ces données et choisissez un algorithme d'apprentissage,

197
0:09:47.268,000 --> 0:09:48,000
car c'est un marché libre

198
0:09:48.855,000 --> 0:09:51,000
où différentes personnes peuvent soumettre différents algorithmes

199
0:09:51.901,000 --> 0:09:55,000
pour prédire comment vous voterez d'après les données fournies.

200
0:09:56.083,000 --> 0:09:59,000
Le système est ouvert, personne ne contrôle les algorithmes ;

201
0:09:59.562,000 --> 0:10:01,000
certains deviennent populaires

202
0:10:01.698,000 --> 0:10:02,000
et d'autres le sont moins.

203
0:10:03.445,000 --> 0:10:04,000
Vous pouvez auditer le système,

204
0:10:05.276,000 --> 0:10:06,000
voir comment votre avatar fonctionne.

205
0:10:07.101,000 --> 0:10:09,000
Si vous l'aimez, laissez-le en pilote automatique.

206
0:10:09.543,000 --> 0:10:1,000
Si vous voulez un peu plus de contrôle,

207
0:10:11.467,000 --> 0:10:12,000
vous pouvez décider qu'il vous demande

208
0:10:13.435,000 --> 0:10:15,000
à chaque fois qu'il prend une décision,

209
0:10:15.527,000 --> 0:10:16,000
ou choisir un compromis.

210
0:10:17.186,000 --> 0:10:19,000
Nous utilisons si peu la démocratie

211
0:10:19.615,000 --> 0:10:22,000
en partie car la démocratie dispose d'une mauvaise interface utilisateur.

212
0:10:23.207,000 --> 0:10:25,000
Si nous améliorons l'interface utilisateur de la démocratie,

213
0:10:26.03,000 --> 0:10:27,000
nous pourrions l'utiliser davantage.

214
0:10:28.452,000 --> 0:10:31,000
Bien sûr, vous pouvez vous poser plein de questions.

215
0:10:32.473,000 --> 0:10:34,000
Comment entraîner ces avatars ?

216
0:10:34.658,000 --> 0:10:36,000
Comment préserver la sécurité des données ?

217
0:10:36.662,000 --> 0:10:39,000
Comment maintenir les systèmes distribués et auditables ?

218
0:10:39.848,000 --> 0:10:41,000
Qu'en est-il de ma grand-mère de 80 ans

219
0:10:41.946,000 --> 0:10:42,000
qui ne sait pas utiliser internet ?

220
0:10:44.262,000 --> 0:10:46,000
Croyez-moi, je les ai toutes entendues.

221
0:10:46.507,000 --> 0:10:5,000
Quand vous pensez à une telle idée, vous devez vous méfier des pessimistes,

222
0:10:51.091,000 --> 0:10:55,000
car ils sont connus pour avoir un problème à chaque solution.

223
0:10:55.434,000 --> 0:10:56,000
(Rires)

224
0:10:57.283,000 --> 0:11:,000
Je veux vous inviter à réfléchir aux plus grandes idées.

225
0:11:00.347,000 --> 0:11:03,000
Les questions que j'ai évoquées ne sont que de petites idées,

226
0:11:03.997,000 --> 0:11:05,000
car ce sont des questions sur le non fonctionnement.

227
0:11:07.502,000 --> 0:11:08,000
Les grandes idées sont :

228
0:11:09.507,000 --> 0:11:1,000
que pouvons-nous faire d'autre

229
0:11:11.338,000 --> 0:11:12,000
si cela fonctionne ?

230
0:11:13.774,000 --> 0:11:16,000
Une de ces idées implique : qui écrit les lois ?

231
0:11:17.854,000 --> 0:11:21,000
Au début, nous pourrions nous servir des avatars que nous possédons déjà ;

232
0:11:22.101,000 --> 0:11:25,000
ils voteraient sur des lois écrites par les sénateurs ou politiciens

233
0:11:25.622,000 --> 0:11:26,000
déjà en place.

234
0:11:27.491,000 --> 0:11:28,000
Mais si cela fonctionnait,

235
0:11:29.902,000 --> 0:11:31,000
vous pourriez écrire un algorithme

236
0:11:32.276,000 --> 0:11:34,000
qui essayerait d'écrire une loi

237
0:11:34.45,000 --> 0:11:36,000
qui recevrait un certain pourcentage d'assentiment,

238
0:11:36.895,000 --> 0:11:37,000
puis vous inverseriez le processus.

239
0:11:38.601,000 --> 0:11:4,000
Vous pensez peut-être que cette idée est ridicule,

240
0:11:40.947,000 --> 0:11:41,000
nous ne devrions pas le faire,

241
0:11:42.377,000 --> 0:11:44,000
mais vous ne pouvez pas nier que c'est une idée possible

242
0:11:45.047,000 --> 0:11:48,000
que dans un monde où la démocratie directe et les agents logiciels

243
0:11:48.207,000 --> 0:11:5,000
sont une forme de participation viable.

244
0:11:52.596,000 --> 0:11:54,000
Comment initier la révolution ?

245
0:11:56.238,000 --> 0:11:59,000
Nous n’initierons pas cette révolution avec des grèves, des manifestations

246
0:11:59.758,000 --> 0:12:03,000
ou en exigeant que nos politiciens actuels soient transformés en robots.

247
0:12:03.81,000 --> 0:12:04,000
Cela ne fonctionnera pas.

248
0:12:05.359,000 --> 0:12:06,000
C'est bien plus simple,

249
0:12:06.995,000 --> 0:12:07,000
plus lent

250
0:12:08.178,000 --> 0:12:09,000
et bien plus humble.

251
0:12:09.616,000 --> 0:12:13,000
Nous commençons par créer de tels systèmes simples dans les universités,

252
0:12:13.989,000 --> 0:12:15,000
les bibliothèques, les organisations.

253
0:12:16.107,000 --> 0:12:18,000
Nous allons trouver une solution à ces petites questions

254
0:12:18.785,000 --> 0:12:19,000
et ces petits problèmes

255
0:12:20.03,000 --> 0:12:23,000
qu'il faudra résoudre pour rendre cette idée viable,

256
0:12:23.955,000 --> 0:12:25,000
pour la rendre fiable.

257
0:12:26.33,000 --> 0:12:29,000
En créant ces systèmes où une centaine, un millier,

258
0:12:29.989,000 --> 0:12:32,000
une centaine de milliers de gens votent de façon politiquement non contractuelle,

259
0:12:33.799,000 --> 0:12:35,000
nous développerons la confiance en l'idée,

260
0:12:35.881,000 --> 0:12:36,000
le monde changera

261
0:12:37.368,000 --> 0:12:4,000
et ceux qui sont aussi jeunes que ma fille actuellement, grandiront.

262
0:12:42.58,000 --> 0:12:44,000
D'ici à ce que ma fille ait mon âge,

263
0:12:44.973,000 --> 0:12:48,000
peut-être que cette idée qui est folle aujourd'hui,

264
0:12:49.433,000 --> 0:12:53,000
ne sera plus si folle pour elle et pour ses amis.

265
0:12:53.956,000 --> 0:12:54,000
À ce moment-là,

266
0:12:55.817,000 --> 0:12:57,000
ce sera la fin de notre histoire,

267
0:12:58.444,000 --> 0:13:,000
mais le début de la leur.

268
0:13:01.646,000 --> 0:13:02,000
Merci.

269
0:13:02.853,000 --> 0:13:05,000
(Applaudissements)

