1
0:00:13.131,000 --> 0:00:17,000
My relationship with the internet reminds me of the setup

2
0:00:17.566,000 --> 0:00:18,000
to a clichéd horror movie.

3
0:00:19.867,000 --> 0:00:23,000
You know, the blissfully happy family moves in to their perfect new home,

4
0:00:24.277,000 --> 0:00:26,000
excited about their perfect future,

5
0:00:26.582,000 --> 0:00:29,000
and it's sunny outside and the birds are chirping ...

6
0:00:30.857,000 --> 0:00:31,000
And then it gets dark.

7
0:00:32.72,000 --> 0:00:34,000
And there are noises from the attic.

8
0:00:35.092,000 --> 0:00:39,000
And we realize that that perfect new house isn't so perfect.

9
0:00:40.485,000 --> 0:00:43,000
When I started working at Google in 2006,

10
0:00:43.64,000 --> 0:00:44,000
Facebook was just a two-year-old,

11
0:00:45.431,000 --> 0:00:47,000
and Twitter hadn't yet been born.

12
0:00:47.848,000 --> 0:00:51,000
And I was in absolute awe of the internet and all of its promise

13
0:00:52.282,000 --> 0:00:53,000
to make us closer

14
0:00:53.743,000 --> 0:00:54,000
and smarter

15
0:00:55.063,000 --> 0:00:56,000
and more free.

16
0:00:57.265,000 --> 0:01:,000
But as we were doing the inspiring work of building search engines

17
0:01:01.003,000 --> 0:01:03,000
and video-sharing sites and social networks,

18
0:01:04.907,000 --> 0:01:08,000
criminals, dictators and terrorists were figuring out

19
0:01:09.235,000 --> 0:01:12,000
how to use those same platforms against us.

20
0:01:13.417,000 --> 0:01:15,000
And we didn't have the foresight to stop them.

21
0:01:16.746,000 --> 0:01:21,000
Over the last few years, geopolitical forces have come online to wreak havoc.

22
0:01:21.869,000 --> 0:01:22,000
And in response,

23
0:01:23.062,000 --> 0:01:27,000
Google supported a few colleagues and me to set up a new group called Jigsaw,

24
0:01:27.864,000 --> 0:01:31,000
with a mandate to make people safer from threats like violent extremism,

25
0:01:32.484,000 --> 0:01:34,000
censorship, persecution --

26
0:01:35.186,000 --> 0:01:39,000
threats that feel very personal to me because I was born in Iran,

27
0:01:39.327,000 --> 0:01:41,000
and I left in the aftermath of a violent revolution.

28
0:01:43.525,000 --> 0:01:47,000
But I've come to realize that even if we had all of the resources

29
0:01:47.895,000 --> 0:01:49,000
of all of the technology companies in the world,

30
0:01:51.595,000 --> 0:01:52,000
we'd still fail

31
0:01:53.586,000 --> 0:01:55,000
if we overlooked one critical ingredient:

32
0:01:57.653,000 --> 0:02:02,000
the human experiences of the victims and perpetrators of those threats.

33
0:02:04.935,000 --> 0:02:06,000
There are many challenges I could talk to you about today.

34
0:02:07.695,000 --> 0:02:08,000
I'm going to focus on just two.

35
0:02:09.623,000 --> 0:02:11,000
The first is terrorism.

36
0:02:13.563,000 --> 0:02:15,000
So in order to understand the radicalization process,

37
0:02:16.144,000 --> 0:02:2,000
we met with dozens of former members of violent extremist groups.

38
0:02:21.59,000 --> 0:02:23,000
One was a British schoolgirl,

39
0:02:25.049,000 --> 0:02:28,000
who had been taken off of a plane at London Heathrow

40
0:02:28.772,000 --> 0:02:32,000
as she was trying to make her way to Syria to join ISIS.

41
0:02:34.281,000 --> 0:02:35,000
And she was 13 years old.

42
0:02:37.792,000 --> 0:02:41,000
So I sat down with her and her father, and I said, "Why?"

43
0:02:42.441,000 --> 0:02:43,000
And she said,

44
0:02:44.182,000 --> 0:02:47,000
"I was looking at pictures of what life is like in Syria,

45
0:02:47.845,000 --> 0:02:5,000
and I thought I was going to go and live in the Islamic Disney World."

46
0:02:52.527,000 --> 0:02:54,000
That's what she saw in ISIS.

47
0:02:54.635,000 --> 0:02:57,000
She thought she'd meet and marry a jihadi Brad Pitt

48
0:02:58.151,000 --> 0:03:01,000
and go shopping in the mall all day and live happily ever after.

49
0:03:02.977,000 --> 0:03:04,000
ISIS understands what drives people,

50
0:03:05.825,000 --> 0:03:08,000
and they carefully craft a message for each audience.

51
0:03:11.122,000 --> 0:03:12,000
Just look at how many languages

52
0:03:12.657,000 --> 0:03:14,000
they translate their marketing material into.

53
0:03:15.677,000 --> 0:03:17,000
They make pamphlets, radio shows and videos

54
0:03:18.362,000 --> 0:03:19,000
in not just English and Arabic,

55
0:03:20.359,000 --> 0:03:24,000
but German, Russian, French, Turkish, Kurdish,

56
0:03:25.15,000 --> 0:03:26,000
Hebrew,

57
0:03:26.846,000 --> 0:03:27,000
Mandarin Chinese.

58
0:03:29.309,000 --> 0:03:33,000
I've even seen an ISIS-produced video in sign language.

59
0:03:34.605,000 --> 0:03:35,000
Just think about that for a second:

60
0:03:36.513,000 --> 0:03:38,000
ISIS took the time and made the effort

61
0:03:38.845,000 --> 0:03:41,000
to ensure their message is reaching the deaf and hard of hearing.

62
0:03:45.143,000 --> 0:03:47,000
It's actually not tech-savviness

63
0:03:47.311,000 --> 0:03:49,000
that is the reason why ISIS wins hearts and minds.

64
0:03:49.93,000 --> 0:03:53,000
It's their insight into the prejudices, the vulnerabilities, the desires

65
0:03:54.117,000 --> 0:03:55,000
of the people they're trying to reach

66
0:03:55.915,000 --> 0:03:56,000
that does that.

67
0:03:57.718,000 --> 0:03:58,000
That's why it's not enough

68
0:03:59.171,000 --> 0:04:03,000
for the online platforms to focus on removing recruiting material.

69
0:04:04.518,000 --> 0:04:07,000
If we want to have a shot at building meaningful technology

70
0:04:08.123,000 --> 0:04:09,000
that's going to counter radicalization,

71
0:04:10.021,000 --> 0:04:12,000
we have to start with the human journey at its core.

72
0:04:13.884,000 --> 0:04:15,000
So we went to Iraq

73
0:04:16.095,000 --> 0:04:18,000
to speak to young men who'd bought into ISIS's promise

74
0:04:18.95,000 --> 0:04:21,000
of heroism and righteousness,

75
0:04:22.165,000 --> 0:04:23,000
who'd taken up arms to fight for them

76
0:04:24.036,000 --> 0:04:25,000
and then who'd defected

77
0:04:25.398,000 --> 0:04:28,000
after they witnessed the brutality of ISIS's rule.

78
0:04:28.88,000 --> 0:04:31,000
And I'm sitting there in this makeshift prison in the north of Iraq

79
0:04:32.096,000 --> 0:04:36,000
with this 23-year-old who had actually trained as a suicide bomber

80
0:04:36.67,000 --> 0:04:37,000
before defecting.

81
0:04:39.08,000 --> 0:04:4,000
And he says,

82
0:04:41.119,000 --> 0:04:44,000
"I arrived in Syria full of hope,

83
0:04:44.363,000 --> 0:04:48,000
and immediately, I had two of my prized possessions confiscated:

84
0:04:48.752,000 --> 0:04:5,000
my passport and my mobile phone."

85
0:04:52.14,000 --> 0:04:54,000
The symbols of his physical and digital liberty

86
0:04:54.57,000 --> 0:04:55,000
were taken away from him on arrival.

87
0:04:57.248,000 --> 0:05:,000
And then this is the way he described that moment of loss to me.

88
0:05:01.356,000 --> 0:05:02,000
He said,

89
0:05:02.966,000 --> 0:05:04,000
"You know in 'Tom and Jerry,'

90
0:05:06.192,000 --> 0:05:09,000
when Jerry wants to escape, and then Tom locks the door

91
0:05:09.319,000 --> 0:05:1,000
and swallows the key

92
0:05:10.499,000 --> 0:05:13,000
and you see it bulging out of his throat as it travels down?"

93
0:05:14.446,000 --> 0:05:17,000
And of course, I really could see the image that he was describing,

94
0:05:17.623,000 --> 0:05:2,000
and I really did connect with the feeling that he was trying to convey,

95
0:05:21.308,000 --> 0:05:23,000
which was one of doom,

96
0:05:23.353,000 --> 0:05:24,000
when you know there's no way out.

97
0:05:26.551,000 --> 0:05:27,000
And I was wondering:

98
0:05:28.644,000 --> 0:05:3,000
What, if anything, could have changed his mind

99
0:05:31.35,000 --> 0:05:32,000
the day that he left home?

100
0:05:32.614,000 --> 0:05:33,000
So I asked,

101
0:05:33.888,000 --> 0:05:36,000
"If you knew everything that you know now

102
0:05:37.09,000 --> 0:05:4,000
about the suffering and the corruption, the brutality --

103
0:05:40.165,000 --> 0:05:41,000
that day you left home,

104
0:05:41.604,000 --> 0:05:42,000
would you still have gone?"

105
0:05:43.786,000 --> 0:05:44,000
And he said, "Yes."

106
0:05:45.846,000 --> 0:05:47,000
And I thought, "Holy crap, he said 'Yes.'"

107
0:05:48.694,000 --> 0:05:49,000
And then he said,

108
0:05:49.937,000 --> 0:05:52,000
"At that point, I was so brainwashed,

109
0:05:52.962,000 --> 0:05:55,000
I wasn't taking in any contradictory information.

110
0:05:56.744,000 --> 0:05:57,000
I couldn't have been swayed."

111
0:05:59.235,000 --> 0:06:01,000
"Well, what if you knew everything that you know now

112
0:06:01.786,000 --> 0:06:03,000
six months before the day that you left?"

113
0:06:05.345,000 --> 0:06:08,000
"At that point, I think it probably would have changed my mind."

114
0:06:10.138,000 --> 0:06:13,000
Radicalization isn't this yes-or-no choice.

115
0:06:14.007,000 --> 0:06:16,000
It's a process, during which people have questions --

116
0:06:17.008,000 --> 0:06:2,000
about ideology, religion, the living conditions.

117
0:06:20.808,000 --> 0:06:22,000
And they're coming online for answers,

118
0:06:23.598,000 --> 0:06:24,000
which is an opportunity to reach them.

119
0:06:25.905,000 --> 0:06:28,000
And there are videos online from people who have answers --

120
0:06:28.943,000 --> 0:06:3,000
defectors, for example, telling the story of their journey

121
0:06:31.843,000 --> 0:06:32,000
into and out of violence;

122
0:06:33.45,000 --> 0:06:36,000
stories like the one from that man I met in the Iraqi prison.

123
0:06:37.914,000 --> 0:06:39,000
There are locals who've uploaded cell phone footage

124
0:06:40.528,000 --> 0:06:43,000
of what life is really like in the caliphate under ISIS's rule.

125
0:06:44.055,000 --> 0:06:47,000
There are clerics who are sharing peaceful interpretations of Islam.

126
0:06:48.83,000 --> 0:06:49,000
But you know what?

127
0:06:50.004,000 --> 0:06:53,000
These people don't generally have the marketing prowess of ISIS.

128
0:06:54.049,000 --> 0:06:58,000
They risk their lives to speak up and confront terrorist propaganda,

129
0:06:58.605,000 --> 0:07:,000
and then they tragically don't reach the people

130
0:07:00.84,000 --> 0:07:01,000
who most need to hear from them.

131
0:07:03.173,000 --> 0:07:05,000
And we wanted to see if technology could change that.

132
0:07:06.205,000 --> 0:07:1,000
So in 2016, we partnered with Moonshot CVE

133
0:07:10.412,000 --> 0:07:13,000
to pilot a new approach to countering radicalization

134
0:07:13.616,000 --> 0:07:14,000
called the "Redirect Method."

135
0:07:16.453,000 --> 0:07:19,000
It uses the power of online advertising

136
0:07:19.489,000 --> 0:07:23,000
to bridge the gap between those susceptible to ISIS's messaging

137
0:07:24.027,000 --> 0:07:27,000
and those credible voices that are debunking that messaging.

138
0:07:28.633,000 --> 0:07:29,000
And it works like this:

139
0:07:29.807,000 --> 0:07:3,000
someone looking for extremist material --

140
0:07:31.792,000 --> 0:07:33,000
say they search for "How do I join ISIS?" --

141
0:07:34.806,000 --> 0:07:36,000
will see an ad appear

142
0:07:37.306,000 --> 0:07:41,000
that invites them to watch a YouTube video of a cleric, of a defector --

143
0:07:42.212,000 --> 0:07:44,000
someone who has an authentic answer.

144
0:07:44.546,000 --> 0:07:47,000
And that targeting is based not on a profile of who they are,

145
0:07:48.193,000 --> 0:07:51,000
but of determining something that's directly relevant

146
0:07:51.27,000 --> 0:07:52,000
to their query or question.

147
0:07:54.122,000 --> 0:07:56,000
During our eight-week pilot in English and Arabic,

148
0:07:56.988,000 --> 0:07:59,000
we reached over 300,000 people

149
0:08:00.291,000 --> 0:08:05,000
who had expressed an interest in or sympathy towards a jihadi group.

150
0:08:06.626,000 --> 0:08:08,000
These people were now watching videos

151
0:08:08.914,000 --> 0:08:11,000
that could prevent them from making devastating choices.

152
0:08:13.405,000 --> 0:08:16,000
And because violent extremism isn't confined to any one language,

153
0:08:17.156,000 --> 0:08:18,000
religion or ideology,

154
0:08:18.984,000 --> 0:08:21,000
the Redirect Method is now being deployed globally

155
0:08:22.509,000 --> 0:08:25,000
to protect people being courted online by violent ideologues,

156
0:08:26.337,000 --> 0:08:28,000
whether they're Islamists, white supremacists

157
0:08:28.957,000 --> 0:08:3,000
or other violent extremists,

158
0:08:31.084,000 --> 0:08:33,000
with the goal of giving them the chance to hear from someone

159
0:08:33.981,000 --> 0:08:35,000
on the other side of that journey;

160
0:08:36.096,000 --> 0:08:38,000
to give them the chance to choose a different path.

161
0:08:40.749,000 --> 0:08:45,000
It turns out that often the bad guys are good at exploiting the internet,

162
0:08:46.753,000 --> 0:08:49,000
not because they're some kind of technological geniuses,

163
0:08:50.521,000 --> 0:08:52,000
but because they understand what makes people tick.

164
0:08:54.855,000 --> 0:08:56,000
I want to give you a second example:

165
0:08:58.019,000 --> 0:08:59,000
online harassment.

166
0:09:00.629,000 --> 0:09:03,000
Online harassers also work to figure out what will resonate

167
0:09:04.016,000 --> 0:09:05,000
with another human being.

168
0:09:05.655,000 --> 0:09:08,000
But not to recruit them like ISIS does,

169
0:09:08.789,000 --> 0:09:09,000
but to cause them pain.

170
0:09:11.259,000 --> 0:09:12,000
Imagine this:

171
0:09:13.347,000 --> 0:09:14,000
you're a woman,

172
0:09:15.03,000 --> 0:09:16,000
you're married,

173
0:09:16.467,000 --> 0:09:17,000
you have a kid.

174
0:09:18.834,000 --> 0:09:19,000
You post something on social media,

175
0:09:20.642,000 --> 0:09:22,000
and in a reply, you're told that you'll be raped,

176
0:09:24.577,000 --> 0:09:25,000
that your son will be watching,

177
0:09:26.825,000 --> 0:09:27,000
details of when and where.

178
0:09:29.148,000 --> 0:09:32,000
In fact, your home address is put online for everyone to see.

179
0:09:33.58,000 --> 0:09:35,000
That feels like a pretty real threat.

180
0:09:37.113,000 --> 0:09:38,000
Do you think you'd go home?

181
0:09:39.999,000 --> 0:09:42,000
Do you think you'd continue doing the thing that you were doing?

182
0:09:43.071,000 --> 0:09:46,000
Would you continue doing that thing that's irritating your attacker?

183
0:09:48.016,000 --> 0:09:51,000
Online abuse has been this perverse art

184
0:09:51.136,000 --> 0:09:54,000
of figuring out what makes people angry,

185
0:09:54.628,000 --> 0:09:56,000
what makes people afraid,

186
0:09:56.784,000 --> 0:09:57,000
what makes people insecure,

187
0:09:58.449,000 --> 0:10:01,000
and then pushing those pressure points until they're silenced.

188
0:10:02.333,000 --> 0:10:04,000
When online harassment goes unchecked,

189
0:10:04.661,000 --> 0:10:05,000
free speech is stifled.

190
0:10:07.196,000 --> 0:10:09,000
And even the people hosting the conversation

191
0:10:09.347,000 --> 0:10:1,000
throw up their arms and call it quits,

192
0:10:11.205,000 --> 0:10:13,000
closing their comment sections and their forums altogether.

193
0:10:14.186,000 --> 0:10:16,000
That means we're actually losing spaces online

194
0:10:17.059,000 --> 0:10:18,000
to meet and exchange ideas.

195
0:10:19.939,000 --> 0:10:21,000
And where online spaces remain,

196
0:10:22.126,000 --> 0:10:26,000
we descend into echo chambers with people who think just like us.

197
0:10:27.688,000 --> 0:10:29,000
But that enables the spread of disinformation;

198
0:10:30.211,000 --> 0:10:32,000
that facilitates polarization.

199
0:10:34.508,000 --> 0:10:39,000
What if technology instead could enable empathy at scale?

200
0:10:40.451,000 --> 0:10:42,000
This was the question that motivated our partnership

201
0:10:42.961,000 --> 0:10:43,000
with Google's Counter Abuse team,

202
0:10:44.804,000 --> 0:10:45,000
Wikipedia

203
0:10:46.006,000 --> 0:10:47,000
and newspapers like the New York Times.

204
0:10:47.964,000 --> 0:10:49,000
We wanted to see if we could build machine-learning models

205
0:10:50.864,000 --> 0:10:53,000
that could understand the emotional impact of language.

206
0:10:55.062,000 --> 0:10:58,000
Could we predict which comments were likely to make someone else leave

207
0:10:58.696,000 --> 0:10:59,000
the online conversation?

208
0:11:00.515,000 --> 0:11:03,000
And that's no mean feat.

209
0:11:04.426,000 --> 0:11:05,000
That's no trivial accomplishment

210
0:11:06.016,000 --> 0:11:08,000
for AI to be able to do something like that.

211
0:11:08.603,000 --> 0:11:11,000
I mean, just consider these two examples of messages

212
0:11:12.356,000 --> 0:11:14,000
that could have been sent to me last week.

213
0:11:15.517,000 --> 0:11:16,000
"Break a leg at TED!"

214
0:11:17.42,000 --> 0:11:18,000
... and

215
0:11:18.608,000 --> 0:11:2,000
"I'll break your legs at TED."

216
0:11:20.758,000 --> 0:11:21,000
(Laughter)

217
0:11:22.028,000 --> 0:11:23,000
You are human,

218
0:11:23.565,000 --> 0:11:25,000
that's why that's an obvious difference to you,

219
0:11:25.799,000 --> 0:11:27,000
even though the words are pretty much the same.

220
0:11:28.047,000 --> 0:11:31,000
But for AI, it takes some training to teach the models

221
0:11:31.15,000 --> 0:11:32,000
to recognize that difference.

222
0:11:32.745,000 --> 0:11:35,000
The beauty of building AI that can tell the difference

223
0:11:36.014,000 --> 0:11:41,000
is that AI can then scale to the size of the online toxicity phenomenon,

224
0:11:41.088,000 --> 0:11:44,000
and that was our goal in building our technology called Perspective.

225
0:11:45.056,000 --> 0:11:46,000
With the help of Perspective,

226
0:11:46.507,000 --> 0:11:47,000
the New York Times, for example,

227
0:11:48.114,000 --> 0:11:5,000
has increased spaces online for conversation.

228
0:11:51.005,000 --> 0:11:52,000
Before our collaboration,

229
0:11:52.339,000 --> 0:11:56,000
they only had comments enabled on just 10 percent of their articles.

230
0:11:57.495,000 --> 0:11:58,000
With the help of machine learning,

231
0:11:59.163,000 --> 0:12:,000
they have that number up to 30 percent.

232
0:12:01.084,000 --> 0:12:02,000
So they've tripled it,

233
0:12:02.264,000 --> 0:12:03,000
and we're still just getting started.

234
0:12:04.872,000 --> 0:12:07,000
But this is about way more than just making moderators more efficient.

235
0:12:10.076,000 --> 0:12:11,000
Right now I can see you,

236
0:12:11.95,000 --> 0:12:14,000
and I can gauge how what I'm saying is landing with you.

237
0:12:16.37,000 --> 0:12:17,000
You don't have that opportunity online.

238
0:12:18.558,000 --> 0:12:21,000
Imagine if machine learning could give commenters,

239
0:12:22.217,000 --> 0:12:23,000
as they're typing,

240
0:12:23.403,000 --> 0:12:26,000
real-time feedback about how their words might land,

241
0:12:27.609,000 --> 0:12:3,000
just like facial expressions do in a face-to-face conversation.

242
0:12:32.926,000 --> 0:12:33,000
Machine learning isn't perfect,

243
0:12:34.792,000 --> 0:12:36,000
and it still makes plenty of mistakes.

244
0:12:37.21,000 --> 0:12:38,000
But if we can build technology

245
0:12:38.791,000 --> 0:12:41,000
that understands the emotional impact of language,

246
0:12:42.108,000 --> 0:12:43,000
we can build empathy.

247
0:12:43.592,000 --> 0:12:45,000
That means that we can have dialogue between people

248
0:12:46.041,000 --> 0:12:47,000
with different politics,

249
0:12:47.881,000 --> 0:12:48,000
different worldviews,

250
0:12:49.121,000 --> 0:12:5,000
different values.

251
0:12:51.359,000 --> 0:12:55,000
And we can reinvigorate the spaces online that most of us have given up on.

252
0:12:57.857,000 --> 0:13:,000
When people use technology to exploit and harm others,

253
0:13:01.666,000 --> 0:13:04,000
they're preying on our human fears and vulnerabilities.

254
0:13:06.461,000 --> 0:13:09,000
If we ever thought that we could build an internet

255
0:13:09.993,000 --> 0:13:11,000
insulated from the dark side of humanity,

256
0:13:12.595,000 --> 0:13:13,000
we were wrong.

257
0:13:14.361,000 --> 0:13:16,000
If we want today to build technology

258
0:13:16.655,000 --> 0:13:19,000
that can overcome the challenges that we face,

259
0:13:19.806,000 --> 0:13:23,000
we have to throw our entire selves into understanding the issues

260
0:13:23.873,000 --> 0:13:24,000
and into building solutions

261
0:13:25.79,000 --> 0:13:28,000
that are as human as the problems they aim to solve.

262
0:13:30.071,000 --> 0:13:31,000
Let's make that happen.

263
0:13:31.924,000 --> 0:13:32,000
Thank you.

264
0:13:33.098,000 --> 0:13:36,000
(Applause)

