1
0:00:,000 --> 0:00:07,000
Traductor: Máximo Hdez Revisor: Denise RQ

2
0:00:13,000 --> 0:00:15,000
Voy a hablar de un fallo intuitivo

3
0:00:15.209,000 --> 0:00:16,000
que muchos sufrimos.

4
0:00:17.48,000 --> 0:00:2,000
En realidad es la capacidad de detectar cierto tipo de peligro.

5
0:00:21.47,000 --> 0:00:27,000
Describiré una situación que creo que es tan aterradora como posible

6
0:00:28.96,000 --> 0:00:3,000
y veremos que eso no es una buena combinación.

7
0:00:31.89,000 --> 0:00:33,000
Y sin embargo, en lugar de sentir miedo,

8
0:00:33.974,000 --> 0:00:36,000
la mayoría pensará que es algo bastante interesante.

9
0:00:37.22,000 --> 0:00:41,000
Describiré cómo los avances en el campo de la inteligencia artificial,

10
0:00:41.952,000 --> 0:00:43,000
en última instancia, podrían destruirnos.

11
0:00:44.037,000 --> 0:00:45,000
Y, de hecho, me resulta muy difícil ver

12
0:00:46.016,000 --> 0:00:49,000
que no nos destruirían o nos ayudaran a destruirnos.

13
0:00:49.424,000 --> 0:00:5,000
Y sin embargo, si son como yo,

14
0:00:51.28,000 --> 0:00:53,000
encontrarán que es divertido pensar en estas cosas.

15
0:00:53.936,000 --> 0:00:56,000
Y esta manera de ver las cosas es parte del problema.

16
0:00:57.312,000 --> 0:00:58,000
Esa reacción debería preocuparles.

17
0:00:59.918,000 --> 0:01:01,000
Si tuviera que convencerles con esta charla

18
0:01:02.582,000 --> 0:01:05,000
de que estamos al borde de sufrir una hambruna a nivel mundial,

19
0:01:05.992,000 --> 0:01:08,000
debido al calentamiento global o cualquier otra catástrofe,

20
0:01:09.048,000 --> 0:01:12,000
y que sus nietos o sus bisnietos,

21
0:01:12.464,000 --> 0:01:13,000
vivirán muy probablemente así,

22
0:01:15.2,000 --> 0:01:16,000
no pensarían:

23
0:01:17.44,000 --> 0:01:18,000
"Interesante.

24
0:01:18.76,000 --> 0:01:19,000
Me gusta esta charla TED".

25
0:01:21.2,000 --> 0:01:22,000
El hambre no es divertida

26
0:01:23.8,000 --> 0:01:26,000
La muerte en la ciencia ficción, por el contrario, es algo divertido,

27
0:01:27.173,000 --> 0:01:3,000
y lo más preocupante en el desarrollo de la IA hoy en día

28
0:01:31.167,000 --> 0:01:35,000
es que parecemos incapaces de ofrecer una respuesta emocional adecuada

29
0:01:35.248,000 --> 0:01:36,000
frente a los peligros que se avecinan.

30
0:01:37.187,000 --> 0:01:4,000
Yo mismo soy incapaz de ello y estoy dando esta charla.

31
0:01:42.125,000 --> 0:01:44,000
Es como si nos encontráramos ante dos puertas.

32
0:01:44.615,000 --> 0:01:45,000
Detrás de la puerta número uno

33
0:01:46.177,000 --> 0:01:49,000
detenemos el progreso de máquinas inteligentes.

34
0:01:49.417,000 --> 0:01:53,000
Nuestro hardware y software se estancan simplemente, por alguna razón.

35
0:01:53.432,000 --> 0:01:56,000
Traten de reflexionar por un momento por qué podría suceder esto.

36
0:01:57.083,000 --> 0:02:,000
Dada la valía de la inteligencia y la automatización,

37
0:02:00.736,000 --> 0:02:03,000
seguiremos mejorando nuestra tecnología si es posible.

38
0:02:05.2,000 --> 0:02:06,000
¿Qué podría impedirnos hacer eso?

39
0:02:07.8,000 --> 0:02:08,000
¿Una guerra nuclear a gran escala?

40
0:02:11,000 --> 0:02:12,000
¿Una pandemia mundial?

41
0:02:14.32,000 --> 0:02:15,000
¿El impacto de un asteroide?

42
0:02:17.375,000 --> 0:02:2,000
¿El hecho que Justin Bieber podría ser el presidente de los EE.UU.?

43
0:02:20.543,000 --> 0:02:21,000
(Risas)

44
0:02:24.76,000 --> 0:02:27,000
El caso es que algo tendría que destruir la civilización tal como la conocemos.

45
0:02:29.36,000 --> 0:02:33,000
Realmente deberíamos imaginar algo terrible

46
0:02:33.656,000 --> 0:02:36,000
para dejar de desarrollar nuestra tecnología

47
0:02:36.992,000 --> 0:02:37,000
para siempre,

48
0:02:38.208,000 --> 0:02:4,000
generación tras generación.

49
0:02:40.224,000 --> 0:02:42,000
Casi por definición, sería lo peor

50
0:02:42.36,000 --> 0:02:43,000
en la historia humana.

51
0:02:44.32,000 --> 0:02:45,000
La única alternativa,

52
0:02:45.816,000 --> 0:02:47,000
y esto es lo que hay detrás de la puerta número dos,

53
0:02:48.392,000 --> 0:02:5,000
es seguir mejorando nuestras máquinas inteligentes

54
0:02:51.336,000 --> 0:02:52,000
año tras año tras año.

55
0:02:53.72,000 --> 0:02:56,000
En algún momento construiremos máquinas más inteligentes que nosotros,

56
0:02:57.84,000 --> 0:02:59,000
y una vez que las tengamos

57
0:03:00.766,000 --> 0:03:01,000
empezarán a mejorarse a sí mismas.

58
0:03:02.626,000 --> 0:03:05,000
Y entonces corremos el riesgo teorizado por el matemático IJ Good

59
0:03:05.71,000 --> 0:03:06,000
llamado "explosión de inteligencia"

60
0:03:07.506,000 --> 0:03:09,000
donde el proceso podría salirse de control.

61
0:03:10.12,000 --> 0:03:12,000
Esto es a menudo caricaturizado, como lo he hecho aquí,

62
0:03:12.936,000 --> 0:03:16,000
como el miedo a que nos ataquen ejércitos de robots maliciosos.

63
0:03:17.408,000 --> 0:03:19,000
Pero ese no es el escenario más probable.

64
0:03:20.104,000 --> 0:03:24,000
No es que nuestras máquinas se volverán malignas espontáneamente.

65
0:03:24.96,000 --> 0:03:25,000
La preocupación verdadera

66
0:03:26.516,000 --> 0:03:29,000
al construir máquinas mucho más competentes que nosotros

67
0:03:29.632,000 --> 0:03:32,000
es que la menor diferencia entre sus objetivos y los nuestros

68
0:03:33.408,000 --> 0:03:34,000
nos podría destruir.

69
0:03:35.96,000 --> 0:03:37,000
Basta con pensar en nuestra relación con las hormigas.

70
0:03:38.63,000 --> 0:03:39,000
No las odiamos.

71
0:03:40.25,000 --> 0:03:41,000
No vamos por la vida lastimándolas.

72
0:03:41.992,000 --> 0:03:43,000
De hecho, a veces nos tomamos la molestia de no hacerles daño.

73
0:03:44.988,000 --> 0:03:45,000
Evitamos pisarlas en la acera.

74
0:03:46.792,000 --> 0:03:48,000
Pero cada vez que su presencia entra seriamente en conflicto

75
0:03:49.788,000 --> 0:03:5,000
con alguno de nuestros objetivos,

76
0:03:51.408,000 --> 0:03:53,000
digamos, en la construcción de un edificio como este,

77
0:03:54.017,000 --> 0:03:55,000
las aniquilamos sin escrúpulos.

78
0:03:56.48,000 --> 0:03:58,000
La preocupación es que algún día construyamos máquinas

79
0:03:59.416,000 --> 0:04:01,000
que, ya sea conscientemente o no,

80
0:04:01.85,000 --> 0:04:03,000
nos puedan tratar con una indiferencia similar.

81
0:04:05.76,000 --> 0:04:07,000
Sospecho que esto pueda parece inverosímil para muchos.

82
0:04:09.375,000 --> 0:04:14,000
Apuesto a que hay quienes dudan de que la superinteligente IA sea posible

83
0:04:15.29,000 --> 0:04:17,000
y mucho menos inevitable.

84
0:04:17.376,000 --> 0:04:2,000
Pero en este caso hay que refutar uno de los siguientes supuestos.

85
0:04:20.996,000 --> 0:04:21,000
Y hay solo tres.

86
0:04:23.8,000 --> 0:04:27,000
La inteligencia es el procesamiento de información en un sistema físico.

87
0:04:29.32,000 --> 0:04:31,000
En realidad, esto es poco más que una suposición.

88
0:04:31.935,000 --> 0:04:34,000
Ya hemos incorporado inteligencia limitada en nuestras máquinas,

89
0:04:35.392,000 --> 0:04:39,000
y aún así, muchas de estas máquinas actúan a un nivel de inteligencia sobrehumana.

90
0:04:40.84,000 --> 0:04:45,000
Y sabemos que la mera materia da lugar a lo que se llama "inteligencia general",

91
0:04:46.046,000 --> 0:04:49,000
la capacidad de pensar con flexibilidad en múltiples campos

92
0:04:49.688,000 --> 0:04:52,000
porque nuestros cerebro humano ya lo ha conseguido.

93
0:04:52.824,000 --> 0:04:55,000
Es decir, solo hay átomos aquí,

94
0:04:56.76,000 --> 0:05:,000
y mientras continuemos construyendo sistemas de átomos

95
0:05:01.256,000 --> 0:05:03,000
que exhiban un comportamiento más y más inteligente,

96
0:05:03.952,000 --> 0:05:08,000
terminaremos implementando, a menos que lo interrumpamos, inteligencia general

97
0:05:09.684,000 --> 0:05:1,000
en nuestras máquinas.

98
0:05:11.16,000 --> 0:05:14,000
Es crucial comprender que la velocidad no es el problema

99
0:05:14.79,000 --> 0:05:17,000
porque cualquier velocidad es suficiente para llegar al fin.

100
0:05:18.256,000 --> 0:05:21,000
No necesitamos la ley de Moore para continuar ni un aumento exponencial.

101
0:05:22.032,000 --> 0:05:23,000
Solo tenemos que seguir adelante.

102
0:05:25.48,000 --> 0:05:27,000
El segundo supuesto es que vamos a seguir adelante.

103
0:05:29,000 --> 0:05:31,000
Vamos a seguir mejorando nuestras máquinas inteligentes.

104
0:05:34.59,000 --> 0:05:36,000
Y teniendo en cuenta el valor de la inteligencia,

105
0:05:37.376,000 --> 0:05:4,000
es decir, la inteligencia es o bien la fuente de todo lo que valoramos

106
0:05:40.912,000 --> 0:05:42,000
o la necesidad por preservar todo lo que valoramos.

107
0:05:43.688,000 --> 0:05:45,000
Es nuestro recurso más valioso.

108
0:05:45.944,000 --> 0:05:46,000
Por eso lo queremos hacer.

109
0:05:47.48,000 --> 0:05:5,000
Tenemos problemas que necesitamos desesperadamente resolver.

110
0:05:50.816,000 --> 0:05:53,000
Queremos curar enfermedades como el Alzheimer y el cáncer.

111
0:05:55.17,000 --> 0:05:57,000
Queremos entender los sistemas económicos.

112
0:05:57.18,000 --> 0:05:59,000
Queremos mejorar el clima.

113
0:05:59.32,000 --> 0:06:,000
Vamos a hacer esto, si podemos.

114
0:06:01.176,000 --> 0:06:04,000
El tren ya salió de la estación y no hay frenos.

115
0:06:05.88,000 --> 0:06:1,000
Por último, no estamos en la cima de la inteligencia,

116
0:06:11.336,000 --> 0:06:12,000
ni siquiera cerca, probablemente.

117
0:06:13.64,000 --> 0:06:14,000
Y esto realmente es crucial.

118
0:06:15.536,000 --> 0:06:17,000
Esto es lo que hace nuestra situación tan precaria,

119
0:06:17.952,000 --> 0:06:19,000
y esto es lo que hace que nuestras intuiciones

120
0:06:20.24,000 --> 0:06:22,000
sobre los riesgos sean poco fiables.

121
0:06:23,000 --> 0:06:26,000
Piensen en la persona más inteligente que jamás haya vivido.

122
0:06:26.667,000 --> 0:06:29,000
En la lista de casi todos está John Von Neumann.

123
0:06:30.056,000 --> 0:06:33,000
La impresión que hacía Von Neumann en las personas a su alrededor,

124
0:06:33.392,000 --> 0:06:37,000
incluyendo los más grandes matemáticos y físicos de su época,

125
0:06:37.448,000 --> 0:06:38,000
está bastante bien documentada.

126
0:06:39.384,000 --> 0:06:42,000
Si solo la mitad de las historias sobre él fueran una verdad a medias,

127
0:06:43.16,000 --> 0:06:44,000
no hay duda

128
0:06:44.256,000 --> 0:06:46,000
de que es una de las personas más inteligentes que ha vivido.

129
0:06:47.142,000 --> 0:06:49,000
Así que consideren el espectro de la inteligencia.

130
0:06:50.32,000 --> 0:06:51,000
Aquí tenemos a John Von Neumann.

131
0:06:53.56,000 --> 0:06:54,000
Y aquí estamos tú y yo.

132
0:06:56.12,000 --> 0:06:57,000
Y luego tenemos un pollo.

133
0:06:59.4,000 --> 0:07:,000
Lo sentimos, una gallina.

134
0:07:01.92,000 --> 0:07:04,000
No hay por qué hacer esta charla más deprimente de lo que ya es.

135
0:07:05.017,000 --> 0:07:06,000
(Risas)

136
0:07:08.339,000 --> 0:07:11,000
Sin embargo, parece muy probable que el espectro de la inteligencia

137
0:07:11.816,000 --> 0:07:14,000
se extienda mucho más allá de lo que actualmente concebimos,

138
0:07:15.88,000 --> 0:07:18,000
y si construimos máquinas más inteligentes que nosotros,

139
0:07:19.096,000 --> 0:07:21,000
muy probablemente explorarán este espectro

140
0:07:21.392,000 --> 0:07:22,000
de maneras que no podemos imaginar,

141
0:07:23.248,000 --> 0:07:25,000
y nos superarán de maneras inimaginables.

142
0:07:27,000 --> 0:07:31,000
Y es importante saber que esto es cierto solo debido a la velocidad.

143
0:07:31.23,000 --> 0:07:36,000
Así que imaginen que acabamos de construir una IA superinteligente

144
0:07:36.416,000 --> 0:07:37,000
que no fuera más inteligente

145
0:07:37.972,000 --> 0:07:41,000
que el promedio del equipo de investigadores en Stanford o el MIT.

146
0:07:42.11,000 --> 0:07:43,000
Los circuitos electrónicos funcionan

147
0:07:43.836,000 --> 0:07:46,000
aproximadamente un millón de veces más rápido que los bioquímicos,

148
0:07:46.996,000 --> 0:07:48,000
así que esta máquina debe pensar un millón de veces más rápido

149
0:07:49.916,000 --> 0:07:5,000
que las mentes que la construyeron.

150
0:07:51.616,000 --> 0:07:52,000
Con una semana funcionando

151
0:07:53.306,000 --> 0:07:57,000
llevará a cabo 20 000 años de trabajo intelectual a nivel humano,

152
0:07:58.4,000 --> 0:07:59,000
semana tras semana tras semana.

153
0:08:01.64,000 --> 0:08:04,000
¿Cómo podríamos siquiera comprender, mucho menos restringir,

154
0:08:04.736,000 --> 0:08:06,000
una mente que progresa de esta manera?

155
0:08:08.84,000 --> 0:08:1,000
Algo más que es francamente preocupante

156
0:08:10.976,000 --> 0:08:14,000
es imaginar el mejor de los casos.

157
0:08:15.952,000 --> 0:08:18,000
Imaginemos que diseñamos una IA superinteligente

158
0:08:19.65,000 --> 0:08:2,000
que no tiene problemas de seguridad.

159
0:08:21.576,000 --> 0:08:24,000
Tenemos el diseño perfecto a la primera.

160
0:08:24.832,000 --> 0:08:26,000
Es como si nos dieran un oráculo

161
0:08:27.048,000 --> 0:08:29,000
que se comporta exactamente como se espera.

162
0:08:29.064,000 --> 0:08:32,000
Esta máquina sería el dispositivo de ahorro de mano de obra perfecta.

163
0:08:33.52,000 --> 0:08:35,000
Puede diseñar la máquina que puede construir la máquina

164
0:08:36.109,000 --> 0:08:37,000
que pueda hacer cualquier trabajo físico,

165
0:08:38.072,000 --> 0:08:39,000
impulsada por la luz solar,

166
0:08:39.386,000 --> 0:08:41,000
más o menos por el costo de las materias primas.

167
0:08:42.072,000 --> 0:08:44,000
Estamos hablando del fin del trabajo pesado humano.

168
0:08:45.49,000 --> 0:08:48,000
También estamos hablando del fin de la mayoría del trabajo intelectual.

169
0:08:49.121,000 --> 0:08:52,000
Entonces, ¿qué harían simios como nosotros en estas circunstancias?

170
0:08:52.292,000 --> 0:08:56,000
Pues, podríamos jugar al Frisbee y darnos masajes.

171
0:08:57.84,000 --> 0:08:59,000
Tomar un poco de LSD e inventar modas ridículas

172
0:09:00.49,000 --> 0:09:02,000
y todo el mundo podríamos parecernos a un festival de rock.

173
0:09:03.306,000 --> 0:09:04,000
(Risas)

174
0:09:06.32,000 --> 0:09:1,000
Puede parecer muy bueno, pero hay que preguntarse

175
0:09:10.746,000 --> 0:09:13,000
qué pasaría con nuestro orden económico y político actual.

176
0:09:14.392,000 --> 0:09:16,000
Podríamos presenciar

177
0:09:16.808,000 --> 0:09:2,000
un nivel de desigualdad de la riqueza y el desempleo

178
0:09:20.944,000 --> 0:09:21,000
nunca antes visto

179
0:09:22.44,000 --> 0:09:25,000
sin la voluntad de poner esta nueva riqueza inmediatamente

180
0:09:25.606,000 --> 0:09:26,000
al servicio de toda la humanidad,

181
0:09:27.459,000 --> 0:09:3,000
y unos poco trillonarios estarían en las portadas de las revistas de negocios

182
0:09:31.167,000 --> 0:09:34,000
mientras que el resto del mundo tendría la libertad de morirse de hambre.

183
0:09:34.646,000 --> 0:09:36,000
Y ¿qué pasaría si los rusos o los chinos se enteraran

184
0:09:37.209,000 --> 0:09:39,000
de que alguna empresa en Silicon Valley

185
0:09:39.243,000 --> 0:09:41,000
está a punto de crear una IA superinteligente?

186
0:09:41.968,000 --> 0:09:43,000
Esta máquina podría ser capaz de hacer la guerra,

187
0:09:44.834,000 --> 0:09:46,000
ya sea terrestre o cibernética,

188
0:09:47.04,000 --> 0:09:49,000
con un poder sin precedentes.

189
0:09:49.92,000 --> 0:09:51,000
En este escenario el ganador se lleva todo.

190
0:09:51.976,000 --> 0:09:54,000
Seis meses adelante en la competencia

191
0:09:55.112,000 --> 0:09:57,000
sería una ventaja de 500 000 años,

192
0:09:57.888,000 --> 0:09:58,000
como mínimo.

193
0:09:59.384,000 --> 0:10:03,000
Parecería que incluso meros rumores de este tipo de avance

194
0:10:04.12,000 --> 0:10:06,000
podría causar que nuestra especie se vuelva loca.

195
0:10:06.496,000 --> 0:10:08,000
Una de las cosas más aterradoras,

196
0:10:09.392,000 --> 0:10:11,000
en mi opinión, en este momento,

197
0:10:12.168,000 --> 0:10:16,000
son el tipo de cosas que dicen los investigadores de IA

198
0:10:16.45,000 --> 0:10:17,000
cuando quieren tranquilizarnos.

199
0:10:18.722,000 --> 0:10:21,000
Y el motivo invocado más frecuentemente de que no nos preocupemos es el tiempo.

200
0:10:22.456,000 --> 0:10:24,000
Falta mucho para eso, no se preocupen.

201
0:10:24.512,000 --> 0:10:26,000
Eso será probablemente dentro de 50 o 100 años.

202
0:10:27.72,000 --> 0:10:28,000
Un investigador dijo,

203
0:10:28.836,000 --> 0:10:3,000
"Preocuparse por la seguridad y todo lo relacionado con la IA

204
0:10:31.501,000 --> 0:10:33,000
es como preocuparse por la superpoblación en Marte".

205
0:10:34.125,000 --> 0:10:37,000
Esta es la manera de Silicon Valley de mostrarse condescendiente.

206
0:10:37.585,000 --> 0:10:38,000
(Risas)

207
0:10:39.52,000 --> 0:10:4,000
Nadie parece darse cuenta

208
0:10:41.417,000 --> 0:10:43,000
que tomar el tiempo con referencia

209
0:10:44.032,000 --> 0:10:46,000
es una incongruencia total.

210
0:10:46.608,000 --> 0:10:49,000
Si la inteligencia es solo una cuestión de procesamiento de la información

211
0:10:50.26,000 --> 0:10:52,000
y seguimos mejorando nuestras máquinas,

212
0:10:52.626,000 --> 0:10:54,000
produciremos algún tipo de superinteligencia.

213
0:10:56.32,000 --> 0:11:,000
Y no tenemos idea de cuánto tiempo nos llevará crear las condiciones

214
0:11:01.166,000 --> 0:11:02,000
para hacerlo de forma segura.

215
0:11:04.2,000 --> 0:11:05,000
Voy a decirlo de nuevo.

216
0:11:05.496,000 --> 0:11:1,000
Y no tenemos idea de cuánto tiempo nos llevará crear las condiciones

217
0:11:10.562,000 --> 0:11:11,000
para hacerlo de forma segura.

218
0:11:12.92,000 --> 0:11:15,000
Y si no lo han notado, 50 años ya no son lo que solían ser.

219
0:11:16.376,000 --> 0:11:18,000
Estos son 50 años en meses.

220
0:11:18.832,000 --> 0:11:2,000
Este es el tiempo que hemos tenido el iPhone.

221
0:11:21.44,000 --> 0:11:23,000
Este es el tiempo que "Los Simpson" ha estado en la televisión.

222
0:11:24.704,000 --> 0:11:26,000
Cincuenta años no es tanto tiempo

223
0:11:27.08,000 --> 0:11:3,000
para lograr uno de los mayores desafíos al que nuestra especie se ha enfrentado.

224
0:11:31.64,000 --> 0:11:35,000
Una vez más, parece que no tenemos una respuesta emocional adecuada

225
0:11:35.656,000 --> 0:11:37,000
para lo que, con toda probabilidad, va a pasar.

226
0:11:38.352,000 --> 0:11:41,000
El científico de la computación Stuart Russell ofrece una gran analogía:

227
0:11:42.328,000 --> 0:11:47,000
"Imaginen que recibimos un mensaje de una civilización extraterrestre que diga:

228
0:11:49.04,000 --> 0:11:5,000
'Gente de la Tierra,

229
0:11:50.576,000 --> 0:11:52,000
llegaremos en su planeta en 50 años.

230
0:11:53.8,000 --> 0:11:54,000
Prepárense'".

231
0:11:55.4,000 --> 0:11:58,000
¿Estaremos contando los meses hasta que llegue la nave nodriza?

232
0:11:59.93,000 --> 0:12:02,000
¿No estaríamos un poco más preocupados?

233
0:12:04.34,000 --> 0:12:06,000
Otra razón que se nos da para no preocuparnos

234
0:12:06.496,000 --> 0:12:08,000
es que estas máquinas no podrán no compartir nuestros valores

235
0:12:09.382,000 --> 0:12:11,000
porque van a ser literalmente extensiones de nosotros mismos.

236
0:12:12.348,000 --> 0:12:13,000
Se injertarán en nuestro cerebro,

237
0:12:14.05,000 --> 0:12:16,000
y prácticamente seremos su sistema límbico.

238
0:12:17.12,000 --> 0:12:18,000
Consideren por un momento

239
0:12:18.95,000 --> 0:12:2,000
que el camino más seguro y prudente hacia adelante,

240
0:12:21.736,000 --> 0:12:22,000
el recomendado,

241
0:12:22.882,000 --> 0:12:25,000
es la implantación de esta tecnología directamente en nuestro cerebro.

242
0:12:26.17,000 --> 0:12:27,000
Ahora bien, esto puede ser de hecho,

243
0:12:27.94,000 --> 0:12:29,000
la manera más segura y prudente de avanzar,

244
0:12:30.25,000 --> 0:12:33,000
pero por lo general, los problemas de seguridad de una nueva tecnología

245
0:12:33.668,000 --> 0:12:36,000
hay que resolverlos antes de implementarla en una cabeza.

246
0:12:36.85,000 --> 0:12:37,000
(Risas)

247
0:12:38.8,000 --> 0:12:39,000
El mayor problema es

248
0:12:40.34,000 --> 0:12:45,000
que construir una IA superinteligente y autónoma parece más fácil

249
0:12:45.776,000 --> 0:12:46,000
que diseñar una IA superinteligente

250
0:12:47.752,000 --> 0:12:48,000
mientras se controla la neurociencia

251
0:12:49.542,000 --> 0:12:51,000
para integrar la máquina y la mente sin problemas.

252
0:12:52.8,000 --> 0:12:55,000
Y dado que las empresas y los gobiernos que trabajan sobre ello

253
0:12:55.976,000 --> 0:12:58,000
probablemente se vean a sí mismos en una carrera contra todos los demás,

254
0:12:59.632,000 --> 0:13:02,000
y tomando en cuenta que ganar esta carrera es ganar el mundo,

255
0:13:02.76,000 --> 0:13:04,000
siempre y cuando no lo destruyan en el momento siguiente,

256
0:13:05.546,000 --> 0:13:07,000
entonces parece probable que lo más fácil de hacer

257
0:13:08.056,000 --> 0:13:09,000
se hará primero.

258
0:13:10.56,000 --> 0:13:12,000
Ahora, por desgracia, no tengo una solución a este problema,

259
0:13:13.417,000 --> 0:13:15,000
además de recomendarles que piensen más sobre ello.

260
0:13:16.002,000 --> 0:13:18,000
Creo que necesitamos un Proyecto Manhattan

261
0:13:18.34,000 --> 0:13:2,000
sobre el tema de la inteligencia artificial.

262
0:13:20.496,000 --> 0:13:22,000
No para construirlo, porque creo que eso es inevitable,

263
0:13:23.232,000 --> 0:13:26,000
sino para entender cómo evitar una carrera armamentística

264
0:13:26.568,000 --> 0:13:29,000
y construirla de manera que concuerde con nuestros intereses.

265
0:13:30.064,000 --> 0:13:32,000
Cuando hablamos de IA superinteligente

266
0:13:32.2,000 --> 0:13:34,000
que puede modificarse a sí misma,

267
0:13:34.89,000 --> 0:13:38,000
solo tenemos una oportunidad de fijar las condiciones iniciales correctas,

268
0:13:39.216,000 --> 0:13:41,000
e incluso entonces necesitaremos gestionar

269
0:13:41.272,000 --> 0:13:44,000
las consecuencias económicas y políticas de dicho diseño.

270
0:13:45.76,000 --> 0:13:47,000
Pero en el momento en que admitamos

271
0:13:47.816,000 --> 0:13:51,000
que el procesamiento de la información es la fuente de la inteligencia,

272
0:13:52.72,000 --> 0:13:56,000
que algún sistema computacional adecuado es la base de la inteligencia,

273
0:13:58.36,000 --> 0:14:01,000
y admitamos que los vamos a mejorar de forma continua,

274
0:14:03.28,000 --> 0:14:07,000
y admitamos que el horizonte del conocimiento supera por mucho

275
0:14:07.4,000 --> 0:14:08,000
lo que actualmente conocemos,

276
0:14:10.12,000 --> 0:14:11,000
entonces tenemos que admitir

277
0:14:11.556,000 --> 0:14:13,000
que estamos en el proceso de construir una especie de dios.

278
0:14:15.4,000 --> 0:14:16,000
Ahora sería un buen momento

279
0:14:16.976,000 --> 0:14:18,000
para asegurarse de que sea un dios con el que podamos vivir.

280
0:14:20.12,000 --> 0:14:21,000
Muchas gracias.

281
0:14:21.166,000 --> 0:14:22,000
(Aplausos)

