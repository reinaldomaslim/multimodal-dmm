1
0:00:,000 --> 0:00:07,000
Translator: Joseph Geni Reviewer: Krystian Aparta

2
0:00:13.047,000 --> 0:00:15,000
There was a day, about 10 years ago,

3
0:00:15.579,000 --> 0:00:18,000
when I asked a friend to hold a baby dinosaur robot upside down.

4
0:00:21.889,000 --> 0:00:24,000
It was this toy called a Pleo that I had ordered,

5
0:00:25.359,000 --> 0:00:29,000
and I was really excited about it because I've always loved robots.

6
0:00:29.784,000 --> 0:00:31,000
And this one has really cool technical features.

7
0:00:32.087,000 --> 0:00:34,000
It had motors and touch sensors

8
0:00:34.23,000 --> 0:00:36,000
and it had an infrared camera.

9
0:00:36.498,000 --> 0:00:38,000
And one of the things it had was a tilt sensor,

10
0:00:39.285,000 --> 0:00:41,000
so it knew what direction it was facing.

11
0:00:42.095,000 --> 0:00:44,000
And when you held it upside down,

12
0:00:44.253,000 --> 0:00:45,000
it would start to cry.

13
0:00:46.527,000 --> 0:00:49,000
And I thought this was super cool, so I was showing it off to my friend,

14
0:00:50.047,000 --> 0:00:52,000
and I said, "Oh, hold it up by the tail. See what it does."

15
0:00:55.268,000 --> 0:00:58,000
So we're watching the theatrics of this robot

16
0:00:58.917,000 --> 0:01:,000
struggle and cry out.

17
0:01:02.767,000 --> 0:01:04,000
And after a few seconds,

18
0:01:04.838,000 --> 0:01:05,000
it starts to bother me a little,

19
0:01:07.744,000 --> 0:01:1,000
and I said, "OK, that's enough now.

20
0:01:11.93,000 --> 0:01:13,000
Let's put him back down."

21
0:01:14.259,000 --> 0:01:16,000
And then I pet the robot to make it stop crying.

22
0:01:18.973,000 --> 0:01:2,000
And that was kind of a weird experience for me.

23
0:01:22.084,000 --> 0:01:26,000
For one thing, I wasn't the most maternal person at the time.

24
0:01:26.677,000 --> 0:01:28,000
Although since then I've become a mother, nine months ago,

25
0:01:29.432,000 --> 0:01:32,000
and I've learned that babies also squirm when you hold them upside down.

26
0:01:32.889,000 --> 0:01:33,000
(Laughter)

27
0:01:35.023,000 --> 0:01:37,000
But my response to this robot was also interesting

28
0:01:37.405,000 --> 0:01:41,000
because I knew exactly how this machine worked,

29
0:01:41.53,000 --> 0:01:44,000
and yet I still felt compelled to be kind to it.

30
0:01:46.45,000 --> 0:01:48,000
And that observation sparked a curiosity

31
0:01:49.181,000 --> 0:01:51,000
that I've spent the past decade pursuing.

32
0:01:52.911,000 --> 0:01:53,000
Why did I comfort this robot?

33
0:01:56.228,000 --> 0:01:59,000
And one of the things I discovered was that my treatment of this machine

34
0:01:59.831,000 --> 0:02:02,000
was more than just an awkward moment in my living room,

35
0:02:03.556,000 --> 0:02:08,000
that in a world where we're increasingly integrating robots into our lives,

36
0:02:09,000 --> 0:02:12,000
an instinct like that might actually have consequences,

37
0:02:13.452,000 --> 0:02:16,000
because the first thing that I discovered is that it's not just me.

38
0:02:19.249,000 --> 0:02:23,000
In 2007, the Washington Post reported that the United States military

39
0:02:24.075,000 --> 0:02:27,000
was testing this robot that defused land mines.

40
0:02:27.329,000 --> 0:02:29,000
And the way it worked was it was shaped like a stick insect

41
0:02:30.265,000 --> 0:02:32,000
and it would walk around a minefield on its legs,

42
0:02:32.94,000 --> 0:02:35,000
and every time it stepped on a mine, one of the legs would blow up,

43
0:02:36.17,000 --> 0:02:39,000
and it would continue on the other legs to blow up more mines.

44
0:02:39.251,000 --> 0:02:42,000
And the colonel who was in charge of this testing exercise

45
0:02:43.061,000 --> 0:02:45,000
ends up calling it off,

46
0:02:45.203,000 --> 0:02:47,000
because, he says, it's too inhumane

47
0:02:47.662,000 --> 0:02:51,000
to watch this damaged robot drag itself along the minefield.

48
0:02:54.978,000 --> 0:02:57,000
Now, what would cause a hardened military officer

49
0:02:58.899,000 --> 0:03:,000
and someone like myself

50
0:03:00.966,000 --> 0:03:01,000
to have this response to robots?

51
0:03:03.537,000 --> 0:03:06,000
Well, of course, we're primed by science fiction and pop culture

52
0:03:06.871,000 --> 0:03:08,000
to really want to personify these things,

53
0:03:09.474,000 --> 0:03:11,000
but it goes a little bit deeper than that.

54
0:03:12.287,000 --> 0:03:17,000
It turns out that we're biologically hardwired to project intent and life

55
0:03:17.62,000 --> 0:03:21,000
onto any movement in our physical space that seems autonomous to us.

56
0:03:23.214,000 --> 0:03:26,000
So people will treat all sorts of robots like they're alive.

57
0:03:26.703,000 --> 0:03:28,000
These bomb-disposal units get names.

58
0:03:29.41,000 --> 0:03:3,000
They get medals of honor.

59
0:03:31.116,000 --> 0:03:33,000
They've had funerals for them with gun salutes.

60
0:03:34.38,000 --> 0:03:37,000
And research shows that we do this even with very simple household robots,

61
0:03:38.237,000 --> 0:03:4,000
like the Roomba vacuum cleaner.

62
0:03:40.396,000 --> 0:03:41,000
(Laughter)

63
0:03:41.711,000 --> 0:03:44,000
It's just a disc that roams around your floor to clean it,

64
0:03:44.824,000 --> 0:03:46,000
but just the fact it's moving around on its own

65
0:03:47.154,000 --> 0:03:49,000
will cause people to name the Roomba

66
0:03:49.345,000 --> 0:03:52,000
and feel bad for the Roomba when it gets stuck under the couch.

67
0:03:52.551,000 --> 0:03:53,000
(Laughter)

68
0:03:54.44,000 --> 0:03:57,000
And we can design robots specifically to evoke this response,

69
0:03:57.804,000 --> 0:04:,000
using eyes and faces or movements

70
0:04:01.289,000 --> 0:04:04,000
that people automatically, subconsciously associate

71
0:04:04.572,000 --> 0:04:06,000
with states of mind.

72
0:04:06.616,000 --> 0:04:09,000
And there's an entire body of research called human-robot interaction

73
0:04:09.933,000 --> 0:04:1,000
that really shows how well this works.

74
0:04:11.783,000 --> 0:04:14,000
So for example, researchers at Stanford University found out

75
0:04:14.933,000 --> 0:04:16,000
that it makes people really uncomfortable

76
0:04:16.958,000 --> 0:04:18,000
when you ask them to touch a robot's private parts.

77
0:04:19.454,000 --> 0:04:21,000
(Laughter)

78
0:04:21.598,000 --> 0:04:23,000
So from this, but from many other studies,

79
0:04:23.645,000 --> 0:04:27,000
we know, we know that people respond to the cues given to them

80
0:04:27.892,000 --> 0:04:29,000
by these lifelike machines,

81
0:04:29.938,000 --> 0:04:31,000
even if they know that they're not real.

82
0:04:33.654,000 --> 0:04:37,000
Now, we're headed towards a world where robots are everywhere.

83
0:04:37.734,000 --> 0:04:4,000
Robotic technology is moving out from behind factory walls.

84
0:04:40.823,000 --> 0:04:43,000
It's entering workplaces, households.

85
0:04:43.86,000 --> 0:04:49,000
And as these machines that can sense and make autonomous decisions and learn

86
0:04:50.093,000 --> 0:04:52,000
enter into these shared spaces,

87
0:04:52.669,000 --> 0:04:54,000
I think that maybe the best analogy we have for this

88
0:04:55.189,000 --> 0:04:56,000
is our relationship with animals.

89
0:04:57.523,000 --> 0:05:,000
Thousands of years ago, we started to domesticate animals,

90
0:05:01.435,000 --> 0:05:05,000
and we trained them for work and weaponry and companionship.

91
0:05:05.504,000 --> 0:05:09,000
And throughout history, we've treated some animals like tools or like products,

92
0:05:10.513,000 --> 0:05:12,000
and other animals, we've treated with kindness

93
0:05:12.711,000 --> 0:05:15,000
and we've given a place in society as our companions.

94
0:05:15.813,000 --> 0:05:18,000
I think it's plausible we might start to integrate robots in similar ways.

95
0:05:21.484,000 --> 0:05:24,000
And sure, animals are alive.

96
0:05:24.604,000 --> 0:05:25,000
Robots are not.

97
0:05:27.626,000 --> 0:05:29,000
And I can tell you, from working with roboticists,

98
0:05:30.23,000 --> 0:05:33,000
that we're pretty far away from developing robots that can feel anything.

99
0:05:35.072,000 --> 0:05:36,000
But we feel for them,

100
0:05:37.835,000 --> 0:05:38,000
and that matters,

101
0:05:39.066,000 --> 0:05:42,000
because if we're trying to integrate robots into these shared spaces,

102
0:05:42.717,000 --> 0:05:46,000
we need to understand that people will treat them differently than other devices,

103
0:05:47.369,000 --> 0:05:48,000
and that in some cases,

104
0:05:49.237,000 --> 0:05:52,000
for example, the case of a soldier who becomes emotionally attached

105
0:05:52.433,000 --> 0:05:54,000
to the robot that they work with,

106
0:05:54.504,000 --> 0:05:56,000
that can be anything from inefficient to dangerous.

107
0:05:58.551,000 --> 0:06:,000
But in other cases, it can actually be useful

108
0:06:00.713,000 --> 0:06:02,000
to foster this emotional connection to robots.

109
0:06:04.184,000 --> 0:06:06,000
We're already seeing some great use cases,

110
0:06:06.342,000 --> 0:06:08,000
for example, robots working with autistic children

111
0:06:08.97,000 --> 0:06:11,000
to engage them in ways that we haven't seen previously,

112
0:06:12.628,000 --> 0:06:16,000
or robots working with teachers to engage kids in learning with new results.

113
0:06:17.433,000 --> 0:06:18,000
And it's not just for kids.

114
0:06:19.75,000 --> 0:06:22,000
Early studies show that robots can help doctors and patients

115
0:06:22.997,000 --> 0:06:23,000
in health care settings.

116
0:06:25.535,000 --> 0:06:26,000
This is the PARO baby seal robot.

117
0:06:27.369,000 --> 0:06:3,000
It's used in nursing homes and with dementia patients.

118
0:06:30.678,000 --> 0:06:31,000
It's been around for a while.

119
0:06:32.272,000 --> 0:06:35,000
And I remember, years ago, being at a party

120
0:06:35.621,000 --> 0:06:37,000
and telling someone about this robot,

121
0:06:38.216,000 --> 0:06:4,000
and her response was,

122
0:06:40.366,000 --> 0:06:41,000
"Oh my gosh.

123
0:06:42.508,000 --> 0:06:43,000
That's horrible.

124
0:06:45.056,000 --> 0:06:48,000
I can't believe we're giving people robots instead of human care."

125
0:06:50.54,000 --> 0:06:51,000
And this is a really common response,

126
0:06:52.439,000 --> 0:06:54,000
and I think it's absolutely correct,

127
0:06:54.962,000 --> 0:06:56,000
because that would be terrible.

128
0:06:57.795,000 --> 0:06:59,000
But in this case, it's not what this robot replaces.

129
0:07:00.858,000 --> 0:07:03,000
What this robot replaces is animal therapy

130
0:07:04.002,000 --> 0:07:07,000
in contexts where we can't use real animals

131
0:07:07.224,000 --> 0:07:08,000
but we can use robots,

132
0:07:08.416,000 --> 0:07:13,000
because people will consistently treat them more like an animal than a device.

133
0:07:15.502,000 --> 0:07:17,000
Acknowledging this emotional connection to robots

134
0:07:17.906,000 --> 0:07:18,000
can also help us anticipate challenges

135
0:07:19.899,000 --> 0:07:22,000
as these devices move into more intimate areas of people's lives.

136
0:07:24.111,000 --> 0:07:27,000
For example, is it OK if your child's teddy bear robot

137
0:07:27.539,000 --> 0:07:29,000
records private conversations?

138
0:07:29.8,000 --> 0:07:33,000
Is it OK if your sex robot has compelling in-app purchases?

139
0:07:33.887,000 --> 0:07:34,000
(Laughter)

140
0:07:35.307,000 --> 0:07:37,000
Because robots plus capitalism

141
0:07:37.832,000 --> 0:07:4,000
equals questions around consumer protection and privacy.

142
0:07:42.549,000 --> 0:07:43,000
And those aren't the only reasons

143
0:07:44.185,000 --> 0:07:46,000
that our behavior around these machines could matter.

144
0:07:48.747,000 --> 0:07:51,000
A few years after that first initial experience I had

145
0:07:52.041,000 --> 0:07:54,000
with this baby dinosaur robot,

146
0:07:54.376,000 --> 0:07:56,000
I did a workshop with my friend Hannes Gassert.

147
0:07:56.901,000 --> 0:07:58,000
And we took five of these baby dinosaur robots

148
0:07:59.822,000 --> 0:08:01,000
and we gave them to five teams of people.

149
0:08:02.299,000 --> 0:08:03,000
And we had them name them

150
0:08:04.02,000 --> 0:08:07,000
and play with them and interact with them for about an hour.

151
0:08:08.707,000 --> 0:08:1,000
And then we unveiled a hammer and a hatchet

152
0:08:10.937,000 --> 0:08:12,000
and we told them to torture and kill the robots.

153
0:08:13.239,000 --> 0:08:16,000
(Laughter)

154
0:08:16.857,000 --> 0:08:18,000
And this turned out to be a little more dramatic

155
0:08:19.175,000 --> 0:08:2,000
than we expected it to be,

156
0:08:20.477,000 --> 0:08:23,000
because none of the participants would even so much as strike

157
0:08:23.573,000 --> 0:08:24,000
these baby dinosaur robots,

158
0:08:24.904,000 --> 0:08:29,000
so we had to improvise a little, and at some point, we said,

159
0:08:30.078,000 --> 0:08:34,000
"OK, you can save your team's robot if you destroy another team's robot."

160
0:08:34.539,000 --> 0:08:35,000
(Laughter)

161
0:08:36.839,000 --> 0:08:38,000
And even that didn't work. They couldn't do it.

162
0:08:39.058,000 --> 0:08:4,000
So finally, we said,

163
0:08:40.233,000 --> 0:08:42,000
"We're going to destroy all of the robots

164
0:08:42.289,000 --> 0:08:44,000
unless someone takes a hatchet to one of them."

165
0:08:45.586,000 --> 0:08:48,000
And this guy stood up, and he took the hatchet,

166
0:08:49.189,000 --> 0:08:51,000
and the whole room winced as he brought the hatchet down

167
0:08:51.919,000 --> 0:08:52,000
on the robot's neck,

168
0:08:53.723,000 --> 0:08:59,000
and there was this half-joking, half-serious moment of silence in the room

169
0:09:00.085,000 --> 0:09:01,000
for this fallen robot.

170
0:09:01.807,000 --> 0:09:02,000
(Laughter)

171
0:09:03.237,000 --> 0:09:06,000
So that was a really interesting experience.

172
0:09:06.955,000 --> 0:09:08,000
Now, it wasn't a controlled study, obviously,

173
0:09:09.438,000 --> 0:09:11,000
but it did lead to some later research that I did at MIT

174
0:09:12.312,000 --> 0:09:14,000
with Palash Nandy and Cynthia Breazeal,

175
0:09:14.564,000 --> 0:09:17,000
where we had people come into the lab and smash these HEXBUGs

176
0:09:18.215,000 --> 0:09:21,000
that move around in a really lifelike way, like insects.

177
0:09:21.326,000 --> 0:09:24,000
So instead of choosing something cute that people are drawn to,

178
0:09:24.484,000 --> 0:09:26,000
we chose something more basic,

179
0:09:26.601,000 --> 0:09:29,000
and what we found was that high-empathy people

180
0:09:30.105,000 --> 0:09:32,000
would hesitate more to hit the HEXBUGS.

181
0:09:33.575,000 --> 0:09:34,000
Now this is just a little study,

182
0:09:35.163,000 --> 0:09:37,000
but it's part of a larger body of research

183
0:09:37.576,000 --> 0:09:39,000
that is starting to indicate that there may be a connection

184
0:09:40.544,000 --> 0:09:42,000
between people's tendencies for empathy

185
0:09:42.941,000 --> 0:09:43,000
and their behavior around robots.

186
0:09:45.721,000 --> 0:09:48,000
But my question for the coming era of human-robot interaction

187
0:09:49.372,000 --> 0:09:52,000
is not: "Do we empathize with robots?"

188
0:09:53.211,000 --> 0:09:55,000
It's: "Can robots change people's empathy?"

189
0:09:57.489,000 --> 0:09:59,000
Is there reason to, for example,

190
0:09:59.8,000 --> 0:10:01,000
prevent your child from kicking a robotic dog,

191
0:10:03.228,000 --> 0:10:05,000
not just out of respect for property,

192
0:10:06.166,000 --> 0:10:08,000
but because the child might be more likely to kick a real dog?

193
0:10:10.507,000 --> 0:10:11,000
And again, it's not just kids.

194
0:10:13.564,000 --> 0:10:17,000
This is the violent video games question, but it's on a completely new level

195
0:10:17.644,000 --> 0:10:21,000
because of this visceral physicality that we respond more intensely to

196
0:10:22.428,000 --> 0:10:23,000
than to images on a screen.

197
0:10:25.674,000 --> 0:10:27,000
When we behave violently towards robots,

198
0:10:28.276,000 --> 0:10:31,000
specifically robots that are designed to mimic life,

199
0:10:31.42,000 --> 0:10:34,000
is that a healthy outlet for violent behavior

200
0:10:35.336,000 --> 0:10:37,000
or is that training our cruelty muscles?

201
0:10:39.511,000 --> 0:10:4,000
We don't know ...

202
0:10:42.622,000 --> 0:10:45,000
But the answer to this question has the potential to impact human behavior,

203
0:10:46.591,000 --> 0:10:48,000
it has the potential to impact social norms,

204
0:10:49.383,000 --> 0:10:52,000
it has the potential to inspire rules around what we can and can't do

205
0:10:53.256,000 --> 0:10:54,000
with certain robots,

206
0:10:54.431,000 --> 0:10:55,000
similar to our animal cruelty laws.

207
0:10:57.228,000 --> 0:10:59,000
Because even if robots can't feel,

208
0:11:00.116,000 --> 0:11:03,000
our behavior towards them might matter for us.

209
0:11:04.889,000 --> 0:11:06,000
And regardless of whether we end up changing our rules,

210
0:11:08.926,000 --> 0:11:11,000
robots might be able to help us come to a new understanding of ourselves.

211
0:11:14.276,000 --> 0:11:16,000
Most of what I've learned over the past 10 years

212
0:11:16.616,000 --> 0:11:18,000
has not been about technology at all.

213
0:11:18.878,000 --> 0:11:2,000
It's been about human psychology

214
0:11:21.405,000 --> 0:11:23,000
and empathy and how we relate to others.

215
0:11:25.524,000 --> 0:11:27,000
Because when a child is kind to a Roomba,

216
0:11:29.262,000 --> 0:11:33,000
when a soldier tries to save a robot on the battlefield,

217
0:11:33.301,000 --> 0:11:36,000
or when a group of people refuses to harm a robotic baby dinosaur,

218
0:11:38.248,000 --> 0:11:41,000
those robots aren't just motors and gears and algorithms.

219
0:11:42.501,000 --> 0:11:43,000
They're reflections of our own humanity.

220
0:11:45.523,000 --> 0:11:46,000
Thank you.

221
0:11:46.698,000 --> 0:11:49,000
(Applause)

