1
0:00:12.739,000 --> 0:00:16,000
So, I started my first job as a computer programmer

2
0:00:16.885,000 --> 0:00:17,000
in my very first year of college --

3
0:00:18.865,000 --> 0:00:19,000
basically, as a teenager.

4
0:00:20.889,000 --> 0:00:21,000
Soon after I started working,

5
0:00:22.645,000 --> 0:00:23,000
writing software in a company,

6
0:00:24.799,000 --> 0:00:27,000
a manager who worked at the company came down to where I was,

7
0:00:28.458,000 --> 0:00:29,000
and he whispered to me,

8
0:00:30.229,000 --> 0:00:32,000
"Can he tell if I'm lying?"

9
0:00:33.806,000 --> 0:00:35,000
There was nobody else in the room.

10
0:00:37.032,000 --> 0:00:41,000
"Can who tell if you're lying? And why are we whispering?"

11
0:00:42.266,000 --> 0:00:45,000
The manager pointed at the computer in the room.

12
0:00:45.397,000 --> 0:00:48,000
"Can he tell if I'm lying?"

13
0:00:49.613,000 --> 0:00:53,000
Well, that manager was having an affair with the receptionist.

14
0:00:53.999,000 --> 0:00:54,000
(Laughter)

15
0:00:55.135,000 --> 0:00:56,000
And I was still a teenager.

16
0:00:57.447,000 --> 0:00:59,000
So I whisper-shouted back to him,

17
0:00:59.49,000 --> 0:01:02,000
"Yes, the computer can tell if you're lying."

18
0:01:03.138,000 --> 0:01:04,000
(Laughter)

19
0:01:04.968,000 --> 0:01:06,000
Well, I laughed, but actually, the laugh's on me.

20
0:01:07.915,000 --> 0:01:1,000
Nowadays, there are computational systems

21
0:01:11.207,000 --> 0:01:14,000
that can suss out emotional states and even lying

22
0:01:14.779,000 --> 0:01:16,000
from processing human faces.

23
0:01:17.248,000 --> 0:01:21,000
Advertisers and even governments are very interested.

24
0:01:22.319,000 --> 0:01:23,000
I had become a computer programmer

25
0:01:24.205,000 --> 0:01:27,000
because I was one of those kids crazy about math and science.

26
0:01:27.942,000 --> 0:01:3,000
But somewhere along the line I'd learned about nuclear weapons,

27
0:01:31.074,000 --> 0:01:33,000
and I'd gotten really concerned with the ethics of science.

28
0:01:34.05,000 --> 0:01:35,000
I was troubled.

29
0:01:35.278,000 --> 0:01:37,000
However, because of family circumstances,

30
0:01:37.943,000 --> 0:01:4,000
I also needed to start working as soon as possible.

31
0:01:41.265,000 --> 0:01:44,000
So I thought to myself, hey, let me pick a technical field

32
0:01:44.588,000 --> 0:01:45,000
where I can get a job easily

33
0:01:46.408,000 --> 0:01:5,000
and where I don't have to deal with any troublesome questions of ethics.

34
0:01:51.022,000 --> 0:01:52,000
So I picked computers.

35
0:01:52.575,000 --> 0:01:53,000
(Laughter)

36
0:01:53.703,000 --> 0:01:56,000
Well, ha, ha, ha! All the laughs are on me.

37
0:01:57.137,000 --> 0:01:59,000
Nowadays, computer scientists are building platforms

38
0:01:59.915,000 --> 0:02:03,000
that control what a billion people see every day.

39
0:02:05.052,000 --> 0:02:08,000
They're developing cars that could decide who to run over.

40
0:02:09.707,000 --> 0:02:12,000
They're even building machines, weapons,

41
0:02:12.944,000 --> 0:02:14,000
that might kill human beings in war.

42
0:02:15.253,000 --> 0:02:17,000
It's ethics all the way down.

43
0:02:19.183,000 --> 0:02:21,000
Machine intelligence is here.

44
0:02:21.823,000 --> 0:02:24,000
We're now using computation to make all sort of decisions,

45
0:02:25.321,000 --> 0:02:26,000
but also new kinds of decisions.

46
0:02:27.231,000 --> 0:02:32,000
We're asking questions to computation that have no single right answers,

47
0:02:32.427,000 --> 0:02:33,000
that are subjective

48
0:02:33.653,000 --> 0:02:35,000
and open-ended and value-laden.

49
0:02:36.002,000 --> 0:02:37,000
We're asking questions like,

50
0:02:37.784,000 --> 0:02:38,000
"Who should the company hire?"

51
0:02:40.096,000 --> 0:02:42,000
"Which update from which friend should you be shown?"

52
0:02:42.879,000 --> 0:02:44,000
"Which convict is more likely to reoffend?"

53
0:02:45.514,000 --> 0:02:48,000
"Which news item or movie should be recommended to people?"

54
0:02:48.592,000 --> 0:02:51,000
Look, yes, we've been using computers for a while,

55
0:02:51.988,000 --> 0:02:52,000
but this is different.

56
0:02:53.529,000 --> 0:02:55,000
This is a historical twist,

57
0:02:55.62,000 --> 0:03:,000
because we cannot anchor computation for such subjective decisions

58
0:03:00.981,000 --> 0:03:05,000
the way we can anchor computation for flying airplanes, building bridges,

59
0:03:06.425,000 --> 0:03:07,000
going to the moon.

60
0:03:08.449,000 --> 0:03:11,000
Are airplanes safer? Did the bridge sway and fall?

61
0:03:11.732,000 --> 0:03:15,000
There, we have agreed-upon, fairly clear benchmarks,

62
0:03:16.254,000 --> 0:03:18,000
and we have laws of nature to guide us.

63
0:03:18.517,000 --> 0:03:21,000
We have no such anchors and benchmarks

64
0:03:21.935,000 --> 0:03:24,000
for decisions in messy human affairs.

65
0:03:25.922,000 --> 0:03:29,000
To make things more complicated, our software is getting more powerful,

66
0:03:30.183,000 --> 0:03:33,000
but it's also getting less transparent and more complex.

67
0:03:34.542,000 --> 0:03:36,000
Recently, in the past decade,

68
0:03:36.606,000 --> 0:03:38,000
complex algorithms have made great strides.

69
0:03:39.359,000 --> 0:03:4,000
They can recognize human faces.

70
0:03:41.985,000 --> 0:03:43,000
They can decipher handwriting.

71
0:03:44.436,000 --> 0:03:46,000
They can detect credit card fraud

72
0:03:46.526,000 --> 0:03:47,000
and block spam

73
0:03:47.739,000 --> 0:03:49,000
and they can translate between languages.

74
0:03:49.8,000 --> 0:03:51,000
They can detect tumors in medical imaging.

75
0:03:52.398,000 --> 0:03:54,000
They can beat humans in chess and Go.

76
0:03:55.264,000 --> 0:03:59,000
Much of this progress comes from a method called "machine learning."

77
0:04:00.175,000 --> 0:04:03,000
Machine learning is different than traditional programming,

78
0:04:03.386,000 --> 0:04:06,000
where you give the computer detailed, exact, painstaking instructions.

79
0:04:07.378,000 --> 0:04:11,000
It's more like you take the system and you feed it lots of data,

80
0:04:11.584,000 --> 0:04:12,000
including unstructured data,

81
0:04:13.264,000 --> 0:04:15,000
like the kind we generate in our digital lives.

82
0:04:15.566,000 --> 0:04:17,000
And the system learns by churning through this data.

83
0:04:18.669,000 --> 0:04:19,000
And also, crucially,

84
0:04:20.219,000 --> 0:04:24,000
these systems don't operate under a single-answer logic.

85
0:04:24.623,000 --> 0:04:26,000
They don't produce a simple answer; it's more probabilistic:

86
0:04:27.606,000 --> 0:04:3,000
"This one is probably more like what you're looking for."

87
0:04:32.023,000 --> 0:04:35,000
Now, the upside is: this method is really powerful.

88
0:04:35.117,000 --> 0:04:37,000
The head of Google's AI systems called it,

89
0:04:37.217,000 --> 0:04:39,000
"the unreasonable effectiveness of data."

90
0:04:39.791,000 --> 0:04:4,000
The downside is,

91
0:04:41.738,000 --> 0:04:44,000
we don't really understand what the system learned.

92
0:04:44.833,000 --> 0:04:45,000
In fact, that's its power.

93
0:04:46.946,000 --> 0:04:49,000
This is less like giving instructions to a computer;

94
0:04:51.2,000 --> 0:04:55,000
it's more like training a puppy-machine-creature

95
0:04:55.288,000 --> 0:04:57,000
we don't really understand or control.

96
0:04:58.362,000 --> 0:04:59,000
So this is our problem.

97
0:05:00.427,000 --> 0:05:04,000
It's a problem when this artificial intelligence system gets things wrong.

98
0:05:04.713,000 --> 0:05:07,000
It's also a problem when it gets things right,

99
0:05:08.277,000 --> 0:05:11,000
because we don't even know which is which when it's a subjective problem.

100
0:05:11.929,000 --> 0:05:13,000
We don't know what this thing is thinking.

101
0:05:15.493,000 --> 0:05:18,000
So, consider a hiring algorithm --

102
0:05:20.123,000 --> 0:05:24,000
a system used to hire people, using machine-learning systems.

103
0:05:25.052,000 --> 0:05:28,000
Such a system would have been trained on previous employees' data

104
0:05:28.655,000 --> 0:05:3,000
and instructed to find and hire

105
0:05:31.27,000 --> 0:05:34,000
people like the existing high performers in the company.

106
0:05:34.814,000 --> 0:05:35,000
Sounds good.

107
0:05:35.991,000 --> 0:05:36,000
I once attended a conference

108
0:05:38.014,000 --> 0:05:41,000
that brought together human resources managers and executives,

109
0:05:41.163,000 --> 0:05:42,000
high-level people,

110
0:05:42.393,000 --> 0:05:43,000
using such systems in hiring.

111
0:05:43.976,000 --> 0:05:44,000
They were super excited.

112
0:05:45.646,000 --> 0:05:49,000
They thought that this would make hiring more objective, less biased,

113
0:05:50.323,000 --> 0:05:53,000
and give women and minorities a better shot

114
0:05:53.347,000 --> 0:05:55,000
against biased human managers.

115
0:05:55.559,000 --> 0:05:57,000
And look -- human hiring is biased.

116
0:05:59.099,000 --> 0:06:,000
I know.

117
0:06:00.308,000 --> 0:06:03,000
I mean, in one of my early jobs as a programmer,

118
0:06:03.337,000 --> 0:06:06,000
my immediate manager would sometimes come down to where I was

119
0:06:07.229,000 --> 0:06:1,000
really early in the morning or really late in the afternoon,

120
0:06:11.006,000 --> 0:06:14,000
and she'd say, "Zeynep, let's go to lunch!"

121
0:06:14.724,000 --> 0:06:16,000
I'd be puzzled by the weird timing.

122
0:06:16.915,000 --> 0:06:18,000
It's 4pm. Lunch?

123
0:06:19.068,000 --> 0:06:22,000
I was broke, so free lunch. I always went.

124
0:06:22.618,000 --> 0:06:24,000
I later realized what was happening.

125
0:06:24.709,000 --> 0:06:28,000
My immediate managers had not confessed to their higher-ups

126
0:06:29.279,000 --> 0:06:32,000
that the programmer they hired for a serious job was a teen girl

127
0:06:32.416,000 --> 0:06:35,000
who wore jeans and sneakers to work.

128
0:06:37.174,000 --> 0:06:39,000
I was doing a good job, I just looked wrong

129
0:06:39.4,000 --> 0:06:4,000
and was the wrong age and gender.

130
0:06:41.123,000 --> 0:06:44,000
So hiring in a gender- and race-blind way

131
0:06:44.493,000 --> 0:06:45,000
certainly sounds good to me.

132
0:06:47.031,000 --> 0:06:5,000
But with these systems, it is more complicated, and here's why:

133
0:06:50.968,000 --> 0:06:55,000
Currently, computational systems can infer all sorts of things about you

134
0:06:56.783,000 --> 0:06:57,000
from your digital crumbs,

135
0:06:58.679,000 --> 0:07:,000
even if you have not disclosed those things.

136
0:07:01.506,000 --> 0:07:03,000
They can infer your sexual orientation,

137
0:07:04.994,000 --> 0:07:05,000
your personality traits,

138
0:07:06.859,000 --> 0:07:07,000
your political leanings.

139
0:07:08.83,000 --> 0:07:11,000
They have predictive power with high levels of accuracy.

140
0:07:13.362,000 --> 0:07:15,000
Remember -- for things you haven't even disclosed.

141
0:07:15.964,000 --> 0:07:16,000
This is inference.

142
0:07:17.579,000 --> 0:07:2,000
I have a friend who developed such computational systems

143
0:07:20.864,000 --> 0:07:23,000
to predict the likelihood of clinical or postpartum depression

144
0:07:24.529,000 --> 0:07:25,000
from social media data.

145
0:07:26.676,000 --> 0:07:27,000
The results are impressive.

146
0:07:28.492,000 --> 0:07:31,000
Her system can predict the likelihood of depression

147
0:07:31.873,000 --> 0:07:34,000
months before the onset of any symptoms --

148
0:07:35.8,000 --> 0:07:36,000
months before.

149
0:07:37.197,000 --> 0:07:39,000
No symptoms, there's prediction.

150
0:07:39.467,000 --> 0:07:43,000
She hopes it will be used for early intervention. Great!

151
0:07:44.911,000 --> 0:07:46,000
But now put this in the context of hiring.

152
0:07:48.027,000 --> 0:07:51,000
So at this human resources managers conference,

153
0:07:51.097,000 --> 0:07:55,000
I approached a high-level manager in a very large company,

154
0:07:55.83,000 --> 0:07:59,000
and I said to her, "Look, what if, unbeknownst to you,

155
0:08:00.432,000 --> 0:08:06,000
your system is weeding out people with high future likelihood of depression?

156
0:08:07.761,000 --> 0:08:1,000
They're not depressed now, just maybe in the future, more likely.

157
0:08:11.923,000 --> 0:08:14,000
What if it's weeding out women more likely to be pregnant

158
0:08:15.353,000 --> 0:08:17,000
in the next year or two but aren't pregnant now?

159
0:08:18.844,000 --> 0:08:23,000
What if it's hiring aggressive people because that's your workplace culture?"

160
0:08:25.173,000 --> 0:08:27,000
You can't tell this by looking at gender breakdowns.

161
0:08:27.888,000 --> 0:08:28,000
Those may be balanced.

162
0:08:29.414,000 --> 0:08:32,000
And since this is machine learning, not traditional coding,

163
0:08:32.995,000 --> 0:08:36,000
there is no variable there labeled "higher risk of depression,"

164
0:08:37.926,000 --> 0:08:38,000
"higher risk of pregnancy,"

165
0:08:39.783,000 --> 0:08:4,000
"aggressive guy scale."

166
0:08:41.995,000 --> 0:08:44,000
Not only do you not know what your system is selecting on,

167
0:08:45.698,000 --> 0:08:47,000
you don't even know where to begin to look.

168
0:08:48.045,000 --> 0:08:49,000
It's a black box.

169
0:08:49.315,000 --> 0:08:51,000
It has predictive power, but you don't understand it.

170
0:08:52.486,000 --> 0:08:54,000
"What safeguards," I asked, "do you have

171
0:08:54.879,000 --> 0:08:57,000
to make sure that your black box isn't doing something shady?"

172
0:09:00.863,000 --> 0:09:03,000
She looked at me as if I had just stepped on 10 puppy tails.

173
0:09:04.765,000 --> 0:09:05,000
(Laughter)

174
0:09:06.037,000 --> 0:09:08,000
She stared at me and she said,

175
0:09:08.556,000 --> 0:09:12,000
"I don't want to hear another word about this."

176
0:09:13.458,000 --> 0:09:15,000
And she turned around and walked away.

177
0:09:16.064,000 --> 0:09:17,000
Mind you -- she wasn't rude.

178
0:09:17.574,000 --> 0:09:23,000
It was clearly: what I don't know isn't my problem, go away, death stare.

179
0:09:23.906,000 --> 0:09:24,000
(Laughter)

180
0:09:25.862,000 --> 0:09:28,000
Look, such a system may even be less biased

181
0:09:29.725,000 --> 0:09:31,000
than human managers in some ways.

182
0:09:31.852,000 --> 0:09:33,000
And it could make monetary sense.

183
0:09:34.573,000 --> 0:09:35,000
But it could also lead

184
0:09:36.247,000 --> 0:09:4,000
to a steady but stealthy shutting out of the job market

185
0:09:41.019,000 --> 0:09:43,000
of people with higher risk of depression.

186
0:09:43.753,000 --> 0:09:45,000
Is this the kind of society we want to build,

187
0:09:46.373,000 --> 0:09:48,000
without even knowing we've done this,

188
0:09:48.682,000 --> 0:09:51,000
because we turned decision-making to machines we don't totally understand?

189
0:09:53.265,000 --> 0:09:54,000
Another problem is this:

190
0:09:55.314,000 --> 0:09:59,000
these systems are often trained on data generated by our actions,

191
0:09:59.79,000 --> 0:10:,000
human imprints.

192
0:10:02.188,000 --> 0:10:05,000
Well, they could just be reflecting our biases,

193
0:10:06.02,000 --> 0:10:09,000
and these systems could be picking up on our biases

194
0:10:09.637,000 --> 0:10:1,000
and amplifying them

195
0:10:10.974,000 --> 0:10:11,000
and showing them back to us,

196
0:10:12.416,000 --> 0:10:13,000
while we're telling ourselves,

197
0:10:13.902,000 --> 0:10:16,000
"We're just doing objective, neutral computation."

198
0:10:18.314,000 --> 0:10:2,000
Researchers found that on Google,

199
0:10:22.134,000 --> 0:10:27,000
women are less likely than men to be shown job ads for high-paying jobs.

200
0:10:28.463,000 --> 0:10:3,000
And searching for African-American names

201
0:10:31.017,000 --> 0:10:35,000
is more likely to bring up ads suggesting criminal history,

202
0:10:35.747,000 --> 0:10:36,000
even when there is none.

203
0:10:38.693,000 --> 0:10:41,000
Such hidden biases and black-box algorithms

204
0:10:42.266,000 --> 0:10:45,000
that researchers uncover sometimes but sometimes we don't know,

205
0:10:46.263,000 --> 0:10:48,000
can have life-altering consequences.

206
0:10:49.958,000 --> 0:10:53,000
In Wisconsin, a defendant was sentenced to six years in prison

207
0:10:54.141,000 --> 0:10:55,000
for evading the police.

208
0:10:56.824,000 --> 0:10:57,000
You may not know this,

209
0:10:58.034,000 --> 0:11:01,000
but algorithms are increasingly used in parole and sentencing decisions.

210
0:11:02.056,000 --> 0:11:04,000
He wanted to know: How is this score calculated?

211
0:11:05.795,000 --> 0:11:06,000
It's a commercial black box.

212
0:11:07.484,000 --> 0:11:11,000
The company refused to have its algorithm be challenged in open court.

213
0:11:12.396,000 --> 0:11:17,000
But ProPublica, an investigative nonprofit, audited that very algorithm

214
0:11:17.952,000 --> 0:11:19,000
with what public data they could find,

215
0:11:19.992,000 --> 0:11:21,000
and found that its outcomes were biased

216
0:11:22.332,000 --> 0:11:25,000
and its predictive power was dismal, barely better than chance,

217
0:11:25.985,000 --> 0:11:29,000
and it was wrongly labeling black defendants as future criminals

218
0:11:30.425,000 --> 0:11:33,000
at twice the rate of white defendants.

219
0:11:35.891,000 --> 0:11:36,000
So, consider this case:

220
0:11:38.103,000 --> 0:11:41,000
This woman was late picking up her godsister

221
0:11:41.979,000 --> 0:11:43,000
from a school in Broward County, Florida,

222
0:11:44.757,000 --> 0:11:46,000
running down the street with a friend of hers.

223
0:11:47.137,000 --> 0:11:51,000
They spotted an unlocked kid's bike and a scooter on a porch

224
0:11:51.26,000 --> 0:11:52,000
and foolishly jumped on it.

225
0:11:52.916,000 --> 0:11:54,000
As they were speeding off, a woman came out and said,

226
0:11:55.539,000 --> 0:11:57,000
"Hey! That's my kid's bike!"

227
0:11:57.768,000 --> 0:12:,000
They dropped it, they walked away, but they were arrested.

228
0:12:01.086,000 --> 0:12:04,000
She was wrong, she was foolish, but she was also just 18.

229
0:12:04.747,000 --> 0:12:06,000
She had a couple of juvenile misdemeanors.

230
0:12:07.808,000 --> 0:12:12,000
Meanwhile, that man had been arrested for shoplifting in Home Depot --

231
0:12:13.017,000 --> 0:12:15,000
85 dollars' worth of stuff, a similar petty crime.

232
0:12:16.766,000 --> 0:12:2,000
But he had two prior armed robbery convictions.

233
0:12:21.955,000 --> 0:12:24,000
But the algorithm scored her as high risk, and not him.

234
0:12:26.746,000 --> 0:12:29,000
Two years later, ProPublica found that she had not reoffended.

235
0:12:30.644,000 --> 0:12:32,000
It was just hard to get a job for her with her record.

236
0:12:33.218,000 --> 0:12:35,000
He, on the other hand, did reoffend

237
0:12:35.318,000 --> 0:12:38,000
and is now serving an eight-year prison term for a later crime.

238
0:12:40.088,000 --> 0:12:43,000
Clearly, we need to audit our black boxes

239
0:12:43.481,000 --> 0:12:45,000
and not have them have this kind of unchecked power.

240
0:12:46.12,000 --> 0:12:48,000
(Applause)

241
0:12:50.087,000 --> 0:12:54,000
Audits are great and important, but they don't solve all our problems.

242
0:12:54.353,000 --> 0:12:56,000
Take Facebook's powerful news feed algorithm --

243
0:12:57.125,000 --> 0:13:01,000
you know, the one that ranks everything and decides what to show you

244
0:13:01.992,000 --> 0:13:03,000
from all the friends and pages you follow.

245
0:13:04.898,000 --> 0:13:06,000
Should you be shown another baby picture?

246
0:13:07.197,000 --> 0:13:08,000
(Laughter)

247
0:13:08.417,000 --> 0:13:1,000
A sullen note from an acquaintance?

248
0:13:11.449,000 --> 0:13:12,000
An important but difficult news item?

249
0:13:13.329,000 --> 0:13:14,000
There's no right answer.

250
0:13:14.835,000 --> 0:13:16,000
Facebook optimizes for engagement on the site:

251
0:13:17.518,000 --> 0:13:18,000
likes, shares, comments.

252
0:13:20.168,000 --> 0:13:22,000
In August of 2014,

253
0:13:22.888,000 --> 0:13:24,000
protests broke out in Ferguson, Missouri,

254
0:13:25.574,000 --> 0:13:29,000
after the killing of an African-American teenager by a white police officer,

255
0:13:30.015,000 --> 0:13:31,000
under murky circumstances.

256
0:13:31.974,000 --> 0:13:33,000
The news of the protests was all over

257
0:13:34.005,000 --> 0:13:36,000
my algorithmically unfiltered Twitter feed,

258
0:13:36.714,000 --> 0:13:37,000
but nowhere on my Facebook.

259
0:13:39.182,000 --> 0:13:4,000
Was it my Facebook friends?

260
0:13:40.94,000 --> 0:13:42,000
I disabled Facebook's algorithm,

261
0:13:43.472,000 --> 0:13:45,000
which is hard because Facebook keeps wanting to make you

262
0:13:46.344,000 --> 0:13:48,000
come under the algorithm's control,

263
0:13:48.404,000 --> 0:13:5,000
and saw that my friends were talking about it.

264
0:13:50.666,000 --> 0:13:52,000
It's just that the algorithm wasn't showing it to me.

265
0:13:53.199,000 --> 0:13:56,000
I researched this and found this was a widespread problem.

266
0:13:56.265,000 --> 0:13:59,000
The story of Ferguson wasn't algorithm-friendly.

267
0:14:00.102,000 --> 0:14:01,000
It's not "likable."

268
0:14:01.297,000 --> 0:14:02,000
Who's going to click on "like?"

269
0:14:03.5,000 --> 0:14:05,000
It's not even easy to comment on.

270
0:14:05.73,000 --> 0:14:06,000
Without likes and comments,

271
0:14:07.125,000 --> 0:14:1,000
the algorithm was likely showing it to even fewer people,

272
0:14:10.441,000 --> 0:14:11,000
so we didn't get to see this.

273
0:14:12.946,000 --> 0:14:13,000
Instead, that week,

274
0:14:14.198,000 --> 0:14:16,000
Facebook's algorithm highlighted this,

275
0:14:16.52,000 --> 0:14:18,000
which is the ALS Ice Bucket Challenge.

276
0:14:18.77,000 --> 0:14:21,000
Worthy cause; dump ice water, donate to charity, fine.

277
0:14:22.536,000 --> 0:14:23,000
But it was super algorithm-friendly.

278
0:14:25.219,000 --> 0:14:27,000
The machine made this decision for us.

279
0:14:27.856,000 --> 0:14:3,000
A very important but difficult conversation

280
0:14:31.377,000 --> 0:14:32,000
might have been smothered,

281
0:14:32.956,000 --> 0:14:34,000
had Facebook been the only channel.

282
0:14:36.117,000 --> 0:14:39,000
Now, finally, these systems can also be wrong

283
0:14:39.938,000 --> 0:14:41,000
in ways that don't resemble human systems.

284
0:14:42.698,000 --> 0:14:44,000
Do you guys remember Watson, IBM's machine-intelligence system

285
0:14:45.644,000 --> 0:14:48,000
that wiped the floor with human contestants on Jeopardy?

286
0:14:49.131,000 --> 0:14:5,000
It was a great player.

287
0:14:50.583,000 --> 0:14:53,000
But then, for Final Jeopardy, Watson was asked this question:

288
0:14:54.659,000 --> 0:14:56,000
"Its largest airport is named for a World War II hero,

289
0:14:57.615,000 --> 0:14:59,000
its second-largest for a World War II battle."

290
0:14:59.891,000 --> 0:15:,000
(Hums Final Jeopardy music)

291
0:15:01.582,000 --> 0:15:02,000
Chicago.

292
0:15:02.788,000 --> 0:15:03,000
The two humans got it right.

293
0:15:04.697,000 --> 0:15:08,000
Watson, on the other hand, answered "Toronto" --

294
0:15:09.069,000 --> 0:15:1,000
for a US city category!

295
0:15:11.596,000 --> 0:15:13,000
The impressive system also made an error

296
0:15:14.521,000 --> 0:15:17,000
that a human would never make, a second-grader wouldn't make.

297
0:15:18.823,000 --> 0:15:21,000
Our machine intelligence can fail

298
0:15:21.956,000 --> 0:15:24,000
in ways that don't fit error patterns of humans,

299
0:15:25.08,000 --> 0:15:27,000
in ways we won't expect and be prepared for.

300
0:15:28.054,000 --> 0:15:31,000
It'd be lousy not to get a job one is qualified for,

301
0:15:31.716,000 --> 0:15:34,000
but it would triple suck if it was because of stack overflow

302
0:15:35.467,000 --> 0:15:36,000
in some subroutine.

303
0:15:36.923,000 --> 0:15:37,000
(Laughter)

304
0:15:38.526,000 --> 0:15:4,000
In May of 2010,

305
0:15:41.336,000 --> 0:15:45,000
a flash crash on Wall Street fueled by a feedback loop

306
0:15:45.404,000 --> 0:15:48,000
in Wall Street's "sell" algorithm

307
0:15:48.456,000 --> 0:15:52,000
wiped a trillion dollars of value in 36 minutes.

308
0:15:53.722,000 --> 0:15:55,000
I don't even want to think what "error" means

309
0:15:55.933,000 --> 0:15:58,000
in the context of lethal autonomous weapons.

310
0:16:01.894,000 --> 0:16:04,000
So yes, humans have always made biases.

311
0:16:05.708,000 --> 0:16:07,000
Decision makers and gatekeepers,

312
0:16:07.908,000 --> 0:16:1,000
in courts, in news, in war ...

313
0:16:11.425,000 --> 0:16:14,000
they make mistakes; but that's exactly my point.

314
0:16:14.487,000 --> 0:16:17,000
We cannot escape these difficult questions.

315
0:16:18.596,000 --> 0:16:21,000
We cannot outsource our responsibilities to machines.

316
0:16:22.676,000 --> 0:16:26,000
(Applause)

317
0:16:29.089,000 --> 0:16:33,000
Artificial intelligence does not give us a "Get out of ethics free" card.

318
0:16:34.742,000 --> 0:16:37,000
Data scientist Fred Benenson calls this math-washing.

319
0:16:38.147,000 --> 0:16:39,000
We need the opposite.

320
0:16:39.56,000 --> 0:16:44,000
We need to cultivate algorithm suspicion, scrutiny and investigation.

321
0:16:45.38,000 --> 0:16:48,000
We need to make sure we have algorithmic accountability,

322
0:16:48.602,000 --> 0:16:5,000
auditing and meaningful transparency.

323
0:16:51.38,000 --> 0:16:54,000
We need to accept that bringing math and computation

324
0:16:54.638,000 --> 0:16:56,000
to messy, value-laden human affairs

325
0:16:57.632,000 --> 0:16:59,000
does not bring objectivity;

326
0:17:00.04,000 --> 0:17:03,000
rather, the complexity of human affairs invades the algorithms.

327
0:17:04.148,000 --> 0:17:07,000
Yes, we can and we should use computation

328
0:17:07.659,000 --> 0:17:09,000
to help us make better decisions.

329
0:17:09.697,000 --> 0:17:14,000
But we have to own up to our moral responsibility to judgment,

330
0:17:15.053,000 --> 0:17:17,000
and use algorithms within that framework,

331
0:17:17.895,000 --> 0:17:21,000
not as a means to abdicate and outsource our responsibilities

332
0:17:22.854,000 --> 0:17:24,000
to one another as human to human.

333
0:17:25.807,000 --> 0:17:27,000
Machine intelligence is here.

334
0:17:28.44,000 --> 0:17:31,000
That means we must hold on ever tighter

335
0:17:31.885,000 --> 0:17:33,000
to human values and human ethics.

336
0:17:34.056,000 --> 0:17:35,000
Thank you.

337
0:17:35.234,000 --> 0:17:4,000
(Applause)

