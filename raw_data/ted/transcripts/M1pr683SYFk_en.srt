1
0:00:13.16,000 --> 0:00:15,000
I thought I'd begin with a scene of war.

2
0:00:15.16,000 --> 0:00:17,000
There was little to warn of the danger ahead.

3
0:00:17.16,000 --> 0:00:19,000
The Iraqi insurgent had placed the IED,

4
0:00:19.16,000 --> 0:00:22,000
an Improvised Explosive Device,

5
0:00:22.16,000 --> 0:00:24,000
along the side of the road with great care.

6
0:00:24.16,000 --> 0:00:28,000
By 2006, there were more than 2,500

7
0:00:28.16,000 --> 0:00:31,000
of these attacks every single month,

8
0:00:31.16,000 --> 0:00:33,000
and they were the leading cause of

9
0:00:33.16,000 --> 0:00:35,000
casualties among American soldiers

10
0:00:35.16,000 --> 0:00:37,000
and Iraqi civilians.

11
0:00:37.16,000 --> 0:00:39,000
The team that was hunting for this IED

12
0:00:39.16,000 --> 0:00:41,000
is called an EOD team—

13
0:00:41.16,000 --> 0:00:43,000
Explosives Ordinance Disposal—and

14
0:00:43.16,000 --> 0:00:45,000
they're the pointy end of the spear in the

15
0:00:45.16,000 --> 0:00:48,000
American effort to suppress these roadside bombs.

16
0:00:48.16,000 --> 0:00:5,000
Each EOD team goes out on about

17
0:00:50.16,000 --> 0:00:52,000
600 of these bomb calls every year,

18
0:00:52.16,000 --> 0:00:55,000
defusing about two bombs a day.

19
0:00:55.16,000 --> 0:00:57,000
Perhaps the best sign of how valuable they

20
0:00:57.16,000 --> 0:00:59,000
are to the war effort, is that

21
0:00:59.16,000 --> 0:01:01,000
the Iraqi insurgents put a $50,000 bounty

22
0:01:01.16,000 --> 0:01:04,000
on the head of a single EOD soldier.

23
0:01:04.16,000 --> 0:01:06,000
Unfortunately, this particular call

24
0:01:06.16,000 --> 0:01:08,000
would not end well.

25
0:01:08.16,000 --> 0:01:1,000
By the time the soldier advanced close

26
0:01:10.16,000 --> 0:01:12,000
enough to see the telltale wires

27
0:01:12.16,000 --> 0:01:15,000
of the bomb, it exploded in a wave of flame.

28
0:01:15.16,000 --> 0:01:17,000
Now, depending how close you are

29
0:01:17.16,000 --> 0:01:19,000
and how much explosive has been packed

30
0:01:19.16,000 --> 0:01:21,000
into that bomb, it can cause death

31
0:01:21.16,000 --> 0:01:23,000
or injury. You have to be as far as

32
0:01:23.16,000 --> 0:01:25,000
50 yards away to escape that.

33
0:01:25.16,000 --> 0:01:27,000
The blast is so strong it can even break

34
0:01:27.16,000 --> 0:01:29,000
your limbs, even if you're not hit.

35
0:01:29.16,000 --> 0:01:31,000
That soldier had been on top of the bomb.

36
0:01:31.16,000 --> 0:01:34,000
And so when the rest of the team advanced

37
0:01:34.16,000 --> 0:01:36,000
they found little left. And that night the unit's

38
0:01:36.16,000 --> 0:01:38,000
commander did a sad duty, and he wrote

39
0:01:38.16,000 --> 0:01:4,000
a condolence letter back to the United

40
0:01:40.16,000 --> 0:01:42,000
States, and he talked about how hard the

41
0:01:42.16,000 --> 0:01:44,000
loss had been on his unit, about the fact

42
0:01:44.16,000 --> 0:01:46,000
that they had lost their bravest soldier,

43
0:01:46.16,000 --> 0:01:48,000
a soldier who had saved their lives

44
0:01:48.16,000 --> 0:01:5,000
many a time.

45
0:01:50.16,000 --> 0:01:52,000
And he apologized

46
0:01:52.16,000 --> 0:01:54,000
for not being able to bring them home.

47
0:01:54.16,000 --> 0:01:56,000
But then he talked up the silver lining

48
0:01:56.16,000 --> 0:01:58,000
that he took away from the loss.

49
0:01:58.16,000 --> 0:02:,000
"At least," as he wrote, "when a robot dies,

50
0:02:00.16,000 --> 0:02:02,000
you don't have to write a letter

51
0:02:02.16,000 --> 0:02:04,000
to its mother."

52
0:02:04.16,000 --> 0:02:06,000
That scene sounds like science fiction,

53
0:02:06.16,000 --> 0:02:08,000
but is battlefield reality already.

54
0:02:08.16,000 --> 0:02:11,000
The soldier in that case

55
0:02:11.16,000 --> 0:02:14,000
was a 42-pound robot called a PackBot.

56
0:02:14.16,000 --> 0:02:17,000
The chief's letter went, not to some

57
0:02:17.16,000 --> 0:02:19,000
farmhouse in Iowa like you see

58
0:02:19.16,000 --> 0:02:22,000
in the old war movies, but went to

59
0:02:22.16,000 --> 0:02:24,000
the iRobot Company, which is

60
0:02:24.16,000 --> 0:02:27,000
named after the Asimov novel

61
0:02:27.16,000 --> 0:02:29,000
and the not-so-great Will Smith movie,

62
0:02:29.16,000 --> 0:02:31,000
and... um... (Laughter)...

63
0:02:31.16,000 --> 0:02:33,000
if you remember that

64
0:02:33.16,000 --> 0:02:35,000
in that fictional world, robots started out

65
0:02:35.16,000 --> 0:02:37,000
carrying out mundane chores, and then

66
0:02:37.16,000 --> 0:02:39,000
they started taking on life-and-death decisions.

67
0:02:39.16,000 --> 0:02:41,000
That's a reality we face today.

68
0:02:41.16,000 --> 0:02:43,000
What we're going to do is actually just

69
0:02:43.16,000 --> 0:02:45,000
flash a series of photos behind me that

70
0:02:45.16,000 --> 0:02:48,000
show you the reality of robots used in war

71
0:02:48.16,000 --> 0:02:5,000
right now or already at the prototype stage.

72
0:02:50.16,000 --> 0:02:53,000
It's just to give you a taste.

73
0:02:53.16,000 --> 0:02:55,000
Another way of putting it is you're not

74
0:02:55.16,000 --> 0:02:57,000
going to see anything that's powered

75
0:02:57.16,000 --> 0:02:59,000
by Vulcan technology, or teenage

76
0:02:59.16,000 --> 0:03:01,000
wizard hormones or anything like that.

77
0:03:01.16,000 --> 0:03:03,000
This is all real. So why don't we

78
0:03:03.16,000 --> 0:03:05,000
go ahead and start those pictures.

79
0:03:05.16,000 --> 0:03:07,000
Something big is going on in war today,

80
0:03:07.16,000 --> 0:03:09,000
and maybe even the history of humanity

81
0:03:09.16,000 --> 0:03:12,000
itself. The U.S. military went into Iraq with

82
0:03:12.16,000 --> 0:03:14,000
a handful of drones in the air.

83
0:03:14.16,000 --> 0:03:17,000
We now have 5,300.

84
0:03:17.16,000 --> 0:03:19,000
We went in with zero unmanned ground

85
0:03:19.16,000 --> 0:03:23,000
systems. We now have 12,000.

86
0:03:23.16,000 --> 0:03:25,000
And the tech term "killer application"

87
0:03:25.16,000 --> 0:03:28,000
takes on new meaning in this space.

88
0:03:28.16,000 --> 0:03:3,000
And we need to remember that we're

89
0:03:30.16,000 --> 0:03:32,000
talking about the Model T Fords,

90
0:03:32.16,000 --> 0:03:34,000
the Wright Flyers, compared

91
0:03:34.16,000 --> 0:03:36,000
to what's coming soon.

92
0:03:36.16,000 --> 0:03:38,000
That's where we're at right now.

93
0:03:38.16,000 --> 0:03:4,000
One of the people that I recently met with

94
0:03:40.16,000 --> 0:03:42,000
was an Air Force three-star general, and he

95
0:03:42.16,000 --> 0:03:44,000
said basically, where we're headed very

96
0:03:44.16,000 --> 0:03:46,000
soon is tens of thousands of robots

97
0:03:46.16,000 --> 0:03:48,000
operating in our conflicts, and these

98
0:03:48.16,000 --> 0:03:5,000
numbers matter, because we're not just

99
0:03:50.16,000 --> 0:03:52,000
talking about tens of thousands of today's

100
0:03:52.16,000 --> 0:03:54,000
robots, but tens of thousands of these

101
0:03:54.16,000 --> 0:03:56,000
prototypes and tomorrow's robots, because

102
0:03:56.16,000 --> 0:03:59,000
of course, one of the things that's operating

103
0:03:59.16,000 --> 0:04:01,000
in technology is Moore's Law,

104
0:04:01.16,000 --> 0:04:03,000
that you can pack in more and more

105
0:04:03.16,000 --> 0:04:05,000
computing power into those robots, and so

106
0:04:05.16,000 --> 0:04:07,000
flash forward around 25 years,

107
0:04:07.16,000 --> 0:04:09,000
if Moore's Law holds true,

108
0:04:09.16,000 --> 0:04:12,000
those robots will be close to a billion times

109
0:04:12.16,000 --> 0:04:15,000
more powerful in their computing than today.

110
0:04:15.16,000 --> 0:04:17,000
And so what that means is the kind of

111
0:04:17.16,000 --> 0:04:19,000
things that we used to only talk about at

112
0:04:19.16,000 --> 0:04:21,000
science fiction conventions like Comic-Con

113
0:04:21.16,000 --> 0:04:23,000
have to be talked about in the halls

114
0:04:23.16,000 --> 0:04:25,000
of power and places like the Pentagon.

115
0:04:25.16,000 --> 0:04:28,000
A robots revolution is upon us.

116
0:04:28.16,000 --> 0:04:3,000
Now, I need to be clear here.

117
0:04:30.16,000 --> 0:04:32,000
I'm not talking about a revolution where you

118
0:04:32.16,000 --> 0:04:34,000
have to worry about the Governor of

119
0:04:34.16,000 --> 0:04:36,000
California showing up at your door,

120
0:04:36.16,000 --> 0:04:38,000
a la the Terminator. (Laughter)

121
0:04:38.16,000 --> 0:04:4,000
When historians look at this period, they're

122
0:04:40.16,000 --> 0:04:42,000
going to conclude that we're in a different

123
0:04:42.16,000 --> 0:04:44,000
type of revolution: a revolution in war,

124
0:04:44.16,000 --> 0:04:46,000
like the invention of the atomic bomb.

125
0:04:46.16,000 --> 0:04:48,000
But it may be even bigger than that,

126
0:04:48.16,000 --> 0:04:5,000
because our unmanned systems don't just

127
0:04:50.16,000 --> 0:04:52,000
affect the "how" of war-fighting,

128
0:04:52.16,000 --> 0:04:54,000
they affect the "who" of fighting

129
0:04:54.16,000 --> 0:04:56,000
at its most fundamental level.

130
0:04:56.16,000 --> 0:04:58,000
That is, every previous revolution in war, be

131
0:04:58.16,000 --> 0:05:,000
it the machine gun, be it the atomic bomb,

132
0:05:00.16,000 --> 0:05:03,000
was about a system that either shot faster,

133
0:05:03.16,000 --> 0:05:06,000
went further, had a bigger boom.

134
0:05:06.16,000 --> 0:05:09,000
That's certainly the case with robotics, but

135
0:05:09.16,000 --> 0:05:12,000
they also change the experience of the warrior

136
0:05:12.16,000 --> 0:05:15,000
and even the very identity of the warrior.

137
0:05:15.16,000 --> 0:05:18,000
Another way of putting this is that

138
0:05:18.16,000 --> 0:05:2,000
mankind's 5,000-year-old monopoly

139
0:05:20.16,000 --> 0:05:23,000
on the fighting of war is breaking down

140
0:05:23.16,000 --> 0:05:25,000
in our very lifetime. I've spent

141
0:05:25.16,000 --> 0:05:27,000
the last several years going around

142
0:05:27.16,000 --> 0:05:29,000
meeting with all the players in this field,

143
0:05:29.16,000 --> 0:05:31,000
from the robot scientists to the science

144
0:05:31.16,000 --> 0:05:33,000
fiction authors who inspired them to the

145
0:05:33.16,000 --> 0:05:35,000
19-year-old drone pilots who are fighting

146
0:05:35.16,000 --> 0:05:37,000
from Nevada, to the four-star generals

147
0:05:37.16,000 --> 0:05:39,000
who command them, to even the Iraqi

148
0:05:39.16,000 --> 0:05:41,000
insurgents who they are targeting and what

149
0:05:41.16,000 --> 0:05:43,000
they think about our systems, and

150
0:05:43.16,000 --> 0:05:45,000
what I found interesting is not just

151
0:05:45.16,000 --> 0:05:47,000
their stories, but how their experiences

152
0:05:47.16,000 --> 0:05:49,000
point to these ripple effects that are going

153
0:05:49.16,000 --> 0:05:51,000
outwards in our society, in our law

154
0:05:51.16,000 --> 0:05:53,000
and our ethics, etc. And so what I'd like

155
0:05:53.16,000 --> 0:05:55,000
to do with my remaining time is basically

156
0:05:55.16,000 --> 0:05:57,000
flesh out a couple of these.

157
0:05:57.16,000 --> 0:05:59,000
So the first is that the future of war,

158
0:05:59.16,000 --> 0:06:01,000
even a robotics one, is not going to be

159
0:06:01.16,000 --> 0:06:03,000
purely an American one.

160
0:06:03.16,000 --> 0:06:05,000
The U.S. is currently ahead in military

161
0:06:05.16,000 --> 0:06:07,000
robotics right now, but we know that in

162
0:06:07.16,000 --> 0:06:09,000
technology there's no such thing as

163
0:06:09.16,000 --> 0:06:12,000
a permanent first move or advantage.

164
0:06:12.16,000 --> 0:06:14,000
In a quick show of hands, how many

165
0:06:14.16,000 --> 0:06:16,000
people in this room still use

166
0:06:16.16,000 --> 0:06:18,000
Wang Computers? (Laughter)

167
0:06:18.16,000 --> 0:06:2,000
It's the same thing in war. The British and

168
0:06:20.16,000 --> 0:06:23,000
the French invented the tank.

169
0:06:23.16,000 --> 0:06:25,000
The Germans figured out how

170
0:06:25.16,000 --> 0:06:27,000
to use it right, and so what we have to

171
0:06:27.16,000 --> 0:06:29,000
think about for the U.S. is that we are

172
0:06:29.16,000 --> 0:06:31,000
ahead right now, but you have

173
0:06:31.16,000 --> 0:06:33,000
43 other countries out there

174
0:06:33.16,000 --> 0:06:35,000
working on military robotics, and they

175
0:06:35.16,000 --> 0:06:37,000
include all the interesting countries like

176
0:06:37.16,000 --> 0:06:4,000
Russia, China, Pakistan, Iran.

177
0:06:40.16,000 --> 0:06:43,000
And this raises a bigger worry for me.

178
0:06:43.16,000 --> 0:06:45,000
How do we move forward in this revolution

179
0:06:45.16,000 --> 0:06:47,000
given the state of our manufacturing

180
0:06:47.16,000 --> 0:06:49,000
and the state of our science and

181
0:06:49.16,000 --> 0:06:51,000
mathematics training in our schools?

182
0:06:51.16,000 --> 0:06:53,000
Or another way of thinking about this is,

183
0:06:53.16,000 --> 0:06:55,000
what does it mean to go to war increasingly

184
0:06:55.16,000 --> 0:06:58,000
with soldiers whose hardware is made

185
0:06:58.16,000 --> 0:07:03,000
in China and software is written in India?

186
0:07:03.16,000 --> 0:07:06,000
But just as software has gone open-source,

187
0:07:06.16,000 --> 0:07:08,000
so has warfare.

188
0:07:08.16,000 --> 0:07:11,000
Unlike an aircraft carrier or an atomic bomb,

189
0:07:11.16,000 --> 0:07:13,000
you don't need a massive manufacturing

190
0:07:13.16,000 --> 0:07:15,000
system to build robotics. A lot of it is

191
0:07:15.16,000 --> 0:07:17,000
off the shelf. A lot of it's even do-it-yourself.

192
0:07:17.16,000 --> 0:07:19,000
One of those things you just saw flashed

193
0:07:19.16,000 --> 0:07:21,000
before you was a raven drone, the handheld

194
0:07:21.16,000 --> 0:07:23,000
tossed one. For about a thousand dollars,

195
0:07:23.16,000 --> 0:07:25,000
you can build one yourself, equivalent to

196
0:07:25.16,000 --> 0:07:27,000
what the soldiers use in Iraq.

197
0:07:27.16,000 --> 0:07:29,000
That raises another wrinkle when it comes

198
0:07:29.16,000 --> 0:07:31,000
to war and conflict. Good guys might play

199
0:07:31.16,000 --> 0:07:33,000
around and work on these as hobby kits,

200
0:07:33.16,000 --> 0:07:35,000
but so might bad guys.

201
0:07:35.16,000 --> 0:07:37,000
This cross between robotics and things like

202
0:07:37.16,000 --> 0:07:39,000
terrorism is going to be fascinating

203
0:07:39.16,000 --> 0:07:41,000
and even disturbing,

204
0:07:41.16,000 --> 0:07:43,000
and we've already seen it start.

205
0:07:43.16,000 --> 0:07:45,000
During the war between Israel, a state,

206
0:07:45.16,000 --> 0:07:48,000
and Hezbollah, a non-state actor,

207
0:07:48.16,000 --> 0:07:5,000
the non-state actor flew

208
0:07:50.16,000 --> 0:07:52,000
four different drones against Israel.

209
0:07:52.16,000 --> 0:07:54,000
There's already a jihadi website

210
0:07:54.16,000 --> 0:07:56,000
that you can go on and remotely

211
0:07:56.16,000 --> 0:07:58,000
detonate an IED in Iraq while sitting

212
0:07:58.16,000 --> 0:08:,000
at your home computer.

213
0:08:00.16,000 --> 0:08:02,000
And so I think what we're going to see is

214
0:08:02.16,000 --> 0:08:04,000
two trends take place with this.

215
0:08:04.16,000 --> 0:08:06,000
First is, you're going to reinforce the power

216
0:08:06.16,000 --> 0:08:1,000
of individuals against governments,

217
0:08:10.16,000 --> 0:08:12,000
but then the second is that

218
0:08:12.16,000 --> 0:08:14,000
we are going to see an expansion

219
0:08:14.16,000 --> 0:08:16,000
in the realm of terrorism.

220
0:08:16.16,000 --> 0:08:18,000
The future of it may be a cross between

221
0:08:18.16,000 --> 0:08:2,000
al Qaeda 2.0 and the

222
0:08:20.16,000 --> 0:08:22,000
next generation of the Unabomber.

223
0:08:22.16,000 --> 0:08:24,000
And another way of thinking about this

224
0:08:24.16,000 --> 0:08:26,000
is the fact that, remember, you don't have

225
0:08:26.16,000 --> 0:08:28,000
to convince a robot that they're gonna

226
0:08:28.16,000 --> 0:08:31,000
receive 72 virgins after they die

227
0:08:31.16,000 --> 0:08:34,000
to convince them to blow themselves up.

228
0:08:34.16,000 --> 0:08:36,000
But the ripple effects of this are going to go

229
0:08:36.16,000 --> 0:08:38,000
out into our politics. One of the people that

230
0:08:38.16,000 --> 0:08:4,000
I met with was a former Assistant Secretary of

231
0:08:40.16,000 --> 0:08:42,000
Defense for Ronald Reagan, and he put it

232
0:08:42.16,000 --> 0:08:44,000
this way: "I like these systems because

233
0:08:44.16,000 --> 0:08:46,000
they save American lives, but I worry about

234
0:08:46.16,000 --> 0:08:48,000
more marketization of wars,

235
0:08:48.16,000 --> 0:08:51,000
more shock-and-awe talk,

236
0:08:51.16,000 --> 0:08:53,000
to defray discussion of the costs.

237
0:08:53.16,000 --> 0:08:55,000
People are more likely to support the use

238
0:08:55.16,000 --> 0:08:58,000
of force if they view it as costless."

239
0:08:58.16,000 --> 0:09:,000
Robots for me take certain trends

240
0:09:00.16,000 --> 0:09:03,000
that are already in play in our body politic,

241
0:09:03.16,000 --> 0:09:05,000
and maybe take them to

242
0:09:05.16,000 --> 0:09:07,000
their logical ending point.

243
0:09:07.16,000 --> 0:09:09,000
We don't have a draft. We don't

244
0:09:09.16,000 --> 0:09:12,000
have declarations of war anymore.

245
0:09:12.16,000 --> 0:09:14,000
We don't buy war bonds anymore.

246
0:09:14.16,000 --> 0:09:16,000
And now we have the fact that we're

247
0:09:16.16,000 --> 0:09:18,000
converting more and more of our American

248
0:09:18.16,000 --> 0:09:2,000
soldiers that we would send into harm's

249
0:09:20.16,000 --> 0:09:23,000
way into machines, and so we may take

250
0:09:23.16,000 --> 0:09:26,000
those already lowering bars to war

251
0:09:26.16,000 --> 0:09:29,000
and drop them to the ground.

252
0:09:29.16,000 --> 0:09:31,000
But the future of war is also going to be

253
0:09:31.16,000 --> 0:09:33,000
a YouTube war.

254
0:09:33.16,000 --> 0:09:35,000
That is, our new technologies don't merely

255
0:09:35.16,000 --> 0:09:37,000
remove humans from risk.

256
0:09:37.16,000 --> 0:09:4,000
They also record everything that they see.

257
0:09:40.16,000 --> 0:09:43,000
So they don't just delink the public:

258
0:09:43.16,000 --> 0:09:46,000
they reshape its relationship with war.

259
0:09:46.16,000 --> 0:09:48,000
There's already several thousand

260
0:09:48.16,000 --> 0:09:5,000
video clips of combat footage from Iraq

261
0:09:50.16,000 --> 0:09:52,000
on YouTube right now,

262
0:09:52.16,000 --> 0:09:54,000
most of it gathered by drones.

263
0:09:54.16,000 --> 0:09:56,000
Now, this could be a good thing.

264
0:09:56.16,000 --> 0:09:58,000
It could be building connections between

265
0:09:58.16,000 --> 0:10:,000
the home front and the war front

266
0:10:00.16,000 --> 0:10:02,000
as never before.

267
0:10:02.16,000 --> 0:10:04,000
But remember, this is taking place

268
0:10:04.16,000 --> 0:10:07,000
in our strange, weird world, and so

269
0:10:07.16,000 --> 0:10:09,000
inevitably the ability to download these

270
0:10:09.16,000 --> 0:10:11,000
video clips to, you know, your iPod

271
0:10:11.16,000 --> 0:10:14,000
or your Zune gives you

272
0:10:14.16,000 --> 0:10:18,000
the ability to turn it into entertainment.

273
0:10:18.16,000 --> 0:10:2,000
Soldiers have a name for these clips.

274
0:10:20.16,000 --> 0:10:22,000
They call it war porn.

275
0:10:22.16,000 --> 0:10:24,000
The typical one that I was sent was

276
0:10:24.16,000 --> 0:10:26,000
an email that had an attachment of

277
0:10:26.16,000 --> 0:10:28,000
video of a Predator strike taking out

278
0:10:28.16,000 --> 0:10:3,000
an enemy site. Missile hits,

279
0:10:30.16,000 --> 0:10:33,000
bodies burst into the air with the explosion.

280
0:10:33.16,000 --> 0:10:35,000
It was set to music.

281
0:10:35.16,000 --> 0:10:37,000
It was set to the pop song

282
0:10:37.16,000 --> 0:10:4,000
"I Just Want To Fly" by Sugar Ray.

283
0:10:40.16,000 --> 0:10:43,000
This ability to watch more

284
0:10:43.16,000 --> 0:10:46,000
but experience less creates a wrinkle

285
0:10:46.16,000 --> 0:10:48,000
in the public's relationship with war.

286
0:10:48.16,000 --> 0:10:5,000
I think about this with a sports parallel.

287
0:10:50.16,000 --> 0:10:53,000
It's like the difference between

288
0:10:53.16,000 --> 0:10:56,000
watching an NBA game, a professional

289
0:10:56.16,000 --> 0:10:59,000
basketball game on TV, where the athletes

290
0:10:59.16,000 --> 0:11:01,000
are tiny figures on the screen, and

291
0:11:01.16,000 --> 0:11:04,000
being at that basketball game in person

292
0:11:04.16,000 --> 0:11:06,000
and realizing what someone seven feet

293
0:11:06.16,000 --> 0:11:08,000
really does look like.

294
0:11:08.16,000 --> 0:11:1,000
But we have to remember,

295
0:11:10.16,000 --> 0:11:12,000
these are just the clips.

296
0:11:12.16,000 --> 0:11:14,000
These are just the ESPN SportsCenter

297
0:11:14.16,000 --> 0:11:16,000
version of the game. They lose the context.

298
0:11:16.16,000 --> 0:11:18,000
They lose the strategy.

299
0:11:18.16,000 --> 0:11:2,000
They lose the humanity. War just

300
0:11:20.16,000 --> 0:11:23,000
becomes slam dunks and smart bombs.

301
0:11:23.16,000 --> 0:11:26,000
Now the irony of all this is that

302
0:11:26.16,000 --> 0:11:28,000
while the future of war may involve

303
0:11:28.16,000 --> 0:11:3,000
more and more machines,

304
0:11:30.16,000 --> 0:11:32,000
it's our human psychology that's driving

305
0:11:32.16,000 --> 0:11:34,000
all of this, it's our human failings

306
0:11:34.16,000 --> 0:11:36,000
that are leading to these wars.

307
0:11:36.16,000 --> 0:11:38,000
So one example of this that has

308
0:11:38.16,000 --> 0:11:4,000
big resonance in the policy realm is

309
0:11:40.16,000 --> 0:11:42,000
how this plays out on our very real

310
0:11:42.16,000 --> 0:11:44,000
war of ideas that we're fighting

311
0:11:44.16,000 --> 0:11:46,000
against radical groups.

312
0:11:46.16,000 --> 0:11:48,000
What is the message that we think we are

313
0:11:48.16,000 --> 0:11:5,000
sending with these machines versus what

314
0:11:50.16,000 --> 0:11:53,000
is being received in terms of the message.

315
0:11:53.16,000 --> 0:11:55,000
So one of the people that I met was

316
0:11:55.16,000 --> 0:11:57,000
a senior Bush Administration official,

317
0:11:57.16,000 --> 0:11:59,000
who had this to say about

318
0:11:59.16,000 --> 0:12:01,000
our unmanning of war:

319
0:12:01.16,000 --> 0:12:03,000
"It plays to our strength. The thing that

320
0:12:03.16,000 --> 0:12:05,000
scares people is our technology."

321
0:12:05.16,000 --> 0:12:07,000
But when you go out and meet with people,

322
0:12:07.16,000 --> 0:12:09,000
for example in Lebanon, it's a very

323
0:12:09.16,000 --> 0:12:11,000
different story. One of the people

324
0:12:11.16,000 --> 0:12:13,000
I met with there was a news editor, and

325
0:12:13.16,000 --> 0:12:15,000
we're talking as a drone is flying above him,

326
0:12:15.16,000 --> 0:12:17,000
and this is what he had to say.

327
0:12:17.162,000 --> 0:12:18,000
"This is just another sign of the coldhearted

328
0:12:19.16,000 --> 0:12:22,000
cruel Israelis and Americans,

329
0:12:22.166,000 --> 0:12:23,000
who are cowards because

330
0:12:24.158,000 --> 0:12:26,000
they send out machines to fight us.

331
0:12:26.164,000 --> 0:12:27,000
They don't want to fight us like real men,

332
0:12:28.156,000 --> 0:12:3,000
but they're afraid to fight,

333
0:12:30.163,000 --> 0:12:31,000
so we just have to kill a few of their soldiers

334
0:12:32.154,000 --> 0:12:35,000
to defeat them."

335
0:12:35.159,000 --> 0:12:37,000
The future of war also is featuring

336
0:12:37.166,000 --> 0:12:38,000
a new type of warrior,

337
0:12:39.159,000 --> 0:12:42,000
and it's actually redefining the experience

338
0:12:42.165,000 --> 0:12:43,000
of going to war.

339
0:12:44.163,000 --> 0:12:45,000
You can call this a cubicle warrior.

340
0:12:46.154,000 --> 0:12:48,000
This is what one Predator drone pilot

341
0:12:48.161,000 --> 0:12:5,000
described of his experience fighting

342
0:12:50.168,000 --> 0:12:52,000
in the Iraq War while never leaving Nevada.

343
0:12:53.157,000 --> 0:12:55,000
"You're going to war for 12 hours,

344
0:12:55.162,000 --> 0:12:56,000
shooting weapons at targets,

345
0:12:57.168,000 --> 0:12:59,000
directing kills on enemy combatants,

346
0:13:00.159,000 --> 0:13:02,000
and then you get in the car

347
0:13:02.165,000 --> 0:13:03,000
and you drive home and within 20 minutes,

348
0:13:04.163,000 --> 0:13:05,000
you're sitting at the dinner table

349
0:13:06.169,000 --> 0:13:07,000
talking to your kids about their homework."

350
0:13:08.162,000 --> 0:13:09,000
Now, the psychological balancing

351
0:13:10.168,000 --> 0:13:11,000
of those experiences is incredibly tough,

352
0:13:12.161,000 --> 0:13:14,000
and in fact those drone pilots have

353
0:13:15.153,000 --> 0:13:17,000
higher rates of PTSD than many

354
0:13:17.161,000 --> 0:13:2,000
of the units physically in Iraq.

355
0:13:20.166,000 --> 0:13:21,000
But some have worries that this

356
0:13:22.158,000 --> 0:13:23,000
disconnection will lead to something else,

357
0:13:24.167,000 --> 0:13:25,000
that it might make the contemplation of war

358
0:13:26.165,000 --> 0:13:28,000
crimes a lot easier when you have

359
0:13:28.17,000 --> 0:13:29,000
this distance. "It's like a video game,"

360
0:13:30.169,000 --> 0:13:31,000
is what one young pilot described to me

361
0:13:32.168,000 --> 0:13:33,000
of taking out enemy troops from afar.

362
0:13:34.158,000 --> 0:13:36,000
As anyone who's played Grand Theft Auto

363
0:13:37.168,000 --> 0:13:4,000
knows, we do things in the video world

364
0:13:40.168,000 --> 0:13:42,000
that we wouldn't do face to face.

365
0:13:43.164,000 --> 0:13:44,000
So much of what you're hearing from me

366
0:13:45.162,000 --> 0:13:46,000
is that there's another side

367
0:13:47.155,000 --> 0:13:49,000
to technologic revolutions,

368
0:13:49.163,000 --> 0:13:5,000
and that it's shaping our present

369
0:13:51.157,000 --> 0:13:54,000
and maybe will shape our future of war.

370
0:13:54.158,000 --> 0:13:56,000
Moore's Law is operative,

371
0:13:56.159,000 --> 0:13:57,000
but so's Murphy's Law.

372
0:13:58.157,000 --> 0:14:,000
The fog of war isn't being lifted.

373
0:14:00.162,000 --> 0:14:02,000
The enemy has a vote.

374
0:14:02.172,000 --> 0:14:03,000
We're gaining incredible new capabilities,

375
0:14:04.161,000 --> 0:14:06,000
but we're also seeing and experiencing

376
0:14:06.167,000 --> 0:14:08,000
new human dilemmas. Now,

377
0:14:08.167,000 --> 0:14:09,000
sometimes these are just "oops" moments,

378
0:14:10.166,000 --> 0:14:11,000
which is what the head of a robotics

379
0:14:12.158,000 --> 0:14:14,000
company described it, you just have

380
0:14:14.169,000 --> 0:14:16,000
"oops" moments. Well, what are

381
0:14:16.169,000 --> 0:14:17,000
"oops" moments with robots in war?

382
0:14:18.165,000 --> 0:14:2,000
Well, sometimes they're funny. Sometimes,

383
0:14:20.166,000 --> 0:14:22,000
they're like that scene from the

384
0:14:22.168,000 --> 0:14:24,000
Eddie Murphy movie "Best Defense,"

385
0:14:24.169,000 --> 0:14:26,000
playing out in reality, where they tested out

386
0:14:26.17,000 --> 0:14:27,000
a machine gun-armed robot, and during

387
0:14:28.158,000 --> 0:14:3,000
the demonstration it started spinning

388
0:14:30.159,000 --> 0:14:33,000
in a circle and pointed its machine gun

389
0:14:33.168,000 --> 0:14:35,000
at the reviewing stand of VIPs.

390
0:14:36.163,000 --> 0:14:37,000
Fortunately the weapon wasn't loaded

391
0:14:38.156,000 --> 0:14:4,000
and no one was hurt, but other times

392
0:14:40.166,000 --> 0:14:41,000
"oops" moments are tragic,

393
0:14:42.158,000 --> 0:14:44,000
such as last year in South Africa, where

394
0:14:44.16,000 --> 0:14:46,000
an anti-aircraft cannon had a

395
0:14:47.159,000 --> 0:14:49,000
"software glitch," and actually did turn on

396
0:14:50.158,000 --> 0:14:53,000
and fired, and nine soldiers were killed.

397
0:14:53.162,000 --> 0:14:55,000
We have new wrinkles in the laws of war

398
0:14:56.159,000 --> 0:14:57,000
and accountability. What do we do

399
0:14:58.156,000 --> 0:15:,000
with things like unmanned slaughter?

400
0:15:00.166,000 --> 0:15:01,000
What is unmanned slaughter?

401
0:15:02.164,000 --> 0:15:04,000
We've already had three instances of

402
0:15:04.164,000 --> 0:15:06,000
Predator drone strikes where we thought

403
0:15:06.164,000 --> 0:15:07,000
we got bin Laden, and it turned out

404
0:15:08.158,000 --> 0:15:1,000
not to be the case.

405
0:15:10.16,000 --> 0:15:12,000
And this is where we're at right now.

406
0:15:12.168,000 --> 0:15:14,000
This is not even talking about armed,

407
0:15:14.168,000 --> 0:15:15,000
autonomous systems

408
0:15:16.157,000 --> 0:15:18,000
with full authority to use force.

409
0:15:18.17,000 --> 0:15:19,000
And do not believe that that isn't coming.

410
0:15:20.164,000 --> 0:15:22,000
During my research I came across

411
0:15:22.171,000 --> 0:15:23,000
four different Pentagon projects

412
0:15:24.156,000 --> 0:15:26,000
on different aspects of that.

413
0:15:26.168,000 --> 0:15:28,000
And so you have this question:

414
0:15:28.17,000 --> 0:15:29,000
what does this lead to issues like

415
0:15:30.163,000 --> 0:15:31,000
war crimes? Robots are emotionless, so

416
0:15:32.155,000 --> 0:15:35,000
they don't get upset if their buddy is killed.

417
0:15:35.167,000 --> 0:15:36,000
They don't commit crimes of rage

418
0:15:37.161,000 --> 0:15:39,000
and revenge.

419
0:15:39.168,000 --> 0:15:41,000
But robots are emotionless.

420
0:15:42.165,000 --> 0:15:43,000
They see an 80-year-old grandmother

421
0:15:44.164,000 --> 0:15:46,000
in a wheelchair the same way they see

422
0:15:46.164,000 --> 0:15:49,000
a T-80 tank: they're both

423
0:15:49.166,000 --> 0:15:51,000
just a series of zeroes and ones.

424
0:15:52.159,000 --> 0:15:55,000
And so we have this question to figure out:

425
0:15:55.162,000 --> 0:15:57,000
How do we catch up our 20th century

426
0:15:57.165,000 --> 0:15:58,000
laws of war, that are so old right now

427
0:15:59.154,000 --> 0:16:02,000
that they could qualify for Medicare,

428
0:16:02.162,000 --> 0:16:05,000
to these 21st century technologies?

429
0:16:05.17,000 --> 0:16:07,000
And so, in conclusion, I've talked about

430
0:16:08.157,000 --> 0:16:11,000
what seems the future of war,

431
0:16:11.168,000 --> 0:16:12,000
but notice that I've only used

432
0:16:13.158,000 --> 0:16:15,000
real world examples and you've only seen

433
0:16:15.17,000 --> 0:16:17,000
real world pictures and videos.

434
0:16:17.171,000 --> 0:16:19,000
And so this sets a great challenge for

435
0:16:19.171,000 --> 0:16:2,000
all of us that we have to worry about well

436
0:16:21.157,000 --> 0:16:23,000
before you have to worry about your

437
0:16:23.166,000 --> 0:16:25,000
Roomba sucking the life away from you.

438
0:16:25.167,000 --> 0:16:26,000
Are we going to let the fact that what's

439
0:16:27.166,000 --> 0:16:3,000
unveiling itself right now in war

440
0:16:30.166,000 --> 0:16:32,000
sounds like science fiction and therefore

441
0:16:33.157,000 --> 0:16:35,000
keeps us in denial?

442
0:16:35.162,000 --> 0:16:37,000
Are we going to face the reality

443
0:16:37.166,000 --> 0:16:38,000
of 21st century war?

444
0:16:39.159,000 --> 0:16:41,000
Is our generation going to make the same

445
0:16:41.16,000 --> 0:16:43,000
mistake that a past generation did

446
0:16:43.169,000 --> 0:16:44,000
with atomic weaponry, and not deal with

447
0:16:45.167,000 --> 0:16:46,000
the issues that surround it until

448
0:16:47.162,000 --> 0:16:49,000
Pandora's box is already opened up?

449
0:16:49.169,000 --> 0:16:5,000
Now, I could be wrong on this, and

450
0:16:51.167,000 --> 0:16:53,000
one Pentagon robot scientist told me

451
0:16:53.171,000 --> 0:16:54,000
that I was. He said, "There's no real

452
0:16:55.165,000 --> 0:16:56,000
social, ethical, moral issues when it comes

453
0:16:57.16,000 --> 0:16:58,000
to robots.

454
0:16:59.158,000 --> 0:17:01,000
That is," he added, "unless the machine

455
0:17:01.159,000 --> 0:17:04,000
kills the wrong people repeatedly.

456
0:17:04.169,000 --> 0:17:06,000
Then it's just a product recall issue."

457
0:17:07.158,000 --> 0:17:1,000
And so the ending point for this is

458
0:17:10.169,000 --> 0:17:15,000
that actually, we can turn to Hollywood.

459
0:17:15.171,000 --> 0:17:16,000
A few years ago, Hollywood gathered

460
0:17:17.167,000 --> 0:17:2,000
all the top characters and created

461
0:17:20.173,000 --> 0:17:21,000
a list of the top 100 heroes and

462
0:17:22.17,000 --> 0:17:24,000
top 100 villains of all of Hollywood history,

463
0:17:25.16,000 --> 0:17:27,000
the characters that represented the best

464
0:17:27.166,000 --> 0:17:28,000
and worst of humanity.

465
0:17:29.158,000 --> 0:17:33,000
Only one character made it onto both lists:

466
0:17:33.173,000 --> 0:17:35,000
The Terminator, a robot killing machine.

467
0:17:36.169,000 --> 0:17:37,000
And so that points to the fact that

468
0:17:38.157,000 --> 0:17:4,000
our machines can be used

469
0:17:40.17,000 --> 0:17:41,000
for both good and evil, but for me

470
0:17:42.169,000 --> 0:17:43,000
it points to the fact that there's a duality

471
0:17:44.165,000 --> 0:17:47,000
of humans as well.

472
0:17:47.165,000 --> 0:17:48,000
This week is a celebration

473
0:17:49.163,000 --> 0:17:5,000
of our creativity. Our creativity

474
0:17:51.162,000 --> 0:17:52,000
has taken our species to the stars.

475
0:17:53.16,000 --> 0:17:55,000
Our creativity has created works of arts

476
0:17:55.167,000 --> 0:17:57,000
and literature to express our love.

477
0:17:58.163,000 --> 0:17:59,000
And now, we're using our creativity

478
0:18:00.161,000 --> 0:18:01,000
in a certain direction, to build fantastic

479
0:18:02.169,000 --> 0:18:04,000
machines with incredible capabilities,

480
0:18:05.156,000 --> 0:18:07,000
maybe even one day

481
0:18:07.165,000 --> 0:18:09,000
an entirely new species.

482
0:18:10.162,000 --> 0:18:12,000
But one of the main reasons that we're

483
0:18:12.167,000 --> 0:18:13,000
doing that is because of our drive

484
0:18:14.165,000 --> 0:18:17,000
to destroy each other, and so the question

485
0:18:17.165,000 --> 0:18:18,000
we all should ask:

486
0:18:19.159,000 --> 0:18:21,000
is it our machines, or is it us

487
0:18:21.159,000 --> 0:18:23,000
that's wired for war?

488
0:18:23.166,000 --> 0:18:25,000
Thank you. (Applause)

