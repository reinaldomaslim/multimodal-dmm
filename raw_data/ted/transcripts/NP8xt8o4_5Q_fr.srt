1
0:00:,000 --> 0:00:07,000
Traducteur: América Aguilera Relecteur: Lucie Langevin

2
0:00:12.78,000 --> 0:00:14,000
Cette histoire commence en 1985

3
0:00:15.78,000 --> 0:00:16,000
quand, à l'âge de 22 ans,

4
0:00:17.78,000 --> 0:00:19,000
je suis devenu champion du monde d'échecs

5
0:00:20.18,000 --> 0:00:23,000
en battant Anatoly Karpov.

6
0:00:24.3,000 --> 0:00:25,000
Plus tôt cette même année,

7
0:00:25.58,000 --> 0:00:28,000
j'ai participé à ce qu'on appelle une série de parties simultanées

8
0:00:29.22,000 --> 0:00:33,000
contre 32 des machines joueuses d'échecs les plus performantes

9
0:00:33.54,000 --> 0:00:34,000
à Hambourg.

10
0:00:35.98,000 --> 0:00:36,000
Je les ai toutes gagnées

11
0:00:38.38,000 --> 0:00:41,000
et à l'époque, ça n'avait rien de surprenant

12
0:00:41.58,000 --> 0:00:45,000
que je puisse battre 32 ordinateurs simultanément.

13
0:00:46.3,000 --> 0:00:48,000
Ah, pour moi, c'était l'âge d'or.

14
0:00:48.9,000 --> 0:00:5,000
(Rires)

15
0:00:51.02,000 --> 0:00:52,000
Les machines avaient peu de puissance

16
0:00:53.5,000 --> 0:00:54,000
et moi, j'avais des cheveux.

17
0:00:54.86,000 --> 0:00:56,000
(Rires)

18
0:00:58.54,000 --> 0:01:,000
Douze ans plus tard seulement,

19
0:01:00.62,000 --> 0:01:04,000
je me retrouvais à m'échiner contre un seul ordinateur

20
0:01:05.26,000 --> 0:01:06,000
dans une partie,

21
0:01:07.18,000 --> 0:01:09,000
surnommée en couverture de Newsweek,

22
0:01:09.26,000 --> 0:01:1,000
« le dernier combat du cerveau ».

23
0:01:11.06,000 --> 0:01:12,000
Pas de pression, donc.

24
0:01:12.3,000 --> 0:01:13,000
(Rires)

25
0:01:14.86,000 --> 0:01:16,000
De la mythologie à la science-fiction,

26
0:01:17.46,000 --> 0:01:19,000
l'affrontement de l'humain et de la machine

27
0:01:20.22,000 --> 0:01:22,000
a souvent été vu comme une question de vie ou de mort.

28
0:01:23.78,000 --> 0:01:24,000
John Henry,

29
0:01:25.38,000 --> 0:01:26,000
qu'on appelle le pousseur d'acier

30
0:01:27.1,000 --> 0:01:3,000
dans la légende folklorique afro-américaine du XIXème siècle,

31
0:01:30.9,000 --> 0:01:31,000
s'est mesuré, pour un pari,

32
0:01:32.3,000 --> 0:01:34,000
à un marteau à vapeur

33
0:01:35.06,000 --> 0:01:37,000
pour creuser un tunnel dans la roche.

34
0:01:38.62,000 --> 0:01:42,000
La légende de John Henry appartient à un corps plus large de récits

35
0:01:43.5,000 --> 0:01:46,000
qui oppose l'humanité à la technologie.

36
0:01:48.02,000 --> 0:01:5,000
Cette rhétorique de la compétition est monnaie courante.

37
0:01:51.38,000 --> 0:01:52,000
Nous faisons la course contre les machines

38
0:01:54.18,000 --> 0:01:56,000
ou nous menons un combat ou même une guerre.

39
0:01:57.7,000 --> 0:01:58,000
Des emplois disparaissent.

40
0:01:59.34,000 --> 0:02:02,000
Des gens sont remplacés comme s'ils n'avaient jamais existé.

41
0:02:04.06,000 --> 0:02:07,000
Il y a de quoi penser que des films comme « Terminator » ou « Matrix »

42
0:02:07.58,000 --> 0:02:08,000
sont du documentaire.

43
0:02:11.46,000 --> 0:02:15,000
Il y a très peu d'exemples de domaines

44
0:02:17.18,000 --> 0:02:21,000
où le corps et l'esprit humain peuvent être à égalité

45
0:02:21.459,000 --> 0:02:22,000
avec un ordinateur ou un robot.

46
0:02:24.1,000 --> 0:02:25,000
J'aimerais en fait qu'il y en ait plus.

47
0:02:27.58,000 --> 0:02:28,000
Au lieu de ça,

48
0:02:29.66,000 --> 0:02:33,000
ça a été une bénédiction et une malédiction pour moi

49
0:02:34.34,000 --> 0:02:36,000
de devenir précisément ce fameux homme

50
0:02:37.06,000 --> 0:02:4,000
dans cette compétition entre l'homme et la machine

51
0:02:40.18,000 --> 0:02:41,000
dont tout le monde parle encore.

52
0:02:44.94,000 --> 0:02:49,000
Lors de l'affrontement homme-machine le plus célèbre depuis John Henry,

53
0:02:49.98,000 --> 0:02:51,000
j'ai joué deux séries de parties

54
0:02:52.58,000 --> 0:02:55,000
contre le superordinateur d'IBM Deep Blue.

55
0:02:58.86,000 --> 0:03:,000
On ne se souvient jamais que j'ai gagné la première...

56
0:03:01.436,000 --> 0:03:02,000
(Rires)

57
0:03:03.42,000 --> 0:03:06,000
(Applaudissements)

58
0:03:07.74,000 --> 0:03:11,000
à Philadelphie, avant de perdre la revanche en 1997 à New York.

59
0:03:12.74,000 --> 0:03:13,000
Mais bon, ce n'est que justice.

60
0:03:16.14,000 --> 0:03:21,000
Il n'y a pas de date anniversaire, de jour spécial au calendrier

61
0:03:21.26,000 --> 0:03:24,000
pour tous les gens qui n'ont pas réussi à escalader l'Everest

62
0:03:24.78,000 --> 0:03:26,000
avant que Sir Edmund Hillary et Tenzing Norgay

63
0:03:27.54,000 --> 0:03:28,000
ne parviennent au sommet.

64
0:03:29.78,000 --> 0:03:32,000
Et en 1997, j'étais toujours champion du monde

65
0:03:36.34,000 --> 0:03:4,000
quand les ordinateurs d'échecs sont enfin arrivés à maturité.

66
0:03:41.34,000 --> 0:03:42,000
Le mont Everest, c'était moi,

67
0:03:43.34,000 --> 0:03:44,000
et Deep Blue a atteint le sommet.

68
0:03:46.42,000 --> 0:03:5,000
Bien sûr, je ferais mieux de dire, non pas que Deep Blue a fait ça,

69
0:03:50.5,000 --> 0:03:52,000
mais ses créateurs humains :

70
0:03:52.66,000 --> 0:03:55,000
Anantharaman, Campbell, Hoane, Hsu.

71
0:03:56.02,000 --> 0:03:57,000
Je leur tire mon chapeau.

72
0:03:58.66,000 --> 0:04:02,000
Comme toujours, le triomphe des machines fut le triomphe des hommes.

73
0:04:03.1,000 --> 0:04:07,000
Nous sommes enclins à l'oublier quand les humains sont surpassés par leur création.

74
0:04:10.18,000 --> 0:04:11,000
Deep Blue a remporté la victoire,

75
0:04:12.96,000 --> 0:04:13,000
mais était-il intelligent ?

76
0:04:15.18,000 --> 0:04:16,000
Non, il ne l'était pas,

77
0:04:18.02,000 --> 0:04:23,000
du moins pas de la façon dont Alan Turing et d'autres fondateurs de l'informatique

78
0:04:23.1,000 --> 0:04:24,000
l'avaient espéré.

79
0:04:25.06,000 --> 0:04:29,000
Il s'est avéré que les échecs pouvaient être dominés par de la force brute,

80
0:04:29.86,000 --> 0:04:33,000
une fois que le matériel informatique est devenu assez rapide

81
0:04:34.14,000 --> 0:04:36,000
et que les algorithmes sont devenus assez malins.

82
0:04:38.58,000 --> 0:04:41,000
Cependant, si l'on s'en tient à ce qu'il produisait,

83
0:04:42.3,000 --> 0:04:45,000
du jeu d'échecs de niveau grand maître,

84
0:04:45.54,000 --> 0:04:46,000
Deep Blue était intelligent.

85
0:04:49.14,000 --> 0:04:51,000
Mais même à cette vitesse incroyable

86
0:04:52.38,000 --> 0:04:55,000
de 200 millions de positions par seconde,

87
0:04:57.18,000 --> 0:04:58,000
la méthode de Deep Blue

88
0:04:59.18,000 --> 0:05:05,000
ne permettait pas de percer le mystère de l'intelligence humaine comme on en rêve.

89
0:05:08.78,000 --> 0:05:09,000
Bientôt,

90
0:05:10.62,000 --> 0:05:12,000
des machines seront chauffeurs de taxi,

91
0:05:13.22,000 --> 0:05:15,000
médecins et professeurs,

92
0:05:15.66,000 --> 0:05:17,000
mais seront-elles pour autant « intelligentes » ?

93
0:05:19.66,000 --> 0:05:21,000
Je m'en remettrai pour ces définitions

94
0:05:22.18,000 --> 0:05:25,000
aux philosophes et au dictionnaire.

95
0:05:27.26,000 --> 0:05:3,000
Ce qui importe vraiment c'est ce que nous, les humains,

96
0:05:32.14,000 --> 0:05:35,000
nous ressentons à vivre et à travailler avec ces machines.

97
0:05:37.98,000 --> 0:05:42,000
Quand j'ai rencontré Deep Blue pour la première fois en février 1996,

98
0:05:43.26,000 --> 0:05:46,000
cela faisait plus de dix ans que j'étais champion du monde

99
0:05:47.9,000 --> 0:05:51,000
et j'avais disputé 182 parties de championnat du monde

100
0:05:51.94,000 --> 0:05:56,000
et des centaines contre des joueurs de haut niveau dans d'autres compétitions.

101
0:05:57.06,000 --> 0:06:02,000
Je savais à quoi m'attendre de la part de mes adversaires

102
0:06:02.14,000 --> 0:06:03,000
et de moi-même.

103
0:06:04.5,000 --> 0:06:09,000
J'avais l'habitude de mesurer leurs coups

104
0:06:09.7,000 --> 0:06:12,000
et d'évaluer leur état emotionnel

105
0:06:13.34,000 --> 0:06:16,000
en observant leur gestuelle et en les regardant dans les yeux.

106
0:06:17.7,000 --> 0:06:21,000
Et puis, me voilà assis de l'autre côté de l'échiquier, face à Deep Blue.

107
0:06:24.78,000 --> 0:06:26,000
J'ai tout de suite ressenti quelque chose de nouveau,

108
0:06:27.66,000 --> 0:06:28,000
de déstabilisant.

109
0:06:31.26,000 --> 0:06:33,000
Vous pourriez avoir la même sensation

110
0:06:35.14,000 --> 0:06:37,000
la première fois que vous prenez une voiture sans conducteur

111
0:06:37.966,000 --> 0:06:41,000
ou la première fois que votre nouveau chef-ordinateur vous donne un ordre.

112
0:06:45.62,000 --> 0:06:48,000
Mais quand je me suis assis pour jouer cette première partie,

113
0:06:49.9,000 --> 0:06:51,000
je ne pouvais pas être sûr

114
0:06:52.06,000 --> 0:06:55,000
de ce dont cette chose était capable.

115
0:06:56.74,000 --> 0:06:59,000
La technologie avance à pas de géant et IBM avait beaucoup investi.

116
0:07:00.5,000 --> 0:07:01,000
J'ai perdu cette partie.

117
0:07:04.14,000 --> 0:07:05,000
Et je ne pouvais m'empêcher de penser

118
0:07:05.94,000 --> 0:07:06,000
« peut-il être invincible ? »

119
0:07:08.42,000 --> 0:07:1,000
Était-ce la fin de ce jeu d'échecs que j'aimais tant ?

120
0:07:12.62,000 --> 0:07:16,000
C'étaient là des inquiétudes et des peurs toutes humaines,

121
0:07:16.78,000 --> 0:07:17,000
et la seule chose dont j'étais sûr,

122
0:07:19.22,000 --> 0:07:21,000
c'était que Deep Blue n'avait pas de tels émois.

123
0:07:22.14,000 --> 0:07:23,000
(Rires)

124
0:07:25.74,000 --> 0:07:26,000
Je me suis battu

125
0:07:28.22,000 --> 0:07:29,000
après ce coup dur

126
0:07:30.82,000 --> 0:07:31,000
pour gagner la première série

127
0:07:32.78,000 --> 0:07:33,000
mais les dés étaient jetés.

128
0:07:36.22,000 --> 0:07:38,000
J'ai finalement perdu contre la machine

129
0:07:38.38,000 --> 0:07:41,000
mais je n'ai pas souffert le même sort que John Henry

130
0:07:41.46,000 --> 0:07:44,000
qui gagna mais mourut le marteau à la main.

131
0:07:49.54,000 --> 0:07:51,000
En fait, le monde des échecs

132
0:07:52.1,000 --> 0:07:55,000
voulait toujours avoir un champion humain.

133
0:07:56.74,000 --> 0:07:57,000
Et aujourd'hui encore,

134
0:07:59.9,000 --> 0:08:02,000
quand une application gratuite de jeu d'échecs pour portable dernier cri

135
0:08:03.38,000 --> 0:08:05,000
est plus performante que Deep Blue,

136
0:08:05.42,000 --> 0:08:07,000
les gens jouent toujours aux échecs,

137
0:08:08.5,000 --> 0:08:1,000
et même plus qu'avant.

138
0:08:11.62,000 --> 0:08:14,000
Les alarmistes avaient prédit que tout le monde déserterait ce jeu

139
0:08:14.86,000 --> 0:08:16,000
qui pouvait être conquis par les machines,

140
0:08:17.14,000 --> 0:08:19,000
et ils ont eu tort, on le voit bien,

141
0:08:19.38,000 --> 0:08:22,000
mais jouer les alarmistes est un passe-temps populaire

142
0:08:22.86,000 --> 0:08:23,000
en matière de technologie.

143
0:08:26.18,000 --> 0:08:28,000
Ce que m'a appris mon expérience personnelle,

144
0:08:28.94,000 --> 0:08:32,000
c'est qu'il nous faut affronter nos peurs

145
0:08:33.62,000 --> 0:08:36,000
si nous voulons tirer le meilleur parti de notre technologie,

146
0:08:38.18,000 --> 0:08:4,000
et nous devons dépasser ces peurs

147
0:08:40.58,000 --> 0:08:45,000
si nous voulons obtenir le meilleur de ce que l'humanité peut donner.

148
0:08:47.94,000 --> 0:08:48,000
Tout en me remettant de ma défaite,

149
0:08:49.739,000 --> 0:08:5,000
j'ai été très inspiré

150
0:08:52.9,000 --> 0:08:54,000
par mes affrontements avec Deep Blue.

151
0:08:55.619,000 --> 0:08:58,000
Comme le dit ce proverbe russe : si on ne peut les vaincre, rejoignons-les.

152
0:09:00.7,000 --> 0:09:01,000
Et puis, je me suis dit,

153
0:09:02.1,000 --> 0:09:04,000
et si je pouvais jouer avec un ordinateur...

154
0:09:04.46,000 --> 0:09:07,000
avec un ordinateur à mes côtés, en combinant nos forces,

155
0:09:08.98,000 --> 0:09:11,000
l'intuition humaine et la capacité de calcul de la machine,

156
0:09:12.78,000 --> 0:09:14,000
la stratégie humaine, la tactique de la machine,

157
0:09:15.5,000 --> 0:09:17,000
l'expérience humaine, la mémoire de la machine.

158
0:09:17.94,000 --> 0:09:19,000
Serait-ce la partie la plus parfaite jamais jouée ?

159
0:09:21.82,000 --> 0:09:22,000
Mon idée est devenue réalité

160
0:09:24.74,000 --> 0:09:27,000
en 1998, sous le nom d'Advanced Chess,

161
0:09:28.14,000 --> 0:09:33,000
quand j'ai disputé cette compétition humain/machine contre un joueur d'élite.

162
0:09:35.1,000 --> 0:09:36,000
Mais lors de ce premier essai,

163
0:09:37.02,000 --> 0:09:43,000
nous avons tous les deux échoué à associer efficacement nos savoir-faire propres.

164
0:09:46.74,000 --> 0:09:48,000
L'Advanced Chess a trouvé sa place sur internet

165
0:09:49.98,000 --> 0:09:53,000
et en 2005, un tournoi d'échecs « freestyle »

166
0:09:54.86,000 --> 0:09:55,000
a été une révélation.

167
0:09:59.06,000 --> 0:10:02,000
Une équipe de grands maîtres et de machines de haut niveau participèrent,

168
0:10:02.62,000 --> 0:10:04,000
mais les gagnants ne furent ni les grands maîtres,

169
0:10:05.38,000 --> 0:10:06,000
ni un superordinateur.

170
0:10:07.5,000 --> 0:10:11,000
Les gagnants furent un duo de joueurs amateurs américains

171
0:10:11.86,000 --> 0:10:14,000
qui contrôlaient trois PC ordinaires à la fois.

172
0:10:17.38,000 --> 0:10:2,000
Leur talent à accompagner leurs machines

173
0:10:20.42,000 --> 0:10:25,000
a de fait contrecarré le savoir supérieur du jeu d'échecs

174
0:10:26.22,000 --> 0:10:27,000
des grands maîtres face à eux

175
0:10:27.82,000 --> 0:10:31,000
et le pouvoir informatique bien plus grand d'autres personnes.

176
0:10:33.42,000 --> 0:10:34,000
Et j'en suis venu à cette idée :

177
0:10:36.38,000 --> 0:10:39,000
un humain de faible niveau auquel s'ajoute une machine

178
0:10:39.78,000 --> 0:10:42,000
et une meilleure méthode est supérieur

179
0:10:43.06,000 --> 0:10:45,000
à une machine très puissante seule,

180
0:10:45.5,000 --> 0:10:48,000
mais plus remarquable encore, il est supérieur à un joueur humain fort

181
0:10:49.42,000 --> 0:10:5,000
auquel s'ajoute une machine

182
0:10:52.94,000 --> 0:10:54,000
et une méthode inférieure.

183
0:10:58.18,000 --> 0:11:,000
Cela m'a convaincu qu'on aurait besoin

184
0:11:01.82,000 --> 0:11:04,000
de meilleures interfaces pour aider à l'accompagnement des machines

185
0:11:06.34,000 --> 0:11:07,000
et rendre cette intelligence plus utile.

186
0:11:10.14,000 --> 0:11:13,000
L'humain plus la machine, ce n'est pas l'avenir,

187
0:11:13.46,000 --> 0:11:14,000
c'est notre présent.

188
0:11:14.7,000 --> 0:11:18,000
Tout le monde a déjà utilisé des outils de traduction en ligne

189
0:11:18.86,000 --> 0:11:22,000
pour comprendre les grandes lignes d'un article de presse étrangère

190
0:11:23.18,000 --> 0:11:24,000
malgré leurs imperfections.

191
0:11:25.5,000 --> 0:11:27,000
Nous utilisons après notre expérience humaine

192
0:11:27.62,000 --> 0:11:29,000
pour faire sens de tout ça,

193
0:11:29.74,000 --> 0:11:31,000
et puis la machine apprend de nos rectifications.

194
0:11:32.54,000 --> 0:11:37,000
Ce modèle se développe en diagnostic médical et en analyse de sécurité.

195
0:11:38.26,000 --> 0:11:4,000
La machine analyse des données,

196
0:11:41.14,000 --> 0:11:42,000
calcule des probabilités,

197
0:11:42.9,000 --> 0:11:45,000
fait 80 ou 90% du chemin,

198
0:11:46.58,000 --> 0:11:5,000
ce qui facilite l'analyse

199
0:11:50.98,000 --> 0:11:52,000
et la prise de décision humaines.

200
0:11:54.1,000 --> 0:11:58,000
Mais vous n'allez pas envoyer vos enfants

201
0:11:59.82,000 --> 0:12:02,000
à l'école dans une voiture sans conducteur fiable à 90%

202
0:12:04.42,000 --> 0:12:05,000
ou même à 99%.

203
0:12:07.38,000 --> 0:12:09,000
Nous avons donc besoin d'une grande avancée

204
0:12:10.26,000 --> 0:12:16,000
pour gagner encore quelques décimales cruciales.

205
0:12:18.98,000 --> 0:12:22,000
Vingt ans après ma série de parties contre Deep Blue,

206
0:12:24.02,000 --> 0:12:25,000
la deuxième,

207
0:12:25.66,000 --> 0:12:31,000
ce gros titre sensationnaliste, « le dernier combat du cerveau »,

208
0:12:31.98,000 --> 0:12:32,000
est omniprésent

209
0:12:33.58,000 --> 0:12:35,000
à l'heure où les machines intelligentes

210
0:12:36.14,000 --> 0:12:37,000
s'invitent

211
0:12:38.38,000 --> 0:12:4,000
dans tous les secteurs d'activité tous les jours.

212
0:12:41.98,000 --> 0:12:44,000
Mais là où par le passé

213
0:12:45.1,000 --> 0:12:46,000
les machines ont remplacé

214
0:12:48.3,000 --> 0:12:5,000
le bétail, le travail manuel,

215
0:12:50.7,000 --> 0:12:52,000
de nos jours, elles s'attaquent à des diplômés

216
0:12:53.22,000 --> 0:12:55,000
ou des personnes politiquement influentes.

217
0:12:55.84,000 --> 0:12:57,000
En tant que personne qui les a combattues et a perdu,

218
0:12:58.326,000 --> 0:13:,000
je suis là pour dire que c'est une excellente nouvelle.

219
0:13:02.82,000 --> 0:13:04,000
Un jour, toutes les professions

220
0:13:05.06,000 --> 0:13:07,000
devront faire face à cette pression

221
0:13:07.18,000 --> 0:13:12,000
ou bien cela voudra dire que l'humanité a cessé de progresser.

222
0:13:14.58,000 --> 0:13:15,000
Ce n'est pas à nous

223
0:13:17.26,000 --> 0:13:18,000
de choisir

224
0:13:20.3,000 --> 0:13:22,000
où et quand le progrès technologique s'arrêtera.

225
0:13:24.98,000 --> 0:13:25,000
On ne peut pas

226
0:13:27.78,000 --> 0:13:28,000
ralentir.

227
0:13:29.3,000 --> 0:13:3,000
À vrai dire,

228
0:13:31.14,000 --> 0:13:32,000
nous devons accélérer.

229
0:13:36.42,000 --> 0:13:38,000
Notre technologie excelle lorsqu'il s'agit d'effacer

230
0:13:41.02,000 --> 0:13:44,000
de nos vies difficultés et incertitudes,

231
0:13:46.82,000 --> 0:13:48,000
et nous devons donc partir à la recherche

232
0:13:49.66,000 --> 0:13:5,000
de défis plus grands,

233
0:13:51.54,000 --> 0:13:55,000
plus incertains encore.

234
0:14:00.02,000 --> 0:14:01,000
Les machines

235
0:14:03.7,000 --> 0:14:04,000
font des calculs.

236
0:14:05.54,000 --> 0:14:06,000
Nous comprenons les choses.

237
0:14:07.14,000 --> 0:14:09,000
Les machines reçoivent des instructions.

238
0:14:10.66,000 --> 0:14:11,000
Nous avons des buts.

239
0:14:12.54,000 --> 0:14:14,000
Les machines ont pour elles

240
0:14:16.9,000 --> 0:14:17,000
l'objectivité.

241
0:14:18.14,000 --> 0:14:19,000
Nous avons la passion.

242
0:14:20.42,000 --> 0:14:25,000
Nous ne devrions pas avoir peur de ce que nos machines peuvent faire aujourd'hui.

243
0:14:26.42,000 --> 0:14:3,000
Nous devrions plutôt nous inquiéter de ce qu'elles ne peuvent toujours pas faire

244
0:14:31.02,000 --> 0:14:36,000
car nous aurons besoin de l'aide de ces nouvelles machines intelligentes

245
0:14:36.54,000 --> 0:14:4,000
pour faire de nos rêves les plus fous une réalité.

246
0:14:41.82,000 --> 0:14:42,000
Et si nous échouons,

247
0:14:44.06,000 --> 0:14:48,000
si nous échouons, ce n'est pas parce que nos machines sont trop intelligentes

248
0:14:48.74,000 --> 0:14:49,000
ou pas assez.

249
0:14:51.02,000 --> 0:14:54,000
Si nous échouons, c'est parce que nous nous sommes laissés aller

250
0:14:55.5,000 --> 0:14:56,000
et avons rogné sur nos ambitions.

251
0:14:58.34,000 --> 0:15:01,000
Notre humanité n'est pas définie par un savoir-faire quelconque,

252
0:15:03.1,000 --> 0:15:05,000
comme manier le marteau ou même jouer aux échecs.

253
0:15:06.38,000 --> 0:15:09,000
L'humanité ne peut faire qu'une chose.

254
0:15:09.42,000 --> 0:15:1,000
Rêver.

255
0:15:11.94,000 --> 0:15:13,000
Alors faisons de grands rêves.

256
0:15:14.5,000 --> 0:15:15,000
Merci.

257
0:15:15.74,000 --> 0:15:18,000
(Applaudissements)

