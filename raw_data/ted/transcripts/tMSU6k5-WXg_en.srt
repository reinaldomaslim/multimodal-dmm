1
0:00:12.485,000 --> 0:00:14,000
Ten years ago, I wrote a book which I entitled

2
0:00:14.707,000 --> 0:00:17,000
"Our Final Century?" Question mark.

3
0:00:17.8,000 --> 0:00:2,000
My publishers cut out the question mark. (Laughter)

4
0:00:21.377,000 --> 0:00:22,000
The American publishers changed our title

5
0:00:23.259,000 --> 0:00:26,000
to "Our Final Hour."

6
0:00:27.168,000 --> 0:00:3,000
Americans like instant gratification and the reverse.

7
0:00:30.66,000 --> 0:00:31,000
(Laughter)

8
0:00:32.368,000 --> 0:00:33,000
And my theme was this:

9
0:00:34.118,000 --> 0:00:38,000
Our Earth has existed for 45 million centuries,

10
0:00:38.284,000 --> 0:00:4,000
but this one is special —

11
0:00:40.297,000 --> 0:00:43,000
it's the first where one species, ours,

12
0:00:43.313,000 --> 0:00:45,000
has the planet's future in its hands.

13
0:00:46.115,000 --> 0:00:47,000
Over nearly all of Earth's history,

14
0:00:48.105,000 --> 0:00:49,000
threats have come from nature —

15
0:00:50.041,000 --> 0:00:53,000
disease, earthquakes, asteroids and so forth —

16
0:00:53.537,000 --> 0:00:58,000
but from now on, the worst dangers come from us.

17
0:00:59.209,000 --> 0:01:02,000
And it's now not just the nuclear threat;

18
0:01:02.48,000 --> 0:01:03,000
in our interconnected world,

19
0:01:04.231,000 --> 0:01:07,000
network breakdowns can cascade globally;

20
0:01:07.394,000 --> 0:01:1,000
air travel can spread pandemics worldwide within days;

21
0:01:11.35,000 --> 0:01:14,000
and social media can spread panic and rumor

22
0:01:14.677,000 --> 0:01:17,000
literally at the speed of light.

23
0:01:17.894,000 --> 0:01:2,000
We fret too much about minor hazards —

24
0:01:21.119,000 --> 0:01:25,000
improbable air crashes, carcinogens in food,

25
0:01:25.15,000 --> 0:01:27,000
low radiation doses, and so forth —

26
0:01:27.376,000 --> 0:01:29,000
but we and our political masters

27
0:01:30.201,000 --> 0:01:34,000
are in denial about catastrophic scenarios.

28
0:01:34.404,000 --> 0:01:37,000
The worst have thankfully not yet happened.

29
0:01:37.442,000 --> 0:01:39,000
Indeed, they probably won't.

30
0:01:39.638,000 --> 0:01:42,000
But if an event is potentially devastating,

31
0:01:42.823,000 --> 0:01:44,000
it's worth paying a substantial premium

32
0:01:45.691,000 --> 0:01:48,000
to safeguard against it, even if it's unlikely,

33
0:01:49.527,000 --> 0:01:53,000
just as we take out fire insurance on our house.

34
0:01:54.04,000 --> 0:01:58,000
And as science offers greater power and promise,

35
0:01:59.037,000 --> 0:02:02,000
the downside gets scarier too.

36
0:02:02.903,000 --> 0:02:04,000
We get ever more vulnerable.

37
0:02:05.142,000 --> 0:02:06,000
Within a few decades,

38
0:02:06.98,000 --> 0:02:08,000
millions will have the capability

39
0:02:09.21,000 --> 0:02:12,000
to misuse rapidly advancing biotech,

40
0:02:12.331,000 --> 0:02:15,000
just as they misuse cybertech today.

41
0:02:15.884,000 --> 0:02:18,000
Freeman Dyson, in a TED Talk,

42
0:02:19.083,000 --> 0:02:22,000
foresaw that children will design and create new organisms

43
0:02:22.679,000 --> 0:02:26,000
just as routinely as his generation played with chemistry sets.

44
0:02:27.19,000 --> 0:02:29,000
Well, this may be on the science fiction fringe,

45
0:02:29.718,000 --> 0:02:32,000
but were even part of his scenario to come about,

46
0:02:32.901,000 --> 0:02:34,000
our ecology and even our species

47
0:02:35.638,000 --> 0:02:38,000
would surely not survive long unscathed.

48
0:02:39.627,000 --> 0:02:42,000
For instance, there are some eco-extremists

49
0:02:43.49,000 --> 0:02:45,000
who think that it would be better for the planet,

50
0:02:45.999,000 --> 0:02:48,000
for Gaia, if there were far fewer humans.

51
0:02:49.402,000 --> 0:02:51,000
What happens when such people have mastered

52
0:02:52.119,000 --> 0:02:54,000
synthetic biology techniques

53
0:02:54.256,000 --> 0:02:56,000
that will be widespread by 2050?

54
0:02:57.108,000 --> 0:03:,000
And by then, other science fiction nightmares

55
0:03:00.15,000 --> 0:03:01,000
may transition to reality:

56
0:03:01.86,000 --> 0:03:03,000
dumb robots going rogue,

57
0:03:03.93,000 --> 0:03:05,000
or a network that develops a mind of its own

58
0:03:06.347,000 --> 0:03:08,000
threatens us all.

59
0:03:08.936,000 --> 0:03:11,000
Well, can we guard against such risks by regulation?

60
0:03:12.206,000 --> 0:03:14,000
We must surely try, but these enterprises

61
0:03:14.613,000 --> 0:03:17,000
are so competitive, so globalized,

62
0:03:18.142,000 --> 0:03:19,000
and so driven by commercial pressure,

63
0:03:20.122,000 --> 0:03:23,000
that anything that can be done will be done somewhere,

64
0:03:23.407,000 --> 0:03:25,000
whatever the regulations say.

65
0:03:25.443,000 --> 0:03:28,000
It's like the drug laws — we try to regulate, but can't.

66
0:03:28.93,000 --> 0:03:31,000
And the global village will have its village idiots,

67
0:03:31.974,000 --> 0:03:34,000
and they'll have a global range.

68
0:03:35.47,000 --> 0:03:37,000
So as I said in my book,

69
0:03:37.761,000 --> 0:03:39,000
we'll have a bumpy ride through this century.

70
0:03:40.65,000 --> 0:03:43,000
There may be setbacks to our society —

71
0:03:44.14,000 --> 0:03:48,000
indeed, a 50 percent chance of a severe setback.

72
0:03:48.255,000 --> 0:03:5,000
But are there conceivable events

73
0:03:51.169,000 --> 0:03:53,000
that could be even worse,

74
0:03:53.33,000 --> 0:03:56,000
events that could snuff out all life?

75
0:03:56.76,000 --> 0:03:58,000
When a new particle accelerator came online,

76
0:03:59.686,000 --> 0:04:,000
some people anxiously asked,

77
0:04:01.475,000 --> 0:04:03,000
could it destroy the Earth or, even worse,

78
0:04:03.725,000 --> 0:04:05,000
rip apart the fabric of space?

79
0:04:06.384,000 --> 0:04:09,000
Well luckily, reassurance could be offered.

80
0:04:09.927,000 --> 0:04:11,000
I and others pointed out that nature

81
0:04:11.971,000 --> 0:04:12,000
has done the same experiments

82
0:04:13.904,000 --> 0:04:15,000
zillions of times already,

83
0:04:16.09,000 --> 0:04:17,000
via cosmic ray collisions.

84
0:04:17.855,000 --> 0:04:2,000
But scientists should surely be precautionary

85
0:04:20.909,000 --> 0:04:22,000
about experiments that generate conditions

86
0:04:23.489,000 --> 0:04:25,000
without precedent in the natural world.

87
0:04:25.972,000 --> 0:04:28,000
Biologists should avoid release of potentially devastating

88
0:04:29.395,000 --> 0:04:31,000
genetically modified pathogens.

89
0:04:32.11,000 --> 0:04:35,000
And by the way, our special aversion

90
0:04:35.627,000 --> 0:04:38,000
to the risk of truly existential disasters

91
0:04:39.088,000 --> 0:04:42,000
depends on a philosophical and ethical question,

92
0:04:42.363,000 --> 0:04:43,000
and it's this:

93
0:04:44.033,000 --> 0:04:46,000
Consider two scenarios.

94
0:04:46.341,000 --> 0:04:51,000
Scenario A wipes out 90 percent of humanity.

95
0:04:51.577,000 --> 0:04:54,000
Scenario B wipes out 100 percent.

96
0:04:55.473,000 --> 0:04:57,000
How much worse is B than A?

97
0:04:58.391,000 --> 0:05:01,000
Some would say 10 percent worse.

98
0:05:01.414,000 --> 0:05:04,000
The body count is 10 percent higher.

99
0:05:04.564,000 --> 0:05:06,000
But I claim that B is incomparably worse.

100
0:05:07.47,000 --> 0:05:09,000
As an astronomer, I can't believe

101
0:05:10.099,000 --> 0:05:12,000
that humans are the end of the story.

102
0:05:12.566,000 --> 0:05:15,000
It is five billion years before the sun flares up,

103
0:05:15.889,000 --> 0:05:17,000
and the universe may go on forever,

104
0:05:18.6,000 --> 0:05:2,000
so post-human evolution,

105
0:05:20.892,000 --> 0:05:22,000
here on Earth and far beyond,

106
0:05:23.082,000 --> 0:05:25,000
could be as prolonged as the Darwinian process

107
0:05:25.796,000 --> 0:05:28,000
that's led to us, and even more wonderful.

108
0:05:29.077,000 --> 0:05:31,000
And indeed, future evolution will happen much faster,

109
0:05:31.741,000 --> 0:05:33,000
on a technological timescale,

110
0:05:33.94,000 --> 0:05:35,000
not a natural selection timescale.

111
0:05:36.239,000 --> 0:05:4,000
So we surely, in view of those immense stakes,

112
0:05:40.434,000 --> 0:05:43,000
shouldn't accept even a one in a billion risk

113
0:05:43.82,000 --> 0:05:45,000
that human extinction would foreclose

114
0:05:46.049,000 --> 0:05:48,000
this immense potential.

115
0:05:48.359,000 --> 0:05:49,000
Some scenarios that have been envisaged

116
0:05:50.131,000 --> 0:05:51,000
may indeed be science fiction,

117
0:05:51.95,000 --> 0:05:54,000
but others may be disquietingly real.

118
0:05:55.336,000 --> 0:05:57,000
It's an important maxim that the unfamiliar

119
0:05:58.21,000 --> 0:06:,000
is not the same as the improbable,

120
0:06:00.907,000 --> 0:06:02,000
and in fact, that's why we at Cambridge University

121
0:06:03.305,000 --> 0:06:06,000
are setting up a center to study how to mitigate

122
0:06:06.68,000 --> 0:06:08,000
these existential risks.

123
0:06:08.712,000 --> 0:06:11,000
It seems it's worthwhile just for a few people

124
0:06:11.775,000 --> 0:06:13,000
to think about these potential disasters.

125
0:06:14.091,000 --> 0:06:17,000
And we need all the help we can get from others,

126
0:06:17.104,000 --> 0:06:19,000
because we are stewards of a precious

127
0:06:19.583,000 --> 0:06:22,000
pale blue dot in a vast cosmos,

128
0:06:23.066,000 --> 0:06:26,000
a planet with 50 million centuries ahead of it.

129
0:06:26.444,000 --> 0:06:28,000
And so let's not jeopardize that future.

130
0:06:29,000 --> 0:06:3,000
And I'd like to finish with a quote

131
0:06:30.795,000 --> 0:06:33,000
from a great scientist called Peter Medawar.

132
0:06:34.296,000 --> 0:06:37,000
I quote, "The bells that toll for mankind

133
0:06:37.569,000 --> 0:06:39,000
are like the bells of Alpine cattle.

134
0:06:40.213,000 --> 0:06:42,000
They are attached to our own necks,

135
0:06:42.499,000 --> 0:06:44,000
and it must be our fault if they do not make

136
0:06:45.174,000 --> 0:06:47,000
a tuneful and melodious sound."

137
0:06:47.305,000 --> 0:06:49,000
Thank you very much.

138
0:06:49.572,000 --> 0:06:51,000
(Applause)

