1
0:00:,000 --> 0:00:07,000
Traductor: María Julia Galles Revisor: Silvina Katz

2
0:00:12.875,000 --> 0:00:15,000
¿Cuántas decisiones sobre Uds. han sido tomadas hoy,

3
0:00:16.667,000 --> 0:00:18,000
o esta semana, o este año,

4
0:00:19.292,000 --> 0:00:2,000
por la inteligencia artificial?

5
0:00:22.958,000 --> 0:00:23,000
Me gano la vida construyendo IA,

6
0:00:24.667,000 --> 0:00:27,000
así que, revelación total, soy una especie de nerda.

7
0:00:27.708,000 --> 0:00:29,000
Y como lo soy,

8
0:00:30.125,000 --> 0:00:32,000
cada vez que sale una noticia

9
0:00:32.5,000 --> 0:00:35,000
sobre la inteligencia artificial que nos roba nuestros trabajos

10
0:00:35.958,000 --> 0:00:39,000
o sobre robots que obtienen la ciudadanía de un país real,

11
0:00:40.167,000 --> 0:00:43,000
yo soy a quien mis amigos y mis seguidores le mandan mensajes

12
0:00:43.333,000 --> 0:00:44,000
asustados por el futuro.

13
0:00:45.833,000 --> 0:00:47,000
Lo vemos en todas partes:

14
0:00:47.958,000 --> 0:00:51,000
este pánico mediático en el que nuestros amos robots toman el control.

15
0:00:52.875,000 --> 0:00:53,000
Podríamos culpar de eso a Hollywood.

16
0:00:56.125,000 --> 0:01:,000
Pero en realidad, ese no es el problema en el que nos debemos concentrar.

17
0:01:01.25,000 --> 0:01:04,000
Con la IA existe un problema más urgente, un riesgo mayor

18
0:01:04.917,000 --> 0:01:05,000
que debemos resolver primero.

19
0:01:07.417,000 --> 0:01:09,000
Así que volvamos a esta pregunta:

20
0:01:09.75,000 --> 0:01:13,000
¿Cuántas decisiones sobre Uds. fueron tomadas hoy mediante la IA?

21
0:01:15.792,000 --> 0:01:16,000
Y cuántas de estas

22
0:01:17.792,000 --> 0:01:21,000
se han basado en su sexo, su raza o su origen?

23
0:01:24.5,000 --> 0:01:26,000
Los algoritmos se usan todo el tiempo

24
0:01:27.292,000 --> 0:01:3,000
para tomar decisiones sobre quiénes somos y qué queremos.

25
0:01:32.208,000 --> 0:01:35,000
Algunas de las mujeres en esta sala saben de lo que hablo

26
0:01:35.875,000 --> 0:01:38,000
si han sido expuestas a esos avisos sobre pruebas de embarazo en YouTube

27
0:01:39.667,000 --> 0:01:41,000
como unas 1000 veces.

28
0:01:41.75,000 --> 0:01:43,000
O han visto avisos pasados sobre clínicas de fertilidad

29
0:01:44.625,000 --> 0:01:46,000
en su página de Facebook.

30
0:01:47.625,000 --> 0:01:49,000
O, en mi caso, oficinas indias de matrimonio.

31
0:01:50.042,000 --> 0:01:51,000
(Risas)

32
0:01:51.333,000 --> 0:01:53,000
Pero la IA no solo se usa para tomar decisiones

33
0:01:54.333,000 --> 0:01:56,000
sobre los productos que queremos comprar

34
0:01:56.958,000 --> 0:01:58,000
o el próximo programa de televisión que queremos ver.

35
0:02:01.042,000 --> 0:02:06,000
Me pregunto qué sentirían ante alguien que piensa cosas como esta:

36
0:02:06.25,000 --> 0:02:07,000
"Una persona negra o latina

37
0:02:08.208,000 --> 0:02:1,000
tiene menos probabilidad de saldar a tiempo un préstamo

38
0:02:11.177,000 --> 0:02:13,000
que una persona blanca".

39
0:02:13.542,000 --> 0:02:15,000
"Una persona llamada Juan es un mejor programador

40
0:02:16.375,000 --> 0:02:17,000
que una persona llamada María".

41
0:02:19.25,000 --> 0:02:24,000
"Es más probable que una persona negra sea reincidente que una persona blanca".

42
0:02:26.958,000 --> 0:02:27,000
Probablemente piensen:

43
0:02:28.25,000 --> 0:02:31,000
"Oh, eso suena como una persona bastante sexista y racista", ¿no?

44
0:02:33,000 --> 0:02:37,000
Estas son algunas de las decisiones reales que ha tomado la IA muy recientemente,

45
0:02:37.875,000 --> 0:02:39,000
basada en los prejuicios que ha aprendido de nosotros,

46
0:02:40.833,000 --> 0:02:41,000
de los humanos.

47
0:02:43.75,000 --> 0:02:47,000
La IA se usa para ayudar a decidir si reciben o no esa entrevista de trabajo,

48
0:02:48.583,000 --> 0:02:5,000
cuánto pagan por el seguro del auto,

49
0:02:51,000 --> 0:02:52,000
cuán bueno es su historial de crédito,

50
0:02:52.917,000 --> 0:02:55,000
e incluso qué calificación reciben en su revisión anual de desempeño.

51
0:02:57.083,000 --> 0:03:,000
Pero estas decisiones se filtran a través

52
0:03:00.25,000 --> 0:03:05,000
de sus supuestos sobre nuestra identidad, nuestra raza, nuestro sexo, nuestra edad.

53
0:03:08.25,000 --> 0:03:1,000
¿Cómo ocurre eso?

54
0:03:10.542,000 --> 0:03:13,000
Ahora imaginen que la IA está ayudando a un gerente de contrataciones

55
0:03:14.083,000 --> 0:03:16,000
a encontrar el siguiente líder de tecnología para la compañía.

56
0:03:16.994,000 --> 0:03:19,000
Hasta ahora el gerente ha estado contratando mayormente hombres.

57
0:03:20.083,000 --> 0:03:22,000
Entonces, la IA aprende que es más probable

58
0:03:22.832,000 --> 0:03:24,000
que los programadores sean hombres, en vez de mujeres.

59
0:03:25.542,000 --> 0:03:27,000
Y de ahí hay un estrecho muy corto hasta:

60
0:03:28.458,000 --> 0:03:3,000
"los hombres son mejores programadores que las mujeres".

61
0:03:31.42,000 --> 0:03:34,000
Hemos reforzado nuestros propios prejuicios en la IA.

62
0:03:35.167,000 --> 0:03:38,000
Y ahora elimina a las candidatas femeninas.

63
0:03:40.917,000 --> 0:03:43,000
Si un gerente de contratación humano hiciera eso

64
0:03:43.958,000 --> 0:03:45,000
estaríamos indignados, no lo permitiríamos.

65
0:03:46.333,000 --> 0:03:49,000
Este tipo de discriminación por el sexo no está bien.

66
0:03:49.833,000 --> 0:03:53,000
Y, sin embargo, de algún modo, la IA se ha situado sobre la ley,

67
0:03:54.375,000 --> 0:03:56,000
ya que una máquina tomó la decisión.

68
0:03:57.833,000 --> 0:03:58,000
No solo eso.

69
0:03:59.375,000 --> 0:04:03,000
Estamos reforzando también nuestro prejuicio en cómo interactuamos con la IA.

70
0:04:04.917,000 --> 0:04:09,000
¿Cuán a menudo usan un asistente de voz como Siri, Alexa o incluso Cortana?

71
0:04:10.917,000 --> 0:04:12,000
Ellos tienen dos cosas en común:

72
0:04:13.5,000 --> 0:04:16,000
una, nunca pueden captar bien mi nombre,

73
0:04:16.625,000 --> 0:04:18,000
y segundo, todas son femeninas.

74
0:04:20.417,000 --> 0:04:22,000
Están designadas para ser nuestros sirvientes obedientes,

75
0:04:23.208,000 --> 0:04:26,000
que encienden y apagan nuestras luces, que encargan nuestra compra.

76
0:04:27.125,000 --> 0:04:3,000
También hay una IA masculina, pero tiende a tener más poder,

77
0:04:30.458,000 --> 0:04:33,000
como IBM Watson, que toma decisiones de negocios,

78
0:04:33.541,000 --> 0:04:36,000
Salesforce Einstein o ROSS, el robot abogado.

79
0:04:38.208,000 --> 0:04:42,000
Así que, pobres robots, incluso ellos sufren el sexismo en el trabajo.

80
0:04:42.292,000 --> 0:04:43,000
(Risas)

81
0:04:44.542,000 --> 0:04:46,000
Piensen en cómo se combinan estas dos cosas

82
0:04:47.417,000 --> 0:04:52,000
y en cómo afectan a un niño que crece en el mundo de hoy, con la IA.

83
0:04:52.75,000 --> 0:04:54,000
Por ejemplo, hacen una investigación para un proyecto escolar

84
0:04:55.708,000 --> 0:04:58,000
y buscan en Google imágenes de directores generales.

85
0:04:58.75,000 --> 0:05:,000
El algoritmo les muestra una mayoría de hombres como resultado.

86
0:05:01.723,000 --> 0:05:03,000
Y ahora buscan en Google "asistente personal".

87
0:05:04.25,000 --> 0:05:07,000
Como se imaginarán, les muestra una mayoría de mujeres.

88
0:05:07.708,000 --> 0:05:1,000
Y luego ellos quieren poner algo de música y quizás encargar algo de comida,

89
0:05:11.333,000 --> 0:05:17,000
y dan órdenes, gritando, a una asistente de voz femenina.

90
0:05:19.542,000 --> 0:05:24,000
Algunas de nuestras mentes más brillantes están creando hoy en día esta tecnología.

91
0:05:24.875,000 --> 0:05:28,000
Una tecnología que podrían haber creado del modo que quisieran.

92
0:05:29.083,000 --> 0:05:32,000
Y, sin embargo, eligieron crearla al estilo

93
0:05:32.492,000 --> 0:05:34,000
de la secretaria de "Mad Men" de los años 50.

94
0:05:34.792,000 --> 0:05:35,000
¡Yey!

95
0:05:36.958,000 --> 0:05:37,000
Pero no se preocupen,

96
0:05:38.292,000 --> 0:05:4,000
no voy a terminar diciéndoles

97
0:05:40.375,000 --> 0:05:43,000
que vamos rumbo hacia que las máquinas sexistas y racistas dominen el mundo.

98
0:05:44.792,000 --> 0:05:49,000
La buena noticia sobre la IA es que está completamente bajo nuestro control.

99
0:05:51.333,000 --> 0:05:55,000
Tenemos que enseñarle a la IA los valores correctos, la ética correcta.

100
0:05:56.167,000 --> 0:05:58,000
Y podemos hacer tres cosas.

101
0:05:58.375,000 --> 0:06:01,000
Una, podemos ser conscientes de nuestros propios prejuicios

102
0:06:01.75,000 --> 0:06:03,000
y de los prejuicios en las máquinas a nuestro alrededor.

103
0:06:04.5,000 --> 0:06:08,000
Dos, podemos asegurarnos de que equipos heterogéneos construyan esta tecnología.

104
0:06:09.042,000 --> 0:06:13,000
Y tres, debemos brindarle experiencias diferentes para que aprenda.

105
0:06:14.875,000 --> 0:06:17,000
Desde mi experiencia personal puedo hablar sobre las dos primeras.

106
0:06:18.208,000 --> 0:06:19,000
Cuando trabajas en tecnología

107
0:06:19.667,000 --> 0:06:22,000
y no te pareces a Mark Zuckerberg o Elon Musk,

108
0:06:23.083,000 --> 0:06:26,000
tu vida es un poco difícil, cuestionan tu capacidad.

109
0:06:27.875,000 --> 0:06:28,000
Este es un ejemplo.

110
0:06:29.292,000 --> 0:06:32,000
Como muchos desarrolladores, a menudo entro en foros tecnológicos en línea

111
0:06:33.042,000 --> 0:06:36,000
y comparto mis conocimientos para ayudar a otros.

112
0:06:36.292,000 --> 0:06:37,000
Y he encontrado

113
0:06:37.625,000 --> 0:06:4,000
que cuando accedo como yo, con mi foto y mi nombre,

114
0:06:41.625,000 --> 0:06:45,000
tiendo a recibir preguntas o comentarios como este:

115
0:06:46.25,000 --> 0:06:49,000
"¿Qué te hace pensar que estás cualificada para hablar sobre IA?".

116
0:06:50.458,000 --> 0:06:53,000
"Qué te hace pensar que sabes sobre el aprendizaje automático?".

117
0:06:53.958,000 --> 0:06:56,000
Así que, como Uds. lo harían, hice un nuevo perfil de Internet,

118
0:06:57.417,000 --> 0:07:01,000
y esta vez, en lugar de mi foto, elegí un gato con una mochila propulsora.

119
0:07:02.292,000 --> 0:07:04,000
Y elegí un nombre que no revelara mi sexo.

120
0:07:05.917,000 --> 0:07:07,000
Pueden imaginarse a qué conduce esto, ¿no?

121
0:07:08.667,000 --> 0:07:14,000
Esta vez no recibí ninguno de estos comentarios despectivos sobre mi capacidad

122
0:07:15.083,000 --> 0:07:18,000
y pude lograr algunas cosas.

123
0:07:19.5,000 --> 0:07:2,000
Y esto apesta, chicos.

124
0:07:21.375,000 --> 0:07:23,000
He estado construyendo robots desde los 15 años,

125
0:07:23.875,000 --> 0:07:25,000
tengo algunos títulos en informática

126
0:07:26.167,000 --> 0:07:28,000
y, sin embargo, tengo que esconder mi sexo

127
0:07:28.625,000 --> 0:07:3,000
para que tomen en serio mi trabajo.

128
0:07:31.875,000 --> 0:07:32,000
Entonces, ¿qué es lo que ocurre aquí?

129
0:07:33.792,000 --> 0:07:36,000
¿Los hombres son mejores que las mujeres en cuanto a la tecnología?

130
0:07:37.917,000 --> 0:07:38,000
Otro estudio descubrió

131
0:07:39.5,000 --> 0:07:43,000
que cuando las mujeres que hacen código en una plataforma también ocultaron su sexo,

132
0:07:44.458,000 --> 0:07:47,000
su código fue aceptado un 4 % más que el de los hombres.

133
0:07:48.542,000 --> 0:07:5,000
Así que no es cuestión de talento.

134
0:07:51.958,000 --> 0:07:53,000
Se trata de un elitismo en la IA

135
0:07:54.875,000 --> 0:07:56,000
que dice que un programador debe tener cierto aspecto.

136
0:07:59.375,000 --> 0:08:02,000
Lo que debemos hacer realmente para mejorar la IA

137
0:08:02.5,000 --> 0:08:05,000
es atraer a personas con orígenes variados.

138
0:08:06.542,000 --> 0:08:08,000
Necesitamos gente que pueda escribir y contar historias

139
0:08:09.161,000 --> 0:08:11,000
para ayudarnos a crear las personalidades de la IA.

140
0:08:12.208,000 --> 0:08:14,000
Necesitamos gente capaz de resolver problemas.

141
0:08:15.125,000 --> 0:08:18,000
Necesitamos gente que enfrente desafíos diferentes

142
0:08:18.917,000 --> 0:08:23,000
y que nos digan cuáles son los problemas reales que necesitan resolverse

143
0:08:24.292,000 --> 0:08:27,000
y que nos ayuden a encontrar los modos en que la tecnología puede resolverlos.

144
0:08:29.833,000 --> 0:08:32,000
Porque cuando las personas de distintos orígenes se unen,

145
0:08:33.583,000 --> 0:08:35,000
cuando construimos las cosas del modo correcto,

146
0:08:35.796,000 --> 0:08:36,000
las posibilidades son ilimitadas.

147
0:08:38.75,000 --> 0:08:41,000
Y eso es de lo que finalmente quiero hablarles.

148
0:08:42.083,000 --> 0:08:46,000
Menos robots racistas, menos máquinas que tomarán nuestros trabajos...

149
0:08:46.332,000 --> 0:08:49,000
y más sobre lo que la tecnología puede lograr.

150
0:08:50.292,000 --> 0:08:53,000
Así que, sí, algo de la energía en el mundo de la IA,

151
0:08:53.75,000 --> 0:08:54,000
en el mundo de la tecnología

152
0:08:55.167,000 --> 0:08:59,000
estará dirigida hacia la publicidad que ven cuando están en Internet.

153
0:08:59.458,000 --> 0:09:04,000
Pero mucho de ella se dirigirá a hacer que el mundo sea mucho mejor.

154
0:09:05.5,000 --> 0:09:08,000
Piensen en una mujer embarazada en la República Democrática del Congo,

155
0:09:09.292,000 --> 0:09:13,000
que debe caminar 17 horas hasta la clínica rural prenatal más cercana

156
0:09:13.5,000 --> 0:09:14,000
para hacerse una revisión médica.

157
0:09:15.375,000 --> 0:09:18,000
¿Y si en lugar de eso ella pudiera obtener un diagnóstico por teléfono?

158
0:09:19.75,000 --> 0:09:2,000
O piensen en lo que la IA podría hacer

159
0:09:21.583,000 --> 0:09:23,000
por esa mujer de cada tres en Sudáfrica

160
0:09:24.333,000 --> 0:09:26,000
que sufre la violencia doméstica.

161
0:09:27.083,000 --> 0:09:29,000
Si fuera peligroso hablar en voz alta,

162
0:09:29.783,000 --> 0:09:31,000
ella podría tener un servicio de IA para lanzar la alarma,

163
0:09:32.539,000 --> 0:09:34,000
para obtener ayuda financiera y legal.

164
0:09:35.958,000 --> 0:09:4,000
Estos son ejemplos de proyectos reales que la gente, incluyéndome a mí,

165
0:09:41,000 --> 0:09:43,000
está trabajando ahora, empleando la IA.

166
0:09:45.542,000 --> 0:09:48,000
Así que estoy segura de que en los próximos días habrá otra noticia

167
0:09:49.167,000 --> 0:09:51,000
sobre el riesgo existencial,

168
0:09:51.875,000 --> 0:09:53,000
robots tomando el control y tomando sus trabajos.

169
0:09:54.333,000 --> 0:09:55,000
(Risas)

170
0:09:55.375,000 --> 0:09:57,000
Y cuando pase algo como eso,

171
0:09:57.708,000 --> 0:10:,000
se que recibiré los mismos mensajes de preocupación con respecto al futuro.

172
0:10:01.333,000 --> 0:10:04,000
Pero me siento muy positiva con respecto a esta tecnología.

173
0:10:07.458,000 --> 0:10:12,000
Esta es nuestra oportunidad de hacer el mundo un lugar más igualitario.

174
0:10:14.458,000 --> 0:10:18,000
Pero para hacerlo debemos construirlo del modo correcto desde el principio.

175
0:10:19.667,000 --> 0:10:24,000
Necesitamos personas de distintos sexos, razas, sexualidades y orígenes.

176
0:10:26.458,000 --> 0:10:28,000
Necesitamos que las mujeres sean hacedoras,

177
0:10:28.958,000 --> 0:10:31,000
y no solo las máquinas que sigan las órdenes de los hombres.

178
0:10:33.875,000 --> 0:10:36,000
Debemos pensar muy cuidadosamente lo que le enseñamos a las máquinas,

179
0:10:37.667,000 --> 0:10:38,000
los datos que les damos,

180
0:10:39.333,000 --> 0:10:42,000
de modo que no repitan nuestros errores pasados.

181
0:10:44.125,000 --> 0:10:47,000
Espero dejarlos pensando en dos cosas.

182
0:10:48.542,000 --> 0:10:52,000
Primero, espero dejarlos pensando en los prejuicios.

183
0:10:53.125,000 --> 0:10:56,000
Y la próxima vez que vean una publicidad en Internet

184
0:10:56.333,000 --> 0:10:58,000
que asume que están interesados en clínicas de fertilidad

185
0:10:59.167,000 --> 0:11:01,000
o en sitios web de apuestas en línea,

186
0:11:02.042,000 --> 0:11:04,000
que piensen y recuerden

187
0:11:04.083,000 --> 0:11:08,000
que la misma tecnología asume que un hombre negro será reincidente;

188
0:11:09.833,000 --> 0:11:13,000
o que es más probable que una mujer sea una asistente personal y no una directora.

189
0:11:14.958,000 --> 0:11:17,000
Y espero que les recuerde que debemos hacer algo sobre eso.

190
0:11:20.917,000 --> 0:11:21,000
Y segundo,

191
0:11:22.792,000 --> 0:11:23,000
espero que piensen sobre el hecho

192
0:11:24.708,000 --> 0:11:26,000
de que no necesitan tener un cierto aspecto

193
0:11:26.714,000 --> 0:11:29,000
o tener una cierta formación en ingeniería o tecnología

194
0:11:30.583,000 --> 0:11:31,000
para crear IA,

195
0:11:31.875,000 --> 0:11:33,000
lo que será una fuerza fenomenal para nuestro futuro.

196
0:11:36.166,000 --> 0:11:38,000
No necesitan ser parecidos a Mark Zuckerberg,

197
0:11:38.333,000 --> 0:11:39,000
pueden parecerse a mí.

198
0:11:41.25,000 --> 0:11:43,000
Y depende de todos Uds. en esta sala

199
0:11:44.167,000 --> 0:11:46,000
poder convencer a los gobiernos y las corporaciones

200
0:11:46.917,000 --> 0:11:48,000
para que construyan una IA para todos,

201
0:11:49.833,000 --> 0:11:51,000
incluso los casos marginales.

202
0:11:52.25,000 --> 0:11:54,000
Y que todos nos eduquemos en el futuro

203
0:11:54.333,000 --> 0:11:56,000
sobre esta tecnología fenomenal.

204
0:11:58.167,000 --> 0:12:,000
Porque si hacemos eso,

205
0:12:00.208,000 --> 0:12:04,000
habremos arañado la superficie de lo que podemos lograr con la IA.

206
0:12:05.125,000 --> 0:12:06,000
Gracias.

207
0:12:06.417,000 --> 0:12:08,000
(Aplausos)

