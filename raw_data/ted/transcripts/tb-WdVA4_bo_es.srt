1
0:00:,000 --> 0:00:07,000
Traductor: Analia Padin Revisor: Sebastian Betti

2
0:00:12.82,000 --> 0:00:16,000
Hoy voy a hablar de tecnología y de la sociedad.

3
0:00:18.86,000 --> 0:00:21,000
El Departamento de Transporte estimó que, el año pasado,

4
0:00:22.58,000 --> 0:00:26,000
hubo 35 000 muertos en accidentes de auto, tan solo en EE.UU.

5
0:00:27.86,000 --> 0:00:31,000
A nivel mundial, mueren 1,2 millones de personas por año en accidentes de auto.

6
0:00:33.58,000 --> 0:00:37,000
Si hubiera una manera de eliminar el 90 % de esos accidentes,

7
0:00:37.7,000 --> 0:00:38,000
¿apoyarían la causa?

8
0:00:39.54,000 --> 0:00:4,000
Por supuesto que lo harían.

9
0:00:40.86,000 --> 0:00:43,000
Esto es lo que promete la tecnología de vehículos autónomos,

10
0:00:44.54,000 --> 0:00:46,000
al eliminar la principal causa de accidentes:

11
0:00:47.38,000 --> 0:00:48,000
el error humano.

12
0:00:49.74,000 --> 0:00:54,000
Imagínense en el año 2030, viajando en un vehículo autónomo,

13
0:00:55.18,000 --> 0:00:58,000
sentados, mirando este video vintage de un evento TEDxCambridge.

14
0:00:58.66,000 --> 0:01:,000
(Risas)

15
0:01:01.34,000 --> 0:01:02,000
De pronto,

16
0:01:02.58,000 --> 0:01:05,000
el auto tiene una falla mecánica y no puede detenerse.

17
0:01:07.18,000 --> 0:01:08,000
Si el auto continúa,

18
0:01:09.54,000 --> 0:01:13,000
va a atropellar a un grupo de peatones que cruza la calle.

19
0:01:14.9,000 --> 0:01:16,000
Pero el auto puede cambiar de dirección,

20
0:01:17.059,000 --> 0:01:18,000
atropellar a un solo transeúnte,

21
0:01:18.94,000 --> 0:01:2,000
matarlo, y así salvar a los peatones.

22
0:01:21.86,000 --> 0:01:23,000
¿Qué debería hacer el auto, y quién debería decidir?

23
0:01:25.34,000 --> 0:01:28,000
¿Y si en cambio el auto pudiera irse contra una pared,

24
0:01:28.9,000 --> 0:01:31,000
chocar y matarte a ti, el pasajero,

25
0:01:32.22,000 --> 0:01:34,000
para salvar a los peatones?

26
0:01:35.06,000 --> 0:01:38,000
Este caso hipotético está inspirado en el dilema del tranvía,

27
0:01:38.78,000 --> 0:01:41,000
que fue inventado por unos filósofos hace unas décadas

28
0:01:42.58,000 --> 0:01:43,000
para reflexionar sobre la ética.

29
0:01:45.94,000 --> 0:01:47,000
Es importante saber cómo pensamos en este problema.

30
0:01:48.46,000 --> 0:01:5,000
Podríamos, por ejemplo, no pensar en esto en absoluto.

31
0:01:51.1,000 --> 0:01:54,000
Podríamos decir que la situación es poco realista,

32
0:01:54.5,000 --> 0:01:56,000
remotamente probable, o simplemente absurda.

33
0:01:57.58,000 --> 0:01:59,000
Pero para mí esta crítica pierde de vista el problema,

34
0:02:00.34,000 --> 0:02:02,000
porque se toma la situación muy al pie de la letra.

35
0:02:03.74,000 --> 0:02:05,000
Por supuesto que ningún accidente es exactamente así;

36
0:02:06.5,000 --> 0:02:09,000
ningún accidente tiene dos o tres opciones

37
0:02:09.86,000 --> 0:02:11,000
donde de una forma u otra muere alguien.

38
0:02:13.3,000 --> 0:02:15,000
Lo que va a ocurrir, es que el auto va a calcular algo,

39
0:02:15.9,000 --> 0:02:19,000
como la probabilidad de atropellar a un determinado grupo de personas

40
0:02:20.82,000 --> 0:02:23,000
si toma una dirección u otra.

41
0:02:24.18,000 --> 0:02:27,000
Se podría aumentar levemente el riesgo de los pasajeros y conductores,

42
0:02:27.66,000 --> 0:02:28,000
en favor de los peatones.

43
0:02:29.22,000 --> 0:02:31,000
Va a ser un cálculo más complejo,

44
0:02:32.3,000 --> 0:02:34,000
pero aun así va a implicar hacer concesiones,

45
0:02:35.66,000 --> 0:02:37,000
y las concesiones normalmente requieren una ética.

46
0:02:39.66,000 --> 0:02:41,000
Podríamos decir: "Bueno, no nos preocupemos por esto.

47
0:02:42.42,000 --> 0:02:46,000
Esperemos a que la tecnología esté totalmente preparada y sea 100 % segura".

48
0:02:48.34,000 --> 0:02:51,000
Supongamos que efectivamente podemos eliminar el 90 % de esos accidentes,

49
0:02:52.9,000 --> 0:02:54,000
o incluso el 99 % en los próximos 10 años.

50
0:02:56.74,000 --> 0:02:59,000
¿Qué pasaría si eliminar el 1 % de los accidentes restante

51
0:02:59.94,000 --> 0:03:02,000
llevara 50 años más de investigación?

52
0:03:04.22,000 --> 0:03:05,000
¿No deberíamos adoptar la tecnología?

53
0:03:06.54,000 --> 0:03:1,000
Estaríamos hablando de 60 millones de muertos en accidentes de auto

54
0:03:11.34,000 --> 0:03:12,000
si seguimos al ritmo al que vamos.

55
0:03:14.58,000 --> 0:03:15,000
Quiere decir que,

56
0:03:15.82,000 --> 0:03:18,000
esperar a tener seguridad total también es una elección

57
0:03:19.46,000 --> 0:03:21,000
y también implica hacer concesiones.

58
0:03:23.38,000 --> 0:03:27,000
En línea, en las redes sociales, la gente ha estado proponiendo todo tipo de ideas

59
0:03:27.74,000 --> 0:03:29,000
para no pensar en este problema.

60
0:03:29.78,000 --> 0:03:32,000
Una persona sugirió que el auto debería zigzaguear de alguna forma

61
0:03:33.02,000 --> 0:03:35,000
entre los peatones...

62
0:03:35.18,000 --> 0:03:36,000
(Risas)

63
0:03:36.22,000 --> 0:03:37,000
y el transeúnte.

64
0:03:37.5,000 --> 0:03:4,000
Por supuesto que, si el auto fuera capaz de hacer eso, debería hacerlo.

65
0:03:41.74,000 --> 0:03:43,000
Pero nos interesan las situaciones donde esto no es posible.

66
0:03:45.1,000 --> 0:03:5,000
Mi favorita fue la idea de un bloguero

67
0:03:50.54,000 --> 0:03:53,000
que propuso un botón de "eyectarse" que se presiona justo antes...

68
0:03:53.58,000 --> 0:03:54,000
(Risas)

69
0:03:54.82,000 --> 0:03:56,000
de que el auto se autodestruya.

70
0:03:56.897,000 --> 0:03:57,000
(Risas)

71
0:03:59.66,000 --> 0:04:04,000
Entonces, si aceptamos que los autos van a tener que hacer concesiones,

72
0:04:06.02,000 --> 0:04:07,000
¿cómo pensamos en esas concesiones,

73
0:04:09.14,000 --> 0:04:1,000
y cómo decidimos cuáles son?

74
0:04:10.74,000 --> 0:04:13,000
Tal vez deberíamos hacer una encuesta y ver qué quiere la sociedad,

75
0:04:13.9,000 --> 0:04:14,000
porque en última instancia,

76
0:04:15.38,000 --> 0:04:18,000
las regulaciones y las leyes son el reflejo de los valores sociales.

77
0:04:19.86,000 --> 0:04:2,000
Y fue lo que hicimos.

78
0:04:21.7,000 --> 0:04:22,000
Con mis colaboradores,

79
0:04:23.34,000 --> 0:04:25,000
Jean-François Bonnefon y Azim Shariff,

80
0:04:25.7,000 --> 0:04:26,000
hicimos una encuesta

81
0:04:27.34,000 --> 0:04:29,000
en la que le propusimos a la gente este tipo de situaciones.

82
0:04:30.219,000 --> 0:04:33,000
Les dimos dos opciones inspiradas en dos filósofos:

83
0:04:34.02,000 --> 0:04:36,000
Jeremy Bentham e Immanuel Kant.

84
0:04:37.42,000 --> 0:04:4,000
Bentham dice que el auto debería seguir la ética del utilitarismo:

85
0:04:40.54,000 --> 0:04:43,000
debería tomar la acción que minimice el daño total,

86
0:04:43.98,000 --> 0:04:45,000
aun si esto implica matar a un transeúnte,

87
0:04:46.82,000 --> 0:04:48,000
y aun si esto implica matar al pasajero.

88
0:04:49.94,000 --> 0:04:53,000
Immanuel Kant dice que el auto debería seguir la ética del deber,

89
0:04:54.94,000 --> 0:04:55,000
tal como "No matarás".

90
0:04:57.3,000 --> 0:05:01,000
O sea que no deberías tomar ninguna acción que implique hacerle daño a un ser humano,

91
0:05:01.78,000 --> 0:05:03,000
y deberías dejar que el auto siga su curso

92
0:05:04.26,000 --> 0:05:05,000
aun si eso resulta en más heridos.

93
0:05:07.46,000 --> 0:05:08,000
¿Uds. qué piensan?

94
0:05:09.18,000 --> 0:05:1,000
¿Bentham o Kant?

95
0:05:11.58,000 --> 0:05:12,000
El resultado fue este:

96
0:05:12.86,000 --> 0:05:13,000
la mayoría optó por Bentham.

97
0:05:15.98,000 --> 0:05:18,000
Así que parece que la gente quiere que los autos sean utilitarios,

98
0:05:19.78,000 --> 0:05:2,000
minimizar el daño total,

99
0:05:21.22,000 --> 0:05:22,000
y eso es lo que deberíamos hacer.

100
0:05:22.82,000 --> 0:05:23,000
Problema solucionado.

101
0:05:25.06,000 --> 0:05:26,000
Pero hay una pequeña trampa.

102
0:05:27.74,000 --> 0:05:3,000
Cuando le preguntamos a la gente si comprarían estos autos,

103
0:05:31.5,000 --> 0:05:32,000
la respuesta fue un "No" rotundo.

104
0:05:33.14,000 --> 0:05:35,000
(Risas)

105
0:05:35.46,000 --> 0:05:38,000
Les gustaría comprar un auto que los proteja a ellos a toda costa,

106
0:05:39.38,000 --> 0:05:42,000
pero quieren que los demás compren autos que minimicen el daño.

107
0:05:43.02,000 --> 0:05:45,000
(Risas)

108
0:05:46.54,000 --> 0:05:47,000
Ya conocemos este tipo de problema.

109
0:05:48.42,000 --> 0:05:49,000
Es un dilema social.

110
0:05:50.98,000 --> 0:05:51,000
Y para entender el dilema social,

111
0:05:52.82,000 --> 0:05:54,000
hay que retroceder un poco en la historia.

112
0:05:55.82,000 --> 0:05:57,000
En el 1800,

113
0:05:58.42,000 --> 0:06:01,000
el economista inglés William Forster Lloyd publicó un folleto

114
0:06:02.18,000 --> 0:06:04,000
que describe la siguiente situación.

115
0:06:04.42,000 --> 0:06:05,000
Hay un grupo de pastores,

116
0:06:06.1,000 --> 0:06:07,000
pastores ingleses,

117
0:06:07.46,000 --> 0:06:09,000
que comparten un prado común donde pastan sus ovejas.

118
0:06:11.34,000 --> 0:06:13,000
Si cada pastor trae una cierta cantidad de ovejas,

119
0:06:13.94,000 --> 0:06:14,000
digamos tres ovejas,

120
0:06:15.46,000 --> 0:06:17,000
el suelo se recupera bien,

121
0:06:17.58,000 --> 0:06:18,000
los pastores contentos,

122
0:06:18.82,000 --> 0:06:19,000
las ovejas contentas,

123
0:06:20.46,000 --> 0:06:21,000
todo va bien.

124
0:06:22.26,000 --> 0:06:24,000
Ahora, si uno de los pastores trae una oveja extra,

125
0:06:25.62,000 --> 0:06:29,000
ese pastor va a estar un poquito mejor, y nadie más va a salir perjudicado.

126
0:06:30.98,000 --> 0:06:33,000
Pero si cada pastor tomara esa decisión individualmente racional,

127
0:06:35.66,000 --> 0:06:37,000
el prado se vería sobreexplotado y el pasto se agotaría,

128
0:06:39.18,000 --> 0:06:41,000
en detrimento de todos los pastores,

129
0:06:41.38,000 --> 0:06:43,000
y por supuesto, en detrimento de las ovejas.

130
0:06:44.54,000 --> 0:06:47,000
Es un problema que se ve mucho:

131
0:06:48.9,000 --> 0:06:51,000
en la dificultad de controlar la sobrepesca,

132
0:06:52.1,000 --> 0:06:56,000
o al reducir las emisiones de carbono para contrarrestar el cambio climático.

133
0:06:58.98,000 --> 0:07:,000
Volviendo al tema de la regulación de vehículos autónomos,

134
0:07:02.9,000 --> 0:07:06,000
el prado común vendría a ser básicamente la seguridad pública;

135
0:07:07.26,000 --> 0:07:08,000
ese es el bien común.

136
0:07:09.22,000 --> 0:07:1,000
Y los pastores serían los pasajeros

137
0:07:11.22,000 --> 0:07:14,000
o los dueños de los autos que eligen viajar en esos vehículos.

138
0:07:16.78,000 --> 0:07:18,000
Y al tomar la decisión individualmente racional

139
0:07:19.42,000 --> 0:07:21,000
de priorizar su propia seguridad,

140
0:07:22.26,000 --> 0:07:25,000
podrían estar disminuyendo colectivamente el bien común,

141
0:07:25.42,000 --> 0:07:27,000
que es minimizar el daño total.

142
0:07:30.14,000 --> 0:07:32,000
Esto se llama "tragedia de los comunes",

143
0:07:32.3,000 --> 0:07:33,000
tradicionalmente,

144
0:07:33.62,000 --> 0:07:36,000
pero creo que en el caso de los vehículos autónomos,

145
0:07:36.74,000 --> 0:07:38,000
el problema es tal vez un poquito más traicionero

146
0:07:39.62,000 --> 0:07:42,000
porque no es necesariamente un ser humano

147
0:07:43.14,000 --> 0:07:44,000
el que toma las decisiones.

148
0:07:44.86,000 --> 0:07:47,000
Entonces, los fabricantes podrían simplemente programar los autos

149
0:07:48.18,000 --> 0:07:5,000
para maximizar la seguridad de sus clientes.

150
0:07:51.9,000 --> 0:07:53,000
Y esos autos podrían aprender, automáticamente y por su cuenta,

151
0:07:54.9,000 --> 0:07:57,000
que hacer eso requiere aumentar levemente el riesgo de los peatones.

152
0:07:59.34,000 --> 0:08:,000
O sea que, volviendo a las ovejas:

153
0:08:00.99,000 --> 0:08:03,000
es como si ahora tuviéramos ovejas eléctricas que piensan solas.

154
0:08:04.42,000 --> 0:08:05,000
(Risas)

155
0:08:05.9,000 --> 0:08:08,000
Y pueden irse a pastar solas aunque el pastor no lo sepa.

156
0:08:10.46,000 --> 0:08:13,000
O sea que a esto podríamos llamarlo "la tragedia de los comunes algorítmicos",

157
0:08:14.46,000 --> 0:08:16,000
y nos presenta nuevos desafíos.

158
0:08:22.34,000 --> 0:08:23,000
Típicamente, tradicionalmente,

159
0:08:24.26,000 --> 0:08:27,000
este tipo de dilemas sociales se resuelven implementando regulación.

160
0:08:27.62,000 --> 0:08:29,000
Ya sea el gobierno o la comunidad, se juntan

161
0:08:30.38,000 --> 0:08:33,000
y deciden colectivamente qué resultado quieren obtener,

162
0:08:34.14,000 --> 0:08:36,000
y qué tipo de restricciones a la conducta individual

163
0:08:36.82,000 --> 0:08:37,000
necesitan implementar.

164
0:08:39.42,000 --> 0:08:41,000
Y luego, a través del control y la imposición de normas,

165
0:08:42.06,000 --> 0:08:44,000
pueden garantizar la preservación del bien común.

166
0:08:45.26,000 --> 0:08:46,000
Entonces ¿por qué no exigimos,

167
0:08:46.859,000 --> 0:08:47,000
en nuestro rol de reguladores,

168
0:08:48.378,000 --> 0:08:5,000
que todos los autos tienen que minimizar el daño?

169
0:08:51.3,000 --> 0:08:53,000
A fin de cuentas, eso es lo que la gente dice que quiere.

170
0:08:55.02,000 --> 0:08:56,000
Y más importante aún,

171
0:08:56.46,000 --> 0:08:59,000
puedo estar seguro de que, como individuo,

172
0:08:59.58,000 --> 0:09:02,000
si compro un auto que en un caso muy extremo podría llegar a sacrificarme,

173
0:09:03.46,000 --> 0:09:04,000
no soy el único papanatas haciéndolo

174
0:09:05.2,000 --> 0:09:07,000
mientras todos los demás gozan de protección ilimitada.

175
0:09:08.94,000 --> 0:09:11,000
En nuestra encuesta indagamos acerca de la idea de la regulación,

176
0:09:12.3,000 --> 0:09:13,000
y el resultado fue este:

177
0:09:14.18,000 --> 0:09:17,000
Primero, la gente dijo "no" a la regulación;

178
0:09:19.1,000 --> 0:09:2,000
y segundo:

179
0:09:20.38,000 --> 0:09:23,000
"Bueno, si van a regular los autos para actuar así y minimizar el daño total,

180
0:09:24.34,000 --> 0:09:25,000
yo no los voy a comprar".

181
0:09:27.22,000 --> 0:09:28,000
Entonces, irónicamente,

182
0:09:28.62,000 --> 0:09:31,000
al regular los autos para minimizar el daño,

183
0:09:32.14,000 --> 0:09:33,000
podríamos acabar con más daño

184
0:09:34.86,000 --> 0:09:37,000
porque la gente no adoptaría esta nueva tecnología

185
0:09:38.54,000 --> 0:09:4,000
aun cuando es mucho más segura que los conductores humanos.

186
0:09:42.18,000 --> 0:09:45,000
No tengo la respuesta final a este acertijo,

187
0:09:45.62,000 --> 0:09:46,000
pero creo que, para empezar,

188
0:09:47.22,000 --> 0:09:5,000
necesitamos que la sociedad se ponga de acuerdo

189
0:09:50.54,000 --> 0:09:53,000
sobre las concesiones que está dispuesta a aceptar,

190
0:09:54.18,000 --> 0:09:57,000
y las posibles maneras de imponer esas concesiones.

191
0:09:58.34,000 --> 0:10:,000
Como punto de partida, mis brillantes alumnos,

192
0:10:00.9,000 --> 0:10:02,000
Edmond Awad y Sohan Dsouza,

193
0:10:03.38,000 --> 0:10:04,000
construyeron el sitio web Moral Machine,

194
0:10:06.02,000 --> 0:10:08,000
que genera y presenta situaciones hipotéticas al azar;

195
0:10:09.9,000 --> 0:10:11,000
es básicamente una secuencia aleatoria de dilemas

196
0:10:12.38,000 --> 0:10:15,000
donde tienes que elegir qué debería hacer el auto en cada caso.

197
0:10:16.86,000 --> 0:10:2,000
Y variamos las edades y hasta las especies de las distintas víctimas.

198
0:10:22.86,000 --> 0:10:25,000
Hasta ahora hemos recolectado más de 5 millones de decisiones,

199
0:10:26.58,000 --> 0:10:28,000
de más de un millón de personas en todo el mundo

200
0:10:30.22,000 --> 0:10:31,000
a través del sitio web.

201
0:10:32.18,000 --> 0:10:34,000
Esto nos está ayudando a pintar un panorama inicial

202
0:10:34.62,000 --> 0:10:36,000
de las concesiones que la gente está dispuesta a hacer

203
0:10:37.26,000 --> 0:10:38,000
y de qué es lo que les importa,

204
0:10:39.18,000 --> 0:10:4,000
incluso en distintas culturas.

205
0:10:42.06,000 --> 0:10:43,000
Pero, lo que es más importante,

206
0:10:43.58,000 --> 0:10:46,000
este ejercicio está ayudando a la gente a comprender

207
0:10:46.98,000 --> 0:10:48,000
lo difícil que es tomar esas decisiones,

208
0:10:49.82,000 --> 0:10:52,000
y que los organismos reguladores se enfrentan con decisiones imposibles.

209
0:10:55.18,000 --> 0:10:58,000
Y tal vez esto nos ayude, como sociedad, a entender el tipo de concesiones

210
0:10:58.78,000 --> 0:11:01,000
que se van a implementar, en última instancia, como normativa.

211
0:11:01.86,000 --> 0:11:02,000
Me alegré mucho cuando me enteré

212
0:11:03.62,000 --> 0:11:05,000
de que el primer conjunto de regulaciones

213
0:11:05.66,000 --> 0:11:07,000
que publicó el Departamento de Transporte

214
0:11:07.82,000 --> 0:11:08,000
la semana pasada

215
0:11:09.22,000 --> 0:11:15,000
incluye una lista de 15 ítems que deben presentar los fabricantes,

216
0:11:15.82,000 --> 0:11:18,000
y el número 14 es "consideraciones éticas";

217
0:11:19.1,000 --> 0:11:2,000
cómo van a manejar ese tema.

218
0:11:23.62,000 --> 0:11:26,000
En el sitio, la gente también puede reflexionar sobre sus decisiones

219
0:11:26.856,000 --> 0:11:28,000
al recibir un resumen de las opciones que eligieron.

220
0:11:30.26,000 --> 0:11:31,000
Les voy a dar un ejemplo.

221
0:11:31.94,000 --> 0:11:34,000
Les advierto que no es un ejemplo típico;

222
0:11:35.5,000 --> 0:11:36,000
no es un usuario típico.

223
0:11:36.9,000 --> 0:11:39,000
Estos son los individuos que esta persona más salvó y más sacrificó.

224
0:11:40.54,000 --> 0:11:45,000
(Risas)

225
0:11:46.5,000 --> 0:11:48,000
Algunos de Uds. estarán de acuerdo con él,

226
0:11:48.53,000 --> 0:11:49,000
o ella, no sabemos.

227
0:11:52.3,000 --> 0:11:58,000
Esta persona también parece inclinarse más a favor del pasajero que del peatón,

228
0:11:58.46,000 --> 0:12:,000
según las opciones que escogió,

229
0:12:00.58,000 --> 0:12:02,000
y no tiene problema en castigar al peatón imprudente.

230
0:12:03.42,000 --> 0:12:06,000
(Risas)

231
0:12:09.14,000 --> 0:12:1,000
Entonces, redondeando.

232
0:12:10.379,000 --> 0:12:13,000
Empezamos con la pregunta, llamémosle dilema ético,

233
0:12:13.82,000 --> 0:12:16,000
de qué debería hacer el auto en una situación específica:

234
0:12:16.9,000 --> 0:12:17,000
¿cambiar de dirección o seguir?

235
0:12:19.06,000 --> 0:12:21,000
Pero luego nos dimos cuenta de que el problema es otro.

236
0:12:21.82,000 --> 0:12:23,000
El problema es que la sociedad se ponga de acuerdo

237
0:12:24.75,000 --> 0:12:27,000
sobre qué concesiones le son aceptables y cómo imponerlas.

238
0:12:28.34,000 --> 0:12:29,000
Es un dilema social.

239
0:12:29.62,000 --> 0:12:34,000
En 1940, Isaac Asimov escribió sus famosas leyes de la robótica;

240
0:12:34.66,000 --> 0:12:35,000
las tres leyes de la robótica.

241
0:12:37.06,000 --> 0:12:39,000
Un robot no hará daño al ser humano,

242
0:12:39.54,000 --> 0:12:41,000
un robot debe obedecer al ser humano,

243
0:12:42.1,000 --> 0:12:45,000
y un robot debe preservarse a sí mismo.

244
0:12:45.38,000 --> 0:12:46,000
En ese orden de importancia.

245
0:12:48.18,000 --> 0:12:5,000
Pero después de 40 años más o menos,

246
0:12:50.34,000 --> 0:12:53,000
y después de tantas historias que llevaron estas leyes al límite,

247
0:12:54.1,000 --> 0:12:57,000
Asimov introdujo la ley cero,

248
0:12:57.82,000 --> 0:12:59,000
que precede a las demás,

249
0:13:00.1,000 --> 0:13:03,000
y es que un robot no hará daño a la Humanidad.

250
0:13:04.3,000 --> 0:13:08,000
No sé qué quiere decir esto en el contexto de los vehículos autónomos,

251
0:13:08.7,000 --> 0:13:1,000
o en cualquier situación específica,

252
0:13:11.46,000 --> 0:13:13,000
y no sé cómo lo podemos implementar,

253
0:13:13.7,000 --> 0:13:14,000
pero creo que reconociendo

254
0:13:15.26,000 --> 0:13:21,000
que la regulación de vehículos autónomos no es solo un problema tecnológico

255
0:13:21.42,000 --> 0:13:24,000
sino también un problema de cooperación social,

256
0:13:25.62,000 --> 0:13:28,000
espero que podamos al menos empezar a hacer las preguntas adecuadas.

257
0:13:29.02,000 --> 0:13:3,000
Gracias.

258
0:13:30.26,000 --> 0:13:32,000
(Aplausos)

