1
0:00:16.477,000 --> 0:00:18,000
I'll tell you a little bit about irrational behavior.

2
0:00:19.16,000 --> 0:00:2,000
Not yours, of course -- other people's.

3
0:00:21.065,000 --> 0:00:22,000
(Laughter)

4
0:00:22.271,000 --> 0:00:25,000
So after being at MIT for a few years,

5
0:00:26.16,000 --> 0:00:3,000
I realized that writing academic papers is not that exciting.

6
0:00:30.841,000 --> 0:00:32,000
You know, I don't know how many of those you read,

7
0:00:33.223,000 --> 0:00:35,000
but it's not fun to read and often not fun to write --

8
0:00:35.782,000 --> 0:00:36,000
even worse to write.

9
0:00:37.16,000 --> 0:00:39,000
So I decided to try and write something more fun.

10
0:00:40.16,000 --> 0:00:43,000
And I came up with an idea that I would write a cookbook.

11
0:00:44.971,000 --> 0:00:46,000
And the title for my cookbook was going to be,

12
0:00:47.147,000 --> 0:00:49,000
"Dining Without Crumbs: The Art of Eating Over the Sink."

13
0:00:50.031,000 --> 0:00:51,000
(Laughter)

14
0:00:51.777,000 --> 0:00:53,000
And it was going to be a look at life through the kitchen.

15
0:00:54.706,000 --> 0:00:55,000
I was quite excited about this.

16
0:00:56.285,000 --> 0:00:58,000
I was going to talk a little bit about research,

17
0:00:58.578,000 --> 0:00:59,000
a little bit about the kitchen.

18
0:01:00.099,000 --> 0:01:03,000
We do so much in the kitchen, I thought this would be interesting.

19
0:01:03.242,000 --> 0:01:06,000
I wrote a couple of chapters, and took it to MIT Press and they said,

20
0:01:07.158,000 --> 0:01:1,000
"Cute, but not for us. Go and find somebody else."

21
0:01:10.276,000 --> 0:01:12,000
I tried other people, and everybody said the same thing,

22
0:01:13.181,000 --> 0:01:15,000
"Cute. Not for us."

23
0:01:15.276,000 --> 0:01:17,000
Until somebody said,

24
0:01:18.16,000 --> 0:01:19,000
"Look, if you're serious about this,

25
0:01:20.01,000 --> 0:01:23,000
you have to write about your research first; you have to publish something,

26
0:01:23.565,000 --> 0:01:25,000
then you'll get the opportunity to write something else.

27
0:01:26.287,000 --> 0:01:28,000
If you really want to do it, you have to do it."

28
0:01:28.582,000 --> 0:01:3,000
I said, "I don't want to write about my research.

29
0:01:30.951,000 --> 0:01:31,000
I do it all day long,

30
0:01:32.192,000 --> 0:01:35,000
I want to write something a bit more free, less constrained."

31
0:01:35.708,000 --> 0:01:37,000
And this person was very forceful and said,

32
0:01:38.16,000 --> 0:01:4,000
"Look, that's the only way you'll ever do it."

33
0:01:40.76,000 --> 0:01:42,000
So I said, "Okay, if I have to do it --"

34
0:01:43.094,000 --> 0:01:44,000
I had a sabbatical.

35
0:01:44.277,000 --> 0:01:46,000
I said, "I'll write about my research, if there's no other way.

36
0:01:47.26,000 --> 0:01:48,000
And then I'll get to do my cookbook."

37
0:01:49.086,000 --> 0:01:52,000
So, I wrote a book on my research.

38
0:01:52.346,000 --> 0:01:54,000
And it turned out to be quite fun in two ways.

39
0:01:54.941,000 --> 0:01:56,000
First of all, I enjoyed writing.

40
0:01:57.489,000 --> 0:02:,000
But the more interesting thing was that I started learning from people.

41
0:02:01.267,000 --> 0:02:02,000
It's a fantastic time to write,

42
0:02:02.876,000 --> 0:02:04,000
because there's so much feedback you can get from people.

43
0:02:05.605,000 --> 0:02:07,000
People write to me about their personal experience,

44
0:02:08.16,000 --> 0:02:1,000
and about their examples, and where they disagree,

45
0:02:10.518,000 --> 0:02:11,000
and their nuances.

46
0:02:12.16,000 --> 0:02:14,000
And even being here -- I mean, the last few days,

47
0:02:14.47,000 --> 0:02:16,000
I've known heights of obsessive behavior

48
0:02:17.16,000 --> 0:02:18,000
I never thought about.

49
0:02:19.16,000 --> 0:02:2,000
(Laughter)

50
0:02:20.816,000 --> 0:02:21,000
Which I think is just fascinating.

51
0:02:22.732,000 --> 0:02:24,000
I will tell you a little bit about irrational behavior,

52
0:02:25.351,000 --> 0:02:28,000
and I want to start by giving you some examples of visual illusion

53
0:02:28.553,000 --> 0:02:29,000
as a metaphor for rationality.

54
0:02:30.51,000 --> 0:02:31,000
So think about these two tables.

55
0:02:32.352,000 --> 0:02:33,000
And you must have seen this illusion.

56
0:02:34.16,000 --> 0:02:37,000
If I asked you what's longer, the vertical line on the table on the left,

57
0:02:38.005,000 --> 0:02:4,000
or the horizontal line on the table on the right,

58
0:02:40.822,000 --> 0:02:42,000
which one seems longer?

59
0:02:43.16,000 --> 0:02:45,000
Can anybody see anything but the left one being longer?

60
0:02:46.159,000 --> 0:02:47,000
No, right? It's impossible.

61
0:02:48.03,000 --> 0:02:51,000
But the nice thing about visual illusion is we can easily demonstrate mistakes.

62
0:02:51.946,000 --> 0:02:53,000
So I can put some lines on; it doesn't help.

63
0:02:54.433,000 --> 0:02:55,000
I can animate the lines.

64
0:02:56.159,000 --> 0:02:58,000
And to the extent you believe I didn't shrink the lines,

65
0:02:58.851,000 --> 0:03:02,000
which I didn't, I've proven to you that your eyes were deceiving you.

66
0:03:03.663,000 --> 0:03:06,000
Now, the interesting thing about this is when I take the lines away,

67
0:03:07.16,000 --> 0:03:09,000
it's as if you haven't learned anything in the last minute.

68
0:03:09.994,000 --> 0:03:11,000
(Laughter)

69
0:03:12.238,000 --> 0:03:15,000
You can't look at this and say, "Now I see reality as it is."

70
0:03:15.953,000 --> 0:03:19,000
Right? It's impossible to overcome this sense that this is indeed longer.

71
0:03:20.017,000 --> 0:03:21,000
Our intuition is really fooling us

72
0:03:21.668,000 --> 0:03:23,000
in a repeatable, predictable, consistent way.

73
0:03:23.839,000 --> 0:03:25,000
and there is almost nothing we can do about it,

74
0:03:26.16,000 --> 0:03:28,000
aside from taking a ruler and starting to measure it.

75
0:03:29.77,000 --> 0:03:31,000
Here's another one. It's one of my favorite illusions.

76
0:03:32.358,000 --> 0:03:34,000
What color is the top arrow pointing to?

77
0:03:36.119,000 --> 0:03:38,000
Audience: Brown. Dan Ariely: Brown. Thank you.

78
0:03:38.315,000 --> 0:03:39,000
The bottom one? Yellow.

79
0:03:40.308,000 --> 0:03:41,000
Turns out they're identical.

80
0:03:41.673,000 --> 0:03:42,000
Can anybody see them as identical?

81
0:03:43.608,000 --> 0:03:44,000
Very, very hard.

82
0:03:45.006,000 --> 0:03:47,000
I can cover the rest of the cube up.

83
0:03:47.16,000 --> 0:03:5,000
If I cover the rest of the cube, you can see that they are identical.

84
0:03:50.59,000 --> 0:03:52,000
If you don't believe me, you can get the slide later

85
0:03:53.051,000 --> 0:03:55,000
and do some arts and crafts and see that they're identical.

86
0:03:55.891,000 --> 0:03:58,000
But again, it's the same story, that if we take the background away,

87
0:03:59.248,000 --> 0:04:01,000
the illusion comes back.

88
0:04:01.477,000 --> 0:04:03,000
There is no way for us not to see this illusion.

89
0:04:04.478,000 --> 0:04:07,000
I guess maybe if you're colorblind, I don't think you can see that.

90
0:04:07.819,000 --> 0:04:09,000
I want you to think about illusion as a metaphor.

91
0:04:10.16,000 --> 0:04:12,000
Vision is one of the best things we do.

92
0:04:12.319,000 --> 0:04:14,000
We have a huge part of our brain dedicated to vision --

93
0:04:14.939,000 --> 0:04:15,000
bigger than dedicated to anything else.

94
0:04:16.858,000 --> 0:04:19,000
We use our vision more hours of the day than anything else.

95
0:04:20.73,000 --> 0:04:22,000
We're evolutionarily designed to use vision.

96
0:04:22.824,000 --> 0:04:24,000
And if we have these predictable repeatable mistakes in vision,

97
0:04:25.824,000 --> 0:04:26,000
which we're so good at,

98
0:04:27.354,000 --> 0:04:29,000
what are the chances we won't make even more mistakes

99
0:04:29.942,000 --> 0:04:32,000
in something we're not as good at, for example, financial decision-making.

100
0:04:33.517,000 --> 0:04:34,000
(Laughter)

101
0:04:35.16,000 --> 0:04:37,000
Something we don't have an evolutionary reason to do,

102
0:04:37.834,000 --> 0:04:39,000
we don't have a specialized part of the brain for,

103
0:04:40.208,000 --> 0:04:42,000
and we don't do that many hours of the day.

104
0:04:42.247,000 --> 0:04:43,000
The argument is in those cases,

105
0:04:44.239,000 --> 0:04:47,000
it might be that we actually make many more mistakes.

106
0:04:48.16,000 --> 0:04:5,000
And worse -- not having an easy way to see them,

107
0:04:50.957,000 --> 0:04:53,000
because in visual illusions, we can easily demonstrate the mistakes;

108
0:04:54.21,000 --> 0:04:56,000
in cognitive illusion it's much, much harder

109
0:04:56.321,000 --> 0:04:57,000
to demonstrate the mistakes to people.

110
0:04:58.16,000 --> 0:05:,000
So I want to show you some cognitive illusions,

111
0:05:01.16,000 --> 0:05:04,000
or decision-making illusions, in the same way.

112
0:05:04.266,000 --> 0:05:07,000
And this is one of my favorite plots in social sciences.

113
0:05:07.766,000 --> 0:05:1,000
It's from a paper by Johnson and Goldstein.

114
0:05:11.628,000 --> 0:05:14,000
It basically shows the percentage of people who indicated

115
0:05:15.35,000 --> 0:05:18,000
they would be interested in donating their organs.

116
0:05:19.104,000 --> 0:05:2,000
These are different countries in Europe.

117
0:05:21.053,000 --> 0:05:22,000
You basically see two types of countries:

118
0:05:23.038,000 --> 0:05:25,000
countries on the right, that seem to be giving a lot;

119
0:05:25.664,000 --> 0:05:27,000
and countries on the left that seem to giving very little,

120
0:05:28.647,000 --> 0:05:29,000
or much less.

121
0:05:30.274,000 --> 0:05:31,000
The question is, why?

122
0:05:31.45,000 --> 0:05:34,000
Why do some countries give a lot and some countries give a little?

123
0:05:34.798,000 --> 0:05:35,000
When you ask people this question,

124
0:05:36.45,000 --> 0:05:38,000
they usually think that it has to be about culture.

125
0:05:39.006,000 --> 0:05:4,000
How much do you care about people?

126
0:05:40.656,000 --> 0:05:41,000
Giving organs to somebody else

127
0:05:42.16,000 --> 0:05:45,000
is probably about how much you care about society, how linked you are.

128
0:05:45.518,000 --> 0:05:46,000
Or maybe it's about religion.

129
0:05:47.16,000 --> 0:05:48,000
But if you look at this plot,

130
0:05:49.139,000 --> 0:05:52,000
you can see that countries that we think about as very similar,

131
0:05:52.59,000 --> 0:05:54,000
actually exhibit very different behavior.

132
0:05:55.368,000 --> 0:05:57,000
For example, Sweden is all the way on the right,

133
0:05:57.653,000 --> 0:05:59,000
and Denmark, which we think is culturally very similar,

134
0:06:00.336,000 --> 0:06:01,000
is all the way on the left.

135
0:06:02.16,000 --> 0:06:05,000
Germany is on the left, and Austria is on the right.

136
0:06:06.16,000 --> 0:06:09,000
The Netherlands is on the left, and Belgium is on the right.

137
0:06:09.5,000 --> 0:06:11,000
And finally, depending on your particular version

138
0:06:12.16,000 --> 0:06:13,000
of European similarity,

139
0:06:14.16,000 --> 0:06:19,000
you can think about the U.K. and France as either similar culturally or not,

140
0:06:19.35,000 --> 0:06:22,000
but it turns out that with organ donation, they are very different.

141
0:06:23.198,000 --> 0:06:25,000
By the way, the Netherlands is an interesting story.

142
0:06:25.661,000 --> 0:06:28,000
You see, the Netherlands is kind of the biggest of the small group.

143
0:06:30.494,000 --> 0:06:32,000
It turns out that they got to 28 percent

144
0:06:33.16,000 --> 0:06:36,000
after mailing every household in the country a letter,

145
0:06:36.463,000 --> 0:06:38,000
begging people to join this organ donation program.

146
0:06:39.672,000 --> 0:06:41,000
You know the expression, "Begging only gets you so far."

147
0:06:42.664,000 --> 0:06:44,000
It's 28 percent in organ donation.

148
0:06:45.16,000 --> 0:06:46,000
(Laughter)

149
0:06:47.433,000 --> 0:06:49,000
But whatever the countries on the right are doing,

150
0:06:49.807,000 --> 0:06:51,000
they're doing a much better job than begging.

151
0:06:51.942,000 --> 0:06:52,000
So what are they doing?

152
0:06:53.274,000 --> 0:06:56,000
Turns out the secret has to do with a form at the DMV.

153
0:06:56.766,000 --> 0:06:57,000
And here is the story.

154
0:06:58.16,000 --> 0:07:,000
The countries on the left have a form at the DMV

155
0:07:00.691,000 --> 0:07:01,000
that looks something like this.

156
0:07:02.406,000 --> 0:07:06,000
"Check the box below if you want to participate in the organ donor program."

157
0:07:06.715,000 --> 0:07:07,000
And what happens?

158
0:07:08.16,000 --> 0:07:1,000
People don't check, and they don't join.

159
0:07:11.16,000 --> 0:07:13,000
The countries on the right, the ones that give a lot,

160
0:07:13.686,000 --> 0:07:14,000
have a slightly different form.

161
0:07:15.312,000 --> 0:07:18,000
It says, "Check the box below if you don't want to participate ..."

162
0:07:18.836,000 --> 0:07:2,000
Interestingly enough, when people get this,

163
0:07:21.05,000 --> 0:07:23,000
they again don't check, but now they join.

164
0:07:23.16,000 --> 0:07:25,000
(Laughter)

165
0:07:26.16,000 --> 0:07:28,000
Now, think about what this means.

166
0:07:29.566,000 --> 0:07:32,000
You know, we wake up in the morning and we feel we make decisions.

167
0:07:33.16,000 --> 0:07:35,000
We wake up in the morning and we open the closet;

168
0:07:35.494,000 --> 0:07:36,000
we feel that we decide what to wear.

169
0:07:37.302,000 --> 0:07:4,000
we open the refrigerator and we feel that we decide what to eat.

170
0:07:40.398,000 --> 0:07:41,000
What this is actually saying,

171
0:07:41.921,000 --> 0:07:43,000
is that many of these decisions are not residing within us.

172
0:07:44.755,000 --> 0:07:46,000
They are residing in the person who is designing that form.

173
0:07:47.881,000 --> 0:07:48,000
When you walk into the DMV,

174
0:07:49.851,000 --> 0:07:51,000
the person who designed the form will have a huge influence

175
0:07:52.874,000 --> 0:07:53,000
on what you'll end up doing.

176
0:07:54.707,000 --> 0:07:56,000
Now, it's also very hard to intuit these results.

177
0:07:57.51,000 --> 0:07:58,000
Think about it for yourself.

178
0:07:58.875,000 --> 0:07:59,000
How many of you believe

179
0:08:00.16,000 --> 0:08:02,000
that if you went to renew your license tomorrow,

180
0:08:02.47,000 --> 0:08:03,000
and you went to the DMV,

181
0:08:04.16,000 --> 0:08:06,000
and you encountered one of these forms,

182
0:08:06.451,000 --> 0:08:08,000
that it would actually change your own behavior?

183
0:08:08.936,000 --> 0:08:1,000
Very hard to think that it would influence us.

184
0:08:11.267,000 --> 0:08:14,000
We can say, "Oh, these funny Europeans, of course it would influence them."

185
0:08:14.83,000 --> 0:08:15,000
But when it comes to us,

186
0:08:16.617,000 --> 0:08:18,000
we have such a feeling that we're in the driver's seat,

187
0:08:19.279,000 --> 0:08:22,000
such a feeling that we're in control and we are making the decision,

188
0:08:22.592,000 --> 0:08:24,000
that it's very hard to even accept the idea

189
0:08:25.215,000 --> 0:08:27,000
that we actually have an illusion of making a decision,

190
0:08:27.914,000 --> 0:08:29,000
rather than an actual decision.

191
0:08:30.002,000 --> 0:08:32,000
Now, you might say,

192
0:08:32.494,000 --> 0:08:34,000
"These are decisions we don't care about."

193
0:08:34.935,000 --> 0:08:36,000
In fact, by definition, these are decisions

194
0:08:37.232,000 --> 0:08:39,000
about something that will happen to us after we die.

195
0:08:39.839,000 --> 0:08:41,000
How could we care about something less

196
0:08:41.983,000 --> 0:08:43,000
than about something that happens after we die?

197
0:08:44.296,000 --> 0:08:46,000
So a standard economist, somebody who believes in rationality,

198
0:08:47.242,000 --> 0:08:48,000
would say, "You know what?

199
0:08:48.519,000 --> 0:08:51,000
The cost of lifting the pencil and marking a "V" is higher

200
0:08:51.995,000 --> 0:08:53,000
than the possible benefit of the decision,

201
0:08:54.16,000 --> 0:08:55,000
so that's why we get this effect."

202
0:08:55.982,000 --> 0:08:56,000
(Laughter)

203
0:08:57.015,000 --> 0:08:59,000
But, in fact, it's not because it's easy.

204
0:08:59.537,000 --> 0:09:02,000
It's not because it's trivial. It's not because we don't care.

205
0:09:02.577,000 --> 0:09:04,000
It's the opposite. It's because we care.

206
0:09:05.022,000 --> 0:09:06,000
It's difficult and it's complex.

207
0:09:06.99,000 --> 0:09:08,000
And it's so complex that we don't know what to do.

208
0:09:09.565,000 --> 0:09:1,000
And because we have no idea what to do,

209
0:09:11.565,000 --> 0:09:14,000
we just pick whatever it was that was chosen for us.

210
0:09:15.769,000 --> 0:09:16,000
I'll give you one more example.

211
0:09:17.334,000 --> 0:09:19,000
This is from a paper by Redelmeier and Shafir.

212
0:09:19.945,000 --> 0:09:22,000
And they said, "Would this effect also happens to experts?

213
0:09:23.475,000 --> 0:09:26,000
People who are well-paid, experts in their decisions,

214
0:09:26.627,000 --> 0:09:27,000
and who make a lot of them?"

215
0:09:28.007,000 --> 0:09:3,000
And they took a group of physicians.

216
0:09:30.216,000 --> 0:09:32,000
They presented to them a case study of a patient.

217
0:09:32.692,000 --> 0:09:35,000
They said, "Here is a patient. He is a 67-year-old farmer.

218
0:09:36.16,000 --> 0:09:38,000
He's been suffering from right hip pain for a while."

219
0:09:38.914,000 --> 0:09:39,000
And then, they said to the physicians,

220
0:09:40.772,000 --> 0:09:41,000
"You decided a few weeks ago

221
0:09:42.16,000 --> 0:09:43,000
that nothing is working for this patient.

222
0:09:44.16,000 --> 0:09:46,000
All these medications, nothing seems to be working.

223
0:09:46.613,000 --> 0:09:48,000
So you refer the patient for hip replacement therapy.

224
0:09:49.409,000 --> 0:09:5,000
Hip replacement. Okay?"

225
0:09:51.16,000 --> 0:09:53,000
So the patient is on a path to have his hip replaced.

226
0:09:54.616,000 --> 0:09:55,000
Then they said to half of the physicians,

227
0:09:56.609,000 --> 0:09:58,000
"Yesterday, you reviewed the patient's case,

228
0:09:58.972,000 --> 0:10:,000
and you realized that you forgot to try one medication.

229
0:10:01.606,000 --> 0:10:02,000
You did not try ibuprofen.

230
0:10:04.16,000 --> 0:10:07,000
What do you do? Do you pull the patient back and try ibuprofen?

231
0:10:07.509,000 --> 0:10:09,000
Or do you let him go and have hip replacement?"

232
0:10:10.303,000 --> 0:10:12,000
Well, the good news is that most physicians in this case

233
0:10:12.978,000 --> 0:10:14,000
decided to pull the patient and try ibuprofen.

234
0:10:15.811,000 --> 0:10:16,000
Very good for the physicians.

235
0:10:17.575,000 --> 0:10:19,000
To the other group of physicians, they said,

236
0:10:19.666,000 --> 0:10:22,000
"Yesterday when you reviewed the case, you discovered there were two medications

237
0:10:23.539,000 --> 0:10:25,000
you didn't try out yet -- ibuprofen and piroxicam."

238
0:10:26.079,000 --> 0:10:28,000
You have two medications you didn't try out yet.

239
0:10:28.354,000 --> 0:10:3,000
What do you do? You let him go, or you pull him back?

240
0:10:31.021,000 --> 0:10:34,000
And if you pull him back, do you try ibuprofen or piroxicam? Which one?"

241
0:10:34.489,000 --> 0:10:35,000
Now, think of it:

242
0:10:35.669,000 --> 0:10:38,000
This decision makes it as easy to let the patient continue with hip replacement,

243
0:10:39.565,000 --> 0:10:42,000
but pulling him back, all of the sudden it becomes more complex.

244
0:10:42.715,000 --> 0:10:43,000
There is one more decision.

245
0:10:44.557,000 --> 0:10:45,000
What happens now?

246
0:10:45.907,000 --> 0:10:48,000
The majority of the physicians now choose to let the patient go

247
0:10:49.55,000 --> 0:10:5,000
for a hip replacement.

248
0:10:51.091,000 --> 0:10:53,000
I hope this worries you, by the way --

249
0:10:53.16,000 --> 0:10:54,000
(Laughter)

250
0:10:54.417,000 --> 0:10:55,000
when you go to see your physician.

251
0:10:56.782,000 --> 0:10:58,000
The thing is that no physician would ever say,

252
0:10:59.528,000 --> 0:11:02,000
"Piroxicam, ibuprofen, hip replacement. Let's go for hip replacement."

253
0:11:03.345,000 --> 0:11:05,000
But the moment you set this as the default,

254
0:11:06.16,000 --> 0:11:09,000
it has a huge power over whatever people end up doing.

255
0:11:10.16,000 --> 0:11:13,000
I'll give you a couple of more examples on irrational decision-making.

256
0:11:13.518,000 --> 0:11:14,000
Imagine I give you a choice:

257
0:11:15.36,000 --> 0:11:18,000
Do you want to go for a weekend to Rome, all expenses paid --

258
0:11:18.997,000 --> 0:11:22,000
hotel, transportation, food, a continental breakfast, everything --

259
0:11:23.583,000 --> 0:11:24,000
or a weekend in Paris?

260
0:11:25.16,000 --> 0:11:28,000
Now, weekend in Paris, weekend in Rome -- these are different things.

261
0:11:28.438,000 --> 0:11:3,000
They have different food, different culture, different art.

262
0:11:31.24,000 --> 0:11:34,000
Imagine I added a choice to the set that nobody wanted.

263
0:11:34.62,000 --> 0:11:35,000
Imagine I said, "A weekend in Rome,

264
0:11:36.396,000 --> 0:11:37,000
a weekend in Paris,

265
0:11:37.788,000 --> 0:11:38,000
or having your car stolen?"

266
0:11:39.404,000 --> 0:11:42,000
(Laughter)

267
0:11:42.461,000 --> 0:11:45,000
It's a funny idea, because why would having your car stolen,

268
0:11:45.731,000 --> 0:11:46,000
in this set, influence anything?

269
0:11:47.351,000 --> 0:11:49,000
(Laughter)

270
0:11:49.489,000 --> 0:11:53,000
But what if the option to have your car stolen was not exactly like this?

271
0:11:53.962,000 --> 0:11:55,000
What if it was a trip to Rome, all expenses paid,

272
0:11:56.518,000 --> 0:11:58,000
transportation, breakfast,

273
0:11:58.82,000 --> 0:12:,000
but it doesn't include coffee in the morning?

274
0:12:01.16,000 --> 0:12:04,000
If you want coffee, you have to pay for it yourself, it's two euros 50.

275
0:12:04.557,000 --> 0:12:05,000
(Laughter)

276
0:12:05.588,000 --> 0:12:06,000
Now in some ways,

277
0:12:07.16,000 --> 0:12:09,000
given that you can have Rome with coffee,

278
0:12:09.483,000 --> 0:12:11,000
why would you possibly want Rome without coffee?

279
0:12:11.888,000 --> 0:12:13,000
It's like having your car stolen. It's an inferior option.

280
0:12:15.474,000 --> 0:12:16,000
But guess what happened?

281
0:12:16.65,000 --> 0:12:18,000
The moment you add Rome without coffee,

282
0:12:18.985,000 --> 0:12:21,000
Rome with coffee becomes more popular, and people choose it.

283
0:12:22.627,000 --> 0:12:24,000
The fact that you have Rome without coffee

284
0:12:25.16,000 --> 0:12:26,000
makes Rome with coffee look superior,

285
0:12:27.16,000 --> 0:12:29,000
and not just to Rome without coffee -- even superior to Paris.

286
0:12:30.16,000 --> 0:12:34,000
(Laughter)

287
0:12:34.261,000 --> 0:12:36,000
Here are two examples of this principle.

288
0:12:36.737,000 --> 0:12:38,000
This was an ad in The Economist a few years ago

289
0:12:39.413,000 --> 0:12:41,000
that gave us three choices:

290
0:12:41.517,000 --> 0:12:43,000
an online subscription for 59 dollars,

291
0:12:44.374,000 --> 0:12:47,000
a print subscription for 125 dollars,

292
0:12:47.883,000 --> 0:12:49,000
or you could get both for 125.

293
0:12:50.16,000 --> 0:12:51,000
(Laughter)

294
0:12:52.16,000 --> 0:12:54,000
Now I looked at this, and I called up The Economist,

295
0:12:54.769,000 --> 0:12:56,000
and I tried to figure out what they were thinking.

296
0:12:57.356,000 --> 0:13:,000
And they passed me from one person to another to another,

297
0:13:00.522,000 --> 0:13:03,000
until eventually I got to the person who was in charge of the website,

298
0:13:04.269,000 --> 0:13:07,000
and I called them up, and they went to check what was going on.

299
0:13:07.888,000 --> 0:13:1,000
The next thing I know, the ad is gone, no explanation.

300
0:13:11.759,000 --> 0:13:12,000
So I decided to do the experiment

301
0:13:13.67,000 --> 0:13:15,000
that I would have loved The Economist to do with me.

302
0:13:16.336,000 --> 0:13:18,000
I took this and I gave it to 100 MIT students.

303
0:13:18.759,000 --> 0:13:19,000
I said, "What would you choose?"

304
0:13:20.347,000 --> 0:13:23,000
These are the market shares -- most people wanted the combo deal.

305
0:13:24.357,000 --> 0:13:26,000
Thankfully, nobody wanted the dominant option.

306
0:13:26.589,000 --> 0:13:27,000
That means our students can read.

307
0:13:28.253,000 --> 0:13:29,000
(Laughter)

308
0:13:29.659,000 --> 0:13:31,000
But now, if you have an option that nobody wants,

309
0:13:32.572,000 --> 0:13:33,000
you can take it off, right?

310
0:13:34.295,000 --> 0:13:35,000
So I printed another version of this,

311
0:13:36.16,000 --> 0:13:37,000
where I eliminated the middle option.

312
0:13:38.001,000 --> 0:13:41,000
I gave it to another 100 students. Here is what happened:

313
0:13:41.508,000 --> 0:13:43,000
Now the most popular option became the least popular,

314
0:13:44.269,000 --> 0:13:46,000
and the least popular became the most popular.

315
0:13:47.801,000 --> 0:13:5,000
What was happening was the option that was useless,

316
0:13:51.16,000 --> 0:13:54,000
in the middle, was useless in the sense that nobody wanted it.

317
0:13:55.035,000 --> 0:13:58,000
But it wasn't useless in the sense that it helped people figure out

318
0:13:58.225,000 --> 0:13:59,000
what they wanted.

319
0:13:59.4,000 --> 0:14:01,000
In fact, relative to the option in the middle,

320
0:14:02.16,000 --> 0:14:06,000
which was get only the print for 125,

321
0:14:06.683,000 --> 0:14:09,000
the print and web for 125 looked like a fantastic deal.

322
0:14:10.46,000 --> 0:14:12,000
And as a consequence, people chose it.

323
0:14:12.587,000 --> 0:14:13,000
The general idea here, by the way,

324
0:14:14.238,000 --> 0:14:16,000
is that we actually don't know our preferences that well.

325
0:14:16.953,000 --> 0:14:18,000
And because we don't know our preferences that well,

326
0:14:19.447,000 --> 0:14:22,000
we're susceptible to all of these influences from the external forces:

327
0:14:22.76,000 --> 0:14:25,000
the defaults, the particular options that are presented to us, and so on.

328
0:14:26.792,000 --> 0:14:27,000
One more example of this.

329
0:14:28.16,000 --> 0:14:31,000
People believe that when we deal with physical attraction,

330
0:14:31.541,000 --> 0:14:34,000
we see somebody, and we know immediately whether we like them or not,

331
0:14:34.828,000 --> 0:14:35,000
if we're attracted or not.

332
0:14:36.16,000 --> 0:14:38,000
This is why we have these four-minute dates.

333
0:14:38.616,000 --> 0:14:4,000
So I decided to do this experiment with people.

334
0:14:41.16,000 --> 0:14:44,000
I'll show you images here, no real people, but the experiment was with people.

335
0:14:45.16,000 --> 0:14:47,000
I showed some people a picture of Tom, and a picture of Jerry.

336
0:14:48.16,000 --> 0:14:5,000
and I said, "Who do you want to date?

337
0:14:50.253,000 --> 0:14:51,000
Tom or Jerry?"

338
0:14:51.752,000 --> 0:14:54,000
But for half the people, I added an ugly version of Jerry.

339
0:14:55.055,000 --> 0:15:,000
I took Photoshop and I made Jerry slightly less attractive.

340
0:15:00.16,000 --> 0:15:01,000
(Laughter)

341
0:15:01.597,000 --> 0:15:04,000
For the other people, I added an ugly version of Tom.

342
0:15:05.16,000 --> 0:15:07,000
And the question was, will ugly Jerry and ugly Tom

343
0:15:08.16,000 --> 0:15:11,000
help their respective, more attractive brothers?

344
0:15:11.952,000 --> 0:15:13,000
The answer was absolutely yes.

345
0:15:14.205,000 --> 0:15:16,000
When ugly Jerry was around, Jerry was popular.

346
0:15:16.375,000 --> 0:15:18,000
When ugly Tom was around, Tom was popular.

347
0:15:18.539,000 --> 0:15:19,000
(Laughter)

348
0:15:20.16,000 --> 0:15:22,000
This of course has two very clear implications

349
0:15:22.696,000 --> 0:15:24,000
for life in general.

350
0:15:25.95,000 --> 0:15:28,000
If you ever go bar-hopping, who do you want to take with you?

351
0:15:29.16,000 --> 0:15:34,000
(Laughter)

352
0:15:34.913,000 --> 0:15:37,000
You want a slightly uglier version of yourself.

353
0:15:38.714,000 --> 0:15:39,000
(Laughter)

354
0:15:40.16,000 --> 0:15:42,000
Similar, but slightly uglier.

355
0:15:42.421,000 --> 0:15:43,000
(Laughter)

356
0:15:43.888,000 --> 0:15:46,000
The second point, or course, is that if somebody invites you to bar hop,

357
0:15:47.681,000 --> 0:15:48,000
you know what they think about you.

358
0:15:49.452,000 --> 0:15:51,000
(Laughter)

359
0:15:52.16,000 --> 0:15:54,000
Now you get it.

360
0:15:54.466,000 --> 0:15:55,000
What is the general point?

361
0:15:56.16,000 --> 0:15:57,000
The general point is that,

362
0:15:57.442,000 --> 0:16:,000
when we think about economics, we have this beautiful view of human nature.

363
0:16:01.16,000 --> 0:16:03,000
"What a piece of work is a man! How noble in reason!"

364
0:16:03.721,000 --> 0:16:05,000
We have this view of ourselves, of others.

365
0:16:06.323,000 --> 0:16:11,000
The behavioral economics perspective is slightly less "generous" to people;

366
0:16:11.561,000 --> 0:16:13,000
in fact, in medical terms,

367
0:16:13.684,000 --> 0:16:14,000
that's our view.

368
0:16:14.9,000 --> 0:16:19,000
(Laughter)

369
0:16:20.051,000 --> 0:16:22,000
But there is a silver lining.

370
0:16:22.16,000 --> 0:16:23,000
The silver lining is, I think,

371
0:16:24.058,000 --> 0:16:28,000
kind of the reason that behavioral economics is interesting and exciting.

372
0:16:28.16,000 --> 0:16:3,000
Are we Superman, or are we Homer Simpson?

373
0:16:31.446,000 --> 0:16:33,000
When it comes to building the physical world,

374
0:16:34.461,000 --> 0:16:36,000
we kind of understand our limitations.

375
0:16:36.517,000 --> 0:16:37,000
We build steps.

376
0:16:37.712,000 --> 0:16:4,000
And we build these things that not everybody can use, obviously.

377
0:16:40.879,000 --> 0:16:41,000
(Laughter)

378
0:16:42.799,000 --> 0:16:43,000
We understand our limitations,

379
0:16:44.37,000 --> 0:16:45,000
and we build around them.

380
0:16:46.044,000 --> 0:16:48,000
But for some reason, when it comes to the mental world,

381
0:16:48.647,000 --> 0:16:51,000
when we design things like healthcare and retirement and stock markets,

382
0:16:52.028,000 --> 0:16:54,000
we somehow forget the idea that we are limited.

383
0:16:54.4,000 --> 0:16:56,000
I think that if we understood our cognitive limitations

384
0:16:57.331,000 --> 0:16:59,000
in the same way we understand our physical limitations,

385
0:16:59.967,000 --> 0:17:01,000
even though they don't stare us in the face the same way,

386
0:17:02.801,000 --> 0:17:04,000
we could design a better world, and that, I think,

387
0:17:05.305,000 --> 0:17:06,000
is the hope of this thing.

388
0:17:06.722,000 --> 0:17:07,000
Thank you very much.

389
0:17:08.16,000 --> 0:17:14,000
(Applause)

