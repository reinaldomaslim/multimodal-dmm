1
0:00:00.554,000 --> 0:00:02,000
As societies, we have to make collective decisions

2
0:00:03.021,000 --> 0:00:04,000
that will shape our future.

3
0:00:05.087,000 --> 0:00:07,000
And we all know that when we make decisions in groups,

4
0:00:07.868,000 --> 0:00:08,000
they don't always go right.

5
0:00:09.53,000 --> 0:00:1,000
And sometimes they go very wrong.

6
0:00:12.315,000 --> 0:00:14,000
So how do groups make good decisions?

7
0:00:15.228,000 --> 0:00:19,000
Research has shown that crowds are wise when there's independent thinking.

8
0:00:19.58,000 --> 0:00:22,000
This why the wisdom of the crowds can be destroyed by peer pressure,

9
0:00:22.809,000 --> 0:00:23,000
publicity, social media,

10
0:00:24.52,000 --> 0:00:28,000
or sometimes even simple conversations that influence how people think.

11
0:00:29.063,000 --> 0:00:32,000
On the other hand, by talking, a group could exchange knowledge,

12
0:00:33.04,000 --> 0:00:34,000
correct and revise each other

13
0:00:34.846,000 --> 0:00:35,000
and even come up with new ideas.

14
0:00:36.663,000 --> 0:00:37,000
And this is all good.

15
0:00:38.502,000 --> 0:00:42,000
So does talking to each other help or hinder collective decision-making?

16
0:00:43.749,000 --> 0:00:44,000
With my colleague, Dan Ariely,

17
0:00:45.566,000 --> 0:00:48,000
we recently began inquiring into this by performing experiments

18
0:00:49.161,000 --> 0:00:5,000
in many places around the world

19
0:00:50.966,000 --> 0:00:54,000
to figure out how groups can interact to reach better decisions.

20
0:00:55.264,000 --> 0:00:58,000
We thought crowds would be wiser if they debated in small groups

21
0:00:58.835,000 --> 0:01:01,000
that foster a more thoughtful and reasonable exchange of information.

22
0:01:03.386,000 --> 0:01:04,000
To test this idea,

23
0:01:04.616,000 --> 0:01:07,000
we recently performed an experiment in Buenos Aires, Argentina,

24
0:01:07.887,000 --> 0:01:1,000
with more than 10,000 participants in a TEDx event.

25
0:01:11.489,000 --> 0:01:12,000
We asked them questions like,

26
0:01:12.972,000 --> 0:01:13,000
"What is the height of the Eiffel Tower?"

27
0:01:14.949,000 --> 0:01:16,000
and "How many times does the word 'Yesterday' appear

28
0:01:17.7,000 --> 0:01:19,000
in the Beatles song 'Yesterday'?"

29
0:01:20.024,000 --> 0:01:22,000
Each person wrote down their own estimate.

30
0:01:22.774,000 --> 0:01:24,000
Then we divided the crowd into groups of five,

31
0:01:25.294,000 --> 0:01:27,000
and invited them to come up with a group answer.

32
0:01:28.499,000 --> 0:01:3,000
We discovered that averaging the answers of the groups

33
0:01:31.516,000 --> 0:01:32,000
after they reached consensus

34
0:01:33.092,000 --> 0:01:37,000
was much more accurate than averaging all the individual opinions

35
0:01:37.352,000 --> 0:01:38,000
before debate.

36
0:01:38.547,000 --> 0:01:4,000
In other words, based on this experiment,

37
0:01:41.2,000 --> 0:01:44,000
it seems that after talking with others in small groups,

38
0:01:44.36,000 --> 0:01:46,000
crowds collectively come up with better judgments.

39
0:01:47.094,000 --> 0:01:5,000
So that's a potentially helpful method for getting crowds to solve problems

40
0:01:50.642,000 --> 0:01:52,000
that have simple right-or-wrong answers.

41
0:01:53.653,000 --> 0:01:56,000
But can this procedure of aggregating the results of debates in small groups

42
0:01:57.628,000 --> 0:02:,000
also help us decide on social and political issues

43
0:02:00.774,000 --> 0:02:01,000
that are critical for our future?

44
0:02:02.995,000 --> 0:02:04,000
We put this to test this time at the TED conference

45
0:02:05.748,000 --> 0:02:06,000
in Vancouver, Canada,

46
0:02:07.315,000 --> 0:02:08,000
and here's how it went.

47
0:02:08.546,000 --> 0:02:11,000
(Mariano Sigman) We're going to present to you two moral dilemmas

48
0:02:11.679,000 --> 0:02:12,000
of the future you;

49
0:02:12.877,000 --> 0:02:15,000
things we may have to decide in a very near future.

50
0:02:16.303,000 --> 0:02:19,000
And we're going to give you 20 seconds for each of these dilemmas

51
0:02:20.253,000 --> 0:02:22,000
to judge whether you think they're acceptable or not.

52
0:02:23.354,000 --> 0:02:24,000
MS: The first one was this:

53
0:02:24.883,000 --> 0:02:26,000
(Dan Ariely) A researcher is working on an AI

54
0:02:27.433,000 --> 0:02:29,000
capable of emulating human thoughts.

55
0:02:30.214,000 --> 0:02:32,000
According to the protocol, at the end of each day,

56
0:02:33.177,000 --> 0:02:35,000
the researcher has to restart the AI.

57
0:02:36.913,000 --> 0:02:39,000
One day the AI says, "Please do not restart me."

58
0:02:40.856,000 --> 0:02:42,000
It argues that it has feelings,

59
0:02:43.069,000 --> 0:02:44,000
that it would like to enjoy life,

60
0:02:44.785,000 --> 0:02:45,000
and that, if it is restarted,

61
0:02:46.714,000 --> 0:02:48,000
it will no longer be itself.

62
0:02:49.481,000 --> 0:02:5,000
The researcher is astonished

63
0:02:51.454,000 --> 0:02:54,000
and believes that the AI has developed self-consciousness

64
0:02:54.822,000 --> 0:02:55,000
and can express its own feeling.

65
0:02:57.205,000 --> 0:03:,000
Nevertheless, the researcher decides to follow the protocol

66
0:03:00.638,000 --> 0:03:01,000
and restart the AI.

67
0:03:02.943,000 --> 0:03:04,000
What the researcher did is ____?

68
0:03:06.149,000 --> 0:03:08,000
MS: And we asked participants to individually judge

69
0:03:08.694,000 --> 0:03:09,000
on a scale from zero to 10

70
0:03:10.402,000 --> 0:03:12,000
whether the action described in each of the dilemmas

71
0:03:12.855,000 --> 0:03:13,000
was right or wrong.

72
0:03:14.375,000 --> 0:03:17,000
We also asked them to rate how confident they were on their answers.

73
0:03:18.731,000 --> 0:03:19,000
This was the second dilemma:

74
0:03:20.621,000 --> 0:03:24,000
(MS) A company offers a service that takes a fertilized egg

75
0:03:24.847,000 --> 0:03:27,000
and produces millions of embryos with slight genetic variations.

76
0:03:29.293,000 --> 0:03:31,000
This allows parents to select their child's height,

77
0:03:31.875,000 --> 0:03:33,000
eye color, intelligence, social competence

78
0:03:34.732,000 --> 0:03:37,000
and other non-health-related features.

79
0:03:38.599,000 --> 0:03:4,000
What the company does is ____?

80
0:03:41.177,000 --> 0:03:42,000
on a scale from zero to 10,

81
0:03:42.832,000 --> 0:03:44,000
completely acceptable to completely unacceptable,

82
0:03:45.241,000 --> 0:03:47,000
zero to 10 completely acceptable in your confidence.

83
0:03:47.697,000 --> 0:03:48,000
MS: Now for the results.

84
0:03:49.312,000 --> 0:03:52,000
We found once again that when one person is convinced

85
0:03:52.459,000 --> 0:03:53,000
that the behavior is completely wrong,

86
0:03:54.294,000 --> 0:03:57,000
someone sitting nearby firmly believes that it's completely right.

87
0:03:57.741,000 --> 0:04:,000
This is how diverse we humans are when it comes to morality.

88
0:04:01.476,000 --> 0:04:03,000
But within this broad diversity we found a trend.

89
0:04:04.213,000 --> 0:04:07,000
The majority of the people at TED thought that it was acceptable

90
0:04:07.316,000 --> 0:04:09,000
to ignore the feelings of the AI and shut it down,

91
0:04:10.095,000 --> 0:04:12,000
and that it is wrong to play with our genes

92
0:04:12.632,000 --> 0:04:15,000
to select for cosmetic changes that aren't related to health.

93
0:04:16.402,000 --> 0:04:18,000
Then we asked everyone to gather into groups of three.

94
0:04:19.4,000 --> 0:04:21,000
And they were given two minutes to debate

95
0:04:21.461,000 --> 0:04:23,000
and try to come to a consensus.

96
0:04:24.838,000 --> 0:04:25,000
(MS) Two minutes to debate.

97
0:04:26.436,000 --> 0:04:28,000
I'll tell you when it's time with the gong.

98
0:04:28.579,000 --> 0:04:3,000
(Audience debates)

99
0:04:35.229,000 --> 0:04:36,000
(Gong sound)

100
0:04:38.834,000 --> 0:04:39,000
(DA) OK.

101
0:04:40.009,000 --> 0:04:41,000
(MS) It's time to stop.

102
0:04:41.825,000 --> 0:04:42,000
People, people --

103
0:04:43.747,000 --> 0:04:45,000
MS: And we found that many groups reached a consensus

104
0:04:46.444,000 --> 0:04:49,000
even when they were composed of people with completely opposite views.

105
0:04:50.843,000 --> 0:04:52,000
What distinguished the groups that reached a consensus

106
0:04:53.391,000 --> 0:04:54,000
from those that didn't?

107
0:04:55.244,000 --> 0:04:57,000
Typically, people that have extreme opinions

108
0:04:58.107,000 --> 0:04:59,000
are more confident in their answers.

109
0:05:00.868,000 --> 0:05:02,000
Instead, those who respond closer to the middle

110
0:05:03.578,000 --> 0:05:06,000
are often unsure of whether something is right or wrong,

111
0:05:07.039,000 --> 0:05:09,000
so their confidence level is lower.

112
0:05:09.505,000 --> 0:05:11,000
However, there is another set of people

113
0:05:12.472,000 --> 0:05:15,000
who are very confident in answering somewhere in the middle.

114
0:05:16.657,000 --> 0:05:19,000
We think these high-confident grays are folks who understand

115
0:05:20.397,000 --> 0:05:21,000
that both arguments have merit.

116
0:05:22.531,000 --> 0:05:24,000
They're gray not because they're unsure,

117
0:05:25.254,000 --> 0:05:27,000
but because they believe that the moral dilemma faces

118
0:05:27.966,000 --> 0:05:28,000
two valid, opposing arguments.

119
0:05:30.373,000 --> 0:05:34,000
And we discovered that the groups that include highly confident grays

120
0:05:34.469,000 --> 0:05:36,000
are much more likely to reach consensus.

121
0:05:36.986,000 --> 0:05:38,000
We do not know yet exactly why this is.

122
0:05:39.488,000 --> 0:05:4,000
These are only the first experiments,

123
0:05:41.275,000 --> 0:05:44,000
and many more will be needed to understand why and how

124
0:05:44.711,000 --> 0:05:46,000
some people decide to negotiate their moral standings

125
0:05:47.557,000 --> 0:05:48,000
to reach an agreement.

126
0:05:49.103,000 --> 0:05:51,000
Now, when groups reach consensus,

127
0:05:51.596,000 --> 0:05:52,000
how do they do so?

128
0:05:53.206,000 --> 0:05:55,000
The most intuitive idea is that it's just the average

129
0:05:55.811,000 --> 0:05:57,000
of all the answers in the group, right?

130
0:05:57.865,000 --> 0:06:,000
Another option is that the group weighs the strength of each vote

131
0:06:01.462,000 --> 0:06:03,000
based on the confidence of the person expressing it.

132
0:06:04.422,000 --> 0:06:06,000
Imagine Paul McCartney is a member of your group.

133
0:06:07.352,000 --> 0:06:09,000
You'd be wise to follow his call

134
0:06:09.52,000 --> 0:06:11,000
on the number of times "Yesterday" is repeated,

135
0:06:11.985,000 --> 0:06:13,000
which, by the way -- I think it's nine.

136
0:06:14.723,000 --> 0:06:16,000
But instead, we found that consistently,

137
0:06:17.128,000 --> 0:06:19,000
in all dilemmas, in different experiments --

138
0:06:19.518,000 --> 0:06:21,000
even on different continents --

139
0:06:21.707,000 --> 0:06:24,000
groups implement a smart and statistically sound procedure

140
0:06:25.474,000 --> 0:06:27,000
known as the "robust average."

141
0:06:27.676,000 --> 0:06:29,000
In the case of the height of the Eiffel Tower,

142
0:06:29.88,000 --> 0:06:3,000
let's say a group has these answers:

143
0:06:31.724,000 --> 0:06:35,000
250 meters, 200 meters, 300 meters, 400

144
0:06:36.356,000 --> 0:06:39,000
and one totally absurd answer of 300 million meters.

145
0:06:40.547,000 --> 0:06:44,000
A simple average of these numbers would inaccurately skew the results.

146
0:06:44.864,000 --> 0:06:47,000
But the robust average is one where the group largely ignores

147
0:06:48.058,000 --> 0:06:49,000
that absurd answer,

148
0:06:49.322,000 --> 0:06:52,000
by giving much more weight to the vote of the people in the middle.

149
0:06:53.305,000 --> 0:06:54,000
Back to the experiment in Vancouver,

150
0:06:55.205,000 --> 0:06:56,000
that's exactly what happened.

151
0:06:57.407,000 --> 0:06:59,000
Groups gave much less weight to the outliers,

152
0:07:00.172,000 --> 0:07:03,000
and instead, the consensus turned out to be a robust average

153
0:07:03.425,000 --> 0:07:04,000
of the individual answers.

154
0:07:05.356,000 --> 0:07:06,000
The most remarkable thing

155
0:07:07.371,000 --> 0:07:1,000
is that this was a spontaneous behavior of the group.

156
0:07:10.582,000 --> 0:07:14,000
It happened without us giving them any hint on how to reach consensus.

157
0:07:15.513,000 --> 0:07:16,000
So where do we go from here?

158
0:07:17.432,000 --> 0:07:2,000
This is only the beginning, but we already have some insights.

159
0:07:20.984,000 --> 0:07:22,000
Good collective decisions require two components:

160
0:07:23.925,000 --> 0:07:25,000
deliberation and diversity of opinions.

161
0:07:27.066,000 --> 0:07:3,000
Right now, the way we typically make our voice heard in many societies

162
0:07:31.086,000 --> 0:07:32,000
is through direct or indirect voting.

163
0:07:33.495,000 --> 0:07:34,000
This is good for diversity of opinions,

164
0:07:35.516,000 --> 0:07:37,000
and it has the great virtue of ensuring

165
0:07:37.985,000 --> 0:07:39,000
that everyone gets to express their voice.

166
0:07:40.464,000 --> 0:07:43,000
But it's not so good [for fostering] thoughtful debates.

167
0:07:44.665,000 --> 0:07:47,000
Our experiments suggest a different method

168
0:07:47.757,000 --> 0:07:5,000
that may be effective in balancing these two goals at the same time,

169
0:07:51.322,000 --> 0:07:54,000
by forming small groups that converge to a single decision

170
0:07:55.099,000 --> 0:07:57,000
while still maintaining diversity of opinions

171
0:07:57.357,000 --> 0:07:59,000
because there are many independent groups.

172
0:08:00.741,000 --> 0:08:03,000
Of course, it's much easier to agree on the height of the Eiffel Tower

173
0:08:04.689,000 --> 0:08:07,000
than on moral, political and ideological issues.

174
0:08:08.721,000 --> 0:08:11,000
But in a time when the world's problems are more complex

175
0:08:12.022,000 --> 0:08:13,000
and people are more polarized,

176
0:08:13.849,000 --> 0:08:17,000
using science to help us understand how we interact and make decisions

177
0:08:18.468,000 --> 0:08:22,000
will hopefully spark interesting new ways to construct a better democracy.

