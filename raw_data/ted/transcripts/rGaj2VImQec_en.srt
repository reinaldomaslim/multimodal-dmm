1
0:00:15.26,000 --> 0:00:18,000
I didn't always love unintended consequences,

2
0:00:18.26,000 --> 0:00:2,000
but I've really learned to appreciate them.

3
0:00:20.26,000 --> 0:00:22,000
I've learned that they're really the essence

4
0:00:22.26,000 --> 0:00:24,000
of what makes for progress,

5
0:00:24.26,000 --> 0:00:27,000
even when they seem to be terrible.

6
0:00:27.26,000 --> 0:00:29,000
And I'd like to review

7
0:00:29.26,000 --> 0:00:32,000
just how unintended consequences

8
0:00:32.26,000 --> 0:00:35,000
play the part that they do.

9
0:00:35.26,000 --> 0:00:4,000
Let's go to 40,000 years before the present,

10
0:00:40.26,000 --> 0:00:44,000
to the time of the cultural explosion,

11
0:00:44.26,000 --> 0:00:49,000
when music, art, technology,

12
0:00:49.26,000 --> 0:00:51,000
so many of the things that we're enjoying today,

13
0:00:51.26,000 --> 0:00:54,000
so many of the things that are being demonstrated at TED

14
0:00:54.26,000 --> 0:00:56,000
were born.

15
0:00:56.26,000 --> 0:00:59,000
And the anthropologist Randall White

16
0:00:59.26,000 --> 0:01:02,000
has made a very interesting observation:

17
0:01:02.26,000 --> 0:01:04,000
that if our ancestors

18
0:01:04.26,000 --> 0:01:06,000
40,000 years ago

19
0:01:06.26,000 --> 0:01:09,000
had been able to see

20
0:01:09.26,000 --> 0:01:11,000
what they had done,

21
0:01:11.26,000 --> 0:01:13,000
they wouldn't have really understood it.

22
0:01:13.26,000 --> 0:01:15,000
They were responding

23
0:01:15.26,000 --> 0:01:18,000
to immediate concerns.

24
0:01:18.26,000 --> 0:01:2,000
They were making it possible for us

25
0:01:20.26,000 --> 0:01:22,000
to do what they do,

26
0:01:22.26,000 --> 0:01:24,000
and yet, they didn't really understand

27
0:01:24.26,000 --> 0:01:26,000
how they did it.

28
0:01:26.26,000 --> 0:01:31,000
Now let's advance to 10,000 years before the present.

29
0:01:31.26,000 --> 0:01:33,000
And this is when it really gets interesting.

30
0:01:33.26,000 --> 0:01:36,000
What about the domestication of grains?

31
0:01:36.26,000 --> 0:01:39,000
What about the origins of agriculture?

32
0:01:39.26,000 --> 0:01:42,000
What would our ancestors 10,000 years ago

33
0:01:42.26,000 --> 0:01:44,000
have said

34
0:01:44.26,000 --> 0:01:46,000
if they really had technology assessment?

35
0:01:46.26,000 --> 0:01:48,000
And I could just imagine the committees

36
0:01:48.26,000 --> 0:01:5,000
reporting back to them

37
0:01:50.26,000 --> 0:01:53,000
on where agriculture was going to take humanity,

38
0:01:53.26,000 --> 0:01:56,000
at least in the next few hundred years.

39
0:01:56.26,000 --> 0:01:58,000
It was really bad news.

40
0:01:58.26,000 --> 0:02:,000
First of all, worse nutrition,

41
0:02:00.26,000 --> 0:02:02,000
maybe shorter life spans.

42
0:02:02.26,000 --> 0:02:04,000
It was simply awful for women.

43
0:02:04.26,000 --> 0:02:06,000
The skeletal remains from that period

44
0:02:06.26,000 --> 0:02:11,000
have shown that they were grinding grain morning, noon and night.

45
0:02:11.26,000 --> 0:02:14,000
And politically, it was awful.

46
0:02:14.26,000 --> 0:02:17,000
It was the beginning of a much higher degree

47
0:02:17.26,000 --> 0:02:2,000
of inequality among people.

48
0:02:20.26,000 --> 0:02:23,000
If there had been rational technology assessment then,

49
0:02:23.26,000 --> 0:02:25,000
I think they very well might have said,

50
0:02:25.26,000 --> 0:02:28,000
"Let's call the whole thing off."

51
0:02:28.26,000 --> 0:02:32,000
Even now, our choices are having unintended effects.

52
0:02:32.26,000 --> 0:02:34,000
Historically, for example,

53
0:02:34.26,000 --> 0:02:37,000
chopsticks -- according to one Japanese anthropologist

54
0:02:37.26,000 --> 0:02:39,000
who wrote a dissertation about it

55
0:02:39.26,000 --> 0:02:41,000
at the University of Michigan --

56
0:02:41.26,000 --> 0:02:44,000
resulted in long-term changes

57
0:02:44.26,000 --> 0:02:46,000
in the dentition, in the teeth,

58
0:02:46.26,000 --> 0:02:48,000
of the Japanese public.

59
0:02:48.26,000 --> 0:02:51,000
And we are also changing our teeth right now.

60
0:02:51.26,000 --> 0:02:53,000
There is evidence

61
0:02:53.26,000 --> 0:02:55,000
that the human mouth and teeth

62
0:02:55.26,000 --> 0:02:57,000
are growing smaller all the time.

63
0:02:57.26,000 --> 0:03:,000
That's not necessarily a bad unintended consequence.

64
0:03:00.26,000 --> 0:03:02,000
But I think from the point of view of a Neanderthal,

65
0:03:02.26,000 --> 0:03:04,000
there would have been a lot of disapproval

66
0:03:04.26,000 --> 0:03:07,000
of the wimpish choppers that we now have.

67
0:03:07.26,000 --> 0:03:1,000
So these things are kind of relative

68
0:03:10.26,000 --> 0:03:14,000
to where you or your ancestors happen to stand.

69
0:03:14.26,000 --> 0:03:16,000
In the ancient world

70
0:03:16.26,000 --> 0:03:19,000
there was a lot of respect for unintended consequences,

71
0:03:19.26,000 --> 0:03:22,000
and there was a very healthy sense of caution,

72
0:03:22.26,000 --> 0:03:24,000
reflected in the Tree of Knowledge,

73
0:03:24.26,000 --> 0:03:26,000
in Pandora's Box,

74
0:03:26.26,000 --> 0:03:28,000
and especially in the myth of Prometheus

75
0:03:28.26,000 --> 0:03:3,000
that's been so important

76
0:03:30.26,000 --> 0:03:32,000
in recent metaphors about technology.

77
0:03:32.26,000 --> 0:03:35,000
And that's all very true.

78
0:03:35.26,000 --> 0:03:37,000
The physicians of the ancient world --

79
0:03:37.26,000 --> 0:03:39,000
especially the Egyptians,

80
0:03:39.26,000 --> 0:03:41,000
who started medicine as we know it --

81
0:03:41.26,000 --> 0:03:43,000
were very conscious

82
0:03:43.26,000 --> 0:03:45,000
of what they could and couldn't treat.

83
0:03:45.26,000 --> 0:03:5,000
And the translations of the surviving texts say,

84
0:03:50.26,000 --> 0:03:52,000
"This I will not treat. This I cannot treat."

85
0:03:52.26,000 --> 0:03:54,000
They were very conscious.

86
0:03:54.26,000 --> 0:03:56,000
So were the followers of Hippocrates.

87
0:03:56.26,000 --> 0:03:58,000
The Hippocratic manuscripts also --

88
0:03:58.26,000 --> 0:04:01,000
repeatedly, according to recent studies --

89
0:04:01.26,000 --> 0:04:04,000
show how important it is not to do harm.

90
0:04:04.26,000 --> 0:04:06,000
More recently,

91
0:04:06.26,000 --> 0:04:08,000
Harvey Cushing,

92
0:04:08.26,000 --> 0:04:1,000
who really developed neurosurgery as we know it,

93
0:04:10.26,000 --> 0:04:13,000
who changed it from a field of medicine

94
0:04:13.26,000 --> 0:04:17,000
that had a majority of deaths resulting from surgery

95
0:04:17.26,000 --> 0:04:2,000
to one in which there was a hopeful outlook,

96
0:04:20.26,000 --> 0:04:22,000
he was very conscious

97
0:04:22.26,000 --> 0:04:25,000
that he was not always going to do the right thing.

98
0:04:25.26,000 --> 0:04:27,000
But he did his best,

99
0:04:27.26,000 --> 0:04:29,000
and he kept meticulous records

100
0:04:29.26,000 --> 0:04:32,000
that let him transform that branch of medicine.

101
0:04:32.26,000 --> 0:04:35,000
Now if we look forward a bit

102
0:04:35.26,000 --> 0:04:37,000
to the 19th century,

103
0:04:37.26,000 --> 0:04:39,000
we find a new style of technology.

104
0:04:39.26,000 --> 0:04:41,000
What we find is,

105
0:04:41.26,000 --> 0:04:44,000
no longer simple tools,

106
0:04:44.26,000 --> 0:04:46,000
but systems.

107
0:04:46.26,000 --> 0:04:48,000
We find more and more

108
0:04:48.26,000 --> 0:04:5,000
complex arrangements of machines

109
0:04:50.26,000 --> 0:04:52,000
that make it harder and harder

110
0:04:52.26,000 --> 0:04:54,000
to diagnose what's going on.

111
0:04:54.26,000 --> 0:04:56,000
And the first people who saw that

112
0:04:56.26,000 --> 0:04:59,000
were the telegraphers of the mid-19th century,

113
0:04:59.26,000 --> 0:05:01,000
who were the original hackers.

114
0:05:01.26,000 --> 0:05:04,000
Thomas Edison would have been very, very comfortable

115
0:05:04.26,000 --> 0:05:07,000
in the atmosphere of a software firm today.

116
0:05:07.26,000 --> 0:05:1,000
And these hackers had a word

117
0:05:10.26,000 --> 0:05:13,000
for those mysterious bugs in telegraph systems

118
0:05:13.26,000 --> 0:05:15,000
that they called bugs.

119
0:05:15.26,000 --> 0:05:19,000
That was the origin of the word "bug."

120
0:05:19.26,000 --> 0:05:21,000
This consciousness, though,

121
0:05:21.26,000 --> 0:05:24,000
was a little slow to seep through the general population,

122
0:05:24.26,000 --> 0:05:27,000
even people who were very, very well informed.

123
0:05:27.26,000 --> 0:05:29,000
Samuel Clemens, Mark Twain,

124
0:05:29.26,000 --> 0:05:31,000
was a big investor

125
0:05:31.26,000 --> 0:05:34,000
in the most complex machine of all times --

126
0:05:34.26,000 --> 0:05:36,000
at least until 1918 --

127
0:05:36.26,000 --> 0:05:38,000
registered with the U.S. Patent Office.

128
0:05:38.26,000 --> 0:05:4,000
That was the Paige typesetter.

129
0:05:40.26,000 --> 0:05:42,000
The Paige typesetter

130
0:05:42.26,000 --> 0:05:44,000
had 18,000 parts.

131
0:05:44.26,000 --> 0:05:47,000
The patent had 64 pages of text

132
0:05:47.26,000 --> 0:05:51,000
and 271 figures.

133
0:05:51.26,000 --> 0:05:53,000
It was such a beautiful machine

134
0:05:53.26,000 --> 0:05:56,000
because it did everything that a human being did

135
0:05:56.26,000 --> 0:05:58,000
in setting type --

136
0:05:58.26,000 --> 0:06:,000
including returning the type to its place,

137
0:06:00.26,000 --> 0:06:02,000
which was a very difficult thing.

138
0:06:02.26,000 --> 0:06:04,000
And Mark Twain, who knew all about typesetting,

139
0:06:04.26,000 --> 0:06:07,000
really was smitten by this machine.

140
0:06:07.26,000 --> 0:06:1,000
Unfortunately, he was smitten in more ways than one,

141
0:06:10.26,000 --> 0:06:12,000
because it made him bankrupt,

142
0:06:12.26,000 --> 0:06:14,000
and he had to tour the world speaking

143
0:06:14.26,000 --> 0:06:17,000
to recoup his money.

144
0:06:17.26,000 --> 0:06:19,000
And this was an important thing

145
0:06:19.26,000 --> 0:06:21,000
about 19th century technology,

146
0:06:21.26,000 --> 0:06:23,000
that all these relationships among parts

147
0:06:23.26,000 --> 0:06:27,000
could make the most brilliant idea fall apart,

148
0:06:27.26,000 --> 0:06:29,000
even when judged by the most expert people.

149
0:06:29.26,000 --> 0:06:32,000
Now there is something else, though, in the early 20th century

150
0:06:32.26,000 --> 0:06:35,000
that made things even more complicated.

151
0:06:35.26,000 --> 0:06:38,000
And that was that safety technology itself

152
0:06:38.26,000 --> 0:06:4,000
could be a source of danger.

153
0:06:40.26,000 --> 0:06:43,000
The lesson of the Titanic, for a lot of the contemporaries,

154
0:06:43.26,000 --> 0:06:45,000
was that you must have enough lifeboats

155
0:06:45.26,000 --> 0:06:47,000
for everyone on the ship.

156
0:06:47.26,000 --> 0:06:5,000
And this was the result

157
0:06:50.26,000 --> 0:06:52,000
of the tragic loss of lives

158
0:06:52.26,000 --> 0:06:54,000
of people who could not get into them.

159
0:06:54.26,000 --> 0:06:57,000
However, there was another case, the Eastland,

160
0:06:57.26,000 --> 0:07:01,000
a ship that capsized in Chicago Harbor in 1915,

161
0:07:01.26,000 --> 0:07:04,000
and it killed 841 people --

162
0:07:04.26,000 --> 0:07:06,000
that was 14 more

163
0:07:06.26,000 --> 0:07:09,000
than the passenger toll of the Titanic.

164
0:07:09.26,000 --> 0:07:11,000
The reason for it, in part, was

165
0:07:11.26,000 --> 0:07:14,000
the extra life boats that were added

166
0:07:14.26,000 --> 0:07:17,000
that made this already unstable ship

167
0:07:17.26,000 --> 0:07:19,000
even more unstable.

168
0:07:19.26,000 --> 0:07:21,000
And that again proves

169
0:07:21.26,000 --> 0:07:24,000
that when you're talking about unintended consequences,

170
0:07:24.26,000 --> 0:07:26,000
it's not that easy to know

171
0:07:26.26,000 --> 0:07:28,000
the right lessons to draw.

172
0:07:28.26,000 --> 0:07:31,000
It's really a question of the system, how the ship was loaded,

173
0:07:31.26,000 --> 0:07:34,000
the ballast and many other things.

174
0:07:35.26,000 --> 0:07:38,000
So the 20th century, then,

175
0:07:38.26,000 --> 0:07:4,000
saw how much more complex reality was,

176
0:07:40.26,000 --> 0:07:43,000
but it also saw a positive side.

177
0:07:43.26,000 --> 0:07:46,000
It saw that invention

178
0:07:46.26,000 --> 0:07:48,000
could actually benefit from emergencies.

179
0:07:48.26,000 --> 0:07:5,000
It could benefit

180
0:07:50.26,000 --> 0:07:53,000
from tragedies.

181
0:07:53.26,000 --> 0:07:55,000
And my favorite example of that --

182
0:07:55.26,000 --> 0:07:57,000
which is not really widely known

183
0:07:57.26,000 --> 0:07:59,000
as a technological miracle,

184
0:07:59.26,000 --> 0:08:02,000
but it may be one of the greatest of all times,

185
0:08:02.26,000 --> 0:08:06,000
was the scaling up of penicillin in the Second World War.

186
0:08:06.26,000 --> 0:08:09,000
Penicillin was discovered in 1928,

187
0:08:09.26,000 --> 0:08:11,000
but even by 1940,

188
0:08:11.26,000 --> 0:08:14,000
no commercially and medically useful quantities of it

189
0:08:14.26,000 --> 0:08:16,000
were being produced.

190
0:08:16.26,000 --> 0:08:19,000
A number of pharmaceutical companies were working on it.

191
0:08:19.26,000 --> 0:08:21,000
They were working on it independently,

192
0:08:21.26,000 --> 0:08:23,000
and they weren't getting anywhere.

193
0:08:23.26,000 --> 0:08:25,000
And the Government Research Bureau

194
0:08:25.26,000 --> 0:08:27,000
brought representatives together

195
0:08:27.26,000 --> 0:08:29,000
and told them that this is something

196
0:08:29.26,000 --> 0:08:31,000
that has to be done.

197
0:08:31.26,000 --> 0:08:33,000
And not only did they do it,

198
0:08:33.26,000 --> 0:08:35,000
but within two years,

199
0:08:35.26,000 --> 0:08:37,000
they scaled up penicillin

200
0:08:37.26,000 --> 0:08:4,000
from preparation in one-liter flasks

201
0:08:40.26,000 --> 0:08:44,000
to 10,000-gallon vats.

202
0:08:44.26,000 --> 0:08:48,000
That was how quickly penicillin was produced

203
0:08:48.26,000 --> 0:08:52,000
and became one of the greatest medical advances of all time.

204
0:08:52.26,000 --> 0:08:54,000
In the Second World War, too,

205
0:08:54.26,000 --> 0:08:56,000
the existence

206
0:08:56.26,000 --> 0:08:58,000
of solar radiation

207
0:08:58.26,000 --> 0:09:01,000
was demonstrated by studies of interference

208
0:09:01.26,000 --> 0:09:05,000
that was detected by the radar stations of Great Britain.

209
0:09:05.26,000 --> 0:09:08,000
So there were benefits in calamities --

210
0:09:08.26,000 --> 0:09:1,000
benefits to pure science,

211
0:09:10.26,000 --> 0:09:12,000
as well as to applied science

212
0:09:12.26,000 --> 0:09:15,000
and medicine.

213
0:09:15.26,000 --> 0:09:18,000
Now when we come to the period after the Second World War,

214
0:09:18.26,000 --> 0:09:22,000
unintended consequences get even more interesting.

215
0:09:22.26,000 --> 0:09:24,000
And my favorite example of that

216
0:09:24.26,000 --> 0:09:27,000
occurred beginning in 1976,

217
0:09:27.26,000 --> 0:09:29,000
when it was discovered

218
0:09:29.26,000 --> 0:09:32,000
that the bacteria causing Legionnaires disease

219
0:09:32.26,000 --> 0:09:35,000
had always been present in natural waters,

220
0:09:35.26,000 --> 0:09:39,000
but it was the precise temperature of the water

221
0:09:39.26,000 --> 0:09:42,000
in heating, ventilating and air conditioning systems

222
0:09:42.26,000 --> 0:09:46,000
that raised the right temperature

223
0:09:46.26,000 --> 0:09:49,000
for the maximum reproduction

224
0:09:49.26,000 --> 0:09:51,000
of Legionella bacillus.

225
0:09:51.26,000 --> 0:09:53,000
Well, technology to the rescue.

226
0:09:53.26,000 --> 0:09:55,000
So chemists got to work,

227
0:09:55.26,000 --> 0:09:57,000
and they developed a bactericide

228
0:09:57.26,000 --> 0:10:,000
that became widely used in those systems.

229
0:10:00.26,000 --> 0:10:04,000
But something else happened in the early 1980s,

230
0:10:04.26,000 --> 0:10:06,000
and that was that there was a mysterious epidemic

231
0:10:06.26,000 --> 0:10:09,000
of failures of tape drives

232
0:10:09.26,000 --> 0:10:11,000
all over the United States.

233
0:10:11.26,000 --> 0:10:14,000
And IBM, which made them,

234
0:10:14.26,000 --> 0:10:17,000
just didn't know what to do.

235
0:10:17.26,000 --> 0:10:2,000
They commissioned a group of their best scientists

236
0:10:20.26,000 --> 0:10:22,000
to investigate,

237
0:10:22.26,000 --> 0:10:24,000
and what they found was

238
0:10:24.26,000 --> 0:10:26,000
that all these tape drives

239
0:10:26.26,000 --> 0:10:29,000
were located near ventilation ducts.

240
0:10:29.26,000 --> 0:10:32,000
What happened was the bactericide was formulated

241
0:10:32.26,000 --> 0:10:34,000
with minute traces of tin.

242
0:10:34.26,000 --> 0:10:37,000
And these tin particles were deposited on the tape heads

243
0:10:37.26,000 --> 0:10:4,000
and were crashing the tape heads.

244
0:10:40.26,000 --> 0:10:43,000
So they reformulated the bactericide.

245
0:10:43.26,000 --> 0:10:45,000
But what's interesting to me

246
0:10:45.26,000 --> 0:10:47,000
is that this was the first case

247
0:10:47.26,000 --> 0:10:49,000
of a mechanical device

248
0:10:49.26,000 --> 0:10:52,000
suffering, at least indirectly, from a human disease.

249
0:10:52.26,000 --> 0:10:55,000
So it shows that we're really all in this together.

250
0:10:55.26,000 --> 0:10:57,000
(Laughter)

251
0:10:57.26,000 --> 0:11:,000
In fact, it also shows something interesting,

252
0:11:00.26,000 --> 0:11:03,000
that although our capabilities and technology

253
0:11:03.26,000 --> 0:11:05,000
have been expanding geometrically,

254
0:11:05.26,000 --> 0:11:08,000
unfortunately, our ability to model their long-term behavior,

255
0:11:08.26,000 --> 0:11:1,000
which has also been increasing,

256
0:11:10.26,000 --> 0:11:13,000
has been increasing only arithmetically.

257
0:11:13.26,000 --> 0:11:16,000
So one of the characteristic problems of our time

258
0:11:16.26,000 --> 0:11:18,000
is how to close this gap

259
0:11:18.26,000 --> 0:11:21,000
between capabilities and foresight.

260
0:11:21.26,000 --> 0:11:24,000
One other very positive consequence

261
0:11:24.26,000 --> 0:11:27,000
of 20th century technology, though,

262
0:11:27.26,000 --> 0:11:31,000
was the way in which other kinds of calamities

263
0:11:31.26,000 --> 0:11:34,000
could lead to positive advances.

264
0:11:34.26,000 --> 0:11:37,000
There are two historians of business

265
0:11:37.26,000 --> 0:11:39,000
at the University of Maryland,

266
0:11:39.26,000 --> 0:11:41,000
Brent Goldfarb and David Kirsch,

267
0:11:41.26,000 --> 0:11:43,000
who have done some extremely interesting work,

268
0:11:43.26,000 --> 0:11:46,000
much of it still unpublished,

269
0:11:46.26,000 --> 0:11:48,000
on the history of major innovations.

270
0:11:48.26,000 --> 0:11:51,000
They have combined the list of major innovations,

271
0:11:51.26,000 --> 0:11:54,000
and they've discovered that the greatest number, the greatest decade,

272
0:11:54.26,000 --> 0:11:56,000
for fundamental innovations,

273
0:11:56.26,000 --> 0:12:,000
as reflected in all of the lists that others have made --

274
0:12:00.26,000 --> 0:12:02,000
a number of lists that they have merged --

275
0:12:02.26,000 --> 0:12:05,000
was the Great Depression.

276
0:12:05.26,000 --> 0:12:08,000
And nobody knows just why this was so,

277
0:12:08.26,000 --> 0:12:11,000
but one story can reflect something of it.

278
0:12:11.26,000 --> 0:12:14,000
It was the origin of the Xerox copier,

279
0:12:14.26,000 --> 0:12:17,000
which celebrated its 50th anniversary

280
0:12:17.26,000 --> 0:12:19,000
last year.

281
0:12:19.26,000 --> 0:12:24,000
And Chester Carlson, the inventor,

282
0:12:24.26,000 --> 0:12:27,000
was a patent attorney.

283
0:12:27.26,000 --> 0:12:3,000
He really was not intending

284
0:12:30.26,000 --> 0:12:32,000
to work in patent research,

285
0:12:32.26,000 --> 0:12:36,000
but he couldn't really find an alternative technical job.

286
0:12:36.26,000 --> 0:12:38,000
So this was the best job he could get.

287
0:12:38.26,000 --> 0:12:42,000
He was upset by the low quality and high cost

288
0:12:42.26,000 --> 0:12:45,000
of existing patent reproductions,

289
0:12:45.26,000 --> 0:12:48,000
and so he started to develop

290
0:12:48.26,000 --> 0:12:51,000
a system of dry photocopying,

291
0:12:51.26,000 --> 0:12:54,000
which he patented in the late 1930s --

292
0:12:54.26,000 --> 0:12:58,000
and which became the first dry photocopier

293
0:12:58.26,000 --> 0:13:,000
that was commercially practical

294
0:13:00.26,000 --> 0:13:02,000
in 1960.

295
0:13:02.26,000 --> 0:13:04,000
So we see that sometimes,

296
0:13:04.26,000 --> 0:13:06,000
as a result of these dislocations,

297
0:13:06.26,000 --> 0:13:08,000
as a result of people

298
0:13:08.26,000 --> 0:13:11,000
leaving their original intended career

299
0:13:11.26,000 --> 0:13:13,000
and going into something else

300
0:13:13.26,000 --> 0:13:15,000
where their creativity could make a difference,

301
0:13:15.26,000 --> 0:13:17,000
that depressions

302
0:13:17.26,000 --> 0:13:2,000
and all kinds of other unfortunate events

303
0:13:20.26,000 --> 0:13:23,000
can have a paradoxically stimulating effect

304
0:13:23.26,000 --> 0:13:25,000
on creativity.

305
0:13:25.26,000 --> 0:13:27,000
What does this mean?

306
0:13:27.26,000 --> 0:13:29,000
It means, I think,

307
0:13:29.26,000 --> 0:13:31,000
that we're living in a time of unexpected possibilities.

308
0:13:31.26,000 --> 0:13:34,000
Think of the financial world, for example.

309
0:13:34.26,000 --> 0:13:37,000
The mentor of Warren Buffett, Benjamin Graham,

310
0:13:37.26,000 --> 0:13:42,000
developed his system of value investing

311
0:13:42.26,000 --> 0:13:44,000
as a result of his own losses

312
0:13:44.26,000 --> 0:13:46,000
in the 1929 crash.

313
0:13:46.26,000 --> 0:13:48,000
And he published that book

314
0:13:48.26,000 --> 0:13:51,000
in the early 1930s,

315
0:13:51.26,000 --> 0:13:53,000
and the book still exists in further editions

316
0:13:53.26,000 --> 0:13:55,000
and is still a fundamental textbook.

317
0:13:55.26,000 --> 0:13:59,000
So many important creative things can happen

318
0:13:59.26,000 --> 0:14:02,000
when people learn from disasters.

319
0:14:02.26,000 --> 0:14:06,000
Now think of the large and small plagues that we have now --

320
0:14:06.26,000 --> 0:14:11,000
bed bugs, killer bees, spam --

321
0:14:11.26,000 --> 0:14:14,000
and it's very possible that the solutions to those

322
0:14:14.26,000 --> 0:14:17,000
will really extend well beyond the immediate question.

323
0:14:17.26,000 --> 0:14:2,000
If we think, for example, of Louis Pasteur,

324
0:14:20.26,000 --> 0:14:22,000
who in the 1860s

325
0:14:22.26,000 --> 0:14:24,000
was asked to study

326
0:14:24.26,000 --> 0:14:28,000
the diseases of silk worms for the silk industry,

327
0:14:28.26,000 --> 0:14:31,000
and his discoveries were really the beginning

328
0:14:31.26,000 --> 0:14:33,000
of the germ theory of disease.

329
0:14:33.26,000 --> 0:14:36,000
So very often, some kind of disaster --

330
0:14:36.26,000 --> 0:14:39,000
sometimes the consequence, for example,

331
0:14:39.26,000 --> 0:14:42,000
of over-cultivation of silk worms,

332
0:14:42.26,000 --> 0:14:44,000
which was a problem in Europe at the time --

333
0:14:44.26,000 --> 0:14:46,000
can be the key to something much bigger.

334
0:14:46.26,000 --> 0:14:48,000
So this means

335
0:14:48.26,000 --> 0:14:5,000
that we need to take a different view

336
0:14:50.26,000 --> 0:14:52,000
of unintended consequences.

337
0:14:52.26,000 --> 0:14:55,000
We need to take a really positive view.

338
0:14:55.26,000 --> 0:14:58,000
We need to see what they can do for us.

339
0:14:58.26,000 --> 0:15:,000
We need to learn

340
0:15:00.26,000 --> 0:15:02,000
from those figures that I mentioned.

341
0:15:02.26,000 --> 0:15:05,000
We need to learn, for example, from Dr. Cushing,

342
0:15:05.26,000 --> 0:15:07,000
who killed patients

343
0:15:07.26,000 --> 0:15:09,000
in the course of his early operations.

344
0:15:09.26,000 --> 0:15:12,000
He had to have some errors. He had to have some mistakes.

345
0:15:12.26,000 --> 0:15:15,000
And he learned meticulously from his mistakes.

346
0:15:15.26,000 --> 0:15:17,000
And as a result,

347
0:15:17.26,000 --> 0:15:2,000
when we say, "This isn't brain surgery,"

348
0:15:20.26,000 --> 0:15:23,000
that pays tribute to how difficult it was

349
0:15:23.26,000 --> 0:15:25,000
for anyone to learn from their mistakes

350
0:15:25.26,000 --> 0:15:27,000
in a field of medicine

351
0:15:27.26,000 --> 0:15:3,000
that was considered so discouraging in its prospects.

352
0:15:30.26,000 --> 0:15:33,000
And we can also remember

353
0:15:33.26,000 --> 0:15:35,000
how the pharmaceutical companies

354
0:15:35.26,000 --> 0:15:37,000
were willing to pool their knowledge,

355
0:15:37.26,000 --> 0:15:39,000
to share their knowledge,

356
0:15:39.26,000 --> 0:15:41,000
in the face of an emergency,

357
0:15:41.26,000 --> 0:15:44,000
which they hadn't really been for years and years.

358
0:15:44.26,000 --> 0:15:47,000
They might have been able to do it earlier.

359
0:15:47.26,000 --> 0:15:5,000
The message, then, for me,

360
0:15:50.26,000 --> 0:15:52,000
about unintended consequences

361
0:15:52.26,000 --> 0:15:55,000
is chaos happens;

362
0:15:55.26,000 --> 0:15:57,000
let's make better use of it.

363
0:15:57.26,000 --> 0:15:59,000
Thank you very much.

364
0:15:59.26,000 --> 0:16:03,000
(Applause)

