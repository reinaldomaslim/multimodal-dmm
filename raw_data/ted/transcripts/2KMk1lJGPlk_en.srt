1
0:00:,000 --> 0:00:07,000
Translator: Ivana Korom Reviewer: Krystian Aparta

2
0:00:12.865,000 --> 0:00:14,000
Let me share a paradox.

3
0:00:16.429,000 --> 0:00:17,000
For the last 10 years,

4
0:00:17.92,000 --> 0:00:2,000
many companies have been trying to become less bureaucratic,

5
0:00:21.792,000 --> 0:00:23,000
to have fewer central rules and procedures,

6
0:00:24.649,000 --> 0:00:27,000
more autonomy for their local teams to be more agile.

7
0:00:28.204,000 --> 0:00:32,000
And now they are pushing artificial intelligence, AI,

8
0:00:32.814,000 --> 0:00:34,000
unaware that cool technology

9
0:00:35.283,000 --> 0:00:38,000
might make them more bureaucratic than ever.

10
0:00:39.378,000 --> 0:00:4,000
Why?

11
0:00:40.553,000 --> 0:00:43,000
Because AI operates just like bureaucracies.

12
0:00:44.403,000 --> 0:00:46,000
The essence of bureaucracy

13
0:00:46.839,000 --> 0:00:5,000
is to favor rules and procedures over human judgment.

14
0:00:51.887,000 --> 0:00:54,000
And AI decides solely based on rules.

15
0:00:56.062,000 --> 0:00:58,000
Many rules inferred from past data

16
0:00:58.919,000 --> 0:00:59,000
but only rules.

17
0:01:01.204,000 --> 0:01:04,000
And if human judgment is not kept in the loop,

18
0:01:04.958,000 --> 0:01:08,000
AI will bring a terrifying form of new bureaucracy --

19
0:01:09.538,000 --> 0:01:11,000
I call it "algocracy" --

20
0:01:12.561,000 --> 0:01:16,000
where AI will take more and more critical decisions by the rules

21
0:01:17.085,000 --> 0:01:19,000
outside of any human control.

22
0:01:20.427,000 --> 0:01:21,000
Is there a real risk?

23
0:01:22.514,000 --> 0:01:23,000
Yes.

24
0:01:23.688,000 --> 0:01:26,000
I'm leading a team of 800 AI specialists.

25
0:01:26.718,000 --> 0:01:29,000
We have deployed over 100 customized AI solutions

26
0:01:30.599,000 --> 0:01:32,000
for large companies around the world.

27
0:01:33.417,000 --> 0:01:38,000
And I see too many corporate executives behaving like bureaucrats from the past.

28
0:01:39.784,000 --> 0:01:43,000
They want to take costly, old-fashioned humans out of the loop

29
0:01:44.72,000 --> 0:01:47,000
and rely only upon AI to take decisions.

30
0:01:49.244,000 --> 0:01:53,000
I call this the "human-zero mindset."

31
0:01:54.26,000 --> 0:01:56,000
And why is it so tempting?

32
0:01:56.879,000 --> 0:02:01,000
Because the other route, "Human plus AI," is long,

33
0:02:02.307,000 --> 0:02:04,000
costly and difficult.

34
0:02:04.94,000 --> 0:02:07,000
Business teams, tech teams, data-science teams

35
0:02:08.257,000 --> 0:02:1,000
have to iterate for months

36
0:02:10.36,000 --> 0:02:15,000
to craft exactly how humans and AI can best work together.

37
0:02:16.16,000 --> 0:02:19,000
Long, costly and difficult.

38
0:02:19.891,000 --> 0:02:21,000
But the reward is huge.

39
0:02:22.343,000 --> 0:02:25,000
A recent survey from BCG and MIT

40
0:02:25.673,000 --> 0:02:29,000
shows that 18 percent of companies in the world

41
0:02:30.204,000 --> 0:02:32,000
are pioneering AI,

42
0:02:32.442,000 --> 0:02:34,000
making money with it.

43
0:02:35.157,000 --> 0:02:4,000
Those companies focus 80 percent of their AI initiatives

44
0:02:40.771,000 --> 0:02:41,000
on effectiveness and growth,

45
0:02:42.748,000 --> 0:02:44,000
taking better decisions --

46
0:02:44.942,000 --> 0:02:47,000
not replacing humans with AI to save costs.

47
0:02:50.159,000 --> 0:02:53,000
Why is it important to keep humans in the loop?

48
0:02:54.032,000 --> 0:02:58,000
Simply because, left alone, AI can do very dumb things.

49
0:02:59.363,000 --> 0:03:02,000
Sometimes with no consequences, like in this tweet.

50
0:03:03.212,000 --> 0:03:04,000
"Dear Amazon,

51
0:03:04.8,000 --> 0:03:05,000
I bought a toilet seat.

52
0:03:06.21,000 --> 0:03:07,000
Necessity, not desire.

53
0:03:07.895,000 --> 0:03:08,000
I do not collect them,

54
0:03:09.371,000 --> 0:03:11,000
I'm not a toilet-seat addict.

55
0:03:11.633,000 --> 0:03:13,000
No matter how temptingly you email me,

56
0:03:13.863,000 --> 0:03:15,000
I am not going to think, 'Oh, go on, then,

57
0:03:16.236,000 --> 0:03:18,000
one more toilet seat, I'll treat myself.' "

58
0:03:18.4,000 --> 0:03:19,000
(Laughter)

59
0:03:19.768,000 --> 0:03:23,000
Sometimes, with more consequence, like in this other tweet.

60
0:03:24.903,000 --> 0:03:25,000
"Had the same situation

61
0:03:26.714,000 --> 0:03:28,000
with my mother's burial urn."

62
0:03:29.197,000 --> 0:03:3,000
(Laughter)

63
0:03:30.229,000 --> 0:03:31,000
"For months after her death,

64
0:03:31.618,000 --> 0:03:34,000
I got messages from Amazon, saying, 'If you liked that ...' "

65
0:03:35.19,000 --> 0:03:37,000
(Laughter)

66
0:03:37.229,000 --> 0:03:39,000
Sometimes with worse consequences.

67
0:03:39.781,000 --> 0:03:43,000
Take an AI engine rejecting a student application for university.

68
0:03:44.535,000 --> 0:03:45,000
Why?

69
0:03:45.709,000 --> 0:03:47,000
Because it has "learned," on past data,

70
0:03:48.403,000 --> 0:03:51,000
characteristics of students that will pass and fail.

71
0:03:51.609,000 --> 0:03:53,000
Some are obvious, like GPAs.

72
0:03:54.069,000 --> 0:03:59,000
But if, in the past, all students from a given postal code have failed,

73
0:03:59.202,000 --> 0:04:02,000
it is very likely that AI will make this a rule

74
0:04:02.758,000 --> 0:04:05,000
and will reject every student with this postal code,

75
0:04:06.552,000 --> 0:04:1,000
not giving anyone the opportunity to prove the rule wrong.

76
0:04:11.857,000 --> 0:04:13,000
And no one can check all the rules,

77
0:04:14.397,000 --> 0:04:17,000
because advanced AI is constantly learning.

78
0:04:18.307,000 --> 0:04:2,000
And if humans are kept out of the room,

79
0:04:20.657,000 --> 0:04:23,000
there comes the algocratic nightmare.

80
0:04:24.466,000 --> 0:04:26,000
Who is accountable for rejecting the student?

81
0:04:27.347,000 --> 0:04:28,000
No one, AI did.

82
0:04:29.014,000 --> 0:04:3,000
Is it fair? Yes.

83
0:04:30.712,000 --> 0:04:33,000
The same set of objective rules has been applied to everyone.

84
0:04:34.367,000 --> 0:04:37,000
Could we reconsider for this bright kid with the wrong postal code?

85
0:04:38.899,000 --> 0:04:41,000
No, algos don't change their mind.

86
0:04:42.974,000 --> 0:04:44,000
We have a choice here.

87
0:04:45.756,000 --> 0:04:47,000
Carry on with algocracy

88
0:04:48.304,000 --> 0:04:5,000
or decide to go to "Human plus AI."

89
0:04:51.193,000 --> 0:04:52,000
And to do this,

90
0:04:52.55,000 --> 0:04:55,000
we need to stop thinking tech first,

91
0:04:56.014,000 --> 0:04:59,000
and we need to start applying the secret formula.

92
0:05:00.601,000 --> 0:05:02,000
To deploy "Human plus AI,"

93
0:05:02.728,000 --> 0:05:04,000
10 percent of the effort is to code algos;

94
0:05:05.673,000 --> 0:05:08,000
20 percent to build tech around the algos,

95
0:05:09.228,000 --> 0:05:13,000
collecting data, building UI, integrating into legacy systems;

96
0:05:13.358,000 --> 0:05:15,000
But 70 percent, the bulk of the effort,

97
0:05:16.286,000 --> 0:05:2,000
is about weaving together AI with people and processes

98
0:05:20.786,000 --> 0:05:22,000
to maximize real outcome.

99
0:05:24.136,000 --> 0:05:28,000
AI fails when cutting short on the 70 percent.

100
0:05:28.794,000 --> 0:05:31,000
The price tag for that can be small,

101
0:05:31.977,000 --> 0:05:34,000
wasting many, many millions of dollars on useless technology.

102
0:05:35.986,000 --> 0:05:36,000
Anyone cares?

103
0:05:38.153,000 --> 0:05:4,000
Or real tragedies:

104
0:05:41.137,000 --> 0:05:48,000
346 casualties in the recent crashes of two B-737 aircrafts

105
0:05:48.776,000 --> 0:05:51,000
when pilots could not interact properly

106
0:05:52.061,000 --> 0:05:54,000
with a computerized command system.

107
0:05:55.974,000 --> 0:05:56,000
For a successful 70 percent,

108
0:05:57.792,000 --> 0:06:02,000
the first step is to make sure that algos are coded by data scientists

109
0:06:02.911,000 --> 0:06:04,000
and domain experts together.

110
0:06:05.427,000 --> 0:06:07,000
Take health care for example.

111
0:06:07.649,000 --> 0:06:11,000
One of our teams worked on a new drug with a slight problem.

112
0:06:12.784,000 --> 0:06:13,000
When taking their first dose,

113
0:06:14.307,000 --> 0:06:17,000
some patients, very few, have heart attacks.

114
0:06:18.117,000 --> 0:06:21,000
So, all patients, when taking their first dose,

115
0:06:21.276,000 --> 0:06:23,000
have to spend one day in hospital,

116
0:06:23.982,000 --> 0:06:25,000
for monitoring, just in case.

117
0:06:26.613,000 --> 0:06:31,000
Our objective was to identify patients who were at zero risk of heart attacks,

118
0:06:32.193,000 --> 0:06:34,000
who could skip the day in hospital.

119
0:06:34.962,000 --> 0:06:38,000
We used AI to analyze data from clinical trials,

120
0:06:40.145,000 --> 0:06:44,000
to correlate ECG signal, blood composition, biomarkers,

121
0:06:44.537,000 --> 0:06:46,000
with the risk of heart attack.

122
0:06:47.232,000 --> 0:06:48,000
In one month,

123
0:06:48.53,000 --> 0:06:54,000
our model could flag 62 percent of patients at zero risk.

124
0:06:54.887,000 --> 0:06:56,000
They could skip the day in hospital.

125
0:06:57.863,000 --> 0:07:,000
Would you be comfortable staying at home for your first dose

126
0:07:01.379,000 --> 0:07:02,000
if the algo said so?

127
0:07:02.927,000 --> 0:07:03,000
(Laughter)

128
0:07:03.966,000 --> 0:07:04,000
Doctors were not.

129
0:07:05.966,000 --> 0:07:07,000
What if we had false negatives,

130
0:07:08.292,000 --> 0:07:13,000
meaning people who are told by AI they can stay at home, and die?

131
0:07:13.545,000 --> 0:07:14,000
(Laughter)

132
0:07:14.934,000 --> 0:07:16,000
There started our 70 percent.

133
0:07:17.41,000 --> 0:07:18,000
We worked with a team of doctors

134
0:07:19.426,000 --> 0:07:22,000
to check the medical logic of each variable in our model.

135
0:07:23.537,000 --> 0:07:27,000
For instance, we were using the concentration of a liver enzyme

136
0:07:28.13,000 --> 0:07:29,000
as a predictor,

137
0:07:29.427,000 --> 0:07:32,000
for which the medical logic was not obvious.

138
0:07:33.149,000 --> 0:07:35,000
The statistical signal was quite strong.

139
0:07:36.3,000 --> 0:07:38,000
But what if it was a bias in our sample?

140
0:07:39.157,000 --> 0:07:41,000
That predictor was taken out of the model.

141
0:07:42.307,000 --> 0:07:45,000
We also took out predictors for which experts told us

142
0:07:45.776,000 --> 0:07:48,000
they cannot be rigorously measured by doctors in real life.

143
0:07:50.371,000 --> 0:07:51,000
After four months,

144
0:07:52.085,000 --> 0:07:55,000
we had a model and a medical protocol.

145
0:07:55.514,000 --> 0:07:56,000
They both got approved

146
0:07:57.204,000 --> 0:08:,000
my medical authorities in the US last spring,

147
0:08:00.45,000 --> 0:08:03,000
resulting in far less stress for half of the patients

148
0:08:04.18,000 --> 0:08:05,000
and better quality of life.

149
0:08:06.355,000 --> 0:08:1,000
And an expected upside on sales over 100 million for that drug.

150
0:08:11.668,000 --> 0:08:15,000
Seventy percent weaving AI with team and processes

151
0:08:15.89,000 --> 0:08:18,000
also means building powerful interfaces

152
0:08:19.485,000 --> 0:08:24,000
for humans and AI to solve the most difficult problems together.

153
0:08:25.286,000 --> 0:08:29,000
Once, we got challenged by a fashion retailer.

154
0:08:31.143,000 --> 0:08:33,000
"We have the best buyers in the world.

155
0:08:33.665,000 --> 0:08:38,000
Could you build an AI engine that would beat them at forecasting sales?

156
0:08:38.8,000 --> 0:08:42,000
At telling how many high-end, light-green, men XL shirts

157
0:08:42.99,000 --> 0:08:44,000
we need to buy for next year?

158
0:08:45.061,000 --> 0:08:47,000
At predicting better what will sell or not

159
0:08:47.895,000 --> 0:08:48,000
than our designers."

160
0:08:50.434,000 --> 0:08:53,000
Our team trained a model in a few weeks, on past sales data,

161
0:08:54.434,000 --> 0:08:57,000
and the competition was organized with human buyers.

162
0:08:58.347,000 --> 0:08:59,000
Result?

163
0:09:00.061,000 --> 0:09:04,000
AI wins, reducing forecasting errors by 25 percent.

164
0:09:05.903,000 --> 0:09:09,000
Human-zero champions could have tried to implement this initial model

165
0:09:10.76,000 --> 0:09:12,000
and create a fight with all human buyers.

166
0:09:13.538,000 --> 0:09:14,000
Have fun.

167
0:09:15.205,000 --> 0:09:2,000
But we knew that human buyers had insights on fashion trends

168
0:09:20.355,000 --> 0:09:22,000
that could not be found in past data.

169
0:09:23.701,000 --> 0:09:25,000
There started our 70 percent.

170
0:09:26.582,000 --> 0:09:27,000
We went for a second test,

171
0:09:28.55,000 --> 0:09:31,000
where human buyers were reviewing quantities

172
0:09:31.677,000 --> 0:09:32,000
suggested by AI

173
0:09:33.363,000 --> 0:09:35,000
and could correct them if needed.

174
0:09:36.18,000 --> 0:09:37,000
Result?

175
0:09:37.704,000 --> 0:09:39,000
Humans using AI ...

176
0:09:39.845,000 --> 0:09:4,000
lose.

177
0:09:41.795,000 --> 0:09:45,000
Seventy-five percent of the corrections made by a human

178
0:09:45.97,000 --> 0:09:47,000
were reducing accuracy.

179
0:09:49.002,000 --> 0:09:52,000
Was it time to get rid of human buyers?

180
0:09:52.2,000 --> 0:09:53,000
No.

181
0:09:53.382,000 --> 0:09:55,000
It was time to recreate a model

182
0:09:56.023,000 --> 0:10:01,000
where humans would not try to guess when AI is wrong,

183
0:10:01.118,000 --> 0:10:05,000
but where AI would take real input from human buyers.

184
0:10:06.962,000 --> 0:10:07,000
We fully rebuilt the model

185
0:10:08.597,000 --> 0:10:13,000
and went away from our initial interface, which was, more or less,

186
0:10:14.585,000 --> 0:10:16,000
"Hey, human! This is what I forecast,

187
0:10:17.046,000 --> 0:10:18,000
correct whatever you want,"

188
0:10:18.831,000 --> 0:10:21,000
and moved to a much richer one, more like,

189
0:10:22.491,000 --> 0:10:23,000
"Hey, humans!

190
0:10:24.491,000 --> 0:10:25,000
I don't know the trends for next year.

191
0:10:26.34,000 --> 0:10:28,000
Could you share with me your top creative bets?"

192
0:10:30.063,000 --> 0:10:31,000
"Hey, humans!

193
0:10:31.563,000 --> 0:10:33,000
Could you help me quantify those few big items?

194
0:10:34.306,000 --> 0:10:37,000
I cannot find any good comparables in the past for them."

195
0:10:38.401,000 --> 0:10:39,000
Result?

196
0:10:40.195,000 --> 0:10:42,000
"Human plus AI" wins,

197
0:10:42.219,000 --> 0:10:45,000
reducing forecast errors by 50 percent.

198
0:10:47.77,000 --> 0:10:49,000
It took one year to finalize the tool.

199
0:10:51.073,000 --> 0:10:54,000
Long, costly and difficult.

200
0:10:55.046,000 --> 0:10:57,000
But profits and benefits

201
0:10:57.276,000 --> 0:11:02,000
were in excess of 100 million of savings per year for that retailer.

202
0:11:03.459,000 --> 0:11:05,000
Seventy percent on very sensitive topics

203
0:11:06.419,000 --> 0:11:09,000
also means human have to decide what is right or wrong

204
0:11:10.221,000 --> 0:11:14,000
and define rules for what AI can do or not,

205
0:11:14.331,000 --> 0:11:17,000
like setting caps on prices to prevent pricing engines

206
0:11:17.839,000 --> 0:11:21,000
[from charging] outrageously high prices to uneducated customers

207
0:11:22.387,000 --> 0:11:23,000
who would accept them.

208
0:11:24.538,000 --> 0:11:26,000
Only humans can define those boundaries --

209
0:11:27.125,000 --> 0:11:3,000
there is no way AI can find them in past data.

210
0:11:31.23,000 --> 0:11:33,000
Some situations are in the gray zone.

211
0:11:34.135,000 --> 0:11:36,000
We worked with a health insurer.

212
0:11:36.904,000 --> 0:11:4,000
He developed an AI engine to identify, among his clients,

213
0:11:41.641,000 --> 0:11:43,000
people who are just about to go to hospital

214
0:11:44.213,000 --> 0:11:46,000
to sell them premium services.

215
0:11:46.506,000 --> 0:11:47,000
And the problem is,

216
0:11:48.045,000 --> 0:11:5,000
some prospects were called by the commercial team

217
0:11:51.038,000 --> 0:11:53,000
while they did not know yet

218
0:11:53.759,000 --> 0:11:55,000
they would have to go to hospital very soon.

219
0:11:57.72,000 --> 0:11:59,000
You are the CEO of this company.

220
0:12:00.061,000 --> 0:12:01,000
Do you stop that program?

221
0:12:02.577,000 --> 0:12:03,000
Not an easy question.

222
0:12:04.514,000 --> 0:12:07,000
And to tackle this question, some companies are building teams,

223
0:12:08.101,000 --> 0:12:13,000
defining ethical rules and standards to help business and tech teams set limits

224
0:12:13.918,000 --> 0:12:16,000
between personalization and manipulation,

225
0:12:17.538,000 --> 0:12:19,000
customization of offers and discrimination,

226
0:12:20.531,000 --> 0:12:22,000
targeting and intrusion.

227
0:12:24.562,000 --> 0:12:27,000
I am convinced that in every company,

228
0:12:28.26,000 --> 0:12:32,000
applying AI where it really matters has massive payback.

229
0:12:33.474,000 --> 0:12:35,000
Business leaders need to be bold

230
0:12:35.649,000 --> 0:12:36,000
and select a few topics,

231
0:12:37.649,000 --> 0:12:41,000
and for each of them, mobilize 10, 20, 30 people from their best teams --

232
0:12:42.609,000 --> 0:12:45,000
tech, AI, data science, ethics --

233
0:12:45.966,000 --> 0:12:49,000
and go through the full 10-, 20-, 70-percent cycle

234
0:12:50.411,000 --> 0:12:51,000
of "Human plus AI,"

235
0:12:52.157,000 --> 0:12:56,000
if they want to land AI effectively in their teams and processes.

236
0:12:57.006,000 --> 0:12:58,000
There is no other way.

237
0:12:58.919,000 --> 0:13:02,000
Citizens in developed economies already fear algocracy.

238
0:13:04.196,000 --> 0:13:07,000
Seven thousand were interviewed in a recent survey.

239
0:13:08.157,000 --> 0:13:11,000
More than 75 percent expressed real concerns

240
0:13:11.736,000 --> 0:13:14,000
on the impact of AI on the workforce, on privacy,

241
0:13:15.697,000 --> 0:13:18,000
on the risk of a dehumanized society.

242
0:13:19.157,000 --> 0:13:24,000
Pushing algocracy creates a real risk of severe backlash against AI

243
0:13:24.561,000 --> 0:13:28,000
within companies or in society at large.

244
0:13:29.014,000 --> 0:13:32,000
"Human plus AI" is our only option

245
0:13:32.323,000 --> 0:13:35,000
to bring the benefits of AI to the real world.

246
0:13:36.038,000 --> 0:13:37,000
And in the end,

247
0:13:37.22,000 --> 0:13:41,000
winning organizations will invest in human knowledge,

248
0:13:41.378,000 --> 0:13:43,000
not just AI and data.

249
0:13:44.792,000 --> 0:13:47,000
Recruiting, training, rewarding human experts.

250
0:13:48.8,000 --> 0:13:51,000
Data is said to be the new oil,

251
0:13:51.966,000 --> 0:13:55,000
but believe me, human knowledge will make the difference,

252
0:13:56.061,000 --> 0:13:59,000
because it is the only derrick available

253
0:13:59.673,000 --> 0:14:02,000
to pump the oil hidden in the data.

254
0:14:04.633,000 --> 0:14:05,000
Thank you.

255
0:14:05.81,000 --> 0:14:08,000
(Applause)

