1
0:00:12.76,000 --> 0:00:15,000
Automation anxiety has been spreading lately,

2
0:00:16.16,000 --> 0:00:18,000
a fear that in the future,

3
0:00:18.84,000 --> 0:00:2,000
many jobs will be performed by machines

4
0:00:21.32,000 --> 0:00:22,000
rather than human beings,

5
0:00:22.68,000 --> 0:00:24,000
given the remarkable advances that are unfolding

6
0:00:25.64,000 --> 0:00:27,000
in artificial intelligence and robotics.

7
0:00:28.44,000 --> 0:00:3,000
What's clear is that there will be significant change.

8
0:00:31.28,000 --> 0:00:34,000
What's less clear is what that change will look like.

9
0:00:34.92,000 --> 0:00:38,000
My research suggests that the future is both troubling and exciting.

10
0:00:39.88,000 --> 0:00:42,000
The threat of technological unemployment is real,

11
0:00:43.64,000 --> 0:00:45,000
and yet it's a good problem to have.

12
0:00:45.72,000 --> 0:00:48,000
And to explain how I came to that conclusion,

13
0:00:48.96,000 --> 0:00:5,000
I want to confront three myths

14
0:00:51.52,000 --> 0:00:55,000
that I think are currently obscuring our vision of this automated future.

15
0:00:56.88,000 --> 0:00:58,000
A picture that we see on our television screens,

16
0:00:59.24,000 --> 0:01:01,000
in books, in films, in everyday commentary

17
0:01:01.48,000 --> 0:01:04,000
is one where an army of robots descends on the workplace

18
0:01:05.2,000 --> 0:01:06,000
with one goal in mind:

19
0:01:06.6,000 --> 0:01:08,000
to displace human beings from their work.

20
0:01:09.12,000 --> 0:01:11,000
And I call this the Terminator myth.

21
0:01:11.84,000 --> 0:01:14,000
Yes, machines displace human beings from particular tasks,

22
0:01:15.84,000 --> 0:01:17,000
but they don't just substitute for human beings.

23
0:01:18.12,000 --> 0:01:19,000
They also complement them in other tasks,

24
0:01:20.12,000 --> 0:01:23,000
making that work more valuable and more important.

25
0:01:23.76,000 --> 0:01:26,000
Sometimes they complement human beings directly,

26
0:01:27.12,000 --> 0:01:31,000
making them more productive or more efficient at a particular task.

27
0:01:31.16,000 --> 0:01:35,000
So a taxi driver can use a satnav system to navigate on unfamiliar roads.

28
0:01:35.8,000 --> 0:01:38,000
An architect can use computer-assisted design software

29
0:01:39.16,000 --> 0:01:42,000
to design bigger, more complicated buildings.

30
0:01:42.28,000 --> 0:01:45,000
But technological progress doesn't just complement human beings directly.

31
0:01:46,000 --> 0:01:49,000
It also complements them indirectly, and it does this in two ways.

32
0:01:49.36,000 --> 0:01:52,000
The first is if we think of the economy as a pie,

33
0:01:52.72,000 --> 0:01:54,000
technological progress makes the pie bigger.

34
0:01:55.64,000 --> 0:01:58,000
As productivity increases, incomes rise and demand grows.

35
0:01:59.52,000 --> 0:02:,000
The British pie, for instance,

36
0:02:01.32,000 --> 0:02:04,000
is more than a hundred times the size it was 300 years ago.

37
0:02:05.92,000 --> 0:02:08,000
And so people displaced from tasks in the old pie

38
0:02:09.16,000 --> 0:02:11,000
could find tasks to do in the new pie instead.

39
0:02:12.8,000 --> 0:02:15,000
But technological progress doesn't just make the pie bigger.

40
0:02:16.76,000 --> 0:02:18,000
It also changes the ingredients in the pie.

41
0:02:19.64,000 --> 0:02:22,000
As time passes, people spend their income in different ways,

42
0:02:23.12,000 --> 0:02:25,000
changing how they spread it across existing goods,

43
0:02:25.96,000 --> 0:02:28,000
and developing tastes for entirely new goods, too.

44
0:02:29.2,000 --> 0:02:3,000
New industries are created,

45
0:02:31,000 --> 0:02:32,000
new tasks have to be done

46
0:02:32.84,000 --> 0:02:34,000
and that means often new roles have to be filled.

47
0:02:35.4,000 --> 0:02:36,000
So again, the British pie:

48
0:02:36.92,000 --> 0:02:38,000
300 years ago, most people worked on farms,

49
0:02:39.92,000 --> 0:02:41,000
150 years ago, in factories,

50
0:02:42.28,000 --> 0:02:44,000
and today, most people work in offices.

51
0:02:45.16,000 --> 0:02:49,000
And once again, people displaced from tasks in the old bit of pie

52
0:02:49.24,000 --> 0:02:51,000
could tumble into tasks in the new bit of pie instead.

53
0:02:52.72,000 --> 0:02:55,000
Economists call these effects complementarities,

54
0:02:56.08,000 --> 0:02:59,000
but really that's just a fancy word to capture the different way

55
0:02:59.36,000 --> 0:03:02,000
that technological progress helps human beings.

56
0:03:02.52,000 --> 0:03:04,000
Resolving this Terminator myth

57
0:03:04.64,000 --> 0:03:06,000
shows us that there are two forces at play:

58
0:03:07,000 --> 0:03:1,000
one, machine substitution that harms workers,

59
0:03:10.56,000 --> 0:03:12,000
but also these complementarities that do the opposite.

60
0:03:13.96,000 --> 0:03:14,000
Now the second myth,

61
0:03:15.36,000 --> 0:03:17,000
what I call the intelligence myth.

62
0:03:18.44,000 --> 0:03:22,000
What do the tasks of driving a car, making a medical diagnosis

63
0:03:23.36,000 --> 0:03:25,000
and identifying a bird at a fleeting glimpse have in common?

64
0:03:27.28,000 --> 0:03:29,000
Well, these are all tasks that until very recently,

65
0:03:30.28,000 --> 0:03:33,000
leading economists thought couldn't readily be automated.

66
0:03:33.64,000 --> 0:03:36,000
And yet today, all of these tasks can be automated.

67
0:03:36.84,000 --> 0:03:39,000
You know, all major car manufacturers have driverless car programs.

68
0:03:40.36,000 --> 0:03:43,000
There's countless systems out there that can diagnose medical problems.

69
0:03:44.36,000 --> 0:03:46,000
And there's even an app that can identify a bird

70
0:03:46.8,000 --> 0:03:47,000
at a fleeting glimpse.

71
0:03:48.92,000 --> 0:03:52,000
Now, this wasn't simply a case of bad luck on the part of economists.

72
0:03:53.32,000 --> 0:03:54,000
They were wrong,

73
0:03:54.64,000 --> 0:03:56,000
and the reason why they were wrong is very important.

74
0:03:57.16,000 --> 0:03:59,000
They've fallen for the intelligence myth,

75
0:03:59.44,000 --> 0:04:01,000
the belief that machines have to copy the way

76
0:04:02.36,000 --> 0:04:04,000
that human beings think and reason

77
0:04:04.44,000 --> 0:04:05,000
in order to outperform them.

78
0:04:06.24,000 --> 0:04:08,000
When these economists were trying to figure out

79
0:04:08.48,000 --> 0:04:09,000
what tasks machines could not do,

80
0:04:10.36,000 --> 0:04:12,000
they imagined the only way to automate a task

81
0:04:12.52,000 --> 0:04:13,000
was to sit down with a human being,

82
0:04:14.36,000 --> 0:04:17,000
get them to explain to you how it was they performed a task,

83
0:04:17.92,000 --> 0:04:19,000
and then try and capture that explanation

84
0:04:20.6,000 --> 0:04:22,000
in a set of instructions for a machine to follow.

85
0:04:23.4,000 --> 0:04:27,000
This view was popular in artificial intelligence at one point, too.

86
0:04:27.6,000 --> 0:04:29,000
I know this because Richard Susskind,

87
0:04:29.8,000 --> 0:04:31,000
who is my dad and my coauthor,

88
0:04:32.68,000 --> 0:04:36,000
wrote his doctorate in the 1980s on artificial intelligence and the law

89
0:04:36.76,000 --> 0:04:37,000
at Oxford University,

90
0:04:38.2,000 --> 0:04:39,000
and he was part of the vanguard.

91
0:04:39.8,000 --> 0:04:41,000
And with a professor called Phillip Capper

92
0:04:42.08,000 --> 0:04:44,000
and a legal publisher called Butterworths,

93
0:04:44.2,000 --> 0:04:49,000
they produced the world's first commercially available

94
0:04:50.12,000 --> 0:04:52,000
artificial intelligence system in the law.

95
0:04:52.92,000 --> 0:04:54,000
This was the home screen design.

96
0:04:55.56,000 --> 0:04:57,000
He assures me this was a cool screen design at the time.

97
0:04:58.28,000 --> 0:04:59,000
(Laughter)

98
0:04:59.32,000 --> 0:05:,000
I've never been entirely convinced.

99
0:05:01.04,000 --> 0:05:03,000
He published it in the form of two floppy disks,

100
0:05:03.68,000 --> 0:05:06,000
at a time where floppy disks genuinely were floppy,

101
0:05:07.24,000 --> 0:05:09,000
and his approach was the same as the economists':

102
0:05:09.6,000 --> 0:05:1,000
sit down with a lawyer,

103
0:05:10.88,000 --> 0:05:13,000
get her to explain to you how it was she solved a legal problem,

104
0:05:14.08,000 --> 0:05:19,000
and then try and capture that explanation in a set of rules for a machine to follow.

105
0:05:19.48,000 --> 0:05:22,000
In economics, if human beings could explain themselves in this way,

106
0:05:23.12,000 --> 0:05:26,000
the tasks are called routine, and they could be automated.

107
0:05:26.44,000 --> 0:05:28,000
But if human beings can't explain themselves,

108
0:05:28.8,000 --> 0:05:32,000
the tasks are called non-routine, and they're thought to be out reach.

109
0:05:33.08,000 --> 0:05:36,000
Today, that routine-nonroutine distinction is widespread.

110
0:05:36.4,000 --> 0:05:38,000
Think how often you hear people say to you

111
0:05:38.48,000 --> 0:05:41,000
machines can only perform tasks that are predictable or repetitive,

112
0:05:41.76,000 --> 0:05:42,000
rules-based or well-defined.

113
0:05:43.68,000 --> 0:05:45,000
Those are all just different words for routine.

114
0:05:46.64,000 --> 0:05:49,000
And go back to those three cases that I mentioned at the start.

115
0:05:50.64,000 --> 0:05:52,000
Those are all classic cases of nonroutine tasks.

116
0:05:53.56,000 --> 0:05:55,000
Ask a doctor, for instance, how she makes a medical diagnosis,

117
0:05:56.56,000 --> 0:05:58,000
and she might be able to give you a few rules of thumb,

118
0:05:59.24,000 --> 0:06:,000
but ultimately she'd struggle.

119
0:06:00.92,000 --> 0:06:04,000
She'd say it requires things like creativity and judgment and intuition.

120
0:06:05.76,000 --> 0:06:07,000
And these things are very difficult to articulate,

121
0:06:08.16,000 --> 0:06:11,000
and so it was thought these tasks would be very hard to automate.

122
0:06:11.28,000 --> 0:06:13,000
If a human being can't explain themselves,

123
0:06:13.84,000 --> 0:06:15,000
where on earth do we begin in writing a set of instructions

124
0:06:16.76,000 --> 0:06:17,000
for a machine to follow?

125
0:06:18.64,000 --> 0:06:2,000
Thirty years ago, this view was right,

126
0:06:21.24,000 --> 0:06:23,000
but today it's looking shaky,

127
0:06:23.4,000 --> 0:06:25,000
and in the future it's simply going to be wrong.

128
0:06:25.68,000 --> 0:06:28,000
Advances in processing power, in data storage capability

129
0:06:28.96,000 --> 0:06:29,000
and in algorithm design

130
0:06:30.64,000 --> 0:06:32,000
mean that this routine-nonroutine distinction

131
0:06:33.16,000 --> 0:06:34,000
is diminishingly useful.

132
0:06:34.92,000 --> 0:06:37,000
To see this, go back to the case of making a medical diagnosis.

133
0:06:38.2,000 --> 0:06:39,000
Earlier in the year,

134
0:06:39.6,000 --> 0:06:42,000
a team of researchers at Stanford announced they'd developed a system

135
0:06:42.92,000 --> 0:06:45,000
which can tell you whether or not a freckle is cancerous

136
0:06:46,000 --> 0:06:48,000
as accurately as leading dermatologists.

137
0:06:49.28,000 --> 0:06:5,000
How does it work?

138
0:06:50.56,000 --> 0:06:55,000
It's not trying to copy the judgment or the intuition of a doctor.

139
0:06:55.88,000 --> 0:06:58,000
It knows or understands nothing about medicine at all.

140
0:06:59.04,000 --> 0:07:01,000
Instead, it's running a pattern recognition algorithm

141
0:07:01.64,000 --> 0:07:05,000
through 129,450 past cases,

142
0:07:06.32,000 --> 0:07:09,000
hunting for similarities between those cases

143
0:07:09.44,000 --> 0:07:11,000
and the particular lesion in question.

144
0:07:12.08,000 --> 0:07:15,000
It's performing these tasks in an unhuman way,

145
0:07:15.32,000 --> 0:07:17,000
based on the analysis of more possible cases

146
0:07:17.68,000 --> 0:07:19,000
than any doctor could hope to review in their lifetime.

147
0:07:20.32,000 --> 0:07:21,000
It didn't matter that that human being,

148
0:07:22.24,000 --> 0:07:24,000
that doctor, couldn't explain how she'd performed the task.

149
0:07:25.64,000 --> 0:07:27,000
Now, there are those who dwell upon that the fact

150
0:07:28,000 --> 0:07:3,000
that these machines aren't built in our image.

151
0:07:30.32,000 --> 0:07:32,000
As an example, take IBM's Watson,

152
0:07:32.4,000 --> 0:07:36,000
the supercomputer that went on the US quiz show "Jeopardy!" in 2011,

153
0:07:37.28,000 --> 0:07:4,000
and it beat the two human champions at "Jeopardy!"

154
0:07:40.32,000 --> 0:07:41,000
The day after it won,

155
0:07:42.04,000 --> 0:07:45,000
The Wall Street Journal ran a piece by the philosopher John Searle

156
0:07:45.36,000 --> 0:07:48,000
with the title "Watson Doesn't Know It Won on 'Jeopardy!'"

157
0:07:48.76,000 --> 0:07:49,000
Right, and it's brilliant, and it's true.

158
0:07:50.76,000 --> 0:07:52,000
You know, Watson didn't let out a cry of excitement.

159
0:07:53.24,000 --> 0:07:56,000
It didn't call up its parents to say what a good job it had done.

160
0:07:56.36,000 --> 0:07:58,000
It didn't go down to the pub for a drink.

161
0:07:58.72,000 --> 0:08:02,000
This system wasn't trying to copy the way that those human contestants played,

162
0:08:03.2,000 --> 0:08:04,000
but it didn't matter.

163
0:08:04.48,000 --> 0:08:05,000
It still outperformed them.

164
0:08:06.48,000 --> 0:08:07,000
Resolving the intelligence myth

165
0:08:08.08,000 --> 0:08:11,000
shows us that our limited understanding about human intelligence,

166
0:08:11.48,000 --> 0:08:12,000
about how we think and reason,

167
0:08:13.4,000 --> 0:08:16,000
is far less of a constraint on automation than it was in the past.

168
0:08:16.88,000 --> 0:08:17,000
What's more, as we've seen,

169
0:08:18.4,000 --> 0:08:21,000
when these machines perform tasks differently to human beings,

170
0:08:21.84,000 --> 0:08:22,000
there's no reason to think

171
0:08:23.12,000 --> 0:08:25,000
that what human beings are currently capable of doing

172
0:08:25.68,000 --> 0:08:26,000
represents any sort of summit

173
0:08:27.16,000 --> 0:08:3,000
in what these machines might be capable of doing in the future.

174
0:08:31.04,000 --> 0:08:32,000
Now the third myth,

175
0:08:32.32,000 --> 0:08:34,000
what I call the superiority myth.

176
0:08:34.8,000 --> 0:08:36,000
It's often said that those who forget

177
0:08:37.04,000 --> 0:08:39,000
about the helpful side of technological progress,

178
0:08:39.52,000 --> 0:08:41,000
those complementarities from before,

179
0:08:42.04,000 --> 0:08:45,000
are committing something known as the lump of labor fallacy.

180
0:08:45.84,000 --> 0:08:47,000
Now, the problem is the lump of labor fallacy

181
0:08:48.159,000 --> 0:08:49,000
is itself a fallacy,

182
0:08:49.679,000 --> 0:08:51,000
and I call this the lump of labor fallacy fallacy,

183
0:08:52.64,000 --> 0:08:54,000
or LOLFF, for short.

184
0:08:56,000 --> 0:08:57,000
Let me explain.

185
0:08:57.44,000 --> 0:08:59,000
The lump of labor fallacy is a very old idea.

186
0:08:59.6,000 --> 0:09:03,000
It was a British economist, David Schloss, who gave it this name in 1892.

187
0:09:03.84,000 --> 0:09:05,000
He was puzzled to come across a dock worker

188
0:09:06.68,000 --> 0:09:08,000
who had begun to use a machine to make washers,

189
0:09:09.04,000 --> 0:09:12,000
the small metal discs that fasten on the end of screws.

190
0:09:13,000 --> 0:09:16,000
And this dock worker felt guilty for being more productive.

191
0:09:17.56,000 --> 0:09:19,000
Now, most of the time, we expect the opposite,

192
0:09:19.76,000 --> 0:09:21,000
that people feel guilty for being unproductive,

193
0:09:22,000 --> 0:09:25,000
you know, a little too much time on Facebook or Twitter at work.

194
0:09:25.04,000 --> 0:09:27,000
But this worker felt guilty for being more productive,

195
0:09:27.6,000 --> 0:09:29,000
and asked why, he said, "I know I'm doing wrong.

196
0:09:29.92,000 --> 0:09:31,000
I'm taking away the work of another man."

197
0:09:32.76,000 --> 0:09:34,000
In his mind, there was some fixed lump of work

198
0:09:35.76,000 --> 0:09:37,000
to be divided up between him and his pals,

199
0:09:37.92,000 --> 0:09:39,000
so that if he used this machine to do more,

200
0:09:4,000 --> 0:09:42,000
there'd be less left for his pals to do.

201
0:09:42.04,000 --> 0:09:43,000
Schloss saw the mistake.

202
0:09:43.92,000 --> 0:09:44,000
The lump of work wasn't fixed.

203
0:09:45.8,000 --> 0:09:47,000
As this worker used the machine and became more productive,

204
0:09:48.64,000 --> 0:09:5,000
the price of washers would fall, demand for washers would rise,

205
0:09:51.64,000 --> 0:09:52,000
more washers would have to be made,

206
0:09:53.36,000 --> 0:09:55,000
and there'd be more work for his pals to do.

207
0:09:55.48,000 --> 0:09:56,000
The lump of work would get bigger.

208
0:09:57.2,000 --> 0:09:59,000
Schloss called this "the lump of labor fallacy."

209
0:10:00.56,000 --> 0:10:02,000
And today you hear people talk about the lump of labor fallacy

210
0:10:03.52,000 --> 0:10:05,000
to think about the future of all types of work.

211
0:10:05.76,000 --> 0:10:07,000
There's no fixed lump of work out there to be divided up

212
0:10:08.44,000 --> 0:10:09,000
between people and machines.

213
0:10:09.84,000 --> 0:10:13,000
Yes, machines substitute for human beings, making the original lump of work smaller,

214
0:10:14.52,000 --> 0:10:15,000
but they also complement human beings,

215
0:10:16.4,000 --> 0:10:18,000
and the lump of work gets bigger and changes.

216
0:10:19.76,000 --> 0:10:2,000
But LOLFF.

217
0:10:21.4,000 --> 0:10:22,000
Here's the mistake:

218
0:10:22.8,000 --> 0:10:24,000
it's right to think that technological progress

219
0:10:25.04,000 --> 0:10:26,000
makes the lump of work to be done bigger.

220
0:10:27.04,000 --> 0:10:3,000
Some tasks become more valuable. New tasks have to be done.

221
0:10:30.08,000 --> 0:10:32,000
But it's wrong to think that necessarily,

222
0:10:32.64,000 --> 0:10:35,000
human beings will be best placed to perform those tasks.

223
0:10:35.92,000 --> 0:10:36,000
And this is the superiority myth.

224
0:10:37.56,000 --> 0:10:4,000
Yes, the lump of work might get bigger and change,

225
0:10:41,000 --> 0:10:42,000
but as machines become more capable,

226
0:10:43,000 --> 0:10:46,000
it's likely that they'll take on the extra lump of work themselves.

227
0:10:46.92,000 --> 0:10:49,000
Technological progress, rather than complement human beings,

228
0:10:50.2,000 --> 0:10:51,000
complements machines instead.

229
0:10:52.92,000 --> 0:10:55,000
To see this, go back to the task of driving a car.

230
0:10:55.96,000 --> 0:10:59,000
Today, satnav systems directly complement human beings.

231
0:11:00.08,000 --> 0:11:02,000
They make some human beings better drivers.

232
0:11:02.92,000 --> 0:11:03,000
But in the future,

233
0:11:04.2,000 --> 0:11:07,000
software is going to displace human beings from the driving seat,

234
0:11:07.32,000 --> 0:11:09,000
and these satnav systems, rather than complement human beings,

235
0:11:10.28,000 --> 0:11:12,000
will simply make these driverless cars more efficient,

236
0:11:12.84,000 --> 0:11:13,000
helping the machines instead.

237
0:11:14.4,000 --> 0:11:18,000
Or go to those indirect complementarities that I mentioned as well.

238
0:11:18.48,000 --> 0:11:19,000
The economic pie may get larger,

239
0:11:20.28,000 --> 0:11:21,000
but as machines become more capable,

240
0:11:22.04,000 --> 0:11:25,000
it's possible that any new demand will fall on goods that machines,

241
0:11:25.207,000 --> 0:11:27,000
rather than human beings, are best placed to produce.

242
0:11:27.88,000 --> 0:11:28,000
The economic pie may change,

243
0:11:29.8,000 --> 0:11:3,000
but as machines become more capable,

244
0:11:31.72,000 --> 0:11:35,000
it's possible that they'll be best placed to do the new tasks that have to be done.

245
0:11:36.6,000 --> 0:11:39,000
In short, demand for tasks isn't demand for human labor.

246
0:11:40.32,000 --> 0:11:41,000
Human beings only stand to benefit

247
0:11:42.28,000 --> 0:11:45,000
if they retain the upper hand in all these complemented tasks,

248
0:11:46.12,000 --> 0:11:49,000
but as machines become more capable, that becomes less likely.

249
0:11:50.76,000 --> 0:11:52,000
So what do these three myths tell us then?

250
0:11:52.8,000 --> 0:11:53,000
Well, resolving the Terminator myth

251
0:11:54.52,000 --> 0:11:57,000
shows us that the future of work depends upon this balance between two forces:

252
0:11:58.24,000 --> 0:12:01,000
one, machine substitution that harms workers

253
0:12:01.4,000 --> 0:12:03,000
but also those complementarities that do the opposite.

254
0:12:04,000 --> 0:12:08,000
And until now, this balance has fallen in favor of human beings.

255
0:12:09.12,000 --> 0:12:1,000
But resolving the intelligence myth

256
0:12:10.88,000 --> 0:12:12,000
shows us that that first force, machine substitution,

257
0:12:13.4,000 --> 0:12:14,000
is gathering strength.

258
0:12:14.72,000 --> 0:12:15,000
Machines, of course, can't do everything,

259
0:12:16.72,000 --> 0:12:17,000
but they can do far more,

260
0:12:18,000 --> 0:12:22,000
encroaching ever deeper into the realm of tasks performed by human beings.

261
0:12:22.6,000 --> 0:12:23,000
What's more, there's no reason to think

262
0:12:24.52,000 --> 0:12:26,000
that what human beings are currently capable of

263
0:12:26.76,000 --> 0:12:27,000
represents any sort of finishing line,

264
0:12:28.64,000 --> 0:12:3,000
that machines are going to draw to a polite stop

265
0:12:30.92,000 --> 0:12:31,000
once they're as capable as us.

266
0:12:32.76,000 --> 0:12:33,000
Now, none of this matters

267
0:12:34.32,000 --> 0:12:36,000
so long as those helpful winds of complementarity

268
0:12:37.16,000 --> 0:12:38,000
blow firmly enough,

269
0:12:38.92,000 --> 0:12:39,000
but resolving the superiority myth

270
0:12:40.88,000 --> 0:12:43,000
shows us that that process of task encroachment

271
0:12:44,000 --> 0:12:47,000
not only strengthens the force of machine substitution,

272
0:12:47.96,000 --> 0:12:5,000
but it wears down those helpful complementarities too.

273
0:12:51.32,000 --> 0:12:52,000
Bring these three myths together

274
0:12:53.28,000 --> 0:12:55,000
and I think we can capture a glimpse of that troubling future.

275
0:12:56.24,000 --> 0:12:58,000
Machines continue to become more capable,

276
0:12:58.28,000 --> 0:13:01,000
encroaching ever deeper on tasks performed by human beings,

277
0:13:01.96,000 --> 0:13:03,000
strengthening the force of machine substitution,

278
0:13:04.56,000 --> 0:13:07,000
weakening the force of machine complementarity.

279
0:13:08.2,000 --> 0:13:12,000
And at some point, that balance falls in favor of machines

280
0:13:12.52,000 --> 0:13:14,000
rather than human beings.

281
0:13:14.6,000 --> 0:13:15,000
This is the path we're currently on.

282
0:13:16.36,000 --> 0:13:19,000
I say "path" deliberately, because I don't think we're there yet,

283
0:13:19.56,000 --> 0:13:22,000
but it is hard to avoid the conclusion that this is our direction of travel.

284
0:13:24.64,000 --> 0:13:25,000
That's the troubling part.

285
0:13:26.12,000 --> 0:13:29,000
Let me say now why I think actually this is a good problem to have.

286
0:13:30.52,000 --> 0:13:33,000
For most of human history, one economic problem has dominated:

287
0:13:34.08,000 --> 0:13:38,000
how to make the economic pie large enough for everyone to live on.

288
0:13:38.16,000 --> 0:13:4,000
Go back to the turn of the first century AD,

289
0:13:40.36,000 --> 0:13:42,000
and if you took the global economic pie

290
0:13:42.48,000 --> 0:13:45,000
and divided it up into equal slices for everyone in the world,

291
0:13:45.8,000 --> 0:13:47,000
everyone would get a few hundred dollars.

292
0:13:47.96,000 --> 0:13:49,000
Almost everyone lived on or around the poverty line.

293
0:13:51.32,000 --> 0:13:53,000
And if you roll forward a thousand years,

294
0:13:53.52,000 --> 0:13:54,000
roughly the same is true.

295
0:13:55.68,000 --> 0:13:58,000
But in the last few hundred years, economic growth has taken off.

296
0:13:59.28,000 --> 0:14:01,000
Those economic pies have exploded in size.

297
0:14:01.68,000 --> 0:14:03,000
Global GDP per head,

298
0:14:03.76,000 --> 0:14:06,000
the value of those individual slices of the pie today,

299
0:14:07.16,000 --> 0:14:09,000
they're about 10,150 dollars.

300
0:14:1,000 --> 0:14:12,000
If economic growth continues at two percent,

301
0:14:12.72,000 --> 0:14:14,000
our children will be twice as rich as us.

302
0:14:14.8,000 --> 0:14:16,000
If it continues at a more measly one percent,

303
0:14:17.12,000 --> 0:14:19,000
our grandchildren will be twice as rich as us.

304
0:14:19.8,000 --> 0:14:22,000
By and large, we've solved that traditional economic problem.

305
0:14:24.2,000 --> 0:14:27,000
Now, technological unemployment, if it does happen,

306
0:14:27.24,000 --> 0:14:3,000
in a strange way will be a symptom of that success,

307
0:14:30.48,000 --> 0:14:33,000
will have solved one problem -- how to make the pie bigger --

308
0:14:34.36,000 --> 0:14:35,000
but replaced it with another --

309
0:14:36.2,000 --> 0:14:38,000
how to make sure that everyone gets a slice.

310
0:14:39.84,000 --> 0:14:42,000
As other economists have noted, solving this problem won't be easy.

311
0:14:43.36,000 --> 0:14:44,000
Today, for most people,

312
0:14:45.04,000 --> 0:14:47,000
their job is their seat at the economic dinner table,

313
0:14:47.56,000 --> 0:14:49,000
and in a world with less work or even without work,

314
0:14:5,000 --> 0:14:52,000
it won't be clear how they get their slice.

315
0:14:52.08,000 --> 0:14:54,000
There's a great deal of discussion, for instance,

316
0:14:54.44,000 --> 0:14:56,000
about various forms of universal basic income

317
0:14:57.16,000 --> 0:14:58,000
as one possible approach,

318
0:14:58.4,000 --> 0:14:59,000
and there's trials underway

319
0:15:00.04,000 --> 0:15:02,000
in the United States and in Finland and in Kenya.

320
0:15:03,000 --> 0:15:06,000
And this is the collective challenge that's right in front of us,

321
0:15:06.2,000 --> 0:15:11,000
to figure out how this material prosperity generated by our economic system

322
0:15:11.28,000 --> 0:15:12,000
can be enjoyed by everyone

323
0:15:13.28,000 --> 0:15:15,000
in a world in which our traditional mechanism

324
0:15:15.72,000 --> 0:15:16,000
for slicing up the pie,

325
0:15:17.6,000 --> 0:15:18,000
the work that people do,

326
0:15:19.56,000 --> 0:15:21,000
withers away and perhaps disappears.

327
0:15:22.28,000 --> 0:15:26,000
Solving this problem is going to require us to think in very different ways.

328
0:15:27.4,000 --> 0:15:31,000
There's going to be a lot of disagreement about what ought to be done,

329
0:15:31.6,000 --> 0:15:34,000
but it's important to remember that this is a far better problem to have

330
0:15:35.04,000 --> 0:15:37,000
than the one that haunted our ancestors for centuries:

331
0:15:37.88,000 --> 0:15:4,000
how to make that pie big enough in the first place.

332
0:15:41.28,000 --> 0:15:42,000
Thank you very much.

333
0:15:42.56,000 --> 0:15:45,000
(Applause)

