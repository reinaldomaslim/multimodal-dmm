1
0:00:13.354,000 --> 0:00:16,000
Technology has brought us so much:

2
0:00:16.489,000 --> 0:00:18,000
the moon landing, the Internet,

3
0:00:18.508,000 --> 0:00:2,000
the ability to sequence the human genome.

4
0:00:21.133,000 --> 0:00:24,000
But it also taps into a lot of our deepest fears,

5
0:00:24.857,000 --> 0:00:25,000
and about 30 years ago,

6
0:00:26.713,000 --> 0:00:28,000
the culture critic Neil Postman wrote a book

7
0:00:29.266,000 --> 0:00:31,000
called "Amusing Ourselves to Death,"

8
0:00:31.381,000 --> 0:00:33,000
which lays this out really brilliantly.

9
0:00:34.14,000 --> 0:00:35,000
And here's what he said,

10
0:00:35.79,000 --> 0:00:37,000
comparing the dystopian visions

11
0:00:38.053,000 --> 0:00:41,000
of George Orwell and Aldous Huxley.

12
0:00:41.626,000 --> 0:00:44,000
He said, Orwell feared we would become

13
0:00:44.752,000 --> 0:00:46,000
a captive culture.

14
0:00:47,000 --> 0:00:5,000
Huxley feared we would become a trivial culture.

15
0:00:50.752,000 --> 0:00:52,000
Orwell feared the truth would be

16
0:00:52.897,000 --> 0:00:53,000
concealed from us,

17
0:00:54.82,000 --> 0:00:56,000
and Huxley feared we would be drowned

18
0:00:57.01,000 --> 0:00:59,000
in a sea of irrelevance.

19
0:00:59.703,000 --> 0:01:01,000
In a nutshell, it's a choice between

20
0:01:01.873,000 --> 0:01:03,000
Big Brother watching you

21
0:01:04.473,000 --> 0:01:06,000
and you watching Big Brother.

22
0:01:06.969,000 --> 0:01:07,000
(Laughter)

23
0:01:08.9,000 --> 0:01:09,000
But it doesn't have to be this way.

24
0:01:10.634,000 --> 0:01:13,000
We are not passive consumers of data and technology.

25
0:01:13.97,000 --> 0:01:15,000
We shape the role it plays in our lives

26
0:01:16.373,000 --> 0:01:18,000
and the way we make meaning from it,

27
0:01:18.503,000 --> 0:01:19,000
but to do that,

28
0:01:20.106,000 --> 0:01:23,000
we have to pay as much attention to how we think

29
0:01:23.619,000 --> 0:01:25,000
as how we code.

30
0:01:25.649,000 --> 0:01:28,000
We have to ask questions, and hard questions,

31
0:01:28.747,000 --> 0:01:29,000
to move past counting things

32
0:01:30.616,000 --> 0:01:32,000
to understanding them.

33
0:01:33.218,000 --> 0:01:35,000
We're constantly bombarded with stories

34
0:01:35.664,000 --> 0:01:37,000
about how much data there is in the world,

35
0:01:38.14,000 --> 0:01:39,000
but when it comes to big data

36
0:01:39.72,000 --> 0:01:41,000
and the challenges of interpreting it,

37
0:01:42.316,000 --> 0:01:44,000
size isn't everything.

38
0:01:44.404,000 --> 0:01:46,000
There's also the speed at which it moves,

39
0:01:47.307,000 --> 0:01:48,000
and the many varieties of data types,

40
0:01:49.003,000 --> 0:01:51,000
and here are just a few examples:

41
0:01:51.501,000 --> 0:01:53,000
images,

42
0:01:53.699,000 --> 0:01:57,000
text,

43
0:01:57.706,000 --> 0:01:59,000
video,

44
0:01:59.801,000 --> 0:02:,000
audio.

45
0:02:01.631,000 --> 0:02:04,000
And what unites this disparate types of data

46
0:02:04.673,000 --> 0:02:06,000
is that they're created by people

47
0:02:06.894,000 --> 0:02:08,000
and they require context.

48
0:02:09.669,000 --> 0:02:11,000
Now, there's a group of data scientists

49
0:02:12.114,000 --> 0:02:14,000
out of the University of Illinois-Chicago,

50
0:02:14.419,000 --> 0:02:16,000
and they're called the Health Media Collaboratory,

51
0:02:16.973,000 --> 0:02:18,000
and they've been working with the Centers for Disease Control

52
0:02:19.56,000 --> 0:02:2,000
to better understand

53
0:02:21.065,000 --> 0:02:23,000
how people talk about quitting smoking,

54
0:02:23.913,000 --> 0:02:25,000
how they talk about electronic cigarettes,

55
0:02:26.593,000 --> 0:02:27,000
and what they can do collectively

56
0:02:28.578,000 --> 0:02:29,000
to help them quit.

57
0:02:30.562,000 --> 0:02:32,000
The interesting thing is, if you want to understand

58
0:02:32.575,000 --> 0:02:34,000
how people talk about smoking,

59
0:02:34.791,000 --> 0:02:35,000
first you have to understand

60
0:02:36.692,000 --> 0:02:38,000
what they mean when they say "smoking."

61
0:02:39.257,000 --> 0:02:42,000
And on Twitter, there are four main categories:

62
0:02:43.183,000 --> 0:02:45,000
number one, smoking cigarettes;

63
0:02:46.18,000 --> 0:02:48,000
number two, smoking marijuana;

64
0:02:48.987,000 --> 0:02:5,000
number three, smoking ribs;

65
0:02:51.63,000 --> 0:02:54,000
and number four, smoking hot women.

66
0:02:55.183,000 --> 0:02:57,000
(Laughter)

67
0:02:58.176,000 --> 0:03:,000
So then you have to think about, well,

68
0:03:00.602,000 --> 0:03:02,000
how do people talk about electronic cigarettes?

69
0:03:02.742,000 --> 0:03:04,000
And there are so many different ways

70
0:03:04.767,000 --> 0:03:06,000
that people do this, and you can see from the slide

71
0:03:07.366,000 --> 0:03:09,000
it's a complex kind of a query.

72
0:03:09.976,000 --> 0:03:12,000
And what it reminds us is that

73
0:03:13.2,000 --> 0:03:15,000
language is created by people,

74
0:03:15.611,000 --> 0:03:17,000
and people are messy and we're complex

75
0:03:17.951,000 --> 0:03:19,000
and we use metaphors and slang and jargon

76
0:03:20.718,000 --> 0:03:23,000
and we do this 24/7 in many, many languages,

77
0:03:23.997,000 --> 0:03:26,000
and then as soon as we figure it out, we change it up.

78
0:03:27.221,000 --> 0:03:32,000
So did these ads that the CDC put on,

79
0:03:32.339,000 --> 0:03:34,000
these television ads that featured a woman

80
0:03:34.769,000 --> 0:03:36,000
with a hole in her throat and that were very graphic

81
0:03:36.79,000 --> 0:03:37,000
and very disturbing,

82
0:03:38.694,000 --> 0:03:39,000
did they actually have an impact

83
0:03:40.579,000 --> 0:03:42,000
on whether people quit?

84
0:03:43.25,000 --> 0:03:46,000
And the Health Media Collaboratory respected the limits of their data,

85
0:03:46.557,000 --> 0:03:48,000
but they were able to conclude

86
0:03:48.562,000 --> 0:03:51,000
that those advertisements — and you may have seen them —

87
0:03:51.874,000 --> 0:03:53,000
that they had the effect of jolting people

88
0:03:54.465,000 --> 0:03:55,000
into a thought process

89
0:03:56.287,000 --> 0:03:59,000
that may have an impact on future behavior.

90
0:03:59.954,000 --> 0:04:02,000
And what I admire and appreciate about this project,

91
0:04:03.845,000 --> 0:04:04,000
aside from the fact, including the fact

92
0:04:05.334,000 --> 0:04:09,000
that it's based on real human need,

93
0:04:09.391,000 --> 0:04:11,000
is that it's a fantastic example of courage

94
0:04:12.237,000 --> 0:04:16,000
in the face of a sea of irrelevance.

95
0:04:16.68,000 --> 0:04:19,000
And so it's not just big data that causes

96
0:04:19.985,000 --> 0:04:21,000
challenges of interpretation, because let's face it,

97
0:04:22.586,000 --> 0:04:24,000
we human beings have a very rich history

98
0:04:25.18,000 --> 0:04:27,000
of taking any amount of data, no matter how small,

99
0:04:27.873,000 --> 0:04:28,000
and screwing it up.

100
0:04:29.49,000 --> 0:04:32,000
So many years ago, you may remember

101
0:04:33.227,000 --> 0:04:35,000
that former President Ronald Reagan

102
0:04:35.5,000 --> 0:04:36,000
was very criticized for making a statement

103
0:04:37.491,000 --> 0:04:4,000
that facts are stupid things.

104
0:04:40.501,000 --> 0:04:42,000
And it was a slip of the tongue, let's be fair.

105
0:04:43.295,000 --> 0:04:45,000
He actually meant to quote John Adams' defense

106
0:04:45.725,000 --> 0:04:47,000
of British soldiers in the Boston Massacre trials

107
0:04:48.476,000 --> 0:04:51,000
that facts are stubborn things.

108
0:04:51.626,000 --> 0:04:53,000
But I actually think there's

109
0:04:54.25,000 --> 0:04:57,000
a bit of accidental wisdom in what he said,

110
0:04:57.668,000 --> 0:04:59,000
because facts are stubborn things,

111
0:05:00.444,000 --> 0:05:02,000
but sometimes they're stupid, too.

112
0:05:03.367,000 --> 0:05:04,000
I want to tell you a personal story

113
0:05:05.255,000 --> 0:05:08,000
about why this matters a lot to me.

114
0:05:08.803,000 --> 0:05:1,000
I need to take a breath.

115
0:05:11.24,000 --> 0:05:13,000
My son Isaac, when he was two,

116
0:05:13.994,000 --> 0:05:15,000
was diagnosed with autism,

117
0:05:16.411,000 --> 0:05:18,000
and he was this happy, hilarious,

118
0:05:18.572,000 --> 0:05:2,000
loving, affectionate little guy,

119
0:05:20.607,000 --> 0:05:22,000
but the metrics on his developmental evaluations,

120
0:05:23.509,000 --> 0:05:25,000
which looked at things like the number of words —

121
0:05:25.579,000 --> 0:05:28,000
at that point, none —

122
0:05:29.236,000 --> 0:05:32,000
communicative gestures and minimal eye contact,

123
0:05:33.176,000 --> 0:05:35,000
put his developmental level

124
0:05:35.179,000 --> 0:05:38,000
at that of a nine-month-old baby.

125
0:05:39.14,000 --> 0:05:41,000
And the diagnosis was factually correct,

126
0:05:42.1,000 --> 0:05:45,000
but it didn't tell the whole story.

127
0:05:45.309,000 --> 0:05:46,000
And about a year and a half later,

128
0:05:46.71,000 --> 0:05:48,000
when he was almost four,

129
0:05:48.812,000 --> 0:05:5,000
I found him in front of the computer one day

130
0:05:51.175,000 --> 0:05:56,000
running a Google image search on women,

131
0:05:56.628,000 --> 0:05:59,000
spelled "w-i-m-e-n."

132
0:06:00.244,000 --> 0:06:02,000
And I did what any obsessed parent would do,

133
0:06:02.984,000 --> 0:06:03,000
which is immediately started hitting the "back" button

134
0:06:04.885,000 --> 0:06:07,000
to see what else he'd been searching for.

135
0:06:08.248,000 --> 0:06:1,000
And they were, in order: men,

136
0:06:10.419,000 --> 0:06:17,000
school, bus and computer.

137
0:06:17.686,000 --> 0:06:19,000
And I was stunned,

138
0:06:19.756,000 --> 0:06:21,000
because we didn't know that he could spell,

139
0:06:21.758,000 --> 0:06:22,000
much less read, and so I asked him,

140
0:06:23.524,000 --> 0:06:25,000
"Isaac, how did you do this?"

141
0:06:25.717,000 --> 0:06:27,000
And he looked at me very seriously and said,

142
0:06:28.395,000 --> 0:06:31,000
"Typed in the box."

143
0:06:31.747,000 --> 0:06:34,000
He was teaching himself to communicate,

144
0:06:35.481,000 --> 0:06:38,000
but we were looking in the wrong place,

145
0:06:38.485,000 --> 0:06:4,000
and this is what happens when assessments

146
0:06:40.78,000 --> 0:06:42,000
and analytics overvalue one metric —

147
0:06:43.176,000 --> 0:06:45,000
in this case, verbal communication —

148
0:06:45.785,000 --> 0:06:5,000
and undervalue others, such as creative problem-solving.

149
0:06:51.488,000 --> 0:06:53,000
Communication was hard for Isaac,

150
0:06:53.795,000 --> 0:06:54,000
and so he found a workaround

151
0:06:55.707,000 --> 0:06:57,000
to find out what he needed to know.

152
0:06:58.564,000 --> 0:06:59,000
And when you think about it, it makes a lot of sense,

153
0:07:00.454,000 --> 0:07:02,000
because forming a question

154
0:07:02.535,000 --> 0:07:04,000
is a really complex process,

155
0:07:05.1,000 --> 0:07:07,000
but he could get himself a lot of the way there

156
0:07:07.622,000 --> 0:07:11,000
by putting a word in a search box.

157
0:07:11.714,000 --> 0:07:13,000
And so this little moment

158
0:07:14.65,000 --> 0:07:16,000
had a really profound impact on me

159
0:07:17.486,000 --> 0:07:18,000
and our family

160
0:07:18.795,000 --> 0:07:21,000
because it helped us change our frame of reference

161
0:07:21.936,000 --> 0:07:23,000
for what was going on with him,

162
0:07:24.144,000 --> 0:07:26,000
and worry a little bit less and appreciate

163
0:07:27.12,000 --> 0:07:29,000
his resourcefulness more.

164
0:07:29.302,000 --> 0:07:31,000
Facts are stupid things.

165
0:07:32.163,000 --> 0:07:34,000
And they're vulnerable to misuse,

166
0:07:34.56,000 --> 0:07:35,000
willful or otherwise.

167
0:07:36.213,000 --> 0:07:39,000
I have a friend, Emily Willingham, who's a scientist,

168
0:07:39.239,000 --> 0:07:41,000
and she wrote a piece for Forbes not long ago

169
0:07:42.04,000 --> 0:07:43,000
entitled "The 10 Weirdest Things

170
0:07:44.02,000 --> 0:07:45,000
Ever Linked to Autism."

171
0:07:45.83,000 --> 0:07:48,000
It's quite a list.

172
0:07:48.835,000 --> 0:07:51,000
The Internet, blamed for everything, right?

173
0:07:52.367,000 --> 0:07:55,000
And of course mothers, because.

174
0:07:56.124,000 --> 0:07:57,000
And actually, wait, there's more,

175
0:07:57.711,000 --> 0:08:,000
there's a whole bunch in the "mother" category here.

176
0:08:01.141,000 --> 0:08:05,000
And you can see it's a pretty rich and interesting list.

177
0:08:05.956,000 --> 0:08:07,000
I'm a big fan of

178
0:08:08.149,000 --> 0:08:11,000
being pregnant near freeways, personally.

179
0:08:11.853,000 --> 0:08:12,000
The final one is interesting,

180
0:08:13.392,000 --> 0:08:16,000
because the term "refrigerator mother"

181
0:08:16.395,000 --> 0:08:18,000
was actually the original hypothesis

182
0:08:19,000 --> 0:08:2,000
for the cause of autism,

183
0:08:20.431,000 --> 0:08:22,000
and that meant somebody who was cold and unloving.

184
0:08:23.166,000 --> 0:08:24,000
And at this point, you might be thinking,

185
0:08:24.728,000 --> 0:08:25,000
"Okay, Susan, we get it,

186
0:08:26.385,000 --> 0:08:27,000
you can take data, you can make it mean anything."

187
0:08:28.167,000 --> 0:08:32,000
And this is true, it's absolutely true,

188
0:08:32.87,000 --> 0:08:37,000
but the challenge is that

189
0:08:38.48,000 --> 0:08:4,000
we have this opportunity

190
0:08:40.928,000 --> 0:08:42,000
to try to make meaning out of it ourselves,

191
0:08:43.212,000 --> 0:08:48,000
because frankly, data doesn't create meaning. We do.

192
0:08:48.564,000 --> 0:08:51,000
So as businesspeople, as consumers,

193
0:08:51.82,000 --> 0:08:53,000
as patients, as citizens,

194
0:08:54.359,000 --> 0:08:56,000
we have a responsibility, I think,

195
0:08:56.755,000 --> 0:08:58,000
to spend more time

196
0:08:58.949,000 --> 0:09:,000
focusing on our critical thinking skills.

197
0:09:01.819,000 --> 0:09:02,000
Why?

198
0:09:02.897,000 --> 0:09:05,000
Because at this point in our history, as we've heard

199
0:09:06.075,000 --> 0:09:07,000
many times over,

200
0:09:07.781,000 --> 0:09:08,000
we can process exabytes of data

201
0:09:09.762,000 --> 0:09:11,000
at lightning speed,

202
0:09:11.915,000 --> 0:09:14,000
and we have the potential to make bad decisions

203
0:09:15.43,000 --> 0:09:16,000
far more quickly, efficiently,

204
0:09:17.264,000 --> 0:09:22,000
and with far greater impact than we did in the past.

205
0:09:22.292,000 --> 0:09:23,000
Great, right?

206
0:09:23.68,000 --> 0:09:26,000
And so what we need to do instead

207
0:09:26.71,000 --> 0:09:28,000
is spend a little bit more time

208
0:09:29.04,000 --> 0:09:31,000
on things like the humanities

209
0:09:31.786,000 --> 0:09:34,000
and sociology, and the social sciences,

210
0:09:35.25,000 --> 0:09:37,000
rhetoric, philosophy, ethics,

211
0:09:37.558,000 --> 0:09:39,000
because they give us context that is so important

212
0:09:40.414,000 --> 0:09:42,000
for big data, and because

213
0:09:42.99,000 --> 0:09:44,000
they help us become better critical thinkers.

214
0:09:45.408,000 --> 0:09:49,000
Because after all, if I can spot

215
0:09:49.615,000 --> 0:09:51,000
a problem in an argument, it doesn't much matter

216
0:09:52.101,000 --> 0:09:54,000
whether it's expressed in words or in numbers.

217
0:09:54.86,000 --> 0:09:56,000
And this means

218
0:09:57.579,000 --> 0:10:01,000
teaching ourselves to find those confirmation biases

219
0:10:02,000 --> 0:10:03,000
and false correlations

220
0:10:03.822,000 --> 0:10:05,000
and being able to spot a naked emotional appeal

221
0:10:05.96,000 --> 0:10:06,000
from 30 yards,

222
0:10:07.622,000 --> 0:10:09,000
because something that happens after something

223
0:10:10.144,000 --> 0:10:13,000
doesn't mean it happened because of it, necessarily,

224
0:10:13.226,000 --> 0:10:15,000
and if you'll let me geek out on you for a second,

225
0:10:15.345,000 --> 0:10:19,000
the Romans called this "post hoc ergo propter hoc,"

226
0:10:19.642,000 --> 0:10:22,000
after which therefore because of which.

227
0:10:22.938,000 --> 0:10:25,000
And it means questioning disciplines like demographics.

228
0:10:26.695,000 --> 0:10:28,000
Why? Because they're based on assumptions

229
0:10:29.215,000 --> 0:10:31,000
about who we all are based on our gender

230
0:10:31.521,000 --> 0:10:32,000
and our age and where we live

231
0:10:32.983,000 --> 0:10:35,000
as opposed to data on what we actually think and do.

232
0:10:36.461,000 --> 0:10:37,000
And since we have this data,

233
0:10:38.124,000 --> 0:10:41,000
we need to treat it with appropriate privacy controls

234
0:10:41.263,000 --> 0:10:44,000
and consumer opt-in,

235
0:10:44.839,000 --> 0:10:46,000
and beyond that, we need to be clear

236
0:10:47.832,000 --> 0:10:49,000
about our hypotheses,

237
0:10:49.935,000 --> 0:10:51,000
the methodologies that we use,

238
0:10:52.531,000 --> 0:10:54,000
and our confidence in the result.

239
0:10:55.335,000 --> 0:10:57,000
As my high school algebra teacher used to say,

240
0:10:57.809,000 --> 0:10:58,000
show your math,

241
0:10:59.34,000 --> 0:11:02,000
because if I don't know what steps you took,

242
0:11:02.781,000 --> 0:11:03,000
I don't know what steps you didn't take,

243
0:11:04.772,000 --> 0:11:06,000
and if I don't know what questions you asked,

244
0:11:07.21,000 --> 0:11:1,000
I don't know what questions you didn't ask.

245
0:11:10.407,000 --> 0:11:11,000
And it means asking ourselves, really,

246
0:11:11.93,000 --> 0:11:12,000
the hardest question of all:

247
0:11:13.409,000 --> 0:11:16,000
Did the data really show us this,

248
0:11:16.909,000 --> 0:11:18,000
or does the result make us feel

249
0:11:19.22,000 --> 0:11:22,000
more successful and more comfortable?

250
0:11:23.098,000 --> 0:11:25,000
So the Health Media Collaboratory,

251
0:11:25.682,000 --> 0:11:26,000
at the end of their project, they were able

252
0:11:27.381,000 --> 0:11:3,000
to find that 87 percent of tweets

253
0:11:30.789,000 --> 0:11:32,000
about those very graphic and disturbing

254
0:11:32.933,000 --> 0:11:36,000
anti-smoking ads expressed fear,

255
0:11:36.971,000 --> 0:11:37,000
but did they conclude

256
0:11:38.827,000 --> 0:11:41,000
that they actually made people stop smoking?

257
0:11:41.988,000 --> 0:11:43,000
No. It's science, not magic.

258
0:11:44.53,000 --> 0:11:47,000
So if we are to unlock

259
0:11:47.72,000 --> 0:11:49,000
the power of data,

260
0:11:50.582,000 --> 0:11:53,000
we don't have to go blindly into

261
0:11:54.03,000 --> 0:11:57,000
Orwell's vision of a totalitarian future,

262
0:11:57.466,000 --> 0:12:,000
or Huxley's vision of a trivial one,

263
0:12:00.583,000 --> 0:12:03,000
or some horrible cocktail of both.

264
0:12:03.603,000 --> 0:12:05,000
What we have to do

265
0:12:05.982,000 --> 0:12:07,000
is treat critical thinking with respect

266
0:12:08.7,000 --> 0:12:1,000
and be inspired by examples

267
0:12:10.729,000 --> 0:12:12,000
like the Health Media Collaboratory,

268
0:12:13.339,000 --> 0:12:15,000
and as they say in the superhero movies,

269
0:12:15.667,000 --> 0:12:16,000
let's use our powers for good.

270
0:12:17.489,000 --> 0:12:19,000
Thank you.

271
0:12:19.84,000 --> 0:12:21,000
(Applause)

