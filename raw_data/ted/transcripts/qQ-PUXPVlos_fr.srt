1
0:00:,000 --> 0:00:07,000
Traducteur: Priscilla R. A. Relecteur: eric vautier

2
0:00:12.944,000 --> 0:00:16,000
Dans les années 80, j'ai donné ma première conférence TED,

3
0:00:16.977,000 --> 0:00:2,000
et j'ai fait quelques-unes des toutes premières démonstrations publiques

4
0:00:21.263,000 --> 0:00:25,000
de la réalité virtuelle sur la scène de TED.

5
0:00:26.375,000 --> 0:00:32,000
À l'époque, on savait qu'on faisait face à un avenir compliqué

6
0:00:33.266,000 --> 0:00:38,000
où la technologie dont on avait besoin,

7
0:00:38.491,000 --> 0:00:39,000
la technologie qu'on aimait,

8
0:00:40.366,000 --> 0:00:42,000
pourrait aussi causer notre perte.

9
0:00:43.266,000 --> 0:00:47,000
On savait que si on considérait notre technologie

10
0:00:47.381,000 --> 0:00:5,000
comme un moyen d'obtenir toujours plus de pouvoir,

11
0:00:50.659,000 --> 0:00:53,000
si ce n'était qu'une soif du pouvoir, on finirait par se détruire.

12
0:00:54.39,000 --> 0:00:55,000
C'est ce qui se passe

13
0:00:55.595,000 --> 0:00:58,000
quand on n'est qu'assoiffé de pouvoir.

14
0:00:59.509,000 --> 0:01:02,000
Donc, à l'époque, l'idéalisme

15
0:01:02.922,000 --> 0:01:06,000
de la culture numérique

16
0:01:07.755,000 --> 0:01:11,000
consistait à d'abord reconnaître l'obscurité possible

17
0:01:12.518,000 --> 0:01:15,000
et à essayer d'imaginer un moyen de la transcender

18
0:01:15.892,000 --> 0:01:17,000
avec beauté et créativité.

19
0:01:19.033,000 --> 0:01:25,000
Je terminais toujours mes premiers talks TED par cette phrase terrifiante :

20
0:01:26.478,000 --> 0:01:29,000
« On a un défi.

21
0:01:30.368,000 --> 0:01:34,000
On doit créer une culture autour de la technologie

22
0:01:34.416,000 --> 0:01:37,000
qui, de par sa beauté, son importance,

23
0:01:38.408,000 --> 0:01:4,000
sa profondeur, sa créativité sans limites,

24
0:01:40.973,000 --> 0:01:43,000
et son potentiel infini,

25
0:01:44.013,000 --> 0:01:47,000
nous éloignera du suicide collectif. »

26
0:01:48.519,000 --> 0:01:53,000
On parlait donc de l'extinction comme étant identique

27
0:01:54.131,000 --> 0:01:59,000
au besoin de créer un avenir séduisant et infiniment créatif.

28
0:01:59.639,000 --> 0:02:04,000
Et je crois toujours que cette alternative créatrice,

29
0:02:05.045,000 --> 0:02:06,000
en tant qu'alternative à la mort,

30
0:02:07.043,000 --> 0:02:08,000
est très réelle et vraie,

31
0:02:09.036,000 --> 0:02:11,000
peut-être la chose la plus vraie qui soit.

32
0:02:11.87,000 --> 0:02:13,000
Dans le cas de la réalité virtuelle --

33
0:02:13.989,000 --> 0:02:15,000
j'en parlais de cette façon :

34
0:02:16.295,000 --> 0:02:18,000
elle serait similaire

35
0:02:18.954,000 --> 0:02:2,000
à ce qui s'est passé lorsqu'on a découvert le langage.

36
0:02:21.828,000 --> 0:02:24,000
Le langage a apporté de nouvelles aventures,

37
0:02:25.407,000 --> 0:02:26,000
une nouvelle profondeur, un nouveau sens,

38
0:02:27.357,000 --> 0:02:29,000
de nouvelles façons de se connecter, de coordonner,

39
0:02:29.737,000 --> 0:02:31,000
d'imaginer, d'élever les enfants.

40
0:02:33.685,000 --> 0:02:36,000
Je pensais qu'avec la réalité virtuelle, il y aurait cette nouvelle chose

41
0:02:37.181,000 --> 0:02:38,000
qui serait comme une conversation

42
0:02:38.808,000 --> 0:02:41,000
mais aussi comme un rêve intentionnel en état de veille.

43
0:02:41.96,000 --> 0:02:43,000
On l'avait appelé « communication post-symbolique »,

44
0:02:44.637,000 --> 0:02:48,000
car ce serait comme créer directement la chose qu'on a vécue

45
0:02:49.019,000 --> 0:02:52,000
au lieu de créer indirectement des symboles pour se référer aux choses.

46
0:02:53.466,000 --> 0:02:57,000
C'était une belle vision, et j'y crois toujours,

47
0:02:57.828,000 --> 0:03:,000
et pourtant, cette belle vision était hantée

48
0:03:01.067,000 --> 0:03:04,000
par le côté obscur de ce qu'il pourrait devenir.

49
0:03:04.241,000 --> 0:03:09,000
Et je suppose que je pourrais mentionner

50
0:03:09.313,000 --> 0:03:12,000
l'un des plus anciens informaticiens,

51
0:03:12.401,000 --> 0:03:14,000
dont le nom était Norbert Wiener,

52
0:03:14.56,000 --> 0:03:17,000
qui a écrit ce livre dans les années 50, avant même que je ne sois né :

53
0:03:18.338,000 --> 0:03:2,000
« L'usage humain des êtres humains ».

54
0:03:25.975,000 --> 0:03:31,000
pour créer un système informatique recueillant des données auprès des gens

55
0:03:32.18,000 --> 0:03:35,000
et leur fournissant des retours en temps réel

56
0:03:35.776,000 --> 0:03:4,000
pour les mettre partiellement, statistiquement, dans une boîte Skinner,

57
0:03:41.131,000 --> 0:03:43,000
dans un système comportementaliste.

58
0:03:43.403,000 --> 0:03:45,000
Il a écrit cette phrase étonnante :

59
0:03:45.928,000 --> 0:03:47,000
on pourrait imaginer, comme une expérience de pensée --

60
0:03:48.69,000 --> 0:03:5,000
et je le paraphrase, ce n'est pas une citation --

61
0:03:51.175,000 --> 0:03:54,000
on pourrait imaginer un système informatique mondial

62
0:03:54.279,000 --> 0:03:56,000
où tout le monde aurait des appareils sur eux en permanence,

63
0:03:57.145,000 --> 0:04:,000
qui leur fourniraient des retours basés sur leurs actions,

64
0:04:00.487,000 --> 0:04:01,000
et toute la population

65
0:04:02.34,000 --> 0:04:05,000
verrait son comportement se modifier dans une certaine mesure.

66
0:04:05.94,000 --> 0:04:08,000
Et une telle société serait insensée,

67
0:04:09.51,000 --> 0:04:12,000
ne survivrait pas, ne pourrait pas affronter ses problèmes.

68
0:04:12.611,000 --> 0:04:14,000
Puis il a dit que ce n'était qu'une expérience de pensée,

69
0:04:15.382,000 --> 0:04:18,000
et qu'un tel avenir était technologiquement irréalisable.

70
0:04:18.72,000 --> 0:04:19,000
(Rires)

71
0:04:19.836,000 --> 0:04:22,000
Et pourtant, bien sûr, c'est ce qu'on a créé,

72
0:04:22.862,000 --> 0:04:25,000
et c'est ce qu'on doit défaire si on veut survivre.

73
0:04:27.457,000 --> 0:04:28,000
Donc --

74
0:04:28.632,000 --> 0:04:31,000
(Applaudissements)

75
0:04:32.631,000 --> 0:04:37,000
je crois qu'on a commis une erreur très particulière,

76
0:04:38.632,000 --> 0:04:4,000
et c'est arrivé très tôt,

77
0:04:40.89,000 --> 0:04:42,000
et en comprenant cette erreur,

78
0:04:42.988,000 --> 0:04:43,000
on peut la réparer.

79
0:04:44.871,000 --> 0:04:46,000
C'est arrivé dans les années 90,

80
0:04:47.454,000 --> 0:04:49,000
au tournant du siècle,

81
0:04:50.22,000 --> 0:04:51,000
et voici ce qu'il s'est passé.

82
0:04:53.2,000 --> 0:04:54,000
La culture numérique à ses débuts,

83
0:04:54.824,000 --> 0:04:58,000
et en fait, la culture numérique actuelle,

84
0:04:59.594,000 --> 0:05:04,000
avait une sorte de mission gauchiste, socialiste :

85
0:05:05.497,000 --> 0:05:07,000
contrairement à d'autres choses réalisées,

86
0:05:07.957,000 --> 0:05:08,000
comme l'invention des livres,

87
0:05:09.575,000 --> 0:05:12,000
tout ce qui se trouve sur Internet devait être exclusivement public

88
0:05:13.302,000 --> 0:05:15,000
et disponible gratuitement,

89
0:05:15.355,000 --> 0:05:18,000
car si une seule personne ne pouvait se l'offrir,

90
0:05:18.767,000 --> 0:05:2,000
cela créerait une terrible injustice.

91
0:05:21.912,000 --> 0:05:23,000
Bien sûr, il y a d'autres moyens de gérer ça.

92
0:05:24.44,000 --> 0:05:28,000
Si les livres coûtent de l'argent, on peut créer des bibliothèques publiques, etc.

93
0:05:28.538,000 --> 0:05:3,000
Mais non, pensait-on. Ceci est une exception.

94
0:05:31.34,000 --> 0:05:35,000
On voulait que ça soit un bien commun exclusivement public.

95
0:05:35.969,000 --> 0:05:37,000
Et donc, cet esprit perdure depuis.

96
0:05:38.627,000 --> 0:05:41,000
On le retrouve dans des projets comme Wikipédia, par exemple,

97
0:05:42.366,000 --> 0:05:43,000
et dans beaucoup d'autres.

98
0:05:43.731,000 --> 0:05:44,000
Mais en même temps,

99
0:05:45.629,000 --> 0:05:47,000
on croyait aussi, avec la même ferveur,

100
0:05:48.241,000 --> 0:05:51,000
dans cette autre chose qui était complètement incompatible :

101
0:05:52.202,000 --> 0:05:55,000
on aimait nos entrepreneurs en technologie.

102
0:05:55.853,000 --> 0:05:58,000
On adorait Steve Jobs ; on aimait ce mythe nietzschéen

103
0:05:59.616,000 --> 0:06:02,000
du technicien qui pourrait empreindre l'univers.

104
0:06:03.108,000 --> 0:06:04,000
N'est-ce pas ?

105
0:06:04.45,000 --> 0:06:09,000
Et ce pouvoir mythique a encore une emprise sur nous.

106
0:06:10.322,000 --> 0:06:14,000
Il y avait donc ces deux passions différentes :

107
0:06:14.805,000 --> 0:06:15,000
le fait de tout rendre gratuit

108
0:06:16.766,000 --> 0:06:21,000
et le pouvoir presque surnaturel de l'entrepreneur en technologie.

109
0:06:21.956,000 --> 0:06:25,000
Comment célébrer l'esprit d'entreprise quand tout est gratuit ?

110
0:06:26.332,000 --> 0:06:29,000
Eh bien, il n'y avait qu'une seule solution à l'époque,

111
0:06:29.481,000 --> 0:06:31,000
à savoir le modèle publicitaire.

112
0:06:31.592,000 --> 0:06:35,000
Et donc, Google est né gratuit, avec des annonces,

113
0:06:35.619,000 --> 0:06:38,000
Facebook est né gratuit, avec des annonces.

114
0:06:39.325,000 --> 0:06:42,000
Au début, c'était mignon,

115
0:06:43.214,000 --> 0:06:44,000
comme avec le tout premier Google.

116
0:06:45.198,000 --> 0:06:46,000
(Rires)

117
0:06:46.508,000 --> 0:06:48,000
Les publicités ressemblaient vraiment à des publicités.

118
0:06:49.429,000 --> 0:06:51,000
Elles parlaient du dentiste du coin par exemple.

119
0:06:51.874,000 --> 0:06:53,000
Mais il y a ce qu'on appelle la loi de Moore

120
0:06:54.088,000 --> 0:06:57,000
qui rend les ordinateurs de plus en plus efficaces et moins chers.

121
0:06:57.184,000 --> 0:06:58,000
Leurs algorithmes s'améliorent.

122
0:06:58.93,000 --> 0:07:,000
On a des universités où les gens les étudient,

123
0:07:01.52,000 --> 0:07:02,000
et ils s'améliorent de plus en plus.

124
0:07:03.238,000 --> 0:07:07,000
Et les clients et autres entités qui utilisent ces systèmes

125
0:07:07.678,000 --> 0:07:11,000
ont acquis plus d'expérience et sont devenus plus intelligents.

126
0:07:11.829,000 --> 0:07:13,000
Et ce qui était de la publicité au début

127
0:07:14.25,000 --> 0:07:16,000
ne l'est plus vraiment.

128
0:07:16.751,000 --> 0:07:18,000
Elle s'est transformée en modification de comportement,

129
0:07:19.687,000 --> 0:07:23,000
tout comme s'en inquiétait Norbert Wiener.

130
0:07:24.204,000 --> 0:07:28,000
Et donc, je ne peux plus appeler ces choses-là des réseaux sociaux.

131
0:07:28.848,000 --> 0:07:31,000
Je les appelle des empires de modification du comportement.

132
0:07:32.686,000 --> 0:07:34,000
(Applaudissements)

133
0:07:34.945,000 --> 0:07:38,000
Et je refuse de calomnier les individus.

134
0:07:39.183,000 --> 0:07:41,000
J'ai de bons amis dans ces entreprises,

135
0:07:41.478,000 --> 0:07:45,000
j'ai vendu une entreprise à Google, même si je pense que c'est l'un de ces empires.

136
0:07:46.262,000 --> 0:07:51,000
Selon moi, il ne s'agit pas de mauvaises personnes ayant commis un mauvais acte.

137
0:07:51.346,000 --> 0:07:55,000
Je pense qu'il s'agit d'une erreur tragique

138
0:07:55.946,000 --> 0:07:59,000
et incroyablement ridicule à l'échelle mondiale,

139
0:08:00.542,000 --> 0:08:04,000
plutôt que d'une vague malfaisante.

140
0:08:04.695,000 --> 0:08:06,000
Laissez-moi vous donner plus de détails

141
0:08:07.401,000 --> 0:08:1,000
sur la façon dont cette erreur particulière fonctionne.

142
0:08:11.337,000 --> 0:08:13,000
Avec le comportementalisme,

143
0:08:14.068,000 --> 0:08:19,000
on donne à la créature, qu'il s'agisse d'un rat, d'un chien ou d'une personne,

144
0:08:19.126,000 --> 0:08:21,000
des petites récompenses et parfois, des petites punitions

145
0:08:22,000 --> 0:08:24,000
en guise de retour par rapport à leurs actions.

146
0:08:24.71,000 --> 0:08:26,000
Donc, si on a un animal en cage,

147
0:08:27.696,000 --> 0:08:29,000
il s'agirait de bonbons et de chocs électriques.

148
0:08:30.216,000 --> 0:08:34,000
Mais c'est différent avec un smartphone,

149
0:08:34.806,000 --> 0:08:39,000
ce serait une punition et une récompense symboliques.

150
0:08:40.36,000 --> 0:08:42,000
Pavlov, l'un des premiers comportementalistes,

151
0:08:42.624,000 --> 0:08:43,000
a démontré le fameux principe.

152
0:08:44.418,000 --> 0:08:46,000
On pourrait entraîner un chien à saliver

153
0:08:47.295,000 --> 0:08:49,000
juste avec la clochette, juste avec le symbole.

154
0:08:49.515,000 --> 0:08:5,000
Ainsi, sur les réseaux sociaux,

155
0:08:51.035,000 --> 0:08:56,000
la punition et la récompense sociales servent de punition et de récompense.

156
0:08:56.096,000 --> 0:08:58,000
Et on en a tous fait l'expérience.

157
0:08:58.364,000 --> 0:08:59,000
On a ce petit frisson --

158
0:08:59.794,000 --> 0:09:01,000
« Quelqu'un a aimé mes posts et ça se répète. »

159
0:09:02.015,000 --> 0:09:04,000
Ou la punition : « Oh mon Dieu, ils ne m'aiment pas,

160
0:09:04.46,000 --> 0:09:05,000
quelqu'un d'autre est plus populaire. »

161
0:09:06.35,000 --> 0:09:08,000
Il y a donc ces deux sentiments très communs,

162
0:09:08.442,000 --> 0:09:12,000
et ils sont distribués de telle sorte qu'on se fait prendre dans cette boucle.

163
0:09:12.551,000 --> 0:09:16,000
Comme l'ont reconnu publiquement bon nombre de fondateurs du système,

164
0:09:16.621,000 --> 0:09:18,000
tout le monde savait ce qui se passait.

165
0:09:19.875,000 --> 0:09:2,000
Mais voici le problème :

166
0:09:21.354,000 --> 0:09:26,000
dans la recherche académique sur les méthodes du comportementalisme,

167
0:09:26.685,000 --> 0:09:31,000
il y a eu des comparaisons de stimuli positifs et négatifs.

168
0:09:32.488,000 --> 0:09:34,000
Dans ce contexte commercial,

169
0:09:35.126,000 --> 0:09:36,000
il y a un nouveau type de différence

170
0:09:37.062,000 --> 0:09:4,000
qui a échappé au monde universitaire depuis un certain temps :

171
0:09:40.404,000 --> 0:09:43,000
si les stimuli positifs sont plus efficaces

172
0:09:43.829,000 --> 0:09:45,000
que les stimuli négatifs dans des circonstances différentes,

173
0:09:46.711,000 --> 0:09:47,000
les stimuli négatifs sont moins chers.

174
0:09:48.683,000 --> 0:09:5,000
Ce sont les stimuli du marché.

175
0:09:51.276,000 --> 0:09:56,000
Ce que je veux dire par là, c'est qu'il est beaucoup plus facile

176
0:09:56.28,000 --> 0:09:58,000
de perdre la confiance que de l'établir.

177
0:09:59.24,000 --> 0:10:02,000
Il faut beaucoup de temps pour construire l'amour.

178
0:10:02.247,000 --> 0:10:04,000
Il en faut peu de temps pour le ruiner.

179
0:10:05.677,000 --> 0:10:08,000
Les clients de ces empires de modification du comportement

180
0:10:09.467,000 --> 0:10:11,000
sont pris dans une boucle rapide.

181
0:10:11.48,000 --> 0:10:13,000
Ils sont presque comme des traders à haute fréquence.

182
0:10:14.048,000 --> 0:10:16,000
Ils reçoivent des retours sur leurs dépenses

183
0:10:16.092,000 --> 0:10:18,000
ou leurs activités s'ils ne dépensent pas,

184
0:10:18.431,000 --> 0:10:21,000
ils voient ce qui fonctionne, puis ils en font plus.

185
0:10:21.487,000 --> 0:10:23,000
Ils reçoivent donc un retour rapide,

186
0:10:23.839,000 --> 0:10:26,000
ce qui signifie qu'ils réagissent davantage aux émotions négatives,

187
0:10:27.027,000 --> 0:10:3,000
car ce sont celles qui se manifestent plus vite, n'est-ce pas ?

188
0:10:30.637,000 --> 0:10:33,000
Par conséquent, même les acteurs bien intentionnés

189
0:10:34.301,000 --> 0:10:37,000
qui pensent qu'ils ne font que de la publicité pour du dentifrice

190
0:10:37.362,000 --> 0:10:39,000
finissent par faire avancer la cause des gens négatifs,

191
0:10:40.294,000 --> 0:10:42,000
des émotions négatives, des excentriques,

192
0:10:42.447,000 --> 0:10:43,000
des paranoïaques,

193
0:10:44.108,000 --> 0:10:46,000
des cyniques, des nihilistes.

194
0:10:47.356,000 --> 0:10:49,000
Le système amplifie ceux-là.

195
0:10:51.33,000 --> 0:10:54,000
Et il est plus difficile de payer l'une de ces entreprises

196
0:10:55.064,000 --> 0:10:57,000
pour améliorer soudainement le monde et la démocratie

197
0:10:57.555,000 --> 0:11:01,000
qu'il ne l'est pour ruiner ces choses.

198
0:11:01.706,000 --> 0:11:04,000
C'est donc face à ce dilemme qu'on s'est retrouvés.

199
0:11:06.186,000 --> 0:11:1,000
L'alternative est de revenir en arrière, avec beaucoup de difficulté,

200
0:11:10.821,000 --> 0:11:12,000
et de repenser cette décision.

201
0:11:13.714,000 --> 0:11:16,000
La repenser signifierait deux choses.

202
0:11:17.91,000 --> 0:11:21,000
D'abord, beaucoup de gens, ceux qui en ont les moyens,

203
0:11:22.041,000 --> 0:11:24,000
paieraient pour ces choses-là.

204
0:11:24.803,000 --> 0:11:27,000
On paierait pour la recherche, pour les réseaux sociaux.

205
0:11:28.795,000 --> 0:11:31,000
Comment paierait-on ? Peut-être avec des frais d'abonnement

206
0:11:32.142,000 --> 0:11:34,000
ou des micro-paiements au fur et à mesure qu'on les utilise.

207
0:11:35.033,000 --> 0:11:36,000
Il y a beaucoup d'options.

208
0:11:37.018,000 --> 0:11:39,000
Si cela répugne certains d'entre vous, et que vous pensez :

209
0:11:39.786,000 --> 0:11:41,000
« Oh mon Dieu, je ne paierais jamais pour ça.

210
0:11:41.984,000 --> 0:11:43,000
Comment pourrait-on faire payer quelqu'un ? »,

211
0:11:44.129,000 --> 0:11:46,000
je voudrais vous rappeler une chose qui vient d'arriver.

212
0:11:46.775,000 --> 0:11:48,000
À peu près à la même époque

213
0:11:48.89,000 --> 0:11:53,000
où Google et Facebook formulaient leur idée de gratuité,

214
0:11:55.256,000 --> 0:11:59,000
une grande partie de la cyber-communauté croyait aussi qu'à l'avenir,

215
0:11:59.34,000 --> 0:12:02,000
les émissions télé et les films se feraient de la même manière,

216
0:12:02.431,000 --> 0:12:03,000
un peu comme Wikipédia.

217
0:12:04.861,000 --> 0:12:08,000
Mais ensuite, des entreprises comme Netflix, Amazon, HBO, ont dit :

218
0:12:09.851,000 --> 0:12:12,000
« En fait, abonnez-vous. On va vous donner une super télé. »

219
0:12:13.366,000 --> 0:12:14,000
Et ça a marché !

220
0:12:14.555,000 --> 0:12:17,000
On est maintenant dans cette période appelée « Peak TV », n'est-ce pas ?

221
0:12:18.398,000 --> 0:12:21,000
Donc, parfois, quand on paie pour une chose, elle s'améliore.

222
0:12:22.578,000 --> 0:12:24,000
On peut imaginer une hypothétique --

223
0:12:25.502,000 --> 0:12:28,000
(Applaudissements)

224
0:12:29.488,000 --> 0:12:33,000
On peut imaginer un monde hypothétique de réseaux sociaux à leur apogée.

225
0:12:33.538,000 --> 0:12:34,000
À quoi cela ressemblerait-il ?

226
0:12:35.167,000 --> 0:12:36,000
Lorsqu'on s'y connecterait,

227
0:12:36.42,000 --> 0:12:4,000
on aurait des conseils médicaux utiles et fiables, et non farfelus.

228
0:12:41.389,000 --> 0:12:43,000
Lorsqu'on voudrait obtenir des informations factuelles,

229
0:12:44.359,000 --> 0:12:47,000
il n'y aurait pas un tas de théories du complot bizarres et paranoïaques.

230
0:12:47.803,000 --> 0:12:51,000
On peut imaginer cette merveilleuse possibilité alternative.

231
0:12:52.007,000 --> 0:12:53,000
Oh.

232
0:12:53.215,000 --> 0:12:55,000
J'en rêve. Je crois que c'est possible.

233
0:12:55.662,000 --> 0:12:56,000
Je suis sûr que c'est possible.

234
0:12:58.819,000 --> 0:13:03,000
Et je suis certain que les entreprises, les Google et les Facebook,

235
0:13:04.249,000 --> 0:13:06,000
s'en sortiraient mieux dans ce monde.

236
0:13:06.337,000 --> 0:13:08,000
Je ne crois pas qu'il faille punir la Silicon Valley.

237
0:13:08.941,000 --> 0:13:1,000
On a juste besoin de repenser la décision.

238
0:13:12.483,000 --> 0:13:14,000
Parmi les grandes entreprises de technologie,

239
0:13:14.599,000 --> 0:13:19,000
seules deux dépendent de la modification du comportement et de l'espionnage

240
0:13:19.8,000 --> 0:13:2,000
comme stratégie commerciale.

241
0:13:21.77,000 --> 0:13:22,000
C'est Google et Facebook.

242
0:13:23.153,000 --> 0:13:24,000
(Rires)

243
0:13:24.96,000 --> 0:13:25,000
Et je vous aime, les gars.

244
0:13:26.289,000 --> 0:13:28,000
C'est vrai. Les gens sont fantastiques.

245
0:13:30.369,000 --> 0:13:33,000
Je tiens à souligner, si vous me le permettez,

246
0:13:33.541,000 --> 0:13:34,000
que si on se penche sur Google,

247
0:13:35.093,000 --> 0:13:39,000
on verra qu'ils peuvent multiplier les centres de coût à l'infini

248
0:13:39.315,000 --> 0:13:42,000
avec toutes ces sociétés, mais pas les centres de profit.

249
0:13:42.373,000 --> 0:13:44,000
Ils ne peuvent pas se diversifier parce qu'ils sont accros.

250
0:13:45.293,000 --> 0:13:48,000
Ils sont dépendants de ce modèle, tout comme leurs propres utilisateurs.

251
0:13:48.687,000 --> 0:13:49,000
Ils sont pris au même piège,

252
0:13:50.502,000 --> 0:13:52,000
et on ne peut pas diriger une grande entreprise ainsi.

253
0:13:53.079,000 --> 0:13:55,000
En définitive, c'est donc tout à l'avantage des actionnaires

254
0:13:55.961,000 --> 0:13:57,000
et d'autres parties prenantes de ces sociétés.

255
0:13:58.575,000 --> 0:14:,000
C'est une solution gagnant-gagnant.

256
0:14:01.038,000 --> 0:14:03,000
Il faudra juste un peu de temps pour tout résoudre.

257
0:14:03.461,000 --> 0:14:05,000
Beaucoup de détails à régler,

258
0:14:05.481,000 --> 0:14:06,000
totalement faisable.

259
0:14:07.068,000 --> 0:14:09,000
(Rires)

260
0:14:10.23,000 --> 0:14:13,000
Je ne crois pas que notre espèce puisse survivre si on n'y remédie pas.

261
0:14:13.944,000 --> 0:14:18,000
On ne peut pas vivre dans une société où deux personnes souhaitant communiquer

262
0:14:19.771,000 --> 0:14:22,000
n'y parviennent que si cette communication est financée par un tiers

263
0:14:22.991,000 --> 0:14:23,000
qui souhaite les manipuler.

264
0:14:24.664,000 --> 0:14:27,000
(Applaudissements)

265
0:14:36.649,000 --> 0:14:39,000
En attendant, si les entreprises ne changent pas,

266
0:14:39.657,000 --> 0:14:4,000
supprimez vos comptes, d'accord ?

267
0:14:41.342,000 --> 0:14:42,000
(Rires)

268
0:14:42.549,000 --> 0:14:43,000
(Applaudissements)

269
0:14:44.095,000 --> 0:14:45,000
C'est tout pour aujourd'hui.

270
0:14:45.718,000 --> 0:14:46,000
Merci beaucoup.

271
0:14:47.254,000 --> 0:14:49,000
(Applaudissements)

