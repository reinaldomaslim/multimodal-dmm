1
0:00:13.16,000 --> 0:00:17,000
You know, I'm struck by how one of the implicit themes of TED

2
0:00:17.16,000 --> 0:00:2,000
is compassion, these very moving demonstrations we've just seen:

3
0:00:21.16,000 --> 0:00:25,000
HIV in Africa, President Clinton last night.

4
0:00:25.16,000 --> 0:00:3,000
And I'd like to do a little collateral thinking, if you will,

5
0:00:30.16,000 --> 0:00:35,000
about compassion and bring it from the global level to the personal.

6
0:00:35.16,000 --> 0:00:37,000
I'm a psychologist, but rest assured,

7
0:00:37.16,000 --> 0:00:38,000
I will not bring it to the scrotal.

8
0:00:39.16,000 --> 0:00:43,000
(Laughter)

9
0:00:44.16,000 --> 0:00:46,000
There was a very important study done a while ago

10
0:00:46.16,000 --> 0:00:5,000
at Princeton Theological Seminary that speaks to why it is

11
0:00:51.16,000 --> 0:00:54,000
that when all of us have so many opportunities to help,

12
0:00:54.16,000 --> 0:00:57,000
we do sometimes, and we don't other times.

13
0:00:58.16,000 --> 0:01:01,000
A group of divinity students at the Princeton Theological Seminary

14
0:01:02.16,000 --> 0:01:06,000
were told that they were going to give a practice sermon

15
0:01:06.16,000 --> 0:01:09,000
and they were each given a sermon topic.

16
0:01:09.16,000 --> 0:01:12,000
Half of those students were given, as a topic,

17
0:01:12.16,000 --> 0:01:14,000
the parable of the Good Samaritan:

18
0:01:14.16,000 --> 0:01:16,000
the man who stopped the stranger in --

19
0:01:17.16,000 --> 0:01:19,000
to help the stranger in need by the side of the road.

20
0:01:19.16,000 --> 0:01:22,000
Half were given random Bible topics.

21
0:01:22.16,000 --> 0:01:25,000
Then one by one, they were told they had to go to another building

22
0:01:26.16,000 --> 0:01:27,000
and give their sermon.

23
0:01:27.16,000 --> 0:01:3,000
As they went from the first building to the second,

24
0:01:30.16,000 --> 0:01:33,000
each of them passed a man who was bent over and moaning,

25
0:01:34.16,000 --> 0:01:38,000
clearly in need. The question is: Did they stop to help?

26
0:01:38.16,000 --> 0:01:39,000
The more interesting question is:

27
0:01:40.16,000 --> 0:01:43,000
Did it matter they were contemplating the parable

28
0:01:43.16,000 --> 0:01:47,000
of the Good Samaritan? Answer: No, not at all.

29
0:01:48.16,000 --> 0:01:51,000
What turned out to determine whether someone would stop

30
0:01:51.16,000 --> 0:01:52,000
and help a stranger in need

31
0:01:52.16,000 --> 0:01:55,000
was how much of a hurry they thought they were in --

32
0:01:56.16,000 --> 0:02:,000
were they feeling they were late, or were they absorbed

33
0:02:00.16,000 --> 0:02:01,000
in what they were going to talk about.

34
0:02:02.16,000 --> 0:02:04,000
And this is, I think, the predicament of our lives:

35
0:02:05.16,000 --> 0:02:09,000
that we don't take every opportunity to help

36
0:02:09.16,000 --> 0:02:12,000
because our focus is in the wrong direction.

37
0:02:12.16,000 --> 0:02:15,000
There's a new field in brain science, social neuroscience.

38
0:02:16.16,000 --> 0:02:2,000
This studies the circuitry in two people's brains

39
0:02:20.16,000 --> 0:02:22,000
that activates while they interact.

40
0:02:22.16,000 --> 0:02:26,000
And the new thinking about compassion from social neuroscience

41
0:02:26.16,000 --> 0:02:3,000
is that our default wiring is to help.

42
0:02:30.16,000 --> 0:02:34,000
That is to say, if we attend to the other person,

43
0:02:35.16,000 --> 0:02:38,000
we automatically empathize, we automatically feel with them.

44
0:02:39.16,000 --> 0:02:41,000
There are these newly identified neurons, mirror neurons,

45
0:02:41.16,000 --> 0:02:45,000
that act like a neuro Wi-Fi, activating in our brain

46
0:02:45.16,000 --> 0:02:49,000
exactly the areas activated in theirs. We feel "with" automatically.

47
0:02:49.16,000 --> 0:02:53,000
And if that person is in need, if that person is suffering,

48
0:02:54.16,000 --> 0:02:58,000
we're automatically prepared to help. At least that's the argument.

49
0:02:58.16,000 --> 0:03:01,000
But then the question is: Why don't we?

50
0:03:01.16,000 --> 0:03:03,000
And I think this speaks to a spectrum

51
0:03:04.16,000 --> 0:03:06,000
that goes from complete self-absorption,

52
0:03:07.16,000 --> 0:03:09,000
to noticing, to empathy and to compassion.

53
0:03:09.16,000 --> 0:03:13,000
And the simple fact is, if we are focused on ourselves,

54
0:03:14.16,000 --> 0:03:17,000
if we're preoccupied, as we so often are throughout the day,

55
0:03:17.16,000 --> 0:03:2,000
we don't really fully notice the other.

56
0:03:20.16,000 --> 0:03:22,000
And this difference between the self and the other focus

57
0:03:22.16,000 --> 0:03:23,000
can be very subtle.

58
0:03:23.16,000 --> 0:03:27,000
I was doing my taxes the other day, and I got to the point

59
0:03:27.16,000 --> 0:03:29,000
where I was listing all of the donations I gave,

60
0:03:30.16,000 --> 0:03:33,000
and I had an epiphany, it was -- I came to my check

61
0:03:33.16,000 --> 0:03:36,000
to the Seva Foundation and I noticed that I thought,

62
0:03:36.16,000 --> 0:03:38,000
boy, my friend Larry Brilliant would really be happy

63
0:03:39.16,000 --> 0:03:4,000
that I gave money to Seva.

64
0:03:40.16,000 --> 0:03:43,000
Then I realized that what I was getting from giving

65
0:03:43.16,000 --> 0:03:47,000
was a narcissistic hit -- that I felt good about myself.

66
0:03:47.16,000 --> 0:03:52,000
Then I started to think about the people in the Himalayas

67
0:03:52.16,000 --> 0:03:54,000
whose cataracts would be helped, and I realized

68
0:03:55.16,000 --> 0:03:58,000
that I went from this kind of narcissistic self-focus

69
0:03:59.16,000 --> 0:04:02,000
to altruistic joy, to feeling good

70
0:04:02.16,000 --> 0:04:06,000
for the people that were being helped. I think that's a motivator.

71
0:04:06.16,000 --> 0:04:09,000
But this distinction between focusing on ourselves

72
0:04:09.16,000 --> 0:04:1,000
and focusing on others

73
0:04:10.16,000 --> 0:04:13,000
is one that I encourage us all to pay attention to.

74
0:04:13.16,000 --> 0:04:16,000
You can see it at a gross level in the world of dating.

75
0:04:17.16,000 --> 0:04:2,000
I was at a sushi restaurant a while back

76
0:04:20.16,000 --> 0:04:23,000
and I overheard two women talking about the brother of one woman,

77
0:04:24.16,000 --> 0:04:27,000
who was in the singles scene. And this woman says,

78
0:04:27.16,000 --> 0:04:29,000
"My brother is having trouble getting dates,

79
0:04:29.16,000 --> 0:04:31,000
so he's trying speed dating." I don't know if you know speed dating?

80
0:04:31.16,000 --> 0:04:35,000
Women sit at tables and men go from table to table,

81
0:04:35.16,000 --> 0:04:38,000
and there's a clock and a bell, and at five minutes, bingo,

82
0:04:39.16,000 --> 0:04:41,000
the conversation ends and the woman can decide

83
0:04:41.16,000 --> 0:04:45,000
whether to give her card or her email address to the man

84
0:04:45.16,000 --> 0:04:47,000
for follow up. And this woman says,

85
0:04:47.16,000 --> 0:04:51,000
"My brother's never gotten a card, and I know exactly why.

86
0:04:51.16,000 --> 0:04:56,000
The moment he sits down, he starts talking non-stop about himself;

87
0:04:56.16,000 --> 0:04:57,000
he never asks about the woman."

88
0:04:58.16,000 --> 0:05:03,000
And I was doing some research in the Sunday Styles section

89
0:05:03.16,000 --> 0:05:06,000
of The New York Times, looking at the back stories of marriages --

90
0:05:06.16,000 --> 0:05:09,000
because they're very interesting -- and I came to the marriage

91
0:05:09.16,000 --> 0:05:12,000
of Alice Charney Epstein. And she said

92
0:05:12.16,000 --> 0:05:14,000
that when she was in the dating scene,

93
0:05:15.16,000 --> 0:05:17,000
she had a simple test she put people to.

94
0:05:18.16,000 --> 0:05:2,000
The test was: from the moment they got together,

95
0:05:20.16,000 --> 0:05:23,000
how long it would take the guy to ask her a question

96
0:05:23.16,000 --> 0:05:25,000
with the word "you" in it.

97
0:05:25.16,000 --> 0:05:29,000
And apparently Epstein aced the test, therefore the article.

98
0:05:29.16,000 --> 0:05:3,000
(Laughter)

99
0:05:30.16,000 --> 0:05:32,000
Now this is a -- it's a little test

100
0:05:32.16,000 --> 0:05:34,000
I encourage you to try out at a party.

101
0:05:34.16,000 --> 0:05:36,000
Here at TED there are great opportunities.

102
0:05:38.16,000 --> 0:05:41,000
The Harvard Business Review recently had an article called

103
0:05:41.16,000 --> 0:05:44,000
"The Human Moment," about how to make real contact

104
0:05:44.16,000 --> 0:05:47,000
with a person at work. And they said, well,

105
0:05:47.16,000 --> 0:05:5,000
the fundamental thing you have to do is turn off your BlackBerry,

106
0:05:51.16,000 --> 0:05:54,000
close your laptop, end your daydream

107
0:05:55.16,000 --> 0:05:57,000
and pay full attention to the person.

108
0:05:58.16,000 --> 0:06:02,000
There is a newly coined word in the English language

109
0:06:03.16,000 --> 0:06:06,000
for the moment when the person we're with whips out their BlackBerry

110
0:06:06.16,000 --> 0:06:09,000
or answers that cell phone, and all of a sudden we don't exist.

111
0:06:10.16,000 --> 0:06:14,000
The word is "pizzled": it's a combination of puzzled and pissed off.

112
0:06:14.16,000 --> 0:06:17,000
(Laughter)

113
0:06:17.16,000 --> 0:06:23,000
I think it's quite apt. It's our empathy, it's our tuning in

114
0:06:24.16,000 --> 0:06:27,000
which separates us from Machiavellians or sociopaths.

115
0:06:27.16,000 --> 0:06:32,000
I have a brother-in-law who's an expert on horror and terror --

116
0:06:32.16,000 --> 0:06:35,000
he wrote the Annotated Dracula, the Essential Frankenstein --

117
0:06:35.16,000 --> 0:06:36,000
he was trained as a Chaucer scholar,

118
0:06:36.16,000 --> 0:06:38,000
but he was born in Transylvania

119
0:06:38.16,000 --> 0:06:4,000
and I think it affected him a little bit.

120
0:06:40.16,000 --> 0:06:44,000
At any rate, at one point my brother-in-law, Leonard,

121
0:06:44.16,000 --> 0:06:46,000
decided to write a book about a serial killer.

122
0:06:46.16,000 --> 0:06:49,000
This is a man who terrorized the very vicinity we're in

123
0:06:50.16,000 --> 0:06:52,000
many years ago. He was known as the Santa Cruz strangler.

124
0:06:53.16,000 --> 0:06:57,000
And before he was arrested, he had murdered his grandparents,

125
0:06:57.16,000 --> 0:07:,000
his mother and five co-eds at UC Santa Cruz.

126
0:07:01.16,000 --> 0:07:03,000
So my brother-in-law goes to interview this killer

127
0:07:04.16,000 --> 0:07:06,000
and he realizes when he meets him

128
0:07:06.16,000 --> 0:07:07,000
that this guy is absolutely terrifying.

129
0:07:08.16,000 --> 0:07:1,000
For one thing, he's almost seven feet tall.

130
0:07:10.16,000 --> 0:07:13,000
But that's not the most terrifying thing about him.

131
0:07:13.16,000 --> 0:07:18,000
The scariest thing is that his IQ is 160: a certified genius.

132
0:07:19.16,000 --> 0:07:23,000
But there is zero correlation between IQ and emotional empathy,

133
0:07:23.16,000 --> 0:07:24,000
feeling with the other person.

134
0:07:25.16,000 --> 0:07:27,000
They're controlled by different parts of the brain.

135
0:07:28.16,000 --> 0:07:3,000
So at one point, my brother-in-law gets up the courage

136
0:07:31.16,000 --> 0:07:33,000
to ask the one question he really wants to know the answer to,

137
0:07:33.16,000 --> 0:07:36,000
and that is: how could you have done it?

138
0:07:36.16,000 --> 0:07:38,000
Didn't you feel any pity for your victims?

139
0:07:38.16,000 --> 0:07:41,000
These were very intimate murders -- he strangled his victims.

140
0:07:42.16,000 --> 0:07:44,000
And the strangler says very matter-of-factly,

141
0:07:44.16,000 --> 0:07:49,000
"Oh no. If I'd felt the distress, I could not have done it.

142
0:07:49.16,000 --> 0:07:55,000
I had to turn that part of me off. I had to turn that part of me off."

143
0:07:55.16,000 --> 0:08:,000
And I think that that is very troubling,

144
0:08:01.16,000 --> 0:08:05,000
and in a sense, I've been reflecting on turning that part of us off.

145
0:08:05.16,000 --> 0:08:07,000
When we focus on ourselves in any activity,

146
0:08:08.16,000 --> 0:08:11,000
we do turn that part of ourselves off if there's another person.

147
0:08:12.16,000 --> 0:08:17,000
Think about going shopping and think about the possibilities

148
0:08:17.16,000 --> 0:08:19,000
of a compassionate consumerism.

149
0:08:20.16,000 --> 0:08:22,000
Right now, as Bill McDonough has pointed out,

150
0:08:24.16,000 --> 0:08:28,000
the objects that we buy and use have hidden consequences.

151
0:08:28.16,000 --> 0:08:31,000
We're all unwitting victims of a collective blind spot.

152
0:08:32.16,000 --> 0:08:34,000
We don't notice and don't notice that we don't notice

153
0:08:35.16,000 --> 0:08:41,000
the toxic molecules emitted by a carpet or by the fabric on the seats.

154
0:08:42.16,000 --> 0:08:47,000
Or we don't know if that fabric is a technological

155
0:08:47.16,000 --> 0:08:51,000
or manufacturing nutrient; it can be reused

156
0:08:51.16,000 --> 0:08:53,000
or does it just end up at landfill? In other words,

157
0:08:53.16,000 --> 0:08:58,000
we're oblivious to the ecological and public health

158
0:08:59.16,000 --> 0:09:02,000
and social and economic justice consequences

159
0:09:02.16,000 --> 0:09:04,000
of the things we buy and use.

160
0:09:06.16,000 --> 0:09:1,000
In a sense, the room itself is the elephant in the room,

161
0:09:10.16,000 --> 0:09:14,000
but we don't see it. And we've become victims

162
0:09:14.16,000 --> 0:09:17,000
of a system that points us elsewhere. Consider this.

163
0:09:18.16,000 --> 0:09:21,000
There's a wonderful book called

164
0:09:22.16,000 --> 0:09:24,000
Stuff: The Hidden Life of Everyday Objects.

165
0:09:25.16,000 --> 0:09:28,000
And it talks about the back story of something like a t-shirt.

166
0:09:28.16,000 --> 0:09:31,000
And it talks about where the cotton was grown

167
0:09:31.16,000 --> 0:09:33,000
and the fertilizers that were used and the consequences

168
0:09:33.16,000 --> 0:09:37,000
for soil of that fertilizer. And it mentions, for instance,

169
0:09:37.16,000 --> 0:09:4,000
that cotton is very resistant to textile dye;

170
0:09:40.16,000 --> 0:09:43,000
about 60 percent washes off into wastewater.

171
0:09:43.16,000 --> 0:09:46,000
And it's well known by epidemiologists that kids

172
0:09:46.16,000 --> 0:09:51,000
who live near textile works tend to have high rates of leukemia.

173
0:09:52.16,000 --> 0:09:56,000
There's a company, Bennett and Company, that supplies Polo.com,

174
0:09:57.16,000 --> 0:10:02,000
Victoria's Secret -- they, because of their CEO, who's aware of this,

175
0:10:03.16,000 --> 0:10:07,000
in China formed a joint venture with their dye works

176
0:10:07.16,000 --> 0:10:09,000
to make sure that the wastewater

177
0:10:09.16,000 --> 0:10:13,000
would be properly taken care of before it returned to the groundwater.

178
0:10:13.16,000 --> 0:10:17,000
Right now, we don't have the option to choose the virtuous t-shirt

179
0:10:18.16,000 --> 0:10:22,000
over the non-virtuous one. So what would it take to do that?

180
0:10:25.16,000 --> 0:10:28,000
Well, I've been thinking. For one thing,

181
0:10:28.16,000 --> 0:10:33,000
there's a new electronic tagging technology that allows any store

182
0:10:33.16,000 --> 0:10:37,000
to know the entire history of any item on the shelves in that store.

183
0:10:38.16,000 --> 0:10:4,000
You can track it back to the factory. Once you can track it

184
0:10:40.16,000 --> 0:10:44,000
back to the factory, you can look at the manufacturing processes

185
0:10:44.16,000 --> 0:10:48,000
that were used to make it, and if it's virtuous,

186
0:10:48.16,000 --> 0:10:52,000
you can label it that way. Or if it's not so virtuous,

187
0:10:52.16,000 --> 0:10:56,000
you can go into -- today, go into any store,

188
0:10:56.16,000 --> 0:10:59,000
put your scanner on a palm onto a barcode,

189
0:10:59.16,000 --> 0:11:01,000
which will take you to a website.

190
0:11:01.16,000 --> 0:11:03,000
They have it for people with allergies to peanuts.

191
0:11:04.16,000 --> 0:11:06,000
That website could tell you things about that object.

192
0:11:07.16,000 --> 0:11:08,000
In other words, at point of purchase,

193
0:11:08.16,000 --> 0:11:12,000
we might be able to make a compassionate choice.

194
0:11:12.16,000 --> 0:11:18,000
There's a saying in the world of information science:

195
0:11:18.16,000 --> 0:11:21,000
ultimately everybody will know everything.

196
0:11:21.16,000 --> 0:11:23,000
And the question is: will it make a difference?

197
0:11:25.16,000 --> 0:11:28,000
Some time ago when I was working for The New York Times,

198
0:11:29.16,000 --> 0:11:31,000
it was in the '80s, I did an article

199
0:11:31.16,000 --> 0:11:33,000
on what was then a new problem in New York --

200
0:11:33.16,000 --> 0:11:35,000
it was homeless people on the streets.

201
0:11:35.16,000 --> 0:11:39,000
And I spent a couple of weeks going around with a social work agency

202
0:11:39.16,000 --> 0:11:42,000
that ministered to the homeless. And I realized seeing the homeless

203
0:11:42.16,000 --> 0:11:47,000
through their eyes that almost all of them were psychiatric patients

204
0:11:47.16,000 --> 0:11:51,000
that had nowhere to go. They had a diagnosis. It made me --

205
0:11:52.16,000 --> 0:11:55,000
what it did was to shake me out of the urban trance where,

206
0:11:56.16,000 --> 0:11:59,000
when we see, when we're passing someone who's homeless

207
0:11:59.16,000 --> 0:12:02,000
in the periphery of our vision, it stays on the periphery.

208
0:12:04.16,000 --> 0:12:06,000
We don't notice and therefore we don't act.

209
0:12:09.16,000 --> 0:12:14,000
One day soon after that -- it was a Friday -- at the end of the day,

210
0:12:14.16,000 --> 0:12:17,000
I went down -- I was going down to the subway. It was rush hour

211
0:12:17.16,000 --> 0:12:19,000
and thousands of people were streaming down the stairs.

212
0:12:19.16,000 --> 0:12:21,000
And all of a sudden as I was going down the stairs

213
0:12:21.16,000 --> 0:12:24,000
I noticed that there was a man slumped to the side,

214
0:12:24.16,000 --> 0:12:28,000
shirtless, not moving, and people were just stepping over him --

215
0:12:29.16,000 --> 0:12:3,000
hundreds and hundreds of people.

216
0:12:31.16,000 --> 0:12:34,000
And because my urban trance had been somehow weakened,

217
0:12:35.16,000 --> 0:12:38,000
I found myself stopping to find out what was wrong.

218
0:12:39.16,000 --> 0:12:41,000
The moment I stopped, half a dozen other people

219
0:12:42.16,000 --> 0:12:43,000
immediately ringed the same guy.

220
0:12:44.16,000 --> 0:12:46,000
And we found out that he was Hispanic, he didn't speak any English,

221
0:12:46.16,000 --> 0:12:51,000
he had no money, he'd been wandering the streets for days, starving,

222
0:12:51.16,000 --> 0:12:52,000
and he'd fainted from hunger.

223
0:12:52.16,000 --> 0:12:54,000
Immediately someone went to get orange juice,

224
0:12:54.16,000 --> 0:12:56,000
someone brought a hotdog, someone brought a subway cop.

225
0:12:57.16,000 --> 0:13:,000
This guy was back on his feet immediately.

226
0:13:00.16,000 --> 0:13:04,000
But all it took was that simple act of noticing,

227
0:13:05.16,000 --> 0:13:06,000
and so I'm optimistic.

228
0:13:06.16,000 --> 0:13:07,000
Thank you very much.

229
0:13:07.16,000 --> 0:13:09,000
(Applause)

