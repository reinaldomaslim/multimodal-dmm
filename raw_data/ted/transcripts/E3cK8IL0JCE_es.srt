1
0:00:,000 --> 0:00:07,000
Traductor: Ciro Gomez

2
0:00:13.131,000 --> 0:00:17,000
Mi relación con Internet me recuerda al montaje

3
0:00:17.566,000 --> 0:00:18,000
de una típica película de terror.

4
0:00:19.867,000 --> 0:00:23,000
Ya saben, la familia dichosamente feliz se muda a su nuevo hogar perfecto,

5
0:00:24.277,000 --> 0:00:26,000
entusiasmada con su futuro perfecto,

6
0:00:26.582,000 --> 0:00:29,000
y hace un día soleado y los pájaros están cantando...

7
0:00:30.857,000 --> 0:00:31,000
Y entonces se pone oscuro.

8
0:00:32.72,000 --> 0:00:34,000
Y hay ruidos en el ático.

9
0:00:35.086,000 --> 0:00:37,000
Y nos damos cuenta de que esa casa perfecta

10
0:00:37.85,000 --> 0:00:39,000
no es tan perfecta.

11
0:00:40.485,000 --> 0:00:43,000
Cuando comencé a trabajar en Google en 2006,

12
0:00:43.64,000 --> 0:00:44,000
Facebook tenía solo dos años

13
0:00:45.431,000 --> 0:00:47,000
y Twitter aún no había nacido.

14
0:00:47.848,000 --> 0:00:51,000
Y estaba absolutamente asombrada con Internet y toda su promesa

15
0:00:52.282,000 --> 0:00:53,000
de hacernos más cercanos,

16
0:00:53.743,000 --> 0:00:54,000
más inteligentes

17
0:00:55.063,000 --> 0:00:56,000
y más libres.

18
0:00:57.265,000 --> 0:00:59,000
Pero mientras hacíamos el inspirador trabajo

19
0:00:59.523,000 --> 0:01:,000
de construir motores de búsqueda

20
0:01:01.103,000 --> 0:01:03,000
y páginas para compartir videos y redes sociales,

21
0:01:04.907,000 --> 0:01:08,000
los criminales, dictadores y terroristas estaban descubriendo

22
0:01:09.235,000 --> 0:01:12,000
cómo usar esas mismas plataformas contra nosotros.

23
0:01:13.417,000 --> 0:01:15,000
Y no tuvimos la previsión de detenerlos.

24
0:01:16.745,000 --> 0:01:21,000
Últimamente, las fuerzas geopolíticas se han conectado para causar estragos.

25
0:01:21.869,000 --> 0:01:22,000
Y en respuesta,

26
0:01:23.062,000 --> 0:01:27,000
Google nos apoyó a algunos colegas y a mí para montar un nuevo grupo llamado Jigsaw,

27
0:01:27.864,000 --> 0:01:29,000
con la pretensión de lograr más seguridad

28
0:01:30.054,000 --> 0:01:32,000
contra amenazas como el extremismo violento,

29
0:01:32.484,000 --> 0:01:34,000
la censura, o persecución,

30
0:01:35.186,000 --> 0:01:39,000
amenazas que considero personales porque yo nací en Irán,

31
0:01:39.327,000 --> 0:01:41,000
y me marché después de una revolución violenta.

32
0:01:43.525,000 --> 0:01:47,000
Pero me he dado cuenta de que aunque tuviéramos todos los recursos

33
0:01:47.895,000 --> 0:01:49,000
de todas las empresas tecnológicas del mundo,

34
0:01:51.595,000 --> 0:01:52,000
aún así fallaríamos

35
0:01:53.586,000 --> 0:01:55,000
si pasáramos por alto un ingrediente crítico:

36
0:01:57.653,000 --> 0:02:02,000
las experiencias humanas de las víctimas y los infractores de esas amenazas.

37
0:02:04.865,000 --> 0:02:06,000
Hay muchos desafíos de los que les podría hablar hoy.

38
0:02:07.365,000 --> 0:02:08,000
Me voy a centrar en solo dos.

39
0:02:09.623,000 --> 0:02:11,000
El primero es el terrorismo.

40
0:02:13.563,000 --> 0:02:15,000
Para entender el proceso de radicalización,

41
0:02:16.144,000 --> 0:02:2,000
nos reunimos con docenas de exmiembros de grupos extremistas violentos.

42
0:02:21.59,000 --> 0:02:23,000
Una era una colegiala británica,

43
0:02:25.049,000 --> 0:02:28,000
que había sido sacada de un avión en Heathrow, en Londres,

44
0:02:28.772,000 --> 0:02:32,000
mientras trataba de abrirse camino hasta Siria para unirse al ISIS.

45
0:02:34.281,000 --> 0:02:35,000
Tenía 13 años.

46
0:02:37.792,000 --> 0:02:41,000
Así que me senté con ella y su padre, y pregunté: "¿Por qué?"

47
0:02:42.441,000 --> 0:02:43,000
Y me dijo,

48
0:02:44.182,000 --> 0:02:47,000
"Estaba mirando fotos de cómo es la vida en Siria,

49
0:02:47.845,000 --> 0:02:5,000
y pensé en irme y vivir en el mundo islámico de Disney".

50
0:02:52.527,000 --> 0:02:54,000
Eso es lo que ella vio en ISIS.

51
0:02:54.635,000 --> 0:02:57,000
Pensó que se encontraría y se casaría con un jihadista Brad Pitt

52
0:02:58.091,000 --> 0:03:01,000
e irían todo el día de compras al centro comercial y vivirían felices.

53
0:03:02.977,000 --> 0:03:04,000
El ISIS entiende qué impulsa a la gente,

54
0:03:05.825,000 --> 0:03:08,000
y elaboran cuidadosamente un mensaje para cada audiencia.

55
0:03:11.122,000 --> 0:03:12,000
Miren a cuántos idiomas

56
0:03:12.657,000 --> 0:03:14,000
traducen su material de mercadotecnia.

57
0:03:15.677,000 --> 0:03:17,000
Hacen panfletos, programas de radio y videos

58
0:03:18.362,000 --> 0:03:19,000
no solo en inglés y árabe,

59
0:03:20.359,000 --> 0:03:24,000
también en alemán, ruso, francés, turco, kurdo,

60
0:03:25.15,000 --> 0:03:26,000
hebreo,

61
0:03:26.846,000 --> 0:03:27,000
chino mandarín.

62
0:03:29.309,000 --> 0:03:33,000
Incluso he visto un video producido por ISIS en lenguaje de signos.

63
0:03:34.605,000 --> 0:03:35,000
Piensen en ello un segundo:

64
0:03:36.513,000 --> 0:03:38,000
El ISIS se tomó su tiempo y se esforzó

65
0:03:38.845,000 --> 0:03:41,000
por asegurarse de que su mensaje llegara a sordos e hipoacúsicos.

66
0:03:45.143,000 --> 0:03:47,000
En realidad, no es por el dominio tecnológico

67
0:03:47.311,000 --> 0:03:49,000
por lo que el ISIS gana corazones y mentes.

68
0:03:49.93,000 --> 0:03:52,000
Lo que lo consigue es su percepción de los prejuicios, las vulnerabilidades

69
0:03:53.753,000 --> 0:03:56,000
y los deseos de las personas a las que intentan llegar.

70
0:03:57.718,000 --> 0:04:,000
Por eso no basta con que las plataformas en línea se centren

71
0:04:01.244,000 --> 0:04:04,000
en eliminar el material de reclutamiento.

72
0:04:04.518,000 --> 0:04:07,000
Si queremos tener una oportunidad de construir una tecnología significativa

73
0:04:08.123,000 --> 0:04:09,000
que contrarreste la radicalización,

74
0:04:10.021,000 --> 0:04:12,000
tenemos que comenzar con un viaje a su esencia más humana.

75
0:04:13.884,000 --> 0:04:15,000
Así que fuimos a Irak

76
0:04:16.095,000 --> 0:04:18,000
a hablar con hombres jóvenes que habían aceptado la promesa

77
0:04:19.036,000 --> 0:04:21,000
de heroísmo y rectitud de ISIS,

78
0:04:22.015,000 --> 0:04:24,000
que se habían armado para luchar por ellos

79
0:04:24.062,000 --> 0:04:25,000
y que habían desertado

80
0:04:25.398,000 --> 0:04:28,000
después de ser testigos de la brutalidad del gobierno del ISIS.

81
0:04:28.88,000 --> 0:04:31,000
Y estoy sentada allí en esa improvisada prisión en el norte de Irak

82
0:04:32.096,000 --> 0:04:33,000
con este chico de 23 años

83
0:04:33.79,000 --> 0:04:35,000
que se había entrenado como terrorista suicida

84
0:04:36.67,000 --> 0:04:37,000
antes de desertar.

85
0:04:39.08,000 --> 0:04:4,000
Y él nos dice:

86
0:04:41.8,000 --> 0:04:43,000
"Yo llegué a Siria lleno de esperanza,

87
0:04:44.363,000 --> 0:04:48,000
e inmediatamente, me confiscaron dos de mis preciadas posesiones:

88
0:04:48.752,000 --> 0:04:5,000
mi pasaporte y mi teléfono móvil".

89
0:04:52.14,000 --> 0:04:54,000
Los símbolos de su libertad física y digital

90
0:04:54.57,000 --> 0:04:55,000
le fueron arrebatados a su llegada.

91
0:04:57.248,000 --> 0:05:,000
Y así fue cómo me describió ese momento de pérdida.

92
0:05:01.356,000 --> 0:05:02,000
Dijo:

93
0:05:02.966,000 --> 0:05:04,000
"Sabes, es como en 'Tom y Jerry',

94
0:05:06.192,000 --> 0:05:09,000
cuando Jerry quiere escapar, y entonces Tom cierra la puerta

95
0:05:09.319,000 --> 0:05:1,000
y se traga la llave

96
0:05:10.499,000 --> 0:05:13,000
y la ve sobresaliendo por su garganta cuando baja".

97
0:05:14.416,000 --> 0:05:17,000
Y por supuesto, de veras, pude ver la imagen que estaba describiendo,

98
0:05:17.689,000 --> 0:05:2,000
y realmente conecté con la sensación que estaba tratando de transmitir,

99
0:05:21.308,000 --> 0:05:23,000
que era un sentimiento de condena,

100
0:05:23.353,000 --> 0:05:25,000
en el que saben que no hay escapatoria.

101
0:05:26.551,000 --> 0:05:27,000
Y me preguntaba:

102
0:05:28.604,000 --> 0:05:3,000
¿Qué podría haberle hecho cambiar de idea, si hubiese algo,

103
0:05:31.416,000 --> 0:05:32,000
el día que se fue de casa?

104
0:05:32.68,000 --> 0:05:33,000
Entonces pregunté,

105
0:05:33.888,000 --> 0:05:36,000
"Si supieras todo lo que sabes ahora

106
0:05:37.09,000 --> 0:05:39,000
--sobre el sufrimiento y la corrupción, la brutalidad--

107
0:05:39.985,000 --> 0:05:4,000
ese día que te fuiste de casa,

108
0:05:41.604,000 --> 0:05:42,000
¿aun así te habrías ido?

109
0:05:43.786,000 --> 0:05:44,000
Y él dijo: "Sí".

110
0:05:45.846,000 --> 0:05:47,000
Y pensé, "madre mía, dijo 'sí'".

111
0:05:48.694,000 --> 0:05:49,000
Y luego dijo:

112
0:05:49.937,000 --> 0:05:52,000
"En ese punto, tenía tan sorbido el seso,

113
0:05:52.962,000 --> 0:05:55,000
no pillaba ninguna información contradictoria.

114
0:05:56.744,000 --> 0:05:57,000
No podían haberme influenciado".

115
0:05:59.235,000 --> 0:06:01,000
"Bueno, ¿y si supieras todo lo que sabes ahora

116
0:06:01.786,000 --> 0:06:03,000
seis meses antes del día en que te fuiste?

117
0:06:05.345,000 --> 0:06:08,000
"En ese momento, creo que probablemente habría cambiado de idea".

118
0:06:10.138,000 --> 0:06:13,000
La radicalización no es esta opción de sí o no.

119
0:06:14.007,000 --> 0:06:16,000
Es un proceso, durante el cual la gente tiene preguntas...

120
0:06:17.008,000 --> 0:06:2,000
sobre ideología, religión, las condiciones de vida.

121
0:06:20.808,000 --> 0:06:22,000
Y entran en la red para obtener respuestas;

122
0:06:23.598,000 --> 0:06:24,000
es una oportunidad para conseguirlas.

123
0:06:25.905,000 --> 0:06:28,000
Y hay videos en la red de personas que tienen respuestas...

124
0:06:28.943,000 --> 0:06:3,000
desertores, por ejemplo, contando la historia de su viaje

125
0:06:31.843,000 --> 0:06:32,000
dentro y fuera de la violencia;

126
0:06:33.45,000 --> 0:06:36,000
historias como la de ese hombre que conocí en la prisión iraquí.

127
0:06:37.864,000 --> 0:06:39,000
Hay lugareños subiendo videos hechos con el móvil

128
0:06:40.534,000 --> 0:06:43,000
sobre cómo es la vida realmente en el califato bajo el mandato del ISIS.

129
0:06:44.055,000 --> 0:06:47,000
Hay clérigos que están compartiendo interpretaciones pacíficas del islam.

130
0:06:48.79,000 --> 0:06:49,000
¿Pero saben? Estas personas

131
0:06:50.2,000 --> 0:06:52,000
generalmente no tienen la destreza de comercialización del ISIS.

132
0:06:54.049,000 --> 0:06:58,000
Arriesgan sus vidas por hablar y enfrentar la propaganda terrorista,

133
0:06:58.605,000 --> 0:07:,000
y luego trágicamente no llegan a esas personas

134
0:07:00.84,000 --> 0:07:01,000
que más necesitan escucharles.

135
0:07:03.173,000 --> 0:07:05,000
Y nosotros quisimos ver si la tecnología podía cambiar eso.

136
0:07:06.205,000 --> 0:07:1,000
Así que, en 2016, nos asociamos con Moonshot CVE

137
0:07:10.412,000 --> 0:07:13,000
para pilotar un nuevo enfoque que contrarreste la radicalización

138
0:07:13.616,000 --> 0:07:15,000
llamado el "Método de Redireccionamiento".

139
0:07:16.453,000 --> 0:07:19,000
Utiliza el poder de la publicidad en red

140
0:07:19.489,000 --> 0:07:23,000
para cerrar la brecha entre esos susceptibles a los mensajes del ISIS

141
0:07:24.027,000 --> 0:07:27,000
y esas voces creíbles que están desacreditando ese mensaje.

142
0:07:28.633,000 --> 0:07:29,000
Y funciona así:

143
0:07:29.807,000 --> 0:07:3,000
alguien buscando material extremista

144
0:07:31.792,000 --> 0:07:33,000
--dicen que buscan: "¿Cómo me uno a ISIS?"--

145
0:07:34.806,000 --> 0:07:36,000
verá aparecer un anuncio

146
0:07:37.306,000 --> 0:07:41,000
que los invita a ver un video de YouTube de un clérigo, de un desertor...

147
0:07:42.212,000 --> 0:07:44,000
alguien que tiene una respuesta auténtica.

148
0:07:44.546,000 --> 0:07:47,000
Y ese foco no se basa en el perfil de quiénes son,

149
0:07:48.193,000 --> 0:07:51,000
sino en determinar algo que es directamente relevante

150
0:07:51.27,000 --> 0:07:52,000
con su consulta o pregunta.

151
0:07:54.122,000 --> 0:07:56,000
Durante nuestra prueba de ocho semanas en inglés y árabe,

152
0:07:56.988,000 --> 0:07:59,000
llegamos a más de 300 000 personas

153
0:08:00.291,000 --> 0:08:05,000
que habían expresado interés o simpatía hacia un grupo yihadista.

154
0:08:06.626,000 --> 0:08:08,000
Estas personas ahora estaban viendo videos

155
0:08:08.914,000 --> 0:08:11,000
que podrían prevenirlos de tomar decisiones devastadoras.

156
0:08:13.405,000 --> 0:08:16,000
Y como el extremismo violento no se limita a ningún idioma,

157
0:08:17.156,000 --> 0:08:18,000
religión o ideología,

158
0:08:18.984,000 --> 0:08:21,000
el método de redirección se está desplegando globalmente ahora

159
0:08:22.509,000 --> 0:08:25,000
para proteger a la gente de ser cortejada en la red por ideólogos violentos,

160
0:08:26.457,000 --> 0:08:28,000
ya sean islamistas, supremacistas blancos

161
0:08:28.957,000 --> 0:08:3,000
u otros extremistas violentos,

162
0:08:31.084,000 --> 0:08:33,000
con el objetivo de darles la oportunidad de escuchar a alguien

163
0:08:33.987,000 --> 0:08:35,000
desde el otro lado de ese viaje;

164
0:08:36.096,000 --> 0:08:38,000
para darles la oportunidad de elegir un camino diferente.

165
0:08:40.749,000 --> 0:08:45,000
Resulta que a menudo los malos son buenos explotando Internet,

166
0:08:46.753,000 --> 0:08:49,000
no porque sean genios tecnológicos,

167
0:08:50.52,000 --> 0:08:52,000
sino porque entienden lo que hace que la gente actúe.

168
0:08:54.855,000 --> 0:08:56,000
Quiero darles un segundo ejemplo:

169
0:08:58.019,000 --> 0:08:59,000
el acoso en línea.

170
0:09:00.629,000 --> 0:09:03,000
Los acosadores en línea también buscan descubrir lo que impactará

171
0:09:04.016,000 --> 0:09:05,000
en otro ser humano.

172
0:09:05.655,000 --> 0:09:08,000
Pero no para reclutarlos como lo hace ISIS,

173
0:09:08.789,000 --> 0:09:09,000
sino para causarles dolor.

174
0:09:11.259,000 --> 0:09:12,000
Imaginen esto:

175
0:09:13.347,000 --> 0:09:14,000
son mujeres,

176
0:09:15.03,000 --> 0:09:16,000
están casadas,

177
0:09:16.467,000 --> 0:09:17,000
tienen un niño.

178
0:09:18.834,000 --> 0:09:19,000
Publican algo en las redes sociales,

179
0:09:20.642,000 --> 0:09:22,000
y como respuesta, les dicen que serán violadas,

180
0:09:24.577,000 --> 0:09:25,000
que su hijo lo verá,

181
0:09:26.825,000 --> 0:09:27,000
detallan cuándo y dónde.

182
0:09:29.148,000 --> 0:09:32,000
De hecho, publican su dirección para que todos lo vean.

183
0:09:33.58,000 --> 0:09:35,000
Parece una amenaza bastante real.

184
0:09:37.113,000 --> 0:09:38,000
¿Creen que se irían a casa?

185
0:09:39.939,000 --> 0:09:41,000
¿Creen que seguirían haciendo lo que estaban haciendo?

186
0:09:42.461,000 --> 0:09:45,000
¿O continuarían haciendo eso que está irritando a tu atacante?

187
0:09:48.016,000 --> 0:09:51,000
El acoso en la red ha sido este arte perverso

188
0:09:51.136,000 --> 0:09:54,000
de descifrar qué hace enojar a la gente,

189
0:09:54.628,000 --> 0:09:56,000
qué hace que la gente tenga miedo,

190
0:09:56.704,000 --> 0:09:57,000
qué las hace sentir inseguras,

191
0:09:58.595,000 --> 0:10:01,000
y luego apretar esos puntos de presión hasta que sean silenciados.

192
0:10:02.333,000 --> 0:10:04,000
Cuando no se controla el acoso en la red,

193
0:10:04.661,000 --> 0:10:05,000
se reprime la libertad de expresión.

194
0:10:07.166,000 --> 0:10:09,000
E incluso las personas que moderan la conversación

195
0:10:09.533,000 --> 0:10:1,000
levantan sus brazos y las cortan,

196
0:10:11.205,000 --> 0:10:13,000
cerrando sus secciones de comentarios y foros.

197
0:10:14.186,000 --> 0:10:16,000
Eso significa que estamos perdiendo espacios en línea

198
0:10:17.059,000 --> 0:10:19,000
para reunirnos e intercambiar ideas.

199
0:10:19.939,000 --> 0:10:21,000
Y esos espacios en línea que perduran,

200
0:10:22.126,000 --> 0:10:23,000
se reducen a cámaras acústicas

201
0:10:23.962,000 --> 0:10:25,000
con personas que piensan igual que nosotros.

202
0:10:27.688,000 --> 0:10:29,000
Pero eso propicia la difusión de la desinformación;

203
0:10:30.211,000 --> 0:10:32,000
eso facilita la polarización.

204
0:10:34.508,000 --> 0:10:39,000
¿Qué pasa si en vez de eso, la tecnología pudiera incluir empatía en la balanza?

205
0:10:40.451,000 --> 0:10:42,000
Esta era la pregunta que motivó a nuestra asociación

206
0:10:42.887,000 --> 0:10:44,000
junto al equipo de Google "Contra el Abuso,"

207
0:10:45.09,000 --> 0:10:46,000
la Wikipedia

208
0:10:46.132,000 --> 0:10:47,000
y periódicos como el New York Times.

209
0:10:47.894,000 --> 0:10:5,000
Queríamos ver si podíamos construir modelos de aprendizaje automático

210
0:10:51.15,000 --> 0:10:54,000
que pudieran entender el impacto emocional del lenguaje.

211
0:10:55.062,000 --> 0:10:58,000
¿Podríamos predecir qué comentarios podrían hacer que otra persona dejara

212
0:10:58.696,000 --> 0:10:59,000
la conversación en la red?

213
0:11:00.515,000 --> 0:11:03,000
Y eso no es poca cosa.

214
0:11:04.426,000 --> 0:11:05,000
No es un logro trivial

215
0:11:06.016,000 --> 0:11:08,000
para la IA conseguir hacer algo así.

216
0:11:08.603,000 --> 0:11:11,000
Es decir, solo consideren estos dos ejemplos de mensajes

217
0:11:12.356,000 --> 0:11:14,000
que podrían haberme enviado la semana pasada.

218
0:11:15.517,000 --> 0:11:16,000
"¡Suerte: Que la rompas en TED!".

219
0:11:17.42,000 --> 0:11:18,000
... y

220
0:11:18.608,000 --> 0:11:2,000
"Te romperemos en TED".

221
0:11:20.758,000 --> 0:11:21,000
(Risas)

222
0:11:22.028,000 --> 0:11:23,000
Son humanos,

223
0:11:23.565,000 --> 0:11:25,000
Por eso ven obvia la diferencia,

224
0:11:25.749,000 --> 0:11:27,000
a pesar de que las palabras son más o menos las mismas.

225
0:11:28.353,000 --> 0:11:29,000
Pero para la IA, lleva algo de práctica

226
0:11:30.26,000 --> 0:11:32,000
enseñar los modelos que reconocen esa diferencia.

227
0:11:32.745,000 --> 0:11:35,000
La belleza de construir una IA que pueda notar la diferencia

228
0:11:36.014,000 --> 0:11:37,000
es que la IA pueda modificar la escala

229
0:11:37.878,000 --> 0:11:4,000
al tamaño del fenómeno de la toxicidad en línea,

230
0:11:40.928,000 --> 0:11:43,000
y ese fue nuestro objetivo al construir nuestra tecnología llamada Perspectiva.

231
0:11:45.056,000 --> 0:11:46,000
Con la ayuda de Perspectiva,

232
0:11:46.507,000 --> 0:11:47,000
el New York Times, por ejemplo,

233
0:11:48.114,000 --> 0:11:5,000
ha aumentado los espacios en la red para la conversación.

234
0:11:50.915,000 --> 0:11:51,000
Antes de nuestra colaboración,

235
0:11:52.345,000 --> 0:11:56,000
tenían comentarios habilitados en solo el 10 % de sus artículos.

236
0:11:57.495,000 --> 0:12:,000
Con ayuda del aprendizaje automático, ha subido a un 30 %.

237
0:12:01.089,000 --> 0:12:04,000
Así que lo han triplicado, y apenas estamos comenzando.

238
0:12:04.868,000 --> 0:12:07,000
Pero esto va más allá de hacer que los moderadores sean más eficientes.

239
0:12:10.076,000 --> 0:12:11,000
En este momento puedo verles,

240
0:12:11.95,000 --> 0:12:14,000
y puedo medir el impacto que tienen mis palabras en Uds.

241
0:12:16.37,000 --> 0:12:17,000
No tienen esa oportunidad en la red.

242
0:12:18.558,000 --> 0:12:21,000
Imagínense si el aprendizaje automático pudiera darles a los comentaristas,

243
0:12:22.217,000 --> 0:12:23,000
mientras escriben,

244
0:12:23.403,000 --> 0:12:26,000
una retroalimentación en tiempo real sobre cómo podrían impactar sus palabras,

245
0:12:27.609,000 --> 0:12:3,000
igual que hacen las expresiones faciales en una conversación cara a cara.

246
0:12:32.926,000 --> 0:12:34,000
El aprendizaje automático no es perfecto,

247
0:12:34.958,000 --> 0:12:36,000
y todavía comete muchos errores.

248
0:12:37.21,000 --> 0:12:38,000
Pero si podemos construir tecnologías

249
0:12:39.047,000 --> 0:12:42,000
que entiendan el impacto emocional del lenguaje,

250
0:12:42.108,000 --> 0:12:43,000
podemos construir empatía.

251
0:12:43.592,000 --> 0:12:45,000
Eso significa que podemos mantener diálogos

252
0:12:46.041,000 --> 0:12:47,000
con diferentes políticas,

253
0:12:47.681,000 --> 0:12:48,000
diferentes visiones del mundo,

254
0:12:49.277,000 --> 0:12:5,000
diferentes valores.

255
0:12:51.359,000 --> 0:12:53,000
Y podemos revitalizar los espacios en línea

256
0:12:53.845,000 --> 0:12:55,000
que la mayoría de nosotros hemos abandonado.

257
0:12:57.857,000 --> 0:13:,000
Cuando usan la tecnología para explotar y dañar a otros,

258
0:13:01.666,000 --> 0:13:04,000
se están aprovechando de nuestros miedos y vulnerabilidades como seres humanos.

259
0:13:06.461,000 --> 0:13:09,000
Si alguna vez pensamos que podríamos construir una Internet

260
0:13:09.993,000 --> 0:13:11,000
aislada del lado oscuro de la humanidad,

261
0:13:12.595,000 --> 0:13:13,000
estábamos equivocados.

262
0:13:14.361,000 --> 0:13:16,000
Si hoy queremos construir una tecnología

263
0:13:16.655,000 --> 0:13:19,000
que pueda superar los desafíos que enfrentamos,

264
0:13:19.806,000 --> 0:13:23,000
tenemos que dedicarnos completamente a comprender los problemas

265
0:13:23.873,000 --> 0:13:24,000
y a construir soluciones

266
0:13:25.79,000 --> 0:13:28,000
que sean tan humanas como los problemas que pretenden resolver.

267
0:13:30.071,000 --> 0:13:31,000
Hagámoslo posible.

268
0:13:31.924,000 --> 0:13:32,000
Gracias.

269
0:13:33.098,000 --> 0:13:36,000
(Aplausos)

