1
0:00:15.651,000 --> 0:00:17,000
We are today talking about moral persuasion:

2
0:00:18.261,000 --> 0:00:21,000
What is moral and immoral in trying to change people's behaviors

3
0:00:22.267,000 --> 0:00:24,000
by using technology and using design?

4
0:00:24.744,000 --> 0:00:25,000
And I don't know what you expect,

5
0:00:26.6,000 --> 0:00:27,000
but when I was thinking about that issue,

6
0:00:28.577,000 --> 0:00:32,000
I early on realized what I'm not able to give you are answers.

7
0:00:33.203,000 --> 0:00:35,000
I'm not able to tell you what is moral or immoral,

8
0:00:35.998,000 --> 0:00:37,000
because we're living in a pluralist society.

9
0:00:38.569,000 --> 0:00:42,000
My values can be radically different from your values,

10
0:00:42.835,000 --> 0:00:45,000
which means that what I consider moral or immoral based on that

11
0:00:46.036,000 --> 0:00:49,000
might not necessarily be what you consider moral or immoral.

12
0:00:50.029,000 --> 0:00:53,000
But I also realized there is one thing that I could give you,

13
0:00:53.267,000 --> 0:00:55,000
and that is what this guy behind me gave the world --

14
0:00:55.824,000 --> 0:00:56,000
Socrates.

15
0:00:56.998,000 --> 0:00:57,000
It is questions.

16
0:00:58.417,000 --> 0:01:,000
What I can do and what I would like to do with you

17
0:01:01.074,000 --> 0:01:02,000
is give you, like that initial question,

18
0:01:03.043,000 --> 0:01:06,000
a set of questions to figure out for yourselves,

19
0:01:06.477,000 --> 0:01:09,000
layer by layer, like peeling an onion,

20
0:01:10.142,000 --> 0:01:15,000
getting at the core of what you believe is moral or immoral persuasion.

21
0:01:15.559,000 --> 0:01:19,000
And I'd like to do that with a couple of examples of technologies

22
0:01:19.624,000 --> 0:01:23,000
where people have used game elements to get people to do things.

23
0:01:25.343,000 --> 0:01:28,000
So it's at first a very simple, very obvious question

24
0:01:28.407,000 --> 0:01:29,000
I would like to give you:

25
0:01:29.627,000 --> 0:01:31,000
What are your intentions if you are designing something?

26
0:01:32.668,000 --> 0:01:35,000
And obviously, intentions are not the only thing,

27
0:01:36.065,000 --> 0:01:39,000
so here is another example for one of these applications.

28
0:01:39.267,000 --> 0:01:42,000
There are a couple of these kinds of Eco dashboards right now --

29
0:01:42.378,000 --> 0:01:43,000
dashboards built into cars --

30
0:01:43.862,000 --> 0:01:45,000
which try to motivate you to drive more fuel-efficiently.

31
0:01:46.718,000 --> 0:01:47,000
This here is Nissan's MyLeaf,

32
0:01:48.515,000 --> 0:01:51,000
where your driving behavior is compared with the driving behavior

33
0:01:51.603,000 --> 0:01:52,000
of other people,

34
0:01:52.778,000 --> 0:01:55,000
so you can compete for who drives a route the most fuel-efficiently.

35
0:01:56.079,000 --> 0:01:58,000
And these things are very effective, it turns out --

36
0:01:58.58,000 --> 0:02:01,000
so effective that they motivate people to engage in unsafe driving behaviors,

37
0:02:02.543,000 --> 0:02:03,000
like not stopping at a red light,

38
0:02:04.329,000 --> 0:02:06,000
because that way you have to stop and restart the engine,

39
0:02:07.068,000 --> 0:02:09,000
and that would use quite some fuel, wouldn't it?

40
0:02:10.338,000 --> 0:02:14,000
So despite this being a very well-intended application,

41
0:02:14.771,000 --> 0:02:16,000
obviously there was a side effect of that.

42
0:02:17.17,000 --> 0:02:19,000
Here's another example for one of these side effects.

43
0:02:19.742,000 --> 0:02:23,000
Commendable: a site that allows parents to give their kids little badges

44
0:02:24.55,000 --> 0:02:26,000
for doing the things that parents want their kids to do,

45
0:02:27.232,000 --> 0:02:28,000
like tying their shoes.

46
0:02:28.572,000 --> 0:02:3,000
And at first that sounds very nice,

47
0:02:30.84,000 --> 0:02:32,000
very benign, well-intended.

48
0:02:33.014,000 --> 0:02:36,000
But it turns out, if you look into research on people's mindset,

49
0:02:36.808,000 --> 0:02:37,000
caring about outcomes,

50
0:02:38.319,000 --> 0:02:39,000
caring about public recognition,

51
0:02:40.116,000 --> 0:02:43,000
caring about these kinds of public tokens of recognition

52
0:02:44.001,000 --> 0:02:45,000
is not necessarily very helpful

53
0:02:46.019,000 --> 0:02:48,000
for your long-term psychological well-being.

54
0:02:48.35,000 --> 0:02:5,000
It's better if you care about learning something.

55
0:02:51.05,000 --> 0:02:52,000
It's better when you care about yourself

56
0:02:52.979,000 --> 0:02:54,000
than how you appear in front of other people.

57
0:02:56.021,000 --> 0:03:01,000
So that kind of motivational tool that is used actually, in and of itself,

58
0:03:01.086,000 --> 0:03:02,000
has a long-term side effect,

59
0:03:03.018,000 --> 0:03:04,000
in that every time we use a technology

60
0:03:04.852,000 --> 0:03:07,000
that uses something like public recognition or status,

61
0:03:08.05,000 --> 0:03:1,000
we're actually positively endorsing this

62
0:03:10.45,000 --> 0:03:13,000
as a good and normal thing to care about --

63
0:03:13.884,000 --> 0:03:15,000
that way, possibly having a detrimental effect

64
0:03:16.772,000 --> 0:03:19,000
on the long-term psychological well-being of ourselves as a culture.

65
0:03:20.651,000 --> 0:03:22,000
So that's a second, very obvious question:

66
0:03:23.319,000 --> 0:03:25,000
What are the effects of what you're doing --

67
0:03:25.654,000 --> 0:03:29,000
the effects you're having with the device, like less fuel,

68
0:03:29.802,000 --> 0:03:31,000
as well as the effects of the actual tools you're using

69
0:03:32.531,000 --> 0:03:33,000
to get people to do things --

70
0:03:34.229,000 --> 0:03:35,000
public recognition?

71
0:03:35.824,000 --> 0:03:37,000
Now is that all -- intention, effect?

72
0:03:38.769,000 --> 0:03:41,000
Well, there are some technologies which obviously combine both.

73
0:03:41.927,000 --> 0:03:43,000
Both good long-term and short-term effects

74
0:03:44.738,000 --> 0:03:46,000
and a positive intention like Fred Stutzman's "Freedom,"

75
0:03:47.571,000 --> 0:03:49,000
where the whole point of that application is --

76
0:03:49.802,000 --> 0:03:52,000
well, we're usually so bombarded with constant requests by other people,

77
0:03:53.551,000 --> 0:03:54,000
with this device,

78
0:03:54.731,000 --> 0:03:57,000
you can shut off the Internet connectivity of your PC of choice

79
0:03:58.235,000 --> 0:03:59,000
for a pre-set amount of time,

80
0:03:59.707,000 --> 0:04:,000
to actually get some work done.

81
0:04:01.702,000 --> 0:04:04,000
And I think most of us will agree that's something well-intended,

82
0:04:04.877,000 --> 0:04:06,000
and also has good consequences.

83
0:04:07.121,000 --> 0:04:08,000
In the words of Michel Foucault,

84
0:04:08.791,000 --> 0:04:09,000
it is a "technology of the self."

85
0:04:10.755,000 --> 0:04:12,000
It is a technology that empowers the individual

86
0:04:13.616,000 --> 0:04:14,000
to determine its own life course,

87
0:04:15.455,000 --> 0:04:16,000
to shape itself.

88
0:04:17.41,000 --> 0:04:19,000
But the problem is, as Foucault points out,

89
0:04:20.418,000 --> 0:04:21,000
that every technology of the self

90
0:04:22.227,000 --> 0:04:25,000
has a technology of domination as its flip side.

91
0:04:25.696,000 --> 0:04:29,000
As you see in today's modern liberal democracies,

92
0:04:30.323,000 --> 0:04:34,000
the society, the state, not only allows us to determine our self,

93
0:04:35.03,000 --> 0:04:36,000
to shape our self,

94
0:04:36.205,000 --> 0:04:37,000
it also demands it of us.

95
0:04:38.22,000 --> 0:04:39,000
It demands that we optimize ourselves,

96
0:04:40.205,000 --> 0:04:41,000
that we control ourselves,

97
0:04:42.069,000 --> 0:04:44,000
that we self-manage continuously,

98
0:04:44.804,000 --> 0:04:47,000
because that's the only way in which such a liberal society works.

99
0:04:48.721,000 --> 0:04:52,000
These technologies want us to stay in the game

100
0:04:53.013,000 --> 0:04:55,000
that society has devised for us.

101
0:04:55.81,000 --> 0:04:57,000
They want us to fit in even better.

102
0:04:58.121,000 --> 0:05:,000
They want us to optimize ourselves to fit in.

103
0:05:01.628,000 --> 0:05:04,000
Now, I don't say that is necessarily a bad thing;

104
0:05:05.293,000 --> 0:05:09,000
I just think that this example points us to a general realization,

105
0:05:09.645,000 --> 0:05:12,000
and that is: no matter what technology or design you look at,

106
0:05:13.472,000 --> 0:05:16,000
even something we consider as well-intended

107
0:05:16.517,000 --> 0:05:18,000
and as good in its effects as Stutzman's Freedom,

108
0:05:19.507,000 --> 0:05:21,000
comes with certain values embedded in it.

109
0:05:22.238,000 --> 0:05:23,000
And we can question these values.

110
0:05:24.198,000 --> 0:05:25,000
We can question: Is it a good thing

111
0:05:26.166,000 --> 0:05:29,000
that all of us continuously self-optimize ourselves

112
0:05:29.674,000 --> 0:05:31,000
to fit better into that society?

113
0:05:31.709,000 --> 0:05:32,000
Or to give you another example:

114
0:05:33.225,000 --> 0:05:35,000
What about a piece of persuasive technology

115
0:05:35.725,000 --> 0:05:38,000
that convinces Muslim women to wear their headscarves?

116
0:05:38.94,000 --> 0:05:4,000
Is that a good or a bad technology

117
0:05:41.016,000 --> 0:05:43,000
in its intentions or in its effects?

118
0:05:43.603,000 --> 0:05:47,000
Well, that basically depends on the kind of values you bring to bear

119
0:05:47.881,000 --> 0:05:49,000
to make these kinds of judgments.

120
0:05:50.142,000 --> 0:05:51,000
So that's a third question:

121
0:05:51.694,000 --> 0:05:52,000
What values do you use to judge?

122
0:05:53.848,000 --> 0:05:54,000
And speaking of values:

123
0:05:55.213,000 --> 0:05:58,000
I've noticed that in the discussion about moral persuasion online

124
0:05:58.593,000 --> 0:05:59,000
and when I'm talking with people,

125
0:06:00.254,000 --> 0:06:02,000
more often than not, there is a weird bias.

126
0:06:03.463,000 --> 0:06:05,000
And that bias is that we're asking:

127
0:06:06.383,000 --> 0:06:08,000
Is this or that "still" ethical?

128
0:06:09.22,000 --> 0:06:11,000
Is it "still" permissible?

129
0:06:11.894,000 --> 0:06:12,000
We're asking things like:

130
0:06:13.116,000 --> 0:06:15,000
Is this Oxfam donation form,

131
0:06:15.329,000 --> 0:06:18,000
where the regular monthly donation is the preset default,

132
0:06:18.401,000 --> 0:06:2,000
and people, maybe without intending it,

133
0:06:20.504,000 --> 0:06:23,000
are encouraged or nudged into giving a regular donation

134
0:06:24.34,000 --> 0:06:25,000
instead of a one-time donation,

135
0:06:25.853,000 --> 0:06:26,000
is that "still' permissible?

136
0:06:27.22,000 --> 0:06:28,000
Is it "still" ethical?

137
0:06:28.607,000 --> 0:06:29,000
We're fishing at the low end.

138
0:06:30.879,000 --> 0:06:32,000
But in fact, that question, "Is it 'still' ethical?"

139
0:06:33.377,000 --> 0:06:34,000
is just one way of looking at ethics.

140
0:06:35.182,000 --> 0:06:39,000
Because if you look at the beginning of ethics in Western culture,

141
0:06:40.098,000 --> 0:06:43,000
you see a very different idea of what ethics also could be.

142
0:06:43.97,000 --> 0:06:46,000
For Aristotle, ethics was not about the question,

143
0:06:47.991,000 --> 0:06:49,000
"Is that still good, or is it bad?"

144
0:06:50.287,000 --> 0:06:53,000
Ethics was about the question of how to live life well.

145
0:06:54.216,000 --> 0:06:56,000
And he put that in the word "arête,"

146
0:06:56.421,000 --> 0:06:58,000
which we, from [Ancient Greek], translate as "virtue."

147
0:06:59.2,000 --> 0:07:,000
But really, it means "excellence."

148
0:07:00.864,000 --> 0:07:05,000
It means living up to your own full potential as a human being.

149
0:07:06.937,000 --> 0:07:07,000
And that is an idea that, I think,

150
0:07:08.617,000 --> 0:07:1,000
Paul Richard Buchanan put nicely in a recent essay,

151
0:07:11.338,000 --> 0:07:13,000
where he said, "Products are vivid arguments

152
0:07:13.505,000 --> 0:07:15,000
about how we should live our lives."

153
0:07:16.086,000 --> 0:07:18,000
Our designs are not ethical or unethical

154
0:07:18.687,000 --> 0:07:22,000
in that they're using ethical or unethical means of persuading us.

155
0:07:23.661,000 --> 0:07:24,000
They have a moral component

156
0:07:25.255,000 --> 0:07:29,000
just in the kind of vision and the aspiration of the good life

157
0:07:29.506,000 --> 0:07:3,000
that they present to us.

158
0:07:31.441,000 --> 0:07:34,000
And if you look into the designed environment around us

159
0:07:34.968,000 --> 0:07:35,000
with that kind of lens,

160
0:07:36.164,000 --> 0:07:38,000
asking, "What is the vision of the good life

161
0:07:38.641,000 --> 0:07:4,000
that our products, our design, present to us?",

162
0:07:41.403,000 --> 0:07:43,000
then you often get the shivers,

163
0:07:43.703,000 --> 0:07:45,000
because of how little we expect of each other,

164
0:07:46.055,000 --> 0:07:49,000
of how little we actually seem to expect of our life,

165
0:07:49.969,000 --> 0:07:51,000
and what the good life looks like.

166
0:07:53.11,000 --> 0:07:56,000
So that's a fourth question I'd like to leave you with:

167
0:07:56.157,000 --> 0:08:,000
What vision of the good life do your designs convey?

168
0:08:01.249,000 --> 0:08:02,000
And speaking of design,

169
0:08:02.645,000 --> 0:08:06,000
you'll notice that I already broadened the discussion,

170
0:08:06.859,000 --> 0:08:1,000
because it's not just persuasive technology that we're talking about here,

171
0:08:11.327,000 --> 0:08:15,000
it's any piece of design that we put out here in the world.

172
0:08:15.428,000 --> 0:08:16,000
I don't know whether you know

173
0:08:16.852,000 --> 0:08:19,000
the great communication researcher Paul Watzlawick who, back in the '60s,

174
0:08:20.431,000 --> 0:08:22,000
made the argument that we cannot not communicate.

175
0:08:22.966,000 --> 0:08:24,000
Even if we choose to be silent, we chose to be silent,

176
0:08:25.591,000 --> 0:08:27,000
and we're communicating something by choosing to be silent.

177
0:08:28.571,000 --> 0:08:3,000
And in the same way that we cannot not communicate,

178
0:08:31.33,000 --> 0:08:32,000
we cannot not persuade:

179
0:08:32.885,000 --> 0:08:34,000
whatever we do or refrain from doing,

180
0:08:34.92,000 --> 0:08:38,000
whatever we put out there as a piece of design, into the world,

181
0:08:39.309,000 --> 0:08:41,000
has a persuasive component.

182
0:08:41.38,000 --> 0:08:42,000
It tries to affect people.

183
0:08:43.277,000 --> 0:08:46,000
It puts a certain vision of the good life out there in front of us,

184
0:08:47.048,000 --> 0:08:48,000
which is what Peter-Paul Verbeek,

185
0:08:48.697,000 --> 0:08:5,000
the Dutch philosopher of technology, says.

186
0:08:51.437,000 --> 0:08:54,000
No matter whether we as designers intend it or not,

187
0:08:55.409,000 --> 0:08:57,000
we materialize morality.

188
0:08:57.575,000 --> 0:08:59,000
We make certain things harder and easier to do.

189
0:09:00.402,000 --> 0:09:02,000
We organize the existence of people.

190
0:09:02.637,000 --> 0:09:03,000
We put a certain vision

191
0:09:03.812,000 --> 0:09:06,000
of what good or bad or normal or usual is

192
0:09:07.24,000 --> 0:09:08,000
in front of people,

193
0:09:08.415,000 --> 0:09:1,000
by everything we put out there in the world.

194
0:09:11.247,000 --> 0:09:14,000
Even something as innocuous as a set of school chairs

195
0:09:14.357,000 --> 0:09:16,000
is a persuasive technology,

196
0:09:16.428,000 --> 0:09:2,000
because it presents and materializes a certain vision of the good life --

197
0:09:21.142,000 --> 0:09:23,000
a good life in which teaching and learning and listening

198
0:09:24.023,000 --> 0:09:27,000
is about one person teaching, the others listening;

199
0:09:27.146,000 --> 0:09:31,000
in which it is about learning-is-done-while-sitting;

200
0:09:31.223,000 --> 0:09:32,000
in which you learn for yourself;

201
0:09:32.842,000 --> 0:09:34,000
in which you're not supposed to change these rules,

202
0:09:35.286,000 --> 0:09:37,000
because the chairs are fixed to the ground.

203
0:09:38.888,000 --> 0:09:4,000
And even something as innocuous as a single-design chair,

204
0:09:41.823,000 --> 0:09:42,000
like this one by Arne Jacobsen,

205
0:09:43.419,000 --> 0:09:44,000
is a persuasive technology,

206
0:09:45.219,000 --> 0:09:48,000
because, again, it communicates an idea of the good life:

207
0:09:48.735,000 --> 0:09:52,000
a good life -- a life that you, as a designer, consent to by saying,

208
0:09:53.684,000 --> 0:09:56,000
"In a good life, goods are produced as sustainably or unsustainably

209
0:09:57.215,000 --> 0:09:58,000
as this chair.

210
0:09:58.85,000 --> 0:09:59,000
Workers are treated as well or as badly

211
0:10:00.851,000 --> 0:10:02,000
as the workers were treated that built that chair."

212
0:10:03.762,000 --> 0:10:05,000
The good life is a life where design is important

213
0:10:06.087,000 --> 0:10:08,000
because somebody obviously took the time and spent the money

214
0:10:09.032,000 --> 0:10:1,000
for that kind of well-designed chair;

215
0:10:10.84,000 --> 0:10:11,000
where tradition is important,

216
0:10:12.268,000 --> 0:10:15,000
because this is a traditional classic and someone cared about this;

217
0:10:15.458,000 --> 0:10:17,000
and where there is something as conspicuous consumption,

218
0:10:18.172,000 --> 0:10:2,000
where it is OK and normal to spend a humongous amount of money

219
0:10:21.152,000 --> 0:10:22,000
on such a chair,

220
0:10:22.327,000 --> 0:10:24,000
to signal to other people what your social status is.

221
0:10:24.924,000 --> 0:10:27,000
So these are the kinds of layers, the kinds of questions

222
0:10:28.265,000 --> 0:10:29,000
I wanted to lead you through today;

223
0:10:30.259,000 --> 0:10:33,000
the question of: What are the intentions that you bring to bear

224
0:10:33.317,000 --> 0:10:34,000
when you're designing something?

225
0:10:34.901,000 --> 0:10:37,000
What are the effects, intended and unintended, that you're having?

226
0:10:38.177,000 --> 0:10:4,000
What are the values you're using to judge those?

227
0:10:41.002,000 --> 0:10:42,000
What are the virtues, the aspirations

228
0:10:42.999,000 --> 0:10:44,000
that you're actually expressing in that?

229
0:10:45.4,000 --> 0:10:46,000
And how does that apply,

230
0:10:47.281,000 --> 0:10:48,000
not just to persuasive technology,

231
0:10:49.291,000 --> 0:10:51,000
but to everything you design?

232
0:10:51.912,000 --> 0:10:52,000
Do we stop there?

233
0:10:53.815,000 --> 0:10:54,000
I don't think so.

234
0:10:55.291,000 --> 0:10:59,000
I think that all of these things are eventually informed

235
0:10:59.753,000 --> 0:11:,000
by the core of all of this,

236
0:11:01.2,000 --> 0:11:04,000
and this is nothing but life itself.

237
0:11:04.594,000 --> 0:11:06,000
Why, when the question of what the good life is

238
0:11:07.335,000 --> 0:11:09,000
informs everything that we design,

239
0:11:09.698,000 --> 0:11:11,000
should we stop at design and not ask ourselves:

240
0:11:12.516,000 --> 0:11:13,000
How does it apply to our own life?

241
0:11:14.881,000 --> 0:11:16,000
"Why should the lamp or the house be an art object,

242
0:11:17.617,000 --> 0:11:18,000
but not our life?"

243
0:11:18.817,000 --> 0:11:19,000
as Michel Foucault puts it.

244
0:11:20.696,000 --> 0:11:23,000
Just to give you a practical example of Buster Benson.

245
0:11:24.331,000 --> 0:11:26,000
This is Buster setting up a pull-up machine

246
0:11:26.613,000 --> 0:11:28,000
at the office of his new start-up, Habit Labs,

247
0:11:29.249,000 --> 0:11:32,000
where they're trying to build other applications like "Health Month"

248
0:11:32.488,000 --> 0:11:33,000
for people.

249
0:11:33.67,000 --> 0:11:34,000
And why is he building a thing like this?

250
0:11:35.656,000 --> 0:11:37,000
Well, here is the set of axioms

251
0:11:37.713,000 --> 0:11:4,000
that Habit Labs, Buster's start-up, put up for themselves

252
0:11:41.135,000 --> 0:11:43,000
on how they wanted to work together as a team

253
0:11:43.864,000 --> 0:11:45,000
when they're building these applications --

254
0:11:45.917,000 --> 0:11:47,000
a set of moral principles they set themselves

255
0:11:48.13,000 --> 0:11:49,000
for working together --

256
0:11:49.515,000 --> 0:11:5,000
one of them being,

257
0:11:50.777,000 --> 0:11:53,000
"We take care of our own health and manage our own burnout."

258
0:11:54.221,000 --> 0:11:57,000
Because ultimately, how can you ask yourselves

259
0:11:57.745,000 --> 0:12:,000
and how can you find an answer on what vision of the good life

260
0:12:01.699,000 --> 0:12:04,000
you want to convey and create with your designs

261
0:12:04.892,000 --> 0:12:05,000
without asking the question:

262
0:12:06.646,000 --> 0:12:09,000
What vision of the good life do you yourself want to live?

263
0:12:11.018,000 --> 0:12:12,000
And with that,

264
0:12:12.945,000 --> 0:12:13,000
I thank you.

265
0:12:14.381,000 --> 0:12:18,000
(Applause)

