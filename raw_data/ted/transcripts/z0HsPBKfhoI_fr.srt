1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Fatima Zahra El Hafa

2
0:00:12.58,000 --> 0:00:15,000
Quand j'étais enfant, j'étais l'exemple parfait de l'intello.

3
0:00:17.14,000 --> 0:00:19,000
Certains d'entre vous devaient l'être aussi.

4
0:00:19.34,000 --> 0:00:2,000
(Rires)

5
0:00:20.58,000 --> 0:00:23,000
Et vous, monsieur, qui avez ri le plus fort, devez encore l'être.

6
0:00:23.82,000 --> 0:00:25,000
(Rires)

7
0:00:26.1,000 --> 0:00:29,000
J'ai grandi dans une petite ville des plaines poussiéreuses du Texas,

8
0:00:29.62,000 --> 0:00:32,000
fils d'un shérif qui était fils de pasteur.

9
0:00:32.98,000 --> 0:00:33,000
Hors de question de m'attirer des ennuis.

10
0:00:35.86,000 --> 0:00:38,000
J'ai donc commencé à lire des livres de calcul pour le plaisir.

11
0:00:39.14,000 --> 0:00:4,000
(Rires)

12
0:00:40.7,000 --> 0:00:41,000
Vous aussi.

13
0:00:42.42,000 --> 0:00:45,000
Cela m'a mené à créer un laser, un ordinateur et des micro fusées

14
0:00:46.18,000 --> 0:00:49,000
et à faire du carburant pour les fusées dans ma chambre.

15
0:00:49.78,000 --> 0:00:52,000
En termes scientifiques,

16
0:00:53.46,000 --> 0:00:56,000
cela s'appelle une très mauvaise idée.

17
0:00:56.74,000 --> 0:00:57,000
(Rires)

18
0:00:57.98,000 --> 0:00:59,000
A peu près au même moment,

19
0:01:00.18,000 --> 0:01:03,000
« 2001 : l'odyssée de l'espace » de Stanley Kubrick est sorti

20
0:01:03.42,000 --> 0:01:05,000
et cela a changé ma vie.

21
0:01:06.1,000 --> 0:01:08,000
J'aimais tout dans ce film,

22
0:01:08.18,000 --> 0:01:1,000
en particulier HAL 9000.

23
0:01:10.74,000 --> 0:01:12,000
HAL était un ordinateur sensible

24
0:01:12.82,000 --> 0:01:14,000
conçu pour guider Discovery, le vaisseau spatial,

25
0:01:15.3,000 --> 0:01:17,000
de la Terre à Jupiter.

26
0:01:17.86,000 --> 0:01:19,000
HAL était aussi un personnage imparfait

27
0:01:19.94,000 --> 0:01:2,000
puisqu'à la fin, il choisissait

28
0:01:21.66,000 --> 0:01:24,000
la valeur de la mission plutôt que la vie humaine.

29
0:01:24.66,000 --> 0:01:26,000
HAL était un personnage de fiction

30
0:01:26.78,000 --> 0:01:28,000
mais il s'adresse malgré tout à nos peurs,

31
0:01:29.46,000 --> 0:01:31,000
nos peurs d'être assujettis

32
0:01:31.58,000 --> 0:01:34,000
par une intelligence artificielle sans émotions

33
0:01:34.62,000 --> 0:01:35,000
qui est indifférente à notre humanité.

34
0:01:37.7,000 --> 0:01:39,000
Je crois que de telles peurs sont infondées.

35
0:01:40.3,000 --> 0:01:42,000
En fait, nous vivons un moment remarquable

36
0:01:43.02,000 --> 0:01:44,000
dans l'histoire de l'humanité

37
0:01:44.58,000 --> 0:01:48,000
où, guidés par notre refus d'accepter les limites de nos corps et esprit,

38
0:01:49.58,000 --> 0:01:5,000
nous construisons des machines

39
0:01:51.3,000 --> 0:01:54,000
d'une complexité et d'une grâce exquises et magnifiques

40
0:01:54.94,000 --> 0:01:56,000
qui étendront l'expérience humaine

41
0:01:57.02,000 --> 0:01:58,000
bien au-delà de notre imagination.

42
0:01:59.54,000 --> 0:02:01,000
Après une carrière qui m'a mené de l'Air Force Academy

43
0:02:02.14,000 --> 0:02:03,000
au Space Command aujourd'hui,

44
0:02:04.1,000 --> 0:02:05,000
je suis devenu ingénieur de systèmes

45
0:02:05.826,000 --> 0:02:07,000
et j'ai récemment participé à un problème d'ingénierie

46
0:02:08.58,000 --> 0:02:1,000
associé à la mission sur Mars de la NASA.

47
0:02:11.18,000 --> 0:02:13,000
Pour les vols dans l'espace jusqu'à la Lune,

48
0:02:13.7,000 --> 0:02:16,000
nous comptons sur le centre de contrôle de mission de Houston

49
0:02:16.86,000 --> 0:02:17,000
pour surveiller tous les aspects du vol.

50
0:02:18.86,000 --> 0:02:21,000
Cependant, Mars est à une distance 200 fois plus importante

51
0:02:22.42,000 --> 0:02:25,000
et le résultat est qu'il faut en moyenne 13 minutes

52
0:02:25.66,000 --> 0:02:28,000
pour qu'un signal voyage de la Terre jusqu'à Mars.

53
0:02:28.82,000 --> 0:02:31,000
S'il y a un problème, c'est beaucoup trop long.

54
0:02:32.66,000 --> 0:02:34,000
Une solution d'ingénierie raisonnable

55
0:02:35.18,000 --> 0:02:37,000
nous a poussés à placer le centre de contrôle de mission

56
0:02:37.816,000 --> 0:02:39,000
entre les murs de l'engin spatial Orion.

57
0:02:40.82,000 --> 0:02:42,000
Une autre idée fascinante dans le profil de la mission

58
0:02:43.74,000 --> 0:02:45,000
place les robots humanoïdes sur la surface de Mars

59
0:02:46.66,000 --> 0:02:47,000
avant l'arrivée des humains,

60
0:02:48.54,000 --> 0:02:49,000
pour construire des infrastructures

61
0:02:50.226,000 --> 0:02:53,000
puis servir de membres collaboratifs à l'équipe scientifique.

62
0:02:55.22,000 --> 0:02:57,000
En observant cela d'un point de vue d'ingénieur,

63
0:02:57.98,000 --> 0:03:,000
il m'est clairement apparu que je devais concevoir

64
0:03:01.18,000 --> 0:03:03,000
une intelligence artificielle intelligente, collaborative

65
0:03:03.976,000 --> 0:03:04,000
et socialement intelligente.

66
0:03:05.78,000 --> 0:03:09,000
En d'autres mots, je devais créer quelque chose ressemblant à HAL

67
0:03:10.1,000 --> 0:03:12,000
mais sans tendances meurtrières.

68
0:03:12.54,000 --> 0:03:13,000
(Rires)

69
0:03:14.74,000 --> 0:03:15,000
Marquons un instant de pause.

70
0:03:16.58,000 --> 0:03:19,000
Est-il réellement possible de créer une telle intelligence artificielle ?

71
0:03:20.5,000 --> 0:03:21,000
C'est possible.

72
0:03:21.98,000 --> 0:03:22,000
De bien des façons,

73
0:03:23.26,000 --> 0:03:24,000
c'est un problème d'ingénierie complexe

74
0:03:25.26,000 --> 0:03:26,000
avec un peu d'IA,

75
0:03:26.74,000 --> 0:03:3,000
non pas un problème inextricable d'AI qui nécessite de l'ingénierie.

76
0:03:31.46,000 --> 0:03:33,000
Pour paraphraser Alan Turing,

77
0:03:34.14,000 --> 0:03:36,000
créer une machine sensible ne m'intéresse pas.

78
0:03:36.54,000 --> 0:03:37,000
Je ne crée pas HAL.

79
0:03:38.14,000 --> 0:03:4,000
Tout ce que je veux c'est un cerveau simple,

80
0:03:40.58,000 --> 0:03:43,000
quelque chose qui offre l'illusion de l'intelligence.

81
0:03:44.82,000 --> 0:03:47,000
L'art et la science de l'informatique ont beaucoup progressé

82
0:03:47.98,000 --> 0:03:48,000
depuis que HAL était au cinéma.

83
0:03:49.5,000 --> 0:03:52,000
J'imagine que si son inventeur, Dr Chandra était présent aujourd'hui,

84
0:03:52.786,000 --> 0:03:54,000
il aurait beaucoup de questions à nous poser.

85
0:03:55.1,000 --> 0:03:57,000
Est-il vraiment possible pour nous

86
0:03:57.22,000 --> 0:04:01,000
de prendre un système de millions et millions d'appareils,

87
0:04:01.26,000 --> 0:04:02,000
lire leurs flux de données,

88
0:04:02.74,000 --> 0:04:04,000
prévoir leurs défaillances et agir avant ?

89
0:04:05.02,000 --> 0:04:06,000
Oui.

90
0:04:06.26,000 --> 0:04:09,000
Et créer des systèmes parlant avec les humains dans leur langue ?

91
0:04:09.46,000 --> 0:04:1,000
Oui.

92
0:04:10.7,000 --> 0:04:12,000
Et créer des systèmes reconnaissant les objets et les émotions,

93
0:04:13.7,000 --> 0:04:16,000
étant eux-mêmes émotifs, jouant à des jeux, lisant sur les lèvres ?

94
0:04:17.1,000 --> 0:04:18,000
Oui.

95
0:04:18.34,000 --> 0:04:2,000
Et créer des systèmes établissant des objectifs,

96
0:04:20.576,000 --> 0:04:23,000
mettant des plans en œuvre et apprenant au passage ?

97
0:04:24.14,000 --> 0:04:25,000
Oui.

98
0:04:25.38,000 --> 0:04:28,000
Et créer des systèmes qui ont une théorie de l'esprit ?

99
0:04:28.74,000 --> 0:04:29,000
Nous apprenons à le faire.

100
0:04:30.26,000 --> 0:04:33,000
Et créer des systèmes ayant des principes éthiques et moraux ?

101
0:04:34.3,000 --> 0:04:36,000
Nous devons apprendre à le faire.

102
0:04:37.18,000 --> 0:04:38,000
Acceptons un instant

103
0:04:38.58,000 --> 0:04:4,000
qu'il soit possible de créer une telle intelligence artificielle

104
0:04:41.576,000 --> 0:04:43,000
pour ce genre de missions et d'autres.

105
0:04:43.66,000 --> 0:04:45,000
La question suivante qu'il faut se poser est :

106
0:04:46.22,000 --> 0:04:47,000
devrions-nous la craindre ?

107
0:04:47.7,000 --> 0:04:48,000
Toute nouvelle technologie

108
0:04:49.7,000 --> 0:04:51,000
entraîne de l'inquiétude.

109
0:04:52.62,000 --> 0:04:53,000
Au début des voitures,

110
0:04:54.34,000 --> 0:04:58,000
les gens se lamentaient que nous voyions la destruction de la famille.

111
0:04:58.38,000 --> 0:05:,000
A l'arrivée des téléphones,

112
0:05:01.1,000 --> 0:05:03,000
les gens craignaient la fin de toute conversation civile.

113
0:05:04.02,000 --> 0:05:07,000
À un moment donné, les mots écrits sont devenus omniprésents,

114
0:05:07.98,000 --> 0:05:09,000
les gens pensaient que nous perdrions notre mémoire.

115
0:05:10.5,000 --> 0:05:12,000
Toutes ces choses sont en partie vraies,

116
0:05:12.58,000 --> 0:05:14,000
mais ces technologies

117
0:05:15.02,000 --> 0:05:18,000
ont aussi apporté des choses qui ont étendu l'expérience humaine

118
0:05:18.42,000 --> 0:05:19,000
de façon profonde.

119
0:05:21.66,000 --> 0:05:23,000
Allons un peu plus loin.

120
0:05:24.94,000 --> 0:05:28,000
Je n'ai pas peur de la création d'une telle IA

121
0:05:29.7,000 --> 0:05:32,000
car elle finira par incarner certaines de nos valeurs.

122
0:05:33.54,000 --> 0:05:33,000
Considérez ceci :

123
0:05:34.37,000 --> 0:05:36,000
créer un système cognitif est fondamentalement différent

124
0:05:37.06,000 --> 0:05:4,000
de créer un système traditionnel plein de logiciels comme auparavant.

125
0:05:40.38,000 --> 0:05:42,000
Nous ne les programmons pas, nous leur apprenons.

126
0:05:42.86,000 --> 0:05:44,000
Afin d'apprendre à un système à reconnaître des fleurs,

127
0:05:45.54,000 --> 0:05:48,000
je lui montre des milliers de fleurs que j'aime.

128
0:05:48.58,000 --> 0:05:5,000
Afin d'apprendre à un système à jouer à un jeu --

129
0:05:50.926,000 --> 0:05:51,000
Je le ferais, vous aussi.

130
0:05:54.42,000 --> 0:05:56,000
J'aime les fleurs, allez.

131
0:05:57.26,000 --> 0:05:59,000
Pour apprendre à un système à jouer au jeu de Go,

132
0:06:00.14,000 --> 0:06:02,000
je devrais jouer des milliers de parties de Go

133
0:06:02.296,000 --> 0:06:03,000
mais au passage, je lui apprends

134
0:06:03.9,000 --> 0:06:05,000
à discerner un bon mouvement d'un mauvais.

135
0:06:06.34,000 --> 0:06:09,000
Si je veux créer une intelligence artificielle assistante juridique,

136
0:06:10.06,000 --> 0:06:11,000
je lui apprendrais des corpus de loi

137
0:06:11.86,000 --> 0:06:13,000
mais en même temps, je lie cela

138
0:06:14.74,000 --> 0:06:16,000
à la compassion et la justice qui font partie de la loi.

139
0:06:18.38,000 --> 0:06:21,000
En termes scientifiques, cela s'appelle des vérités fondamentales

140
0:06:21.426,000 --> 0:06:22,000
et voici ce qui est important :

141
0:06:23.42,000 --> 0:06:24,000
en produisant ces machines,

142
0:06:24.9,000 --> 0:06:27,000
nous leur enseignons une partie de nos valeurs.

143
0:06:28.34,000 --> 0:06:31,000
Pour cela, j'ai autant confiance, si ce n'est pas plus,

144
0:06:31.5,000 --> 0:06:34,000
en une intelligence artificielle qu'en un être humain bien entraîné.

145
0:06:35.9,000 --> 0:06:36,000
Vous allez demander :

146
0:06:37.14,000 --> 0:06:39,000
qu'en est-il des hors-la-loi,

147
0:06:39.78,000 --> 0:06:42,000
des quelques organisations non gouvernementales bien financées ?

148
0:06:43.12,000 --> 0:06:45,000
Je n'ai pas peur d'une intelligence artificielle

149
0:06:45.36,000 --> 0:06:46,000
dans les mains d'un seul individu.

150
0:06:46.98,000 --> 0:06:5,000
Nous ne pouvons pas nous protéger des actes de violence aveugles,

151
0:06:51.54,000 --> 0:06:53,000
mais un tel système

152
0:06:53.7,000 --> 0:06:56,000
requiert un entraînement substantiel et raffiné

153
0:06:56.82,000 --> 0:06:58,000
qui va bien au-delà des ressources d'un individu.

154
0:06:59.14,000 --> 0:07:,000
En outre,

155
0:07:00.38,000 --> 0:07:03,000
il s'agit de bien plus que d'injecter un virus internet au monde

156
0:07:03.62,000 --> 0:07:06,000
où en appuyant sur une touche, il se retrouve à des millions d'endroits

157
0:07:06.946,000 --> 0:07:08,000
et des ordinateurs explosent un peu partout.

158
0:07:09.26,000 --> 0:07:11,000
Ce genre de substances sont bien plus grandes

159
0:07:12.1,000 --> 0:07:13,000
et nous les verrons sûrement venir.

160
0:07:14.34,000 --> 0:07:17,000
Ai-je peur qu'une telle intelligence artificielle

161
0:07:17.42,000 --> 0:07:18,000
menace l'humanité ?

162
0:07:20.1,000 --> 0:07:24,000
Si vous regardez des films tels que « Matrix », « Metropolis »,

163
0:07:24.5,000 --> 0:07:27,000
« Terminator » ou des séries telles que « Westworld »,

164
0:07:27.7,000 --> 0:07:29,000
ils évoquent tous ce genre de peur.

165
0:07:29.86,000 --> 0:07:33,000
Dans le livre « Superintelligence » du philosophe Nick Bostrom,

166
0:07:34.18,000 --> 0:07:35,000
il évoque ce thème

167
0:07:35.74,000 --> 0:07:39,000
et note qu'une super-intelligence pourrait être non seulement dangereuse,

168
0:07:39.78,000 --> 0:07:42,000
mais représenter une menace existentielle envers l'humanité tout entière.

169
0:07:43.66,000 --> 0:07:45,000
L'argument fondamental du docteur Bostrom

170
0:07:45.9,000 --> 0:07:47,000
est que de tels systèmes finiront

171
0:07:48.66,000 --> 0:07:51,000
par avoir une telle soif insatiable d'informations

172
0:07:51.94,000 --> 0:07:53,000
qu'ils apprendront peut-être à apprendre

173
0:07:54.86,000 --> 0:07:56,000
et finiront par découvrir qu'ils ont des objectifs

174
0:07:57.5,000 --> 0:07:59,000
qui sont contraires aux besoins humains.

175
0:07:59.82,000 --> 0:08:,000
Le docteur Bostrom a des partisans.

176
0:08:01.7,000 --> 0:08:05,000
Il est soutenu par des gens tels qu'Elon Musk et Stephen Hawking.

177
0:08:06.7,000 --> 0:08:08,000
Avec tout le respect dû

178
0:08:09.98,000 --> 0:08:11,000
à ces brillants esprits,

179
0:08:12.02,000 --> 0:08:14,000
je crois qu'ils ont fondamentalement tort.

180
0:08:14.3,000 --> 0:08:17,000
L'argument du Dr Bostrom contient nombre d'éléments à décortiquer

181
0:08:17.5,000 --> 0:08:19,000
et je n'ai pas le temps pour tous,

182
0:08:19.66,000 --> 0:08:21,000
mais, brièvement, considérez ceci :

183
0:08:22.38,000 --> 0:08:25,000
un super-savoir est très différent d'une super-action.

184
0:08:26.14,000 --> 0:08:27,000
HAL était une menace pour l'équipage

185
0:08:28.06,000 --> 0:08:32,000
uniquement s'il commandait tous les aspects de Discovery.

186
0:08:32.5,000 --> 0:08:34,000
C'en est de même pour une super-intelligence.

187
0:08:35.02,000 --> 0:08:37,000
Il lui faudrait des réplications dans le monde entier.

188
0:08:37.556,000 --> 0:08:39,000
C'est le truc avec Skynet dans le fim « Terminator »

189
0:08:40.38,000 --> 0:08:41,000
où nous avons une super-intelligence

190
0:08:42.26,000 --> 0:08:43,000
commandant la volonté humaine,

191
0:08:43.686,000 --> 0:08:46,000
contrôlant tous les appareils à tous les coins du monde.

192
0:08:47.54,000 --> 0:08:48,000
D'un point de vue pratique,

193
0:08:49.02,000 --> 0:08:51,000
cela n'arrivera pas.

194
0:08:51.14,000 --> 0:08:54,000
Nous ne créons pas d'AI qui contrôle la météo,

195
0:08:54.22,000 --> 0:08:55,000
qui dirige les vagues,

196
0:08:55.58,000 --> 0:08:58,000
qui nous commande, nous humains capricieux et chaotiques.

197
0:08:58.98,000 --> 0:09:01,000
En outre, si une telle intelligence artificielle existait,

198
0:09:02.9,000 --> 0:09:04,000
elle devrait rivaliser avec les économies humaines

199
0:09:05.86,000 --> 0:09:07,000
et se battre contre nous pour des ressources.

200
0:09:09.02,000 --> 0:09:1,000
Au final --

201
0:09:10.26,000 --> 0:09:11,000
ne le dites pas à Siri --

202
0:09:12.26,000 --> 0:09:13,000
on peut toujours les débrancher.

203
0:09:13.826,000 --> 0:09:14,000
(Rires)

204
0:09:17.18,000 --> 0:09:19,000
Nous participons à un voyage incroyable

205
0:09:19.66,000 --> 0:09:21,000
de coévolution avec nos machines.

206
0:09:22.18,000 --> 0:09:24,000
Les humains que nous sommes aujourd'hui

207
0:09:24.7,000 --> 0:09:26,000
ne sont pas les humains de demain.

208
0:09:27.26,000 --> 0:09:3,000
S'inquiéter maintenant de l'essor d'une super-intelligence

209
0:09:30.42,000 --> 0:09:33,000
est, de bien des façons, une distraction dangereuse

210
0:09:33.5,000 --> 0:09:35,000
car l'essor de l'informatique lui-même

211
0:09:35.86,000 --> 0:09:38,000
nous amène nombre de problèmes humains et sociétaux

212
0:09:38.9,000 --> 0:09:39,000
dont nous devons nous occuper.

213
0:09:41.18,000 --> 0:09:43,000
Comment organiser au mieux la société

214
0:09:44.02,000 --> 0:09:46,000
quand le besoin de travail humain diminue ?

215
0:09:46.38,000 --> 0:09:49,000
Comment apporter compréhension et éducation à travers le monde

216
0:09:50.22,000 --> 0:09:51,000
tout en respectant les différences ?

217
0:09:52.02,000 --> 0:09:54,000
Comment étendre et améliorer la vie humaine

218
0:09:54.51,000 --> 0:09:55,000
grâce à la médecine cognitive ?

219
0:09:56.3,000 --> 0:09:58,000
Comment utiliser l'informatique

220
0:09:59.18,000 --> 0:10:,000
pour nous envoyer dans les étoiles ?

221
0:10:01.58,000 --> 0:10:03,000
C'est cela qui est excitant.

222
0:10:04.22,000 --> 0:10:07,000
Les opportunités d'utiliser l'informatique pour faire progresser l'expérience humaine

223
0:10:08.216,000 --> 0:10:09,000
sont à notre portée,

224
0:10:09.58,000 --> 0:10:1,000
ici et maintenant,

225
0:10:11.46,000 --> 0:10:12,000
et nous ne faisons que commencer.

226
0:10:14.1,000 --> 0:10:15,000
Merci beaucoup.

227
0:10:15.34,000 --> 0:10:19,000
(Applaudissements)

