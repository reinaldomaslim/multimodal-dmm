1
0:00:12.8,000 --> 0:00:15,000
So why do you think the rich should pay more in taxes?

2
0:00:16.4,000 --> 0:00:18,000
Why did you buy the latest iPhone?

3
0:00:18.8,000 --> 0:00:2,000
Why did you pick your current partner?

4
0:00:21.28,000 --> 0:00:24,000
And why did so many people vote for Donald Trump?

5
0:00:24.72,000 --> 0:00:26,000
What were the reasons, why did they do it?

6
0:00:27.99,000 --> 0:00:29,000
So we ask this kind of question all the time,

7
0:00:30.12,000 --> 0:00:31,000
and we expect to get an answer.

8
0:00:31.88,000 --> 0:00:34,000
And when being asked, we expect ourselves to know the answer,

9
0:00:35.04,000 --> 0:00:37,000
to simply tell why we did as we did.

10
0:00:38.44,000 --> 0:00:39,000
But do we really know why?

11
0:00:41,000 --> 0:00:44,000
So when you say that you prefer George Clooney to Tom Hanks,

12
0:00:44.479,000 --> 0:00:46,000
due to his concern for the environment,

13
0:00:46.56,000 --> 0:00:47,000
is that really true?

14
0:00:48.56,000 --> 0:00:5,000
So you can be perfectly sincere and genuinely believe

15
0:00:51.08,000 --> 0:00:53,000
that this is the reason that drives your choice,

16
0:00:54.04,000 --> 0:00:56,000
but to me, it may still feel like something is missing.

17
0:00:57.56,000 --> 0:01:,000
As it stands, due to the nature of subjectivity,

18
0:01:00.76,000 --> 0:01:04,000
it is actually very hard to ever prove that people are wrong about themselves.

19
0:01:06.6,000 --> 0:01:08,000
So I'm an experimental psychologist,

20
0:01:08.76,000 --> 0:01:11,000
and this is the problem we've been trying to solve in our lab.

21
0:01:12.32,000 --> 0:01:14,000
So we wanted to create an experiment

22
0:01:14.52,000 --> 0:01:17,000
that would allow us to challenge what people say about themselves,

23
0:01:18.08,000 --> 0:01:2,000
regardless of how certain they may seem.

24
0:01:21.96,000 --> 0:01:23,000
But tricking people about their own mind is hard.

25
0:01:24.72,000 --> 0:01:26,000
So we turned to the professionals.

26
0:01:27.12,000 --> 0:01:28,000
The magicians.

27
0:01:29.12,000 --> 0:01:31,000
So they're experts at creating the illusion of a free choice.

28
0:01:32.04,000 --> 0:01:34,000
So when they say, "Pick a card, any card,"

29
0:01:34.36,000 --> 0:01:36,000
the only thing you know is that your choice is no longer free.

30
0:01:38.2,000 --> 0:01:4,000
So we had a few fantastic brainstorming sessions

31
0:01:40.6,000 --> 0:01:41,000
with a group of Swedish magicians,

32
0:01:42.48,000 --> 0:01:43,000
and they helped us create a method

33
0:01:44.147,000 --> 0:01:47,000
in which we would be able to manipulate the outcome of people's choices.

34
0:01:48.76,000 --> 0:01:5,000
This way we would know when people are wrong about themselves,

35
0:01:51.72,000 --> 0:01:53,000
even if they don't know this themselves.

36
0:01:54.48,000 --> 0:01:58,000
So I will now show you a short movie showing this manipulation.

37
0:01:59.16,000 --> 0:02:,000
So it's quite simple.

38
0:02:00.6,000 --> 0:02:02,000
The participants make a choice,

39
0:02:02.76,000 --> 0:02:04,000
but I end up giving them the opposite.

40
0:02:05.04,000 --> 0:02:08,000
And then we want to see: How did they react, and what did they say?

41
0:02:09.24,000 --> 0:02:12,000
So it's quite simple, but see if you can spot the magic going on.

42
0:02:13.44,000 --> 0:02:16,000
And this was shot with real participants, they don't know what's going on.

43
0:02:19,000 --> 0:02:21,000
(Video) Petter Johansson: Hi, my name's Petter.

44
0:02:21.24,000 --> 0:02:22,000
Woman: Hi, I'm Becka.

45
0:02:22.479,000 --> 0:02:24,000
PJ: I'm going to show you pictures like this.

46
0:02:24.64,000 --> 0:02:26,000
And you'll have to decide which one you find more attractive.

47
0:02:27.56,000 --> 0:02:28,000
Becka: OK.

48
0:02:28.8,000 --> 0:02:31,000
PJ: And then sometimes, I will ask you why you prefer that face.

49
0:02:32,000 --> 0:02:33,000
Becka: OK.

50
0:02:33.24,000 --> 0:02:34,000
PJ: Ready? Becka: Yeah.

51
0:02:43.12,000 --> 0:02:44,000
PJ: Why did you prefer that one?

52
0:02:44.96,000 --> 0:02:45,000
Becka: The smile, I think.

53
0:02:46.48,000 --> 0:02:47,000
PJ: Smile.

54
0:02:52.4,000 --> 0:02:53,000
Man: One on the left.

55
0:02:57.52,000 --> 0:02:58,000
Again, this one just struck me.

56
0:02:59.76,000 --> 0:03:,000
Interesting shot.

57
0:03:01.4,000 --> 0:03:04,000
Since I'm a photographer, I like the way it's lit and looks.

58
0:03:06.28,000 --> 0:03:08,000
Petter Johansson: But now comes the trick.

59
0:03:10.12,000 --> 0:03:11,000
(Video) Woman 1: This one.

60
0:03:16.24,000 --> 0:03:18,000
PJ: So they get the opposite of their choice.

61
0:03:20.52,000 --> 0:03:21,000
And let's see what happens.

62
0:03:28.24,000 --> 0:03:29,000
Woman 2: Um ...

63
0:03:35.76,000 --> 0:03:37,000
I think he seems a little more innocent than the other guy.

64
0:03:45.36,000 --> 0:03:46,000
Man: The one on the left.

65
0:03:49.28,000 --> 0:03:52,000
I like her smile and contour of the nose and face.

66
0:03:53,000 --> 0:03:55,000
So it's a little more interesting to me, and her haircut.

67
0:04:00.04,000 --> 0:04:01,000
Woman 3: This one.

68
0:04:03.52,000 --> 0:04:04,000
I like the smirky look better.

69
0:04:05.12,000 --> 0:04:07,000
PJ: You like the smirky look better?

70
0:04:09.68,000 --> 0:04:12,000
(Laughter)

71
0:04:12.88,000 --> 0:04:13,000
Woman 3: This one.

72
0:04:15.28,000 --> 0:04:16,000
PJ: What made you choose him?

73
0:04:17.52,000 --> 0:04:19,000
Woman 3: I don't know, he looks a little bit like the Hobbit.

74
0:04:20.44,000 --> 0:04:22,000
(Laughter)

75
0:04:22.52,000 --> 0:04:23,000
PJ: And what happens in the end

76
0:04:24.04,000 --> 0:04:27,000
when I tell them the true nature of the experiment?

77
0:04:27.16,000 --> 0:04:29,000
Yeah, that's it. I just have to ask a few questions.

78
0:04:29.64,000 --> 0:04:3,000
Man: Sure.

79
0:04:30.88,000 --> 0:04:32,000
PJ: What did you think of this experiment, was it easy or hard?

80
0:04:33.88,000 --> 0:04:34,000
Man: It was easy.

81
0:04:36.04,000 --> 0:04:37,000
PJ: During the experiments,

82
0:04:37.4,000 --> 0:04:4,000
I actually switched the pictures three times.

83
0:04:40.76,000 --> 0:04:41,000
Was this anything you noticed?

84
0:04:42.36,000 --> 0:04:43,000
Man: No. I didn't notice any of that.

85
0:04:44.2,000 --> 0:04:45,000
PJ: Not at all? Man: No.

86
0:04:45.72,000 --> 0:04:47,000
Switching the pictures as far as ...

87
0:04:47.84,000 --> 0:04:5,000
PJ: Yeah, you were pointing at one of them but I actually gave you the opposite.

88
0:04:51.68,000 --> 0:04:52,000
Man: The opposite one. OK, when you --

89
0:04:53.52,000 --> 0:04:55,000
No. Shows you how much my attention span was.

90
0:04:55.8,000 --> 0:04:56,000
(Laughter)

91
0:04:58.88,000 --> 0:05:01,000
PJ: Did you notice that sometimes during the experiment

92
0:05:01.92,000 --> 0:05:03,000
I switched the pictures?

93
0:05:04.08,000 --> 0:05:06,000
Woman 2: No, I did not notice that.

94
0:05:06.12,000 --> 0:05:09,000
PJ: You were pointing at one, but then I gave you the other one.

95
0:05:09.92,000 --> 0:05:1,000
No inclination of that happening?

96
0:05:11.56,000 --> 0:05:12,000
Woman 2: No.

97
0:05:13.16,000 --> 0:05:14,000
Woman 2: I did not notice.

98
0:05:14.44,000 --> 0:05:15,000
(Laughs)

99
0:05:16.4,000 --> 0:05:17,000
PJ: Thank you.

100
0:05:17.64,000 --> 0:05:18,000
Woman 2: Thank you.

101
0:05:19.04,000 --> 0:05:21,000
PJ: OK, so as you probably figured out now,

102
0:05:21.12,000 --> 0:05:23,000
the trick is that I have two cards in each hand,

103
0:05:23.4,000 --> 0:05:24,000
and when I hand one of them over,

104
0:05:25,000 --> 0:05:29,000
the black one kind of disappears into the black surface on the table.

105
0:05:30.64,000 --> 0:05:31,000
So using pictures like this,

106
0:05:32.4,000 --> 0:05:36,000
normally not more than 20 percent of the participants detect these tries.

107
0:05:36.8,000 --> 0:05:37,000
And as you saw in the movie,

108
0:05:38.24,000 --> 0:05:41,000
when in the end we explain what's going on,

109
0:05:41.44,000 --> 0:05:45,000
they're very surprised and often refuse to believe the trick has been made.

110
0:05:45.84,000 --> 0:05:49,000
So this shows that this effect is quite robust and a genuine effect.

111
0:05:50.64,000 --> 0:05:52,000
But if you're interested in self-knowledge, as I am,

112
0:05:53.32,000 --> 0:05:54,000
the more interesting bit is,

113
0:05:54.68,000 --> 0:05:57,000
OK, so what did they say when they explained these choices?

114
0:05:58.64,000 --> 0:05:59,000
So we've done a lot of analysis

115
0:06:00.16,000 --> 0:06:02,000
of the verbal reports in these experiments.

116
0:06:03.36,000 --> 0:06:05,000
And this graph simply shows

117
0:06:05.84,000 --> 0:06:09,000
that if you compare what they say in a manipulated trial

118
0:06:10.64,000 --> 0:06:11,000
with a nonmanipulated trial,

119
0:06:12.04,000 --> 0:06:14,000
that is when they explain a normal choice they've made

120
0:06:14.84,000 --> 0:06:16,000
and one where we manipulated the outcome,

121
0:06:17.36,000 --> 0:06:19,000
we find that they are remarkably similar.

122
0:06:19.84,000 --> 0:06:22,000
So they are just as emotional, just as specific,

123
0:06:22.92,000 --> 0:06:25,000
and they are expressed with the same level of certainty.

124
0:06:27.12,000 --> 0:06:29,000
So the strong conclusion to draw from this

125
0:06:29.48,000 --> 0:06:31,000
is that if there are no differences

126
0:06:31.72,000 --> 0:06:34,000
between a real choice and a manipulated choice,

127
0:06:35.44,000 --> 0:06:37,000
perhaps we make things up all the time.

128
0:06:38.68,000 --> 0:06:39,000
But we've also done studies

129
0:06:40.04,000 --> 0:06:43,000
where we try to match what they say with the actual faces.

130
0:06:43.08,000 --> 0:06:44,000
And then we find things like this.

131
0:06:45.76,000 --> 0:06:5,000
So here, this male participant, he preferred the girl to the left,

132
0:06:50.84,000 --> 0:06:51,000
he ended up with the one to the right.

133
0:06:52.72,000 --> 0:06:54,000
And then, he explained his choice like this.

134
0:06:55.56,000 --> 0:06:56,000
"She is radiant.

135
0:06:56.88,000 --> 0:06:59,000
I would rather have approached her at the bar than the other one.

136
0:07:,000 --> 0:07:01,000
And I like earrings."

137
0:07:01.64,000 --> 0:07:04,000
And whatever made him choose the girl on the left to begin with,

138
0:07:05.16,000 --> 0:07:06,000
it can't have been the earrings,

139
0:07:06.76,000 --> 0:07:08,000
because they were actually sitting on the girl on the right.

140
0:07:09.64,000 --> 0:07:12,000
So this is a clear example of a post hoc construction.

141
0:07:13.44,000 --> 0:07:15,000
So they just explained the choice afterwards.

142
0:07:17.32,000 --> 0:07:19,000
So what this experiment shows is,

143
0:07:19.64,000 --> 0:07:22,000
OK, so if we fail to detect that our choices have been changed,

144
0:07:23.32,000 --> 0:07:26,000
we will immediately start to explain them in another way.

145
0:07:27.52,000 --> 0:07:28,000
And what we also found

146
0:07:28.8,000 --> 0:07:31,000
is that the participants often come to prefer the alternative,

147
0:07:32.04,000 --> 0:07:34,000
that they were led to believe they liked.

148
0:07:34.32,000 --> 0:07:36,000
So if we let them do the choice again,

149
0:07:36.36,000 --> 0:07:39,000
they will now choose the face they had previously rejected.

150
0:07:41.52,000 --> 0:07:43,000
So this is the effect we call "choice blindness."

151
0:07:43.84,000 --> 0:07:45,000
And we've done a number of different studies --

152
0:07:46.08,000 --> 0:07:48,000
we've tried consumer choices,

153
0:07:48.64,000 --> 0:07:52,000
choices based on taste and smell and even reasoning problems.

154
0:07:53.08,000 --> 0:07:55,000
But what you all want to know is of course

155
0:07:55.16,000 --> 0:07:58,000
does this extend also to more complex, more meaningful choices?

156
0:07:59.12,000 --> 0:08:02,000
Like those concerning moral and political issues.

157
0:08:04.4,000 --> 0:08:08,000
So the next experiment, it needs a little bit of a background.

158
0:08:08.64,000 --> 0:08:12,000
So in Sweden, the political landscape

159
0:08:12.92,000 --> 0:08:15,000
is dominated by a left-wing and a right-wing coalition.

160
0:08:17.72,000 --> 0:08:21,000
And the voters may move a little bit between the parties within each coalition,

161
0:08:22.16,000 --> 0:08:24,000
but there is very little movement between the coalitions.

162
0:08:25.68,000 --> 0:08:26,000
And before each elections,

163
0:08:27.68,000 --> 0:08:31,000
the newspapers and the polling institutes

164
0:08:31.92,000 --> 0:08:33,000
put together what they call "an election compass"

165
0:08:34.56,000 --> 0:08:37,000
which consists of a number of dividing issues

166
0:08:37.92,000 --> 0:08:39,000
that sort of separates the two coalitions.

167
0:08:40.28,000 --> 0:08:43,000
Things like if tax on gasoline should be increased

168
0:08:44.039,000 --> 0:08:48,000
or if the 13 months of paid parental leave

169
0:08:48.159,000 --> 0:08:5,000
should be split equally between the two parents

170
0:08:50.679,000 --> 0:08:52,000
in order to increase gender equality.

171
0:08:54.84,000 --> 0:08:56,000
So, before the last Swedish election,

172
0:08:57.08,000 --> 0:08:59,000
we created an election compass of our own.

173
0:09:00.48,000 --> 0:09:02,000
So we walked up to people in the street

174
0:09:02.64,000 --> 0:09:05,000
and asked if they wanted to do a quick political survey.

175
0:09:06,000 --> 0:09:08,000
So first we had them state their voting intention

176
0:09:08.48,000 --> 0:09:09,000
between the two coalitions.

177
0:09:10.56,000 --> 0:09:13,000
Then we asked them to answer 12 of these questions.

178
0:09:14.36,000 --> 0:09:15,000
They would fill in their answers,

179
0:09:16.36,000 --> 0:09:17,000
and we would ask them to discuss,

180
0:09:18,000 --> 0:09:23,000
so OK, why do you think tax on gas should be increased?

181
0:09:23.52,000 --> 0:09:25,000
And we'd go through the questions.

182
0:09:25.64,000 --> 0:09:28,000
Then we had a color coded template

183
0:09:29.56,000 --> 0:09:31,000
that would allow us to tally their overall score.

184
0:09:32.52,000 --> 0:09:35,000
So this person would have one, two, three, four

185
0:09:36,000 --> 0:09:39,000
five, six, seven, eight, nine scores to the left,

186
0:09:39.32,000 --> 0:09:41,000
so he would lean to the left, basically.

187
0:09:42.8,000 --> 0:09:46,000
And in the end, we also had them fill in their voting intention once more.

188
0:09:48.16,000 --> 0:09:5,000
But of course, there was also a trick involved.

189
0:09:51.36,000 --> 0:09:53,000
So first, we walked up to people,

190
0:09:53.56,000 --> 0:09:55,000
we asked them about their voting intention

191
0:09:55.64,000 --> 0:09:57,000
and then when they started filling in,

192
0:09:57.92,000 --> 0:10:02,000
we would fill in a set of answers going in the opposite direction.

193
0:10:03.4,000 --> 0:10:05,000
We would put it under the notepad.

194
0:10:06,000 --> 0:10:08,000
And when we get the questionnaire,

195
0:10:08.8,000 --> 0:10:11,000
we would simply glue it on top of the participant's own answer.

196
0:10:16,000 --> 0:10:17,000
So there, it's gone.

197
0:10:24.28,000 --> 0:10:26,000
And then we would ask about each of the questions:

198
0:10:26.68,000 --> 0:10:27,000
How did you reason here?

199
0:10:28.24,000 --> 0:10:29,000
And they'll state the reasons,

200
0:10:3,000 --> 0:10:32,000
together we will sum up their overall score.

201
0:10:34.8,000 --> 0:10:37,000
And in the end, they will state their voting intention again.

202
0:10:41.96,000 --> 0:10:42,000
So what we find first of all here,

203
0:10:43.64,000 --> 0:10:47,000
is that very few of these manipulations are detected.

204
0:10:47.88,000 --> 0:10:49,000
And they're not detected in the sense that they realize,

205
0:10:50.56,000 --> 0:10:51,000
"OK, you must have changed my answer,"

206
0:10:52.44,000 --> 0:10:53,000
it was more the case that,

207
0:10:53.72,000 --> 0:10:56,000
"OK, I must've misunderstood the question the first time I read it.

208
0:10:56.92,000 --> 0:10:57,000
Can I please change it?"

209
0:10:59.08,000 --> 0:11:04,000
And even if a few of these manipulations were changed,

210
0:11:04.24,000 --> 0:11:06,000
the overall majority was missed.

211
0:11:06.4,000 --> 0:11:09,000
So we managed to switch 90 percent of the participants' answers

212
0:11:10.08,000 --> 0:11:13,000
from left to right, right to left, their overall profile.

213
0:11:14.8,000 --> 0:11:18,000
And what happens then when they are asked to motivate their choices?

214
0:11:20.16,000 --> 0:11:23,000
And here we find much more interesting verbal reports

215
0:11:23.24,000 --> 0:11:25,000
than compared to the faces.

216
0:11:25.28,000 --> 0:11:28,000
People say things like this, and I'll read it to you.

217
0:11:29.72,000 --> 0:11:32,000
So, "Large-scale governmental surveillance of email and internet traffic

218
0:11:33.48,000 --> 0:11:37,000
ought to be permissible as means to combat international crime and terrorism."

219
0:11:37.84,000 --> 0:11:39,000
"So you agree to some extent with this statement." "Yes."

220
0:11:40.58,000 --> 0:11:41,000
"So how did you reason here?"

221
0:11:43.6,000 --> 0:11:47,000
"Well, like, as it is so hard to get at international crime and terrorism,

222
0:11:48.56,000 --> 0:11:5,000
I think there should be those kinds of tools."

223
0:11:51.36,000 --> 0:11:54,000
And then the person remembers an argument from the newspaper in the morning.

224
0:11:55,000 --> 0:11:56,000
"Like in the newspaper today,

225
0:11:56.64,000 --> 0:11:59,000
it said they can like, listen to mobile phones from prison,

226
0:12:00.04,000 --> 0:12:03,000
if a gang leader tries to continue his crimes from inside.

227
0:12:03.6,000 --> 0:12:05,000
And I think it's madness that we have so little power

228
0:12:06.44,000 --> 0:12:07,000
that we can't stop those things

229
0:12:08.12,000 --> 0:12:1,000
when we actually have the possibility to do so."

230
0:12:11.08,000 --> 0:12:13,000
And then there's a little bit back and forth in the end:

231
0:12:13.8,000 --> 0:12:15,000
"I don't like that they have access to everything I do,

232
0:12:16.4,000 --> 0:12:18,000
but I still think it's worth it in the long run."

233
0:12:19,000 --> 0:12:21,000
So, if you didn't know that this person

234
0:12:21.56,000 --> 0:12:23,000
just took part in a choice blindness experiment,

235
0:12:23.84,000 --> 0:12:24,000
I don't think you would question

236
0:12:25.72,000 --> 0:12:28,000
that this is the true attitude of that person.

237
0:12:29.8,000 --> 0:12:31,000
And what happens in the end, with the voting intention?

238
0:12:32.68,000 --> 0:12:36,000
What we find -- that one is also clearly affected by the questionnaire.

239
0:12:37.4,000 --> 0:12:38,000
So we have 10 participants

240
0:12:39.16,000 --> 0:12:41,000
shifting from left to right or from right to left.

241
0:12:42.16,000 --> 0:12:44,000
We have another 19 that go from clear voting intention

242
0:12:44.72,000 --> 0:12:45,000
to being uncertain.

243
0:12:46.2,000 --> 0:12:49,000
Some go from being uncertain to clear voting intention.

244
0:12:49.32,000 --> 0:12:53,000
And then there is a number of participants staying uncertain throughout.

245
0:12:54.08,000 --> 0:12:55,000
And that number is interesting

246
0:12:55.68,000 --> 0:12:59,000
because if you look at what the polling institutes say

247
0:13:00.32,000 --> 0:13:01,000
the closer you get to an election,

248
0:13:02,000 --> 0:13:04,000
the only people that are sort of in play

249
0:13:04.16,000 --> 0:13:06,000
are the ones that are considered uncertain.

250
0:13:06.84,000 --> 0:13:09,000
But we show there is a much larger number

251
0:13:10.08,000 --> 0:13:12,000
that would actually consider shifting their attitudes.

252
0:13:13.64,000 --> 0:13:16,000
And here I must point out, of course, that you are not allowed to use this

253
0:13:17.16,000 --> 0:13:19,000
as an actual method to change people's votes

254
0:13:19.8,000 --> 0:13:2,000
before an election,

255
0:13:21.32,000 --> 0:13:24,000
and we clearly debriefed them afterwards

256
0:13:24.96,000 --> 0:13:26,000
and gave them every opportunity to change back

257
0:13:27.28,000 --> 0:13:29,000
to whatever they thought first.

258
0:13:30.6,000 --> 0:13:32,000
But what this shows is that if you can get people

259
0:13:32.96,000 --> 0:13:37,000
to see the opposite view and engage in a conversation with themselves,

260
0:13:38.52,000 --> 0:13:4,000
that could actually make them change their views.

261
0:13:42.4,000 --> 0:13:43,000
OK.

262
0:13:44.76,000 --> 0:13:45,000
So what does it all mean?

263
0:13:46.44,000 --> 0:13:48,000
What do I think is going on here?

264
0:13:48.88,000 --> 0:13:49,000
So first of all,

265
0:13:50.12,000 --> 0:13:54,000
a lot of what we call self-knowledge is actually self-interpretation.

266
0:13:55,000 --> 0:13:57,000
So I see myself make a choice,

267
0:13:57.52,000 --> 0:13:59,000
and then when I'm asked why,

268
0:14:00.32,000 --> 0:14:02,000
I just try to make as much sense of it as possible

269
0:14:02.88,000 --> 0:14:03,000
when I make an explanation.

270
0:14:04.84,000 --> 0:14:07,000
But we do this so quickly and with such ease

271
0:14:07.88,000 --> 0:14:11,000
that we think we actually know the answer when we answer why.

272
0:14:13.04,000 --> 0:14:16,000
And as it is an interpretation,

273
0:14:16.16,000 --> 0:14:18,000
of course we sometimes make mistakes.

274
0:14:18.48,000 --> 0:14:21,000
The same way we make mistakes when we try to understand other people.

275
0:14:23.16,000 --> 0:14:26,000
So beware when you ask people the question "why"

276
0:14:26.88,000 --> 0:14:3,000
because what may happen is that, if you asked them,

277
0:14:31.8,000 --> 0:14:35,000
"So why do you support this issue?"

278
0:14:35.84,000 --> 0:14:38,000
"Why do you stay in this job or this relationship?" --

279
0:14:39.08,000 --> 0:14:42,000
what may happen when you ask why is that you actually create an attitude

280
0:14:42.52,000 --> 0:14:44,000
that wasn't there before you asked the question.

281
0:14:45.44,000 --> 0:14:48,000
And this is of course important in your professional life, as well,

282
0:14:48.64,000 --> 0:14:49,000
or it could be.

283
0:14:49.88,000 --> 0:14:51,000
If, say, you design something and then you ask people,

284
0:14:52.44,000 --> 0:14:54,000
"Why do you think this is good or bad?"

285
0:14:54.72,000 --> 0:14:57,000
Or if you're a journalist asking a politician,

286
0:14:57.8,000 --> 0:14:59,000
"So, why did you make this decision?"

287
0:15:00.2,000 --> 0:15:01,000
Or if indeed you are a politician

288
0:15:02.16,000 --> 0:15:04,000
and try to explain why a certain decision was made.

289
0:15:06.08,000 --> 0:15:09,000
So this may all seem a bit disturbing.

290
0:15:09.68,000 --> 0:15:12,000
But if you want to look at it from a positive direction,

291
0:15:13.2,000 --> 0:15:14,000
it could be seen as showing,

292
0:15:14.96,000 --> 0:15:17,000
OK, so we're actually a little bit more flexible than we think.

293
0:15:18.36,000 --> 0:15:19,000
We can change our minds.

294
0:15:20.28,000 --> 0:15:22,000
Our attitudes are not set in stone.

295
0:15:22.76,000 --> 0:15:25,000
And we can also change the minds of others,

296
0:15:25.96,000 --> 0:15:27,000
if we can only get them to engage with the issue

297
0:15:28.36,000 --> 0:15:29,000
and see it from the opposite view.

298
0:15:31.4,000 --> 0:15:34,000
And in my own personal life, since starting with this research --

299
0:15:35.36,000 --> 0:15:37,000
So my partner and I, we've always had the rule

300
0:15:37.96,000 --> 0:15:39,000
that you're allowed to take things back.

301
0:15:40.28,000 --> 0:15:42,000
Just because I said I liked something a year ago,

302
0:15:42.64,000 --> 0:15:44,000
doesn't mean I have to like it still.

303
0:15:45.48,000 --> 0:15:47,000
And getting rid of the need to stay consistent

304
0:15:48.32,000 --> 0:15:52,000
is actually a huge relief and makes relational life so mush easier to live.

305
0:15:53.72,000 --> 0:15:55,000
Anyway, so the conclusion must be:

306
0:15:57.32,000 --> 0:15:59,000
know that you don't know yourself.

307
0:15:59.84,000 --> 0:16:01,000
Or at least not as well as you think you do.

308
0:16:03.48,000 --> 0:16:04,000
Thanks.

309
0:16:04.72,000 --> 0:16:08,000
(Applause)

