1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: eric vautier

2
0:00:12.36,000 --> 0:00:15,000
[Cette intervention contient du contenu réservé aux adultes]

3
0:00:16.913,000 --> 0:00:2,000
Moritz Riesewieck : Le 23 mars 2013,

4
0:00:21.389,000 --> 0:00:25,000
des utilisateurs du monde entier ont découvert dans leur fil d'actualités

5
0:00:25.47,000 --> 0:00:3,000
la vidéo d'une jeune fille se faisant violer par un homme plus âgé.

6
0:00:31.478,000 --> 0:00:34,000
Avant que cette vidéo ne soit retirée de Facebook,

7
0:00:35.358,000 --> 0:00:39,000
elle avait déjà été partagée 16 000 fois

8
0:00:39.998,000 --> 0:00:42,000
et avait même été aimée 4 000 fois.

9
0:00:45.268,000 --> 0:00:48,000
La vidéo est devenue virale et a infecté internet.

10
0:00:49.873,000 --> 0:00:52,000
Hans Block : C'est le moment où nous nous sommes demandé :

11
0:00:52.992,000 --> 0:00:54,000
comment une telle chose a-t-elle pu arriver sur Facebook ?

12
0:00:55.794,000 --> 0:00:56,000
Et en même temps,

13
0:00:57.06,000 --> 0:01:,000
pourquoi ne voyons-nous pas des contenus similaires plus souvent ?

14
0:01:00.22,000 --> 0:01:03,000
Il y a beaucoup de contenus révoltants en ligne,

15
0:01:03.927,000 --> 0:01:05,000
mais pourquoi voyons-nous si rarement

16
0:01:06.128,000 --> 0:01:08,000
de telles merdes sur Facebook, Twitter ou Google ?

17
0:01:08.958,000 --> 0:01:1,000
MR : Si les logiciels de reconnaissance des images

18
0:01:11.313,000 --> 0:01:15,000
peuvent identifier l’esquisse d'organes sexuels,

19
0:01:15.53,000 --> 0:01:19,000
de sang ou de peau nue dans des images et des vidéos,

20
0:01:20.482,000 --> 0:01:25,000
ils ont d'immenses difficultés à distinguer du contenu pornographique

21
0:01:26.046,000 --> 0:01:3,000
de photos de vacances, de statues d'Adonis

22
0:01:30.418,000 --> 0:01:32,000
ou de campagnes de dépistage du cancer du sein.

23
0:01:33.14,000 --> 0:01:37,000
Ils ne peuvent pas distinguer Roméo et Juliette mourant sur scène

24
0:01:37.419,000 --> 0:01:39,000
d'une vraie attaque au couteau.

25
0:01:39.998,000 --> 0:01:44,000
Ils ne peuvent pas distinguer la satire de la propagande

26
0:01:45.244,000 --> 0:01:48,000
ou l'ironie de la haine et ainsi de suite.

27
0:01:50.077,000 --> 0:01:54,000
Par conséquent, des êtres humains sont nécessaires pour décider

28
0:01:54.179,000 --> 0:01:58,000
quels contenus suspicieux doivent être supprimés

29
0:01:58.204,000 --> 0:01:59,000
et lesquels doivent être conservés.

30
0:02:00.909,000 --> 0:02:02,000
Des êtres humains au sujet desquels nous ne savons presque rien

31
0:02:03.864,000 --> 0:02:04,000
car ils travaillent dans le secret.

32
0:02:06.053,000 --> 0:02:08,000
Ils signent des accords de confidentialité

33
0:02:08.072,000 --> 0:02:1,000
qui leur interdisent de parler ou de partager

34
0:02:10.959,000 --> 0:02:13,000
ce qu'ils voient sur leur écran et ce que ce travail leur fait.

35
0:02:14.887,000 --> 0:02:18,000
Ils sont obligés d'utiliser des codes pour dissimuler pour qui ils travaillent.

36
0:02:19.585,000 --> 0:02:21,000
Ils sont surveillés par des sociétés de sécurité privée

37
0:02:22.514,000 --> 0:02:25,000
afin d'assurer qu'ils ne parlent pas à des journalistes.

38
0:02:26.048,000 --> 0:02:29,000
Ils sont menacés d'amende s'ils parlent.

39
0:02:30.421,000 --> 0:02:33,000
Tout cela semble être une histoire policière étrange,

40
0:02:34.207,000 --> 0:02:35,000
mais c'est vrai.

41
0:02:35.561,000 --> 0:02:36,000
Ces gens existent

42
0:02:37.633,000 --> 0:02:41,000
et ils sont appelés modérateurs de contenu.

43
0:02:42.942,000 --> 0:02:45,000
HB : Nous sommes les réalisateurs du documentaire « The Cleaners »

44
0:02:46.411,000 --> 0:02:47,000
et nous aimerions vous emmener

45
0:02:48.395,000 --> 0:02:5,000
dans un monde que beaucoup ne connaissent pas encore.

46
0:02:51.006,000 --> 0:02:53,000
Voici un court extrait de notre film.

47
0:02:58.639,000 --> 0:03:01,000
(Musique)

48
0:03:04.4,000 --> 0:03:07,000
(Vidéo) Modérateur : Je dois être anonyme car nous avons signé un contrat.

49
0:03:09.784,000 --> 0:03:12,000
Nous n'avons pas le droit de déclarer avec qui nous travaillons.

50
0:03:14.807,000 --> 0:03:15,000
Je vous parle

51
0:03:16.594,000 --> 0:03:2,000
car le monde devrait savoir que nous existons.

52
0:03:22.544,000 --> 0:03:24,000
Il y a quelqu'un qui vérifie les réseaux sociaux.

53
0:03:26.317,000 --> 0:03:29,000
Nous faisons de notre mieux pour rendre cette plateforme

54
0:03:29.945,000 --> 0:03:3,000
sûre pour tous.

55
0:03:42.438,000 --> 0:03:43,000
Supprimer.

56
0:03:44.278,000 --> 0:03:45,000
Ignorer.

57
0:03:45.596,000 --> 0:03:46,000
Supprimer.

58
0:03:47.279,000 --> 0:03:48,000
Ignorer.

59
0:03:48.6,000 --> 0:03:49,000
Supprimer.

60
0:03:50.68,000 --> 0:03:51,000
Ignorer.

61
0:03:51.855,000 --> 0:03:52,000
Ignorer.

62
0:03:53.625,000 --> 0:03:54,000
Supprimer.

63
0:03:58.03,000 --> 0:04:,000
HB : Les soi-disant modérateurs de contenu

64
0:04:00.031,000 --> 0:04:04,000
ne reçoivent leur salaire de Facebook, Twitter ou Google,

65
0:04:04.055,000 --> 0:04:06,000
mais d'entreprises de sous-traitance

66
0:04:06.396,000 --> 0:04:08,000
afin de maintenir leurs rémunérations basses.

67
0:04:08.833,000 --> 0:04:09,000
Des dizaines de milliers de jeunes gens

68
0:04:10.824,000 --> 0:04:13,000
qui regardent tout ce que nous ne sommes pas censés voir.

69
0:04:14.061,000 --> 0:04:17,000
Nous parlons de décapitations, de mutilations,

70
0:04:17.633,000 --> 0:04:2,000
d'exécutions, de nécrophilie, de torture, de maltraitance des enfants.

71
0:04:21.743,000 --> 0:04:23,000
Des milliers d'images chaque jour --

72
0:04:24.041,000 --> 0:04:26,000
ignorer, supprimer, jour et nuit.

73
0:04:27.393,000 --> 0:04:3,000
La plupart de ce travail est réalisé à Manille,

74
0:04:30.815,000 --> 0:04:33,000
où les déchets toxiques analogiques du monde occidental

75
0:04:34.141,000 --> 0:04:36,000
étaient transportés pendant des années par porte-conteneurs

76
0:04:36.923,000 --> 0:04:38,000
et on s'y débarrasse maintenant de nos déchets numériques

77
0:04:39.8,000 --> 0:04:4,000
par fibre optique.

78
0:04:40.8,000 --> 0:04:43,000
Tout comme les éboueurs

79
0:04:43.871,000 --> 0:04:46,000
fouillent de gigantesques décharges en lisière de la ville,

80
0:04:47.371,000 --> 0:04:51,000
les modérateurs de contenu cliquent dans un océan toxique et intarissable

81
0:04:52.228,000 --> 0:04:56,000
d'images et de vidéos et de tous genres de déchets intellectuels

82
0:04:56.339,000 --> 0:04:58,000
pour que nous n'ayons pas à les regarder.

83
0:04:58.665,000 --> 0:05:01,000
MR : Mais contrairement aux plaies des éboueurs,

84
0:05:02.229,000 --> 0:05:05,000
celles des modérateurs de contenu demeurent invisibles.

85
0:05:06.117,000 --> 0:05:09,000
Pleines de contenus choquants et dérangeants,

86
0:05:09.221,000 --> 0:05:12,000
ces images et vidéos s'enfouissent dans leurs souvenirs

87
0:05:12.508,000 --> 0:05:15,000
où, n'importe quand, elles peuvent avoir des effets imprévisibles :

88
0:05:15.977,000 --> 0:05:18,000
troubles alimentaires, perte de la libido,

89
0:05:19.358,000 --> 0:05:22,000
troubles de l'anxiété, alcoolisme,

90
0:05:22.641,000 --> 0:05:24,000
dépression, pouvant même mener au suicide.

91
0:05:26.1,000 --> 0:05:28,000
Les images et vidéos les infectent

92
0:05:28.569,000 --> 0:05:3,000
et souvent ne les laissent jamais s'en tirer.

93
0:05:30.982,000 --> 0:05:34,000
S'ils sont malchanceux, ils développent un trouble du stress post-traumatique

94
0:05:35.847,000 --> 0:05:37,000
comme les soldats après des opératons de guerre.

95
0:05:39.445,000 --> 0:05:42,000
Dans notre film, nous racontons l'histoire d'un jeune homme

96
0:05:43.112,000 --> 0:05:45,000
qui a dû contrôler des diffusions en direct

97
0:05:45.834,000 --> 0:05:47,000
d'automutilation et de tentatives de suicide,

98
0:05:48.334,000 --> 0:05:49,000
à répétition,

99
0:05:50.033,000 --> 0:05:53,000
et qui a fini par se suicider.

100
0:05:53.787,000 --> 0:05:55,000
Ce n'est pas un cas isolé, d'après ce que l'on nous a dit.

101
0:05:57.184,000 --> 0:06:,000
C'est le prix que nous payons tous

102
0:06:01.188,000 --> 0:06:03,000
pour nos environnements sur les réseaux sociaux

103
0:06:04.066,000 --> 0:06:08,000
soi-disant propres, sûrs et « sains ».

104
0:06:10.482,000 --> 0:06:12,000
Jamais auparavant dans l'histoire de l'humanité,

105
0:06:13.101,000 --> 0:06:16,000
il n'a été plus simple de toucher des millions de gens à travers le monde

106
0:06:16.527,000 --> 0:06:17,000
en quelques secondes.

107
0:06:18.148,000 --> 0:06:21,000
Ce qui est posté sur les réseaux sociaux se propage si rapidement,

108
0:06:22.117,000 --> 0:06:25,000
cela devient viral et excite l'esprit de gens à travers le monde.

109
0:06:26.45,000 --> 0:06:28,000
Avant que ce ne soit supprimé,

110
0:06:28.538,000 --> 0:06:29,000
il est souvent déjà trop tard.

111
0:06:30.966,000 --> 0:06:32,000
Des millions de gens ont déjà été infectés

112
0:06:33.22,000 --> 0:06:34,000
par la haine et la colère

113
0:06:35.101,000 --> 0:06:37,000
et soit ils deviennent actifs en ligne

114
0:06:37.855,000 --> 0:06:4,000
en diffusant ou en amplifiant la haine

115
0:06:41.022,000 --> 0:06:44,000
ou ils descendent dans les rues et prennent les armes.

116
0:06:45.236,000 --> 0:06:47,000
HB : Par conséquent, une armée de modérateurs de contenu

117
0:06:47.86,000 --> 0:06:5,000
s'assoient devant un écran pour éviter de nouveaux dégâts collatéraux.

118
0:06:52.434,000 --> 0:06:54,000
Ils décident dès que possible

119
0:06:54.577,000 --> 0:06:58,000
si le contenu demeure sur la plateforme -- ignorer --

120
0:06:58.696,000 --> 0:07:,000
ou disparaît -- supprimer.

121
0:07:01.823,000 --> 0:07:03,000
Mais chaque décision n'est pas aussi claire

122
0:07:04.474,000 --> 0:07:06,000
que pour une vidéo de maltraitance d'enfants.

123
0:07:07.395,000 --> 0:07:09,000
Qu'en est-il de contenu controversé, ambivalent,

124
0:07:10.196,000 --> 0:07:12,000
téléchargé par des activistes des droits civils

125
0:07:12.538,000 --> 0:07:13,000
ou des journalistes citoyens ?

126
0:07:14.048,000 --> 0:07:17,000
Les modérateurs de contenu prennent souvent de telles décisions

127
0:07:17.294,000 --> 0:07:19,000
à la même vitesse que pour les cas évidents.

128
0:07:21.515,000 --> 0:07:23,000
MR : Nous allons maintenant vous montrer une vidéo

129
0:07:24.198,000 --> 0:07:27,000
et nous aimerions que vous décidiez :

130
0:07:27.531,000 --> 0:07:3,000
est-ce que vous la supprimeriez ou pas ?

131
0:07:31.07,000 --> 0:07:32,000
(Vidéo) (Bruits de frappe aérienne)

132
0:07:33.1,000 --> 0:07:35,000
(Explosion)

133
0:07:40.076,000 --> 0:07:45,000
(Gens parlant en arabe)

134
0:07:46.053,000 --> 0:07:48,000
MR : Nous avons un peu flouté cela pour vous.

135
0:07:49.196,000 --> 0:07:52,000
Un enfant serait potentiellement dangereusement perturbé

136
0:07:52.975,000 --> 0:07:54,000
et extrêmement effrayé par un tel contenu.

137
0:07:55.808,000 --> 0:07:57,000
Préféreriez-vous le supprimer ?

138
0:07:59.61,000 --> 0:08:02,000
Mais... et si cette vidéo pouvait aider à enquêter

139
0:08:02.627,000 --> 0:08:04,000
sur les crimes de guerre en Syrie ?

140
0:08:04.717,000 --> 0:08:07,000
Et si personne n'entendait parler de cette frappe aérienne

141
0:08:07.908,000 --> 0:08:1,000
car Facebook, YouTube, Twitter avaient décidé de retirer ce contenu ?

142
0:08:12.895,000 --> 0:08:16,000
Airwars, une organisation non gouvernementale basée à Londres,

143
0:08:17.244,000 --> 0:08:19,000
essaye de trouver ces vidéos aussi vite que possible,

144
0:08:20.165,000 --> 0:08:22,000
dès qu'elles sont publiées sur les réseaux sociaux,

145
0:08:22.749,000 --> 0:08:23,000
afin de les archiver.

146
0:08:24.693,000 --> 0:08:26,000
Car ils savent que, tôt ou tard,

147
0:08:27.55,000 --> 0:08:3,000
Facebook, YouTube, Twitter retireront un tel contenu.

148
0:08:31.345,000 --> 0:08:33,000
Les gens armés de leur téléphone portable

149
0:08:33.577,000 --> 0:08:37,000
peuvent rendre visible ce à quoi les journalistes n'ont souvent pas accès.

150
0:08:37.8,000 --> 0:08:4,000
Les groupes pour les droits civils n'ont souvent pas de meilleure option

151
0:08:41.177,000 --> 0:08:44,000
pour rendre leurs enregistrements rapidement accessibles à un large public

152
0:08:44.712,000 --> 0:08:46,000
que de les publier sur les réseaux sociaux.

153
0:08:47.95,000 --> 0:08:51,000
N'est-ce pas le potentiel responsabilisant que le World Wide Web devait avoir ?

154
0:08:52.966,000 --> 0:08:53,000
N'était-ce pas les rêves

155
0:08:54.95,000 --> 0:08:58,000
que les gens avaient au début du World Wide Web ?

156
0:08:59.608,000 --> 0:09:01,000
De telles images et vidéos ne peuvent-elles pas

157
0:09:02.427,000 --> 0:09:07,000
persuader les gens qui sont devenus insensibles aux faits

158
0:09:07.585,000 --> 0:09:08,000
de réfléchir à nouveau ?

159
0:09:09.917,000 --> 0:09:12,000
HB : Mais au lieu de cela, tout ce qui pourrait être perturbant est supprimé.

160
0:09:13.543,000 --> 0:09:15,000
Il y a un changement généralisé dans la société.

161
0:09:15.805,000 --> 0:09:18,000
Les médias, par exemple, utilisent de plus en plus souvent des avertissements

162
0:09:19.546,000 --> 0:09:2,000
en haut des articles

163
0:09:21.363,000 --> 0:09:24,000
que certains pourraient percevoir comme blessants ou perturbants.

164
0:09:24.696,000 --> 0:09:27,000
De plus en plus d'étudiants dans les universités américaines

165
0:09:28.634,000 --> 0:09:3,000
exigent l'exclusion du programme de classiques antiques

166
0:09:31.475,000 --> 0:09:34,000
qui dépeignent de la violence ou une agression sexuelle.

167
0:09:34.991,000 --> 0:09:36,000
Jusqu'où devrions-nous aller ?

168
0:09:37.875,000 --> 0:09:4,000
L'intégrité physique est garantie comme un droit humain

169
0:09:41.279,000 --> 0:09:42,000
dans les constitutions du monde.

170
0:09:43.422,000 --> 0:09:46,000
Dans la Charte des droits fondamentaux de l'Union européenne,

171
0:09:47.2,000 --> 0:09:5,000
ce droit est expressément appliqué à l'intégrité mentale.

172
0:09:51.347,000 --> 0:09:53,000
Mais même si les effets potentiellement traumatiques

173
0:09:54.029,000 --> 0:09:56,000
des images et des vidéos est difficile à prédire,

174
0:09:56.879,000 --> 0:09:57,000
voulons-nous devenir si prudents

175
0:09:58.86,000 --> 0:10:01,000
que nous risquons de perdre la conscience sociale de l'injustice ?

176
0:10:03.203,000 --> 0:10:04,000
Alors que faire ?

177
0:10:04.942,000 --> 0:10:06,000
Mark Zuckerberg a récemment déclaré qu'à l'avenir,

178
0:10:07.958,000 --> 0:10:1,000
les utilisateurs, nous ou presque tout le monde,

179
0:10:11.784,000 --> 0:10:13,000
décideraient individuellement

180
0:10:14.069,000 --> 0:10:16,000
de ce qu'ils aimeraient voir sur la plateforme

181
0:10:16.231,000 --> 0:10:17,000
en paramétrant des filtres personnels.

182
0:10:18.196,000 --> 0:10:21,000
Tout le monde pourrait facilement prétendre demeurer non perturbé

183
0:10:21.292,000 --> 0:10:24,000
par des images de guerre et d'autres conflits violents, comme...

184
0:10:25.849,000 --> 0:10:29,000
MR : Je suis le genre de gars qui n'est pas gêné par la vue de seins

185
0:10:30.319,000 --> 0:10:33,000
et je suis très intéressé par le réchauffement climatique,

186
0:10:34.109,000 --> 0:10:36,000
mais je n'aime pas trop la guerre.

187
0:10:37.109,000 --> 0:10:38,000
HB : Pour moi, c'est plutôt le contraire,

188
0:10:39.065,000 --> 0:10:42,000
je n'ai aucun intérêt pour les seins et corps nus.

189
0:10:43.209,000 --> 0:10:45,000
Mais pourquoi pas des armes ? J'aime les armes, oui.

190
0:10:46.901,000 --> 0:10:49,000
MR : Allons, si nous ne partageons pas une conscience sociale similaire,

191
0:10:50.67,000 --> 0:10:52,000
comment sommes-nous censés discuter de problèmes sociaux ?

192
0:10:53.413,000 --> 0:10:55,000
Comment appeler les gens à agir ?

193
0:10:55.784,000 --> 0:10:58,000
Des bulles encore plus isolées émergeraient.

194
0:10:59.665,000 --> 0:11:02,000
Une des questions centrales est : « Comment, à l'avenir,

195
0:11:02.92,000 --> 0:11:06,000
la liberté d'expression sera considérée face au besoin de protection des gens ? »

196
0:11:08.441,000 --> 0:11:09,000
C'est une question de principes.

197
0:11:10.602,000 --> 0:11:14,000
Voulons-nous concevoir une société ouverte ou fermée

198
0:11:14.874,000 --> 0:11:15,000
pour l'espace numérique ?

199
0:11:17.054,000 --> 0:11:22,000
Au cœur de la question, il y a « la liberté contre la sécurité ».

200
0:11:24.388,000 --> 0:11:28,000
Facebook a toujours voulu être une plateforme « saine ».

201
0:11:28.896,000 --> 0:11:31,000
Avant tout, les utilisateurs devraient se sentir en sécurité.

202
0:11:32.618,000 --> 0:11:34,000
C'est le même choix de mots

203
0:11:34.762,000 --> 0:11:36,000
que les modérateurs de contenu aux Philippines ont utilisés

204
0:11:37.744,000 --> 0:11:38,000
dans nombre de nos interviews.

205
0:11:40.188,000 --> 0:11:42,000
(Vidéo) Le monde dans lequel nous vivons,

206
0:11:42.593,000 --> 0:11:44,000
à mon sens, n'est pas vraiment sain.

207
0:11:44.783,000 --> 0:11:45,000
(Musique)

208
0:11:46.355,000 --> 0:11:49,000
Dans ce monde, le mal existe vraiment.

209
0:11:49.537,000 --> 0:11:52,000
(Musique)

210
0:11:52.799,000 --> 0:11:54,000
Nous devons le surveiller.

211
0:11:54.886,000 --> 0:11:55,000
(Musique)

212
0:11:56.792,000 --> 0:11:59,000
Nous devons le contrôler -- bon ou mauvais.

213
0:12:00.646,000 --> 0:12:07,000
(Musique)

214
0:12:10.193,000 --> 0:12:14,000
[Lève la tête, jeune homme ! -- Dieu]

215
0:12:14.952,000 --> 0:12:16,000
MR : Pour les jeunes modérateurs de contenu

216
0:12:17.254,000 --> 0:12:19,000
vivant dans un pays pratiquant un catholicisme strict,

217
0:12:19.784,000 --> 0:12:21,000
cela s'apparente à une mission chrétienne.

218
0:12:22.833,000 --> 0:12:24,000
Contrer les péchés du monde

219
0:12:25.823,000 --> 0:12:27,000
qui se propagent sur internet.

220
0:12:28.641,000 --> 0:12:31,000
« La propreté est proche de la piété »

221
0:12:32.077,000 --> 0:12:35,000
est un proverbe que tout le monde connaît aux Philippines.

222
0:12:36.035,000 --> 0:12:37,000
HB : D'autres se motivent

223
0:12:37.718,000 --> 0:12:4,000
en se comparant à leur président, Rodrigo Duterte.

224
0:12:41.837,000 --> 0:12:44,000
Il dirige les Philippines depuis 2016

225
0:12:45.352,000 --> 0:12:48,000
et il a gagné les élections avec la promesse : « Je nettoierai ».

226
0:12:49.892,000 --> 0:12:52,000
Cela signifie éliminer tous genres de problèmes

227
0:12:53.233,000 --> 0:12:55,000
en tuant littéralement des gens dans les rues,

228
0:12:55.712,000 --> 0:12:57,000
des gens censés être des criminels, quoi que cela signifie.

229
0:12:58.601,000 --> 0:12:59,000
Depuis qu'il a été élu,

230
0:12:59.895,000 --> 0:13:02,000
on estime que 20 000 personnes ont été tuées.

231
0:13:03.655,000 --> 0:13:05,000
Dans notre film, un modérateur dit :

232
0:13:06.18,000 --> 0:13:08,000
« Ce que Duterte fait dans les rues,

233
0:13:08.259,000 --> 0:13:09,000
je le fais pour Internet. »

234
0:13:10.934,000 --> 0:13:13,000
Et les voilà, nos super-héros autoproclamés

235
0:13:14.522,000 --> 0:13:17,000
qui font respecter la loi et l'ordre dans notre monde numérique.

236
0:13:17.522,000 --> 0:13:19,000
Ils nettoient, ils astiquent tout,

237
0:13:19.927,000 --> 0:13:21,000
ils nous libèrent de tout ce qui est mauvais.

238
0:13:22.284,000 --> 0:13:25,000
Des tâches auparavant réservées aux autorités gouvernementales

239
0:13:26.037,000 --> 0:13:29,000
ont été reprises par des jeunes diplômés qui ont la vingtaine

240
0:13:29.736,000 --> 0:13:31,000
et sont préparés via une formation de trois à cinq jours --

241
0:13:32.653,000 --> 0:13:33,000
c'est la qualification --

242
0:13:34.613,000 --> 0:13:37,000
qui travaillent pour rien de moins que secourir le monde.

243
0:13:38.756,000 --> 0:13:42,000
MR : Les souverainetés nationales ont été sous-traitées à des entreprises privées

244
0:13:42.999,000 --> 0:13:46,000
qui refilent leurs responsabilités à des prestataires.

245
0:13:47.031,000 --> 0:13:5,000
C'est de la sous-traitance de sous-traitance de sous-traitance

246
0:13:50.118,000 --> 0:13:51,000
qui a lieu.

247
0:13:51.618,000 --> 0:13:52,000
Avec les réseaux sociaux,

248
0:13:53.038,000 --> 0:13:56,000
nous faisons face à une structure complètement nouvelle

249
0:13:56.077,000 --> 0:13:57,000
avec ses propres mécanismes,

250
0:13:57.617,000 --> 0:13:58,000
sa propre logique de fonctionnement

251
0:13:59.3,000 --> 0:14:04,000
et de ce fait, également ses propres dangers

252
0:14:04.489,000 --> 0:14:08,000
qui n'existaient pas encore dans la sphère publique pré-numérique.

253
0:14:08.538,000 --> 0:14:1,000
HB : Quand Mark Zuckerberg était au Congrès américain

254
0:14:11.011,000 --> 0:14:12,000
ou au Parlement européen,

255
0:14:12.565,000 --> 0:14:14,000
il a été confronté à tous genres de critiques.

256
0:14:15.224,000 --> 0:14:17,000
Sa réaction était toujours la même :

257
0:14:18.501,000 --> 0:14:19,000
« Nous arrangerons cela

258
0:14:19.993,000 --> 0:14:21,000
et je vais en assurer le suivi avec mon équipe. »

259
0:14:23.167,000 --> 0:14:26,000
Mais un tel débat ne devra pas se tenir dans une arrière-salle de Facebook,

260
0:14:26.969,000 --> 0:14:27,000
Twitter ou Google --

261
0:14:28.278,000 --> 0:14:3,000
un tel débat devrait être ouvertement discuté

262
0:14:31.028,000 --> 0:14:33,000
dans de nouveaux parlements cosmopolites,

263
0:14:33.118,000 --> 0:14:37,000
dans de nouvelles institutions reflétant la diversité des gens

264
0:14:38.002,000 --> 0:14:42,000
contribuant au projet utopique d'un réseau mondial.

265
0:14:42.568,000 --> 0:14:45,000
Si cela peut sembler impossible de considérer les valeurs

266
0:14:45.969,000 --> 0:14:47,000
des utilisateurs à travers le monde,

267
0:14:48.235,000 --> 0:14:49,000
cela vaut la peine de croire

268
0:14:49.941,000 --> 0:14:52,000
qu'il y a plus de choses qui nous lient que de choses qui nous séparent.

269
0:14:53.624,000 --> 0:14:56,000
MR : A une époque où le populisme gagne en vigueur,

270
0:14:57.355,000 --> 0:15:,000
il devient populaire de justifier les symptômes,

271
0:15:00.577,000 --> 0:15:01,000
de les éradiquer,

272
0:15:01.879,000 --> 0:15:02,000
de les rendre invisibles.

273
0:15:04.919,000 --> 0:15:07,000
Cette idéologie se propage à travers le monde,

274
0:15:08.292,000 --> 0:15:1,000
analogue ainsi que numérique,

275
0:15:11.903,000 --> 0:15:14,000
et il est de notre devoir de l'arrêter

276
0:15:15.419,000 --> 0:15:16,000
avant qu'il ne soit trop tard.

277
0:15:17.665,000 --> 0:15:2,000
La question de la liberté et de la démocratie

278
0:15:21.673,000 --> 0:15:23,000
ne doit pas avoir seulement ces deux options.

279
0:15:25.053,000 --> 0:15:26,000
HB : Supprimer.

280
0:15:26.243,000 --> 0:15:28,000
MR : Ou ignorer.

281
0:15:29.3,000 --> 0:15:3,000
HB : Merci beaucoup.

282
0:15:30.921,000 --> 0:15:35,000
(Applaudissements)

