1
0:00:12.787,000 --> 0:00:14,000
I'm going to begin with a scary question:

2
0:00:15.659,000 --> 0:00:17,000
Are we headed toward a future without jobs?

3
0:00:18.987,000 --> 0:00:2,000
The remarkable progress that we're seeing

4
0:00:21.08,000 --> 0:00:22,000
in technologies like self-driving cars

5
0:00:22.965,000 --> 0:00:25,000
has led to an explosion of interest in this question,

6
0:00:26.054,000 --> 0:00:28,000
but because it's something that's been asked

7
0:00:28.228,000 --> 0:00:29,000
so many times in the past,

8
0:00:29.508,000 --> 0:00:3,000
maybe what we should really be asking

9
0:00:31.372,000 --> 0:00:33,000
is whether this time is really different.

10
0:00:35.252,000 --> 0:00:37,000
The fear that automation might displace workers

11
0:00:38.237,000 --> 0:00:4,000
and potentially lead to lots of unemployment

12
0:00:40.378,000 --> 0:00:43,000
goes back at a minimum 200 years to the Luddite revolts in England.

13
0:00:44.29,000 --> 0:00:47,000
And since then, this concern has come up again and again.

14
0:00:47.51,000 --> 0:00:48,000
I'm going to guess

15
0:00:48.695,000 --> 0:00:52,000
that most of you have probably never heard of the Triple Revolution report,

16
0:00:53.185,000 --> 0:00:55,000
but this was a very prominent report.

17
0:00:55.502,000 --> 0:00:57,000
It was put together by a brilliant group of people --

18
0:00:58.057,000 --> 0:01:01,000
it actually included two Nobel laureates --

19
0:01:01.138,000 --> 0:01:04,000
and this report was presented to the President of the United States,

20
0:01:04.385,000 --> 0:01:09,000
and it argued that the US was on the brink of economic and social upheaval

21
0:01:09.903,000 --> 0:01:12,000
because industrial automation was going to put millions of people

22
0:01:13.029,000 --> 0:01:14,000
out of work.

23
0:01:14.205,000 --> 0:01:17,000
Now, that report was delivered to President Lyndon Johnson

24
0:01:17.886,000 --> 0:01:18,000
in March of 1964.

25
0:01:19.711,000 --> 0:01:21,000
So that's now over 50 years,

26
0:01:21.951,000 --> 0:01:23,000
and, of course, that hasn't really happened.

27
0:01:24.033,000 --> 0:01:26,000
And that's been the story again and again.

28
0:01:26.201,000 --> 0:01:28,000
This alarm has been raised repeatedly,

29
0:01:28.334,000 --> 0:01:3,000
but it's always been a false alarm.

30
0:01:30.371,000 --> 0:01:31,000
And because it's been a false alarm,

31
0:01:32.204,000 --> 0:01:34,000
it's led to a very conventional way of thinking about this.

32
0:01:35.035,000 --> 0:01:37,000
And that says essentially that yes,

33
0:01:37.591,000 --> 0:01:39,000
technology may devastate entire industries.

34
0:01:40.163,000 --> 0:01:43,000
It may wipe out whole occupations and types of work.

35
0:01:43.919,000 --> 0:01:44,000
But at the same time, of course,

36
0:01:45.551,000 --> 0:01:47,000
progress is going to lead to entirely new things.

37
0:01:47.926,000 --> 0:01:49,000
So there will be new industries that will arise in the future,

38
0:01:50.912,000 --> 0:01:52,000
and those industries, of course, will have to hire people.

39
0:01:53.794,000 --> 0:01:55,000
There'll be new kinds of work that will appear,

40
0:01:56.056,000 --> 0:01:59,000
and those might be things that today we can't really even imagine.

41
0:01:59.29,000 --> 0:02:,000
And that has been the story so far,

42
0:02:01.061,000 --> 0:02:02,000
and it's been a positive story.

43
0:02:03.095,000 --> 0:02:06,000
It turns out that the new jobs that have been created

44
0:02:06.444,000 --> 0:02:08,000
have generally been a lot better than the old ones.

45
0:02:08.938,000 --> 0:02:1,000
They have, for example, been more engaging.

46
0:02:11.618,000 --> 0:02:14,000
They've been in safer, more comfortable work environments,

47
0:02:15.071,000 --> 0:02:16,000
and, of course, they've paid more.

48
0:02:16.775,000 --> 0:02:17,000
So it has been a positive story.

49
0:02:18.664,000 --> 0:02:2,000
That's the way things have played out so far.

50
0:02:21.292,000 --> 0:02:23,000
But there is one particular class of worker

51
0:02:24.264,000 --> 0:02:26,000
for whom the story has been quite different.

52
0:02:27.938,000 --> 0:02:28,000
For these workers,

53
0:02:29.112,000 --> 0:02:32,000
technology has completely decimated their work,

54
0:02:32.157,000 --> 0:02:35,000
and it really hasn't created any new opportunities at all.

55
0:02:35.395,000 --> 0:02:37,000
And these workers, of course,

56
0:02:37.614,000 --> 0:02:38,000
are horses.

57
0:02:38.926,000 --> 0:02:39,000
(Laughter)

58
0:02:40.393,000 --> 0:02:42,000
So I can ask a very provocative question:

59
0:02:43.167,000 --> 0:02:46,000
Is it possible that at some point in the future,

60
0:02:46.626,000 --> 0:02:5,000
a significant fraction of the human workforce is going to be made redundant

61
0:02:51.278,000 --> 0:02:52,000
in the way that horses were?

62
0:02:53.485,000 --> 0:02:56,000
Now, you might have a very visceral, reflexive reaction to that.

63
0:02:56.509,000 --> 0:02:57,000
You might say, "That's absurd.

64
0:02:58.18,000 --> 0:03:01,000
How can you possibly compare human beings to horses?"

65
0:03:02.437,000 --> 0:03:03,000
Horses, of course, are very limited,

66
0:03:04.23,000 --> 0:03:06,000
and when cars and trucks and tractors came along,

67
0:03:07.147,000 --> 0:03:09,000
horses really had nowhere else to turn.

68
0:03:09.844,000 --> 0:03:11,000
People, on the other hand, are intelligent;

69
0:03:12.228,000 --> 0:03:13,000
we can learn, we can adapt.

70
0:03:14.037,000 --> 0:03:15,000
And in theory,

71
0:03:15.225,000 --> 0:03:18,000
that ought to mean that we can always find something new to do,

72
0:03:18.376,000 --> 0:03:21,000
and that we can always remain relevant to the future economy.

73
0:03:21.706,000 --> 0:03:23,000
But here's the really critical thing to understand.

74
0:03:24.79,000 --> 0:03:26,000
The machines that will threaten workers in the future

75
0:03:27.679,000 --> 0:03:3,000
are really nothing like those cars and trucks and tractors

76
0:03:30.937,000 --> 0:03:31,000
that displaced horses.

77
0:03:32.577,000 --> 0:03:36,000
The future is going to be full of thinking, learning, adapting machines.

78
0:03:37.44,000 --> 0:03:38,000
And what that really means

79
0:03:38.872,000 --> 0:03:4,000
is that technology is finally beginning to encroach

80
0:03:41.73,000 --> 0:03:43,000
on that fundamental human capability --

81
0:03:44.603,000 --> 0:03:46,000
the thing that makes us so different from horses,

82
0:03:47.43,000 --> 0:03:49,000
and the very thing that, so far,

83
0:03:49.688,000 --> 0:03:51,000
has allowed us to stay ahead of the march of progress

84
0:03:52.359,000 --> 0:03:53,000
and remain relevant,

85
0:03:53.572,000 --> 0:03:56,000
and, in fact, indispensable to the economy.

86
0:03:58.407,000 --> 0:04:,000
So what is it that is really so different

87
0:04:00.926,000 --> 0:04:02,000
about today's information technology

88
0:04:02.993,000 --> 0:04:03,000
relative to what we've seen in the past?

89
0:04:04.964,000 --> 0:04:06,000
I would point to three fundamental things.

90
0:04:07.641,000 --> 0:04:11,000
The first thing is that we have seen this ongoing process

91
0:04:12.074,000 --> 0:04:13,000
of exponential acceleration.

92
0:04:14.42,000 --> 0:04:16,000
I know you all know about Moore's law,

93
0:04:16.539,000 --> 0:04:18,000
but in fact, it's more broad-based than that;

94
0:04:18.859,000 --> 0:04:21,000
it extends in many cases, for example, to software,

95
0:04:22.033,000 --> 0:04:25,000
it extends to communications, bandwidth and so forth.

96
0:04:25.057,000 --> 0:04:26,000
But the really key thing to understand

97
0:04:27.065,000 --> 0:04:3,000
is that this acceleration has now been going on for a really long time.

98
0:04:30.96,000 --> 0:04:31,000
In fact, it's been going on for decades.

99
0:04:32.909,000 --> 0:04:34,000
If you measure from the late 1950s,

100
0:04:35.689,000 --> 0:04:37,000
when the first integrated circuits were fabricated,

101
0:04:38.138,000 --> 0:04:42,000
we've seen something on the order of 30 doublings in computational power

102
0:04:42.947,000 --> 0:04:43,000
since then.

103
0:04:44.127,000 --> 0:04:47,000
That's just an extraordinary number of times to double any quantity,

104
0:04:47.839,000 --> 0:04:48,000
and what it really means

105
0:04:49.103,000 --> 0:04:51,000
is that we're now at a point where we're going to see

106
0:04:51.651,000 --> 0:04:53,000
just an extraordinary amount of absolute progress,

107
0:04:54.086,000 --> 0:04:56,000
and, of course, things are going to continue to also accelerate

108
0:04:57.085,000 --> 0:04:58,000
from this point.

109
0:04:58.268,000 --> 0:05:,000
So as we look forward to the coming years and decades,

110
0:05:00.832,000 --> 0:05:02,000
I think that means that we're going to see things

111
0:05:03.194,000 --> 0:05:04,000
that we're really not prepared for.

112
0:05:04.891,000 --> 0:05:06,000
We're going to see things that astonish us.

113
0:05:06.992,000 --> 0:05:07,000
The second key thing

114
0:05:08.282,000 --> 0:05:11,000
is that the machines are, in a limited sense, beginning to think.

115
0:05:12.212,000 --> 0:05:14,000
And by this, I don't mean human-level AI,

116
0:05:14.693,000 --> 0:05:16,000
or science fiction artificial intelligence;

117
0:05:17.653,000 --> 0:05:21,000
I simply mean that machines and algorithms are making decisions.

118
0:05:22.139,000 --> 0:05:25,000
They're solving problems, and most importantly, they're learning.

119
0:05:26.023,000 --> 0:05:29,000
In fact, if there's one technology that is truly central to this

120
0:05:29.35,000 --> 0:05:32,000
and has really become the driving force behind this,

121
0:05:32.451,000 --> 0:05:33,000
it's machine learning,

122
0:05:33.647,000 --> 0:05:35,000
which is just becoming this incredibly powerful,

123
0:05:36.391,000 --> 0:05:38,000
disruptive, scalable technology.

124
0:05:39.561,000 --> 0:05:41,000
One of the best examples I've seen of that recently

125
0:05:42.054,000 --> 0:05:44,000
was what Google's DeepMind division was able to do

126
0:05:44.829,000 --> 0:05:45,000
with its AlphaGo system.

127
0:05:46.406,000 --> 0:05:5,000
Now, this is the system that was able to beat the best player in the world

128
0:05:50.73,000 --> 0:05:51,000
at the ancient game of Go.

129
0:05:52.733,000 --> 0:05:53,000
Now, at least to me,

130
0:05:53.907,000 --> 0:05:56,000
there are two things that really stand out about the game of Go.

131
0:05:57.048,000 --> 0:05:59,000
One is that as you're playing the game,

132
0:05:59.368,000 --> 0:06:01,000
the number of configurations that the board can be in

133
0:06:02.258,000 --> 0:06:03,000
is essentially infinite.

134
0:06:03.693,000 --> 0:06:06,000
There are actually more possibilities than there are atoms in the universe.

135
0:06:07.98,000 --> 0:06:08,000
So what that means is,

136
0:06:09.188,000 --> 0:06:12,000
you're never going to be able to build a computer to win at the game of Go

137
0:06:12.809,000 --> 0:06:14,000
the way chess was approached, for example,

138
0:06:15.013,000 --> 0:06:19,000
which is basically to throw brute-force computational power at it.

139
0:06:19.563,000 --> 0:06:23,000
So clearly, a much more sophisticated, thinking-like approach is needed.

140
0:06:24.368,000 --> 0:06:27,000
The second thing that really stands out is that,

141
0:06:27.663,000 --> 0:06:29,000
if you talk to one of the championship Go players,

142
0:06:30.334,000 --> 0:06:34,000
this person cannot necessarily even really articulate what exactly it is

143
0:06:34.843,000 --> 0:06:36,000
they're thinking about as they play the game.

144
0:06:37.082,000 --> 0:06:39,000
It's often something that's very intuitive,

145
0:06:39.299,000 --> 0:06:42,000
it's almost just like a feeling about which move they should make.

146
0:06:42.645,000 --> 0:06:43,000
So given those two qualities,

147
0:06:44.076,000 --> 0:06:47,000
I would say that playing Go at a world champion level

148
0:06:48.037,000 --> 0:06:51,000
really ought to be something that's safe from automation,

149
0:06:51.299,000 --> 0:06:55,000
and the fact that it isn't should really raise a cautionary flag for us.

150
0:06:55.769,000 --> 0:06:58,000
And the reason is that we tend to draw a very distinct line,

151
0:06:59.71,000 --> 0:07:02,000
and on one side of that line are all the jobs and tasks

152
0:07:03.243,000 --> 0:07:07,000
that we perceive as being on some level fundamentally routine and repetitive

153
0:07:08.015,000 --> 0:07:09,000
and predictable.

154
0:07:09.389,000 --> 0:07:11,000
And we know that these jobs might be in different industries,

155
0:07:12.271,000 --> 0:07:15,000
they might be in different occupations and at different skill levels,

156
0:07:15.668,000 --> 0:07:17,000
but because they are innately predictable,

157
0:07:17.902,000 --> 0:07:2,000
we know they're probably at some point going to be susceptible

158
0:07:21.053,000 --> 0:07:22,000
to machine learning,

159
0:07:22.254,000 --> 0:07:23,000
and therefore, to automation.

160
0:07:23.697,000 --> 0:07:25,000
And make no mistake -- that's a lot of jobs.

161
0:07:25.818,000 --> 0:07:27,000
That's probably something on the order of roughly half

162
0:07:28.521,000 --> 0:07:29,000
the jobs in the economy.

163
0:07:30.112,000 --> 0:07:32,000
But then on the other side of that line,

164
0:07:32.295,000 --> 0:07:36,000
we have all the jobs that require some capability

165
0:07:36.39,000 --> 0:07:38,000
that we perceive as being uniquely human,

166
0:07:38.786,000 --> 0:07:4,000
and these are the jobs that we think are safe.

167
0:07:41.033,000 --> 0:07:43,000
Now, based on what I know about the game of Go,

168
0:07:43.322,000 --> 0:07:46,000
I would've guessed that it really ought to be on the safe side of that line.

169
0:07:47.049,000 --> 0:07:5,000
But the fact that it isn't, and that Google solved this problem,

170
0:07:50.251,000 --> 0:07:52,000
suggests that that line is going to be very dynamic.

171
0:07:52.707,000 --> 0:07:53,000
It's going to shift,

172
0:07:53.91,000 --> 0:07:57,000
and it's going to shift in a way that consumes more and more jobs and tasks

173
0:07:58.069,000 --> 0:08:01,000
that we currently perceive as being safe from automation.

174
0:08:01.921,000 --> 0:08:02,000
The other key thing to understand

175
0:08:03.602,000 --> 0:08:08,000
is that this is by no means just about low-wage jobs or blue-collar jobs,

176
0:08:08.764,000 --> 0:08:09,000
or jobs and tasks done by people

177
0:08:10.663,000 --> 0:08:12,000
that have relatively low levels of education.

178
0:08:12.791,000 --> 0:08:13,000
There's lots of evidence to show

179
0:08:14.339,000 --> 0:08:17,000
that these technologies are rapidly climbing the skills ladder.

180
0:08:17.523,000 --> 0:08:2,000
So we already see an impact on professional jobs --

181
0:08:21.163,000 --> 0:08:25,000
tasks done by people like accountants,

182
0:08:25.622,000 --> 0:08:26,000
financial analysts,

183
0:08:26.963,000 --> 0:08:27,000
journalists,

184
0:08:28.283,000 --> 0:08:3,000
lawyers, radiologists and so forth.

185
0:08:30.684,000 --> 0:08:31,000
So a lot of the assumptions that we make

186
0:08:32.646,000 --> 0:08:35,000
about the kind of occupations and tasks and jobs

187
0:08:35.89,000 --> 0:08:37,000
that are going to be threatened by automation in the future

188
0:08:38.733,000 --> 0:08:4,000
are very likely to be challenged going forward.

189
0:08:40.955,000 --> 0:08:41,000
So as we put these trends together,

190
0:08:42.679,000 --> 0:08:45,000
I think what it shows is that we could very well end up in a future

191
0:08:45.995,000 --> 0:08:46,000
with significant unemployment.

192
0:08:48.254,000 --> 0:08:49,000
Or at a minimum,

193
0:08:49.43,000 --> 0:08:52,000
we could face lots of underemployment or stagnant wages,

194
0:08:53.235,000 --> 0:08:55,000
maybe even declining wages.

195
0:08:56.142,000 --> 0:08:58,000
And, of course, soaring levels of inequality.

196
0:08:58.976,000 --> 0:09:02,000
All of that, of course, is going to put a terrific amount of stress

197
0:09:03.033,000 --> 0:09:04,000
on the fabric of society.

198
0:09:04.974,000 --> 0:09:07,000
But beyond that, there's also a fundamental economic problem,

199
0:09:08.057,000 --> 0:09:13,000
and that arises because jobs are currently the primary mechanism

200
0:09:13.276,000 --> 0:09:16,000
that distributes income, and therefore purchasing power,

201
0:09:16.845,000 --> 0:09:21,000
to all the consumers that buy the products and services we're producing.

202
0:09:22.831,000 --> 0:09:24,000
In order to have a vibrant market economy,

203
0:09:25.37,000 --> 0:09:27,000
you've got to have lots and lots of consumers

204
0:09:27.514,000 --> 0:09:3,000
that are really capable of buying the products and services

205
0:09:30.567,000 --> 0:09:31,000
that are being produced.

206
0:09:31.742,000 --> 0:09:33,000
If you don't have that, then you run the risk

207
0:09:34.152,000 --> 0:09:35,000
of economic stagnation,

208
0:09:35.591,000 --> 0:09:38,000
or maybe even a declining economic spiral,

209
0:09:39.284,000 --> 0:09:41,000
as there simply aren't enough customers out there

210
0:09:41.622,000 --> 0:09:43,000
to buy the products and services being produced.

211
0:09:44.105,000 --> 0:09:45,000
It's really important to realize

212
0:09:46.057,000 --> 0:09:52,000
that all of us as individuals rely on access to that market economy

213
0:09:52.095,000 --> 0:09:53,000
in order to be successful.

214
0:09:53.848,000 --> 0:09:57,000
You can visualize that by thinking in terms of one really exceptional person.

215
0:09:58.308,000 --> 0:10:,000
Imagine for a moment you take, say, Steve Jobs,

216
0:10:01.32,000 --> 0:10:03,000
and you drop him on an island all by himself.

217
0:10:03.925,000 --> 0:10:05,000
On that island, he's going to be running around,

218
0:10:06.243,000 --> 0:10:08,000
gathering coconuts just like anyone else.

219
0:10:08.805,000 --> 0:10:1,000
He's really not going to be anything special,

220
0:10:11.017,000 --> 0:10:14,000
and the reason, of course, is that there is no market

221
0:10:14.213,000 --> 0:10:16,000
for him to scale his incredible talents across.

222
0:10:17.023,000 --> 0:10:2,000
So access to this market is really critical to us as individuals,

223
0:10:20.517,000 --> 0:10:24,000
and also to the entire system in terms of it being sustainable.

224
0:10:25.063,000 --> 0:10:28,000
So the question then becomes: What exactly could we do about this?

225
0:10:29.285,000 --> 0:10:32,000
And I think you can view this through a very utopian framework.

226
0:10:32.541,000 --> 0:10:34,000
You can imagine a future where we all have to work less,

227
0:10:35.208,000 --> 0:10:38,000
we have more time for leisure,

228
0:10:38.233,000 --> 0:10:39,000
more time to spend with our families,

229
0:10:40.185,000 --> 0:10:43,000
more time to do things that we find genuinely rewarding

230
0:10:43.464,000 --> 0:10:44,000
and so forth.

231
0:10:44.645,000 --> 0:10:45,000
And I think that's a terrific vision.

232
0:10:46.524,000 --> 0:10:49,000
That's something that we should absolutely strive to move toward.

233
0:10:50.177,000 --> 0:10:52,000
But at the same time, I think we have to be realistic,

234
0:10:52.877,000 --> 0:10:53,000
and we have to realize

235
0:10:54.294,000 --> 0:10:58,000
that we're very likely to face a significant income distribution problem.

236
0:10:59.178,000 --> 0:11:01,000
A lot of people are likely to be left behind.

237
0:11:03.186,000 --> 0:11:05,000
And I think that in order to solve that problem,

238
0:11:05.614,000 --> 0:11:07,000
we're ultimately going to have to find a way

239
0:11:07.736,000 --> 0:11:09,000
to decouple incomes from traditional work.

240
0:11:10.366,000 --> 0:11:12,000
And the best, more straightforward way I know to do that

241
0:11:13.256,000 --> 0:11:16,000
is some kind of a guaranteed income or universal basic income.

242
0:11:16.848,000 --> 0:11:18,000
Now, basic income is becoming a very important idea.

243
0:11:19.36,000 --> 0:11:21,000
It's getting a lot of traction and attention,

244
0:11:21.523,000 --> 0:11:23,000
there are a lot of important pilot projects

245
0:11:23.82,000 --> 0:11:25,000
and experiments going on throughout the world.

246
0:11:26.628,000 --> 0:11:29,000
My own view is that a basic income is not a panacea;

247
0:11:29.852,000 --> 0:11:31,000
it's not necessarily a plug-and-play solution,

248
0:11:32.408,000 --> 0:11:33,000
but rather, it's a place to start.

249
0:11:34.067,000 --> 0:11:36,000
It's an idea that we can build on and refine.

250
0:11:36.873,000 --> 0:11:38,000
For example, one thing that I have written quite a lot about

251
0:11:39.714,000 --> 0:11:43,000
is the possibility of incorporating explicit incentives into a basic income.

252
0:11:44.93,000 --> 0:11:45,000
To illustrate that,

253
0:11:46.123,000 --> 0:11:48,000
imagine that you are a struggling high school student.

254
0:11:48.915,000 --> 0:11:5,000
Imagine that you are at risk of dropping out of school.

255
0:11:52.289,000 --> 0:11:55,000
And yet, suppose you know that at some point in the future,

256
0:11:55.691,000 --> 0:11:56,000
no matter what,

257
0:11:56.939,000 --> 0:11:59,000
you're going to get the same basic income as everyone else.

258
0:12:00.66,000 --> 0:12:03,000
Now, to my mind, that creates a very perverse incentive

259
0:12:03.726,000 --> 0:12:05,000
for you to simply give up and drop out of school.

260
0:12:06.247,000 --> 0:12:08,000
So I would say, let's not structure things that way.

261
0:12:08.776,000 --> 0:12:13,000
Instead, let's pay people who graduate from high school somewhat more

262
0:12:14.116,000 --> 0:12:15,000
than those who simply drop out.

263
0:12:16.329,000 --> 0:12:19,000
And we can take that idea of building incentives into a basic income,

264
0:12:19.831,000 --> 0:12:2,000
and maybe extend it to other areas.

265
0:12:21.522,000 --> 0:12:24,000
For example, we might create an incentive to work in the community

266
0:12:25.123,000 --> 0:12:26,000
to help others,

267
0:12:26.305,000 --> 0:12:29,000
or perhaps to do positive things for the environment,

268
0:12:29.393,000 --> 0:12:3,000
and so forth.

269
0:12:30.587,000 --> 0:12:33,000
So by incorporating incentives into a basic income,

270
0:12:33.622,000 --> 0:12:34,000
we might actually improve it,

271
0:12:35.275,000 --> 0:12:37,000
and also, perhaps, take at least a couple of steps

272
0:12:37.925,000 --> 0:12:39,000
towards solving another problem

273
0:12:40.374,000 --> 0:12:42,000
that I think we're quite possibly going to face in the future,

274
0:12:43.342,000 --> 0:12:46,000
and that is, how do we all find meaning and fulfillment,

275
0:12:47.118,000 --> 0:12:49,000
and how do we occupy our time

276
0:12:49.46,000 --> 0:12:53,000
in a world where perhaps there's less demand for traditional work?

277
0:12:54.201,000 --> 0:12:56,000
So by extending and refining a basic income,

278
0:12:57.03,000 --> 0:12:59,000
I think we can make it look better,

279
0:12:59.39,000 --> 0:13:04,000
and we can also, perhaps, make it more politically and socially acceptable

280
0:13:04.712,000 --> 0:13:05,000
and feasible --

281
0:13:05.9,000 --> 0:13:06,000
and, of course, by doing that,

282
0:13:07.398,000 --> 0:13:1,000
we increase the odds that it will actually come to be.

283
0:13:11.731,000 --> 0:13:13,000
I think one of the most fundamental,

284
0:13:14.025,000 --> 0:13:16,000
almost instinctive objections

285
0:13:16.217,000 --> 0:13:19,000
that many of us have to the idea of a basic income,

286
0:13:19.694,000 --> 0:13:22,000
or really to any significant expansion of the safety net,

287
0:13:23.45,000 --> 0:13:26,000
is this fear that we're going to end up with too many people

288
0:13:27.234,000 --> 0:13:28,000
riding in the economic cart,

289
0:13:28.996,000 --> 0:13:3,000
and not enough people pulling that cart.

290
0:13:31.067,000 --> 0:13:33,000
And yet, really, the whole point I'm making here, of course,

291
0:13:33.925,000 --> 0:13:34,000
is that in the future,

292
0:13:35.31,000 --> 0:13:38,000
machines are increasingly going to be capable of pulling that cart for us.

293
0:13:39.16,000 --> 0:13:4,000
That should give us more options

294
0:13:41.174,000 --> 0:13:44,000
for the way we structure our society and our economy,

295
0:13:45.009,000 --> 0:13:48,000
And I think eventually, it's going to go beyond simply being an option,

296
0:13:48.475,000 --> 0:13:49,000
and it's going to become an imperative.

297
0:13:50.4,000 --> 0:13:52,000
The reason, of course, is that all of this is going to put

298
0:13:53.246,000 --> 0:13:55,000
such a degree of stress on our society,

299
0:13:55.284,000 --> 0:13:57,000
and also because jobs are that mechanism

300
0:13:57.822,000 --> 0:13:58,000
that gets purchasing power to consumers

301
0:13:59.811,000 --> 0:14:01,000
so they can then drive the economy.

302
0:14:02.351,000 --> 0:14:05,000
If, in fact, that mechanism begins to erode in the future,

303
0:14:05.922,000 --> 0:14:07,000
then we're going to need to replace it with something else

304
0:14:08.761,000 --> 0:14:09,000
or we're going to face the risk

305
0:14:10.348,000 --> 0:14:12,000
that our whole system simply may not be sustainable.

306
0:14:12.939,000 --> 0:14:14,000
But the bottom line here is that I really think

307
0:14:15.345,000 --> 0:14:17,000
that solving these problems,

308
0:14:17.805,000 --> 0:14:2,000
and especially finding a way to build a future economy

309
0:14:21.229,000 --> 0:14:23,000
that works for everyone,

310
0:14:23.266,000 --> 0:14:24,000
at every level of our society,

311
0:14:25.151,000 --> 0:14:28,000
is going to be one of the most important challenges that we all face

312
0:14:28.715,000 --> 0:14:3,000
in the coming years and decades.

313
0:14:30.782,000 --> 0:14:31,000
Thank you very much.

314
0:14:32.054,000 --> 0:14:33,000
(Applause)

