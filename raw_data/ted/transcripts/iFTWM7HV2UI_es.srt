1
0:00:,000 --> 0:00:07,000
Traductor: Claudia Viveros Revisor: Analia Padin

2
0:00:12.76,000 --> 0:00:15,000
Cuando la gente manifiesta temor por la inteligencia artificial,

3
0:00:16.32,000 --> 0:00:19,000
muchas veces recurre a imágenes de robots humanoides enloquecidos.

4
0:00:20.32,000 --> 0:00:21,000
Ya saben: Terminator.

5
0:00:22.4,000 --> 0:00:24,000
Quizá debamos considerarlo,

6
0:00:24.76,000 --> 0:00:25,000
pero es una amenaza lejana.

7
0:00:26.64,000 --> 0:00:29,000
O si no, nos inquietamos por la vigilancia electrónica

8
0:00:30.12,000 --> 0:00:31,000
con metáforas del pasado.

9
0:00:31.92,000 --> 0:00:33,000
"1984", el "1984" de George Orwell,

10
0:00:34.6,000 --> 0:00:36,000
es un libro superventas otra vez.

11
0:00:37.96,000 --> 0:00:38,000
Es un gran libro,

12
0:00:39.4,000 --> 0:00:42,000
pero no es la distopía correcta para el siglo XXI.

13
0:00:44.08,000 --> 0:00:45,000
Lo que debemos temer más

14
0:00:45.52,000 --> 0:00:49,000
no es lo que la inteligencia artificial nos hará por sí misma,

15
0:00:50.32,000 --> 0:00:54,000
sino cómo la gente en el poder usará la inteligencia artificial

16
0:00:55.08,000 --> 0:00:57,000
para controlarnos y manipularnos

17
0:00:57.92,000 --> 0:01:,000
de maneras nuevas, a veces escondidas,

18
0:01:01.08,000 --> 0:01:04,000
sutiles e inesperadas.

19
0:01:04.12,000 --> 0:01:05,000
Mucha de la tecnología

20
0:01:06,000 --> 0:01:1,000
que amenaza nuestra libertad y dignidad en un futuro cercano

21
0:01:10.36,000 --> 0:01:11,000
la están desarrollando compañías

22
0:01:12.24,000 --> 0:01:16,000
que se dedican a capturar y vender nuestra información y nuestra atención

23
0:01:17.2,000 --> 0:01:19,000
a anunciantes y demás:

24
0:01:19.48,000 --> 0:01:22,000
Facebook, Google, Amazon,

25
0:01:22.92,000 --> 0:01:23,000
Alibaba, Tencent.

26
0:01:26.04,000 --> 0:01:31,000
La inteligencia artificial ha comenzado a respaldar esos negocios también.

27
0:01:31.56,000 --> 0:01:33,000
Y parece que la inteligencia artificial

28
0:01:33.68,000 --> 0:01:35,000
es lo que le sigue a los anuncios en línea.

29
0:01:36.56,000 --> 0:01:37,000
Pero no lo es.

30
0:01:37.8,000 --> 0:01:39,000
Es un salto de categoría.

31
0:01:40.28,000 --> 0:01:42,000
Es un mundo totalmente distinto,

32
0:01:42.88,000 --> 0:01:44,000
y tiene un gran potencial.

33
0:01:45.52,000 --> 0:01:48,000
Podría acelerar nuestro entendimiento

34
0:01:48.888,000 --> 0:01:51,000
de muchas áreas de estudio e investigación.

35
0:01:53.12,000 --> 0:01:56,000
Pero, parafraseando a un famoso filósofo hollywoodense,

36
0:01:56.64,000 --> 0:01:59,000
"Un enorme potencial viene con un enorme riesgo".

37
0:02:01.12,000 --> 0:02:04,000
Ahora hablemos de un hecho básico de nuestra vida digital: los anuncios.

38
0:02:05.08,000 --> 0:02:07,000
Como que los ignoramos, ¿no?

39
0:02:08,000 --> 0:02:09,000
Parecen ordinarios, inefectivos.

40
0:02:1,000 --> 0:02:14,000
Todos hemos tenido esa experiencia de ser perseguidos en la web

41
0:02:14.28,000 --> 0:02:16,000
por un anuncio basado en algo que buscamos o leímos.

42
0:02:17.08,000 --> 0:02:18,000
Ya saben, buscas un par de botas

43
0:02:18.96,000 --> 0:02:21,000
y durante una semana, esas botas te siguen a todos lados donde vayas.

44
0:02:22.36,000 --> 0:02:25,000
Incluso después de haber sucumbido a comprarlas, te continúan siguiendo.

45
0:02:26.04,000 --> 0:02:29,000
Estamos habituados a ese tipo de manipulación simple y barata.

46
0:02:29.08,000 --> 0:02:32,000
Ponemos los ojos en blanco y pensamos, "¿Sabes qué? Esto no funciona".

47
0:02:33.72,000 --> 0:02:35,000
Excepto que, en línea,

48
0:02:35.84,000 --> 0:02:38,000
las tecnologías digitales no son solo anuncios.

49
0:02:40.24,000 --> 0:02:43,000
Para entender eso, pensemos en un ejemplo del mundo físico.

50
0:02:43.84,000 --> 0:02:47,000
¿Han visto que en la caja de cobro del supermercado, cerca del cajero,

51
0:02:48.52,000 --> 0:02:51,000
hay dulces y goma de mascar a la altura de los ojos de los niños?

52
0:02:52.8,000 --> 0:02:55,000
Eso está diseñado para hacerlos rogar a sus padres

53
0:02:56.32,000 --> 0:02:59,000
justo cuando los padres están por pagar.

54
0:03:00.04,000 --> 0:03:02,000
Eso es arquitectura de la persuasión.

55
0:03:03.16,000 --> 0:03:06,000
No es agradable, pero funciona.

56
0:03:06.28,000 --> 0:03:08,000
Por eso se ve en todos los supermercados.

57
0:03:08.72,000 --> 0:03:09,000
Ahora, en el mundo físico,

58
0:03:10.436,000 --> 0:03:12,000
la arquitectura de la persuasión es un poco limitada,

59
0:03:12.976,000 --> 0:03:16,000
porque solo puedes poner unas cuantas cosas cerca de la caja, ¿no?

60
0:03:17.8,000 --> 0:03:21,000
Y los dulces y goma de mascar son iguales para todos,

61
0:03:22.12,000 --> 0:03:23,000
aunque en general funciona

62
0:03:23.6,000 --> 0:03:27,000
solo con la gente que va acompañada de personitas caprichosas.

63
0:03:29.16,000 --> 0:03:32,000
En el mundo físico, vivimos con esas limitantes.

64
0:03:34.28,000 --> 0:03:35,000
No obstante, en el mundo digital,

65
0:03:36.24,000 --> 0:03:4,000
las arquitecturas de la persuasión pueden tener miles de millones de opciones

66
0:03:41.84,000 --> 0:03:44,000
y pueden apuntar, inferir, entender

67
0:03:45.72,000 --> 0:03:47,000
y ser aplicadas a cada individuo

68
0:03:48.64,000 --> 0:03:49,000
uno por uno

69
0:03:49.88,000 --> 0:03:51,000
descubriendo nuestras debilidades,

70
0:03:52.04,000 --> 0:03:57,000
y pueden ser enviadas directamente a la pantalla personal de cada teléfono,

71
0:03:57.68,000 --> 0:03:59,000
así que son invisibles para nosotros.

72
0:03:59.96,000 --> 0:04:,000
Y eso es diferente.

73
0:04:01.24,000 --> 0:04:04,000
Es solo una de las cosas que puede hacer la inteligencia artificial.

74
0:04:04.68,000 --> 0:04:05,000
Pongamos un ejemplo.

75
0:04:05.75,000 --> 0:04:08,000
Digamos que quieren vender boletos de avión a Las Vegas.

76
0:04:08.92,000 --> 0:04:11,000
En el viejo mundo, se dirigirían a ciertos sectores demográficos

77
0:04:12.44,000 --> 0:04:14,000
basándose en la experiencia y lo que podían suponer.

78
0:04:15.56,000 --> 0:04:17,000
Podrían tratar de anunciar para

79
0:04:18.4,000 --> 0:04:2,000
hombres entre los 25 y 35 años,

80
0:04:20.92,000 --> 0:04:23,000
o gente con un límite alto en la tarjeta de crédito,

81
0:04:24.88,000 --> 0:04:25,000
o parejas retiradas, ¿no?

82
0:04:26.28,000 --> 0:04:27,000
Eso es lo que harían en el pasado.

83
0:04:28.12,000 --> 0:04:3,000
Con big data y aprendizaje automático,

84
0:04:31.04,000 --> 0:04:32,000
ya no funciona más así.

85
0:04:33.32,000 --> 0:04:35,000
Así que, para imaginar eso,

86
0:04:35.52,000 --> 0:04:38,000
piensen en todos los datos que Facebook tiene sobre Uds.:

87
0:04:39.4,000 --> 0:04:41,000
cada actualización de estado que jamás hayan escrito,

88
0:04:41.96,000 --> 0:04:43,000
cada conversación en el Messenger,

89
0:04:44,000 --> 0:04:45,000
cada lugar desde donde accedieron,

90
0:04:48.4,000 --> 0:04:51,000
todas las fotografías que hayan subido.

91
0:04:51.6,000 --> 0:04:54,000
Lo que comenzaron a escribir y borraron porque cambiaron de opinión;

92
0:04:55.4,000 --> 0:04:58,000
Facebook guarda y analiza eso también.

93
0:04:59.16,000 --> 0:05:02,000
Cada vez trata de aproximarse más a los datos de tu vida fuera de línea.

94
0:05:03.12,000 --> 0:05:06,000
También adquiere muchos datos de los corredores de datos.

95
0:05:06.32,000 --> 0:05:09,000
Podría ser cualquier cosa, desde registros financieros

96
0:05:09.76,000 --> 0:05:11,000
hasta una buena parte de tu historial de búsqueda.

97
0:05:13.08,000 --> 0:05:17,000
En EE. UU. esos datos son habitualmente recolectados,

98
0:05:17.8,000 --> 0:05:18,000
cotejados y vendidos.

99
0:05:20.32,000 --> 0:05:22,000
En Europa tienen reglas más estrictas.

100
0:05:23.68,000 --> 0:05:25,000
Entonces, lo que ocurre es lo siguiente:

101
0:05:26.92,000 --> 0:05:3,000
al procesar todos esos datos, esos algoritmos de aprendizaje automático,

102
0:05:30.96,000 --> 0:05:32,000
y por esto se llaman algoritmos de aprendizaje,

103
0:05:33.88,000 --> 0:05:37,000
aprenden a entender las características de la gente

104
0:05:38,000 --> 0:05:4,000
que compró boletos a Las Vegas anteriormente.

105
0:05:41.76,000 --> 0:05:44,000
Una vez que aprenden esto de los datos existentes,

106
0:05:45.32,000 --> 0:05:48,000
también aprenden cómo aplicarlo a un nuevo grupo de gente.

107
0:05:49.16,000 --> 0:05:52,000
Entonces, si se encuentran con una persona nueva,

108
0:05:52.24,000 --> 0:05:56,000
pueden clasificar si esa persona compraría un boleto a Las Vegas o no.

109
0:05:57.72,000 --> 0:05:58,000
Ahora bien, Uds. están pensando:

110
0:05:59.56,000 --> 0:06:03,000
"Una oferta para comprar boletos a Las Vegas... Puedo ignorarlo".

111
0:06:04.68,000 --> 0:06:06,000
Pero el problema no es ese.

112
0:06:06.92,000 --> 0:06:07,000
El problema es

113
0:06:08.52,000 --> 0:06:12,000
que ya no comprendemos realmente cómo funcionan estos algoritmos complejos.

114
0:06:12.68,000 --> 0:06:15,000
No entendemos cómo categorizan.

115
0:06:16.16,000 --> 0:06:2,000
Son matrices gigantes, miles de filas y columnas,

116
0:06:20.6,000 --> 0:06:21,000
quizá millones de filas y columnas,

117
0:06:23.32,000 --> 0:06:25,000
y ya ni los programadores

118
0:06:26.76,000 --> 0:06:27,000
ni nadie que los analice,

119
0:06:29.44,000 --> 0:06:3,000
aun teniendo todos los datos,

120
0:06:30.96,000 --> 0:06:34,000
comprende cómo operan exactamente;

121
0:06:35.6,000 --> 0:06:38,000
no más de lo que Uds. sabrían lo que estoy pensado en este momento

122
0:06:39.4,000 --> 0:06:42,000
si les enseñaran una disección de mi cerebro.

123
0:06:44.36,000 --> 0:06:46,000
Es como que ya no estamos programando;

124
0:06:46.96,000 --> 0:06:5,000
estamos creando inteligencia que no comprendemos totalmente.

125
0:06:52.52,000 --> 0:06:55,000
Y estas cosas solo funcionan con una enorme cantidad de datos,

126
0:06:56.52,000 --> 0:07:,000
así que también fomentan una vigilancia profunda de todos nosotros

127
0:07:01.51,000 --> 0:07:03,000
para que los algoritmos de aprendizaje funcionen.

128
0:07:03.876,000 --> 0:07:06,000
Por eso Facebook quiere acumular todos los datos que pueda sobre Uds.

129
0:07:07.2,000 --> 0:07:08,000
Los algoritmos funcionan mejor.

130
0:07:08.8,000 --> 0:07:1,000
Así que continuemos con el ejemplo de Las Vegas.

131
0:07:11.52,000 --> 0:07:14,000
¿Qué pasaría si ese sistema que no entendemos

132
0:07:16.2,000 --> 0:07:21,000
aprendiera que es más fácil venderle boletos a Las Vegas

133
0:07:21.36,000 --> 0:07:24,000
a gente bipolar a punto de entrar en un episodio maníaco?

134
0:07:25.64,000 --> 0:07:29,000
Esa gente tiende a gastar de más y a apostar compulsivamente.

135
0:07:31.28,000 --> 0:07:35,000
Podrían hacerlo y no tendríamos ni idea de que se fijaron en eso.

136
0:07:35.76,000 --> 0:07:38,000
Di este ejemplo una vez a un grupo de científicos de la computación

137
0:07:39.4,000 --> 0:07:41,000
y después, uno se me acercó.

138
0:07:41.48,000 --> 0:07:44,000
Estaba afligido y dijo: "Por eso no pude publicarlo".

139
0:07:45.6,000 --> 0:07:46,000
Yo le pregunté: "¿Publicar qué?".

140
0:07:47.8,000 --> 0:07:52,000
Él había tratado de ver si realmente se podría predecir el arranque maníaco

141
0:07:53.68,000 --> 0:07:56,000
en publicaciones de redes sociales antes de los síntomas clínicos,

142
0:07:56.92,000 --> 0:07:57,000
y había funcionado,

143
0:07:58.72,000 --> 0:08:,000
había funcionado muy bien,

144
0:08:00.8,000 --> 0:08:04,000
y no tenía idea de cómo funcionaba o qué había descubierto.

145
0:08:06.84,000 --> 0:08:1,000
El problema no se resuelve si él no lo publica,

146
0:08:11.28,000 --> 0:08:12,000
porque ya hay compañías

147
0:08:13.2,000 --> 0:08:15,000
que están desarrollando este tipo de tecnología,

148
0:08:15.76,000 --> 0:08:17,000
y muchas de estas cosas ya están a la venta.

149
0:08:19.24,000 --> 0:08:21,000
Esto ya no es tan difícil.

150
0:08:21.84,000 --> 0:08:24,000
¿Les ha pasado entrar a YouTube para ver un video en específico

151
0:08:25.32,000 --> 0:08:27,000
y una hora más tarde vieron 27?

152
0:08:28.76,000 --> 0:08:3,000
¿Vieron que YouTube tiene una columna a la derecha

153
0:08:31.28,000 --> 0:08:33,000
que dice: "A continuación"

154
0:08:33.52,000 --> 0:08:34,000
y que reproduce algo automáticamente?

155
0:08:35.36,000 --> 0:08:36,000
Es un algoritmo

156
0:08:36.516,000 --> 0:08:41,000
que elige lo que cree que les interesaría y que quizá no encuentren por sí mismos.

157
0:08:41.8,000 --> 0:08:42,000
No es un editor humano.

158
0:08:43.08,000 --> 0:08:44,000
Eso hacen los algoritmos.

159
0:08:44.52,000 --> 0:08:48,000
Se fijan en lo que han mirado y lo que la gente como Uds. ha mirado,

160
0:08:49.28,000 --> 0:08:53,000
e infiere que eso debe ser lo que les interesa,

161
0:08:53.52,000 --> 0:08:54,000
y de lo que quieren más,

162
0:08:54.799,000 --> 0:08:55,000
y entonces les muestra más.

163
0:08:56.159,000 --> 0:08:58,000
Parece una herramienta benigna y útil,

164
0:08:59.28,000 --> 0:09:,000
excepto cuando no lo es.

165
0:09:01.64,000 --> 0:09:07,000
En 2016 asistí a actos electorales del entonces candidato Donald Trump

166
0:09:09.84,000 --> 0:09:12,000
para estudiar, como académica, el movimiento que lo apoyaba.

167
0:09:13.2,000 --> 0:09:16,000
Estudio los movimientos sociales, así que por eso lo estudiaba también.

168
0:09:16.68,000 --> 0:09:19,000
Quise escribir algo sobre uno de sus actos,

169
0:09:20.04,000 --> 0:09:21,000
así que lo miré varias veces en YouTube.

170
0:09:23.24,000 --> 0:09:26,000
YouTube comenzó a recomendarme

171
0:09:26.36,000 --> 0:09:3,000
una lista de reproducción de videos de supremacistas blancos

172
0:09:30.64,000 --> 0:09:32,000
en orden de extremismo creciente.

173
0:09:33.32,000 --> 0:09:34,000
Si miraba uno,

174
0:09:35.16,000 --> 0:09:37,000
me llevaba a otro incluso más extremo

175
0:09:38.16,000 --> 0:09:39,000
y se reproducía automáticamente.

176
0:09:40.32,000 --> 0:09:44,000
Si miran contenido sobre Hillary Clinton o Bernie Sanders,

177
0:09:44.88,000 --> 0:09:48,000
YouTube les recomienda y reproduce conspiraciones de izquierda,

178
0:09:49.6,000 --> 0:09:5,000
de ahí hacia abajo.

179
0:09:52.48,000 --> 0:09:55,000
Quizá estén pensando que se trata de política, pero no.

180
0:09:55.56,000 --> 0:09:56,000
No se trata de política.

181
0:09:56.84,000 --> 0:09:59,000
Es solo el algoritmo entendiendo la conducta humana.

182
0:09:59.96,000 --> 0:10:03,000
Una vez miré un video sobre el vegetarianismo en YouTube

183
0:10:04.76,000 --> 0:10:08,000
y YouTube me recomendó y reprodujo un video sobre veganismo.

184
0:10:09.72,000 --> 0:10:12,000
Uno nunca es lo suficientemente extremo para YouTube.

185
0:10:12.76,000 --> 0:10:13,000
(Risas)

186
0:10:14.36,000 --> 0:10:15,000
¿Qué está ocurriendo?

187
0:10:16.52,000 --> 0:10:19,000
El algoritmo de YouTube está patentado,

188
0:10:20.08,000 --> 0:10:22,000
pero esto es lo que creo que está pasando.

189
0:10:23.36,000 --> 0:10:25,000
El algoritmo ha descubierto

190
0:10:25.48,000 --> 0:10:28,000
que si puedes persuadir a la gente

191
0:10:29.2,000 --> 0:10:32,000
haciéndoles pensar que puedes mostrarles algo más extremo,

192
0:10:32.96,000 --> 0:10:34,000
son más propensos a quedarse en el sitio

193
0:10:35.4,000 --> 0:10:39,000
viendo video tras video adentrándose en el agujero de conejo

194
0:10:39.84,000 --> 0:10:4,000
mientras Google les sirve anuncios.

195
0:10:43.76,000 --> 0:10:46,000
Sin alguien a quien le importe la ética de la tienda,

196
0:10:47.72,000 --> 0:10:51,000
estos sitios pueden retratar gente

197
0:10:53.68,000 --> 0:10:54,000
que odia a los judíos,

198
0:10:56.36,000 --> 0:10:58,000
que cree que los judíos son parásitos

199
0:11:00.32,000 --> 0:11:04,000
y que tiene contenido antisemita explícito,

200
0:11:06.08,000 --> 0:11:08,000
y permitirte que les envíes anuncios.

201
0:11:09.2,000 --> 0:11:12,000
También pueden movilizar algoritmos

202
0:11:12.76,000 --> 0:11:15,000
para encontrar audiencias parecidas,

203
0:11:15.92,000 --> 0:11:2,000
gente que no tiene ese contenido antisemita explícito en su perfil,

204
0:11:21.52,000 --> 0:11:27,000
pero a quien el algoritmo detecta como susceptible a esos mensajes,

205
0:11:27.72,000 --> 0:11:28,000
y te permite enviarles anuncios también.

206
0:11:30.68,000 --> 0:11:32,000
Esto puede sonar como un ejemplo inverosímil,

207
0:11:33.44,000 --> 0:11:34,000
pero es real.

208
0:11:35.48,000 --> 0:11:37,000
ProPublica lo investigó

209
0:11:37.64,000 --> 0:11:4,000
y encontró que de hecho se puede hacer esto en Facebook,

210
0:11:41.28,000 --> 0:11:43,000
y Facebook amablemente nos ofreció sugerencias

211
0:11:43.72,000 --> 0:11:44,000
de cómo ampliar la audiencia.

212
0:11:46.72,000 --> 0:11:47,000
BuzzFeed lo intentó con Google,

213
0:11:48.45,000 --> 0:11:51,000
y en seguida vieron que sí, que se puede hacer en Google también.

214
0:11:51.52,000 --> 0:11:52,000
Y no fue ni siquiera costoso.

215
0:11:53.24,000 --> 0:11:57,000
El reportero de ProPublica gastó alrededor de 30 dólares

216
0:11:57.68,000 --> 0:11:59,000
para anunciarle a esa categoría.

217
0:12:02.6,000 --> 0:12:07,000
El año pasado, el asesor de medios de Donald Trump reveló

218
0:12:07.92,000 --> 0:12:12,000
que usaban publicaciones de página oculta en Facebook para desmovilizar gente.

219
0:12:13.28,000 --> 0:12:14,000
No para persuadirlos,

220
0:12:14.68,000 --> 0:12:16,000
sino para convencerlos de no votar.

221
0:12:18.52,000 --> 0:12:21,000
Y para lograr eso, se dirigieron a grupos específicos,

222
0:12:22.12,000 --> 0:12:25,000
por ejemplo, hombres afroamericanos en ciudades clave como Filadelfia,

223
0:12:26.04,000 --> 0:12:28,000
y voy a leer exactamente lo que dijo.

224
0:12:28.52,000 --> 0:12:29,000
Cito.

225
0:12:29.76,000 --> 0:12:32,000
Estaban usando "publicaciones no públicas

226
0:12:32.8,000 --> 0:12:34,000
cuya audiencia controla la campaña

227
0:12:35,000 --> 0:12:38,000
para que solo la gente que queremos pueda verlas".

228
0:12:38.8,000 --> 0:12:39,000
"Modelamos esto".

229
0:12:40.04,000 --> 0:12:44,000
"Esto afectará dramáticamente la habilidad de ella para llevar votantes a las urnas".

230
0:12:45.72,000 --> 0:12:47,000
¿Qué hay en esas publicaciones de página oculta?

231
0:12:48.48,000 --> 0:12:49,000
No tenemos idea.

232
0:12:50.16,000 --> 0:12:51,000
Facebook no nos lo dirá.

233
0:12:52.48,000 --> 0:12:56,000
Facebook también organiza con un algoritmo las publicaciones

234
0:12:56.88,000 --> 0:12:59,000
que nuestros amigos ponen en Facebook, o las páginas que seguimos.

235
0:13:00.64,000 --> 0:13:02,000
No nos muestra todo cronológicamente.

236
0:13:02.88,000 --> 0:13:06,000
Ordena según la manera en que el algoritmo piensa que nos va a persuadir

237
0:13:07.72,000 --> 0:13:08,000
para quedarnos más tiempo en el sitio.

238
0:13:11.04,000 --> 0:13:14,000
Esto tiene muchas consecuencias.

239
0:13:14.44,000 --> 0:13:17,000
Quizá estén pensando que alguien los está desairando en Facebook.

240
0:13:18.8,000 --> 0:13:21,000
Pero quizá el algoritmo nunca les muestra su publicación a ellos.

241
0:13:22.08,000 --> 0:13:27,000
El algoritmo prioriza algunas publicaciones e ignora otras.

242
0:13:29.32,000 --> 0:13:3,000
Los experimentos muestran

243
0:13:30.64,000 --> 0:13:34,000
que lo que el algoritmo escoge para mostrar, puede afectar las emociones.

244
0:13:36.6,000 --> 0:13:37,000
Pero eso no es todo.

245
0:13:38.28,000 --> 0:13:4,000
También afecta la conducta política.

246
0:13:41.36,000 --> 0:13:45,000
Así que en 2010, en la votación a mitad de legislatura,

247
0:13:46.04,000 --> 0:13:51,000
Facebook hizo un experimento con 61 millones de personas en EE. UU.

248
0:13:51.96,000 --> 0:13:52,000
y lo dio a conocer después.

249
0:13:53.88,000 --> 0:13:56,000
A algunas personas les mostraron la publicación "Hoy es día de votación",

250
0:13:57.32,000 --> 0:13:58,000
la más sencilla,

251
0:13:58.72,000 --> 0:14:01,000
y a otras personas les mostraron la que tiene esa pequeña modificación,

252
0:14:02.64,000 --> 0:14:04,000
con esas miniaturas

253
0:14:04.76,000 --> 0:14:06,000
de tus amigos que cliquearon "Voté".

254
0:14:09,000 --> 0:14:1,000
Solo esa pequeña modificación.

255
0:14:11.52,000 --> 0:14:15,000
Así que las fotos fueron el único cambio,

256
0:14:15.84,000 --> 0:14:18,000
y esa publicación que fue mostrada solo una vez

257
0:14:19.12,000 --> 0:14:25,000
añadió 340 000 votantes

258
0:14:25.2,000 --> 0:14:26,000
en esa elección,

259
0:14:26.92,000 --> 0:14:27,000
según este estudio

260
0:14:28.64,000 --> 0:14:3,000
confirmado por los padrones electorales.

261
0:14:32.92,000 --> 0:14:33,000
¿Suerte? No.

262
0:14:34.6,000 --> 0:14:39,000
Porque en 2012, repitieron el experimento.

263
0:14:40.84,000 --> 0:14:41,000
Y esa vez,

264
0:14:42.6,000 --> 0:14:45,000
ese mensaje cívico mostrado una sola vez

265
0:14:45.92,000 --> 0:14:49,000
añadió 270 000 votantes.

266
0:14:51.16,000 --> 0:14:56,000
Como referencia, la elección presidencial de 2016 en EE. UU.

267
0:14:56.4,000 --> 0:14:59,000
se decidió por unos 100 000 votos.

268
0:15:01.36,000 --> 0:15:05,000
Facebook también puede inferir muy fácilmente sus opiniones políticas,

269
0:15:06.12,000 --> 0:15:08,000
aun si nunca las han revelado en el sitio.

270
0:15:08.4,000 --> 0:15:1,000
Estos algoritmos pueden lograrlo de manera sencilla.

271
0:15:11.96,000 --> 0:15:14,000
¿Qué pasaría si una plataforma con ese tipo de poder

272
0:15:15.88,000 --> 0:15:2,000
decide ganar seguidores para un candidato y no para el otro?

273
0:15:21.68,000 --> 0:15:23,000
¿Cómo lo sabríamos?

274
0:15:25.56,000 --> 0:15:29,000
Comenzamos en un lugar aparentemente inocuo,

275
0:15:29.72,000 --> 0:15:31,000
anuncios en línea siguiéndonos a todas partes,

276
0:15:31.96,000 --> 0:15:32,000
y hemos terminado en otro lugar.

277
0:15:35.48,000 --> 0:15:37,000
Como público y como ciudadanos,

278
0:15:37.96,000 --> 0:15:4,000
no sabemos ya si estamos viendo la misma información

279
0:15:41.4,000 --> 0:15:42,000
o qué es lo que los demás ven,

280
0:15:43.68,000 --> 0:15:45,000
y sin una base común de información,

281
0:15:46.28,000 --> 0:15:47,000
poco a poco,

282
0:15:47.92,000 --> 0:15:5,000
el debate público se está volviendo imposible,

283
0:15:51.16,000 --> 0:15:53,000
y eso que solo estamos en las etapas iniciales de esto.

284
0:15:54.16,000 --> 0:15:57,000
Estos algoritmos pueden inferir fácilmente

285
0:15:57.64,000 --> 0:16:01,000
cosas como el origen étnico de la gente, las ideas religiosas y políticas,

286
0:16:02.27,000 --> 0:16:03,000
la personalidad,

287
0:16:03.41,000 --> 0:16:06,000
la inteligencia, la felicidad, el uso de sustancias adictivas,

288
0:16:06.68,000 --> 0:16:09,000
la separación de los padres, la edad y género,

289
0:16:09.84,000 --> 0:16:1,000
solo con los "me gusta" de Facebook.

290
0:16:13.44,000 --> 0:16:17,000
Estos algoritmos pueden identificar manifestantes

291
0:16:17.52,000 --> 0:16:19,000
incluso si sus caras están parcialmente cubiertas.

292
0:16:21.72,000 --> 0:16:27,000
Estos algoritmos podrían detectar la orientación sexual de la gente

293
0:16:28.36,000 --> 0:16:31,000
solo con sus fotos de perfil.

294
0:16:33.56,000 --> 0:16:35,000
Estas son especulaciones probables

295
0:16:36.2,000 --> 0:16:38,000
así que no serán 100 % atinadas,

296
0:16:39.12,000 --> 0:16:4,000
pero yo no me imagino a los poderosos

297
0:16:41.01,000 --> 0:16:44,000
resistiendo la tentación de usar estas tecnologías

298
0:16:44.05,000 --> 0:16:46,000
solo porque haya algunos falsos positivos,

299
0:16:46.24,000 --> 0:16:49,000
lo cual creará, por supuesto, otra oleada de problemas.

300
0:16:49.52,000 --> 0:16:51,000
Imaginen lo que puede hacer un estado

301
0:16:52.48,000 --> 0:16:55,000
con la enorme cantidad de datos que tiene de sus ciudadanos.

302
0:16:56.68,000 --> 0:17:,000
China ya está usando tecnología de detección de rostros

303
0:17:01.48,000 --> 0:17:03,000
para identificar y arrestar gente.

304
0:17:05.28,000 --> 0:17:07,000
Y esta es la tragedia:

305
0:17:07.44,000 --> 0:17:12,000
Estamos construyendo esta infraestructura de vigilancia y autoritarismo

306
0:17:13,000 --> 0:17:15,000
solo para obtener más clics en los anuncios.

307
0:17:17.24,000 --> 0:17:19,000
Este no será el autoritarismo de Orwell.

308
0:17:19.839,000 --> 0:17:2,000
Esto no es "1984".

309
0:17:21.76,000 --> 0:17:25,000
Si el autoritarismo usa el miedo para aterrorizarnos,

310
0:17:26.359,000 --> 0:17:28,000
todos estaremos asustados, pero lo sabremos,

311
0:17:29.28,000 --> 0:17:31,000
lo odiaremos y lo resistiremos.

312
0:17:32.88,000 --> 0:17:36,000
Pero si la gente en el poder está usando estos algoritmos

313
0:17:37.319,000 --> 0:17:4,000
para vigilarnos calladamente,

314
0:17:40.72,000 --> 0:17:42,000
para juzgarnos y empujarnos,

315
0:17:43.72,000 --> 0:17:47,000
para predecir e identificar a los agitadores y a los rebeldes,

316
0:17:47.92,000 --> 0:17:5,000
para desplegar arquitecturas de persuasión a escala

317
0:17:51.84,000 --> 0:17:55,000
y para manipularnos uno por uno

318
0:17:56,000 --> 0:18:01,000
usando las debilidades y vulnerabilidades personales e individuales de cada uno,

319
0:18:02.72,000 --> 0:18:04,000
y si lo están haciendo a escala

320
0:18:05.94,000 --> 0:18:06,000
con nuestras pantallas personales

321
0:18:07.84,000 --> 0:18:08,000
para que ni siquiera sepamos

322
0:18:09.52,000 --> 0:18:11,000
lo que ven nuestros compañeros ciudadanos y vecinos,

323
0:18:13.56,000 --> 0:18:17,000
ese autoritarismo nos atrapará como una telaraña

324
0:18:18.4,000 --> 0:18:2,000
y quizá ni siquiera sepamos que estamos en ella.

325
0:18:22.44,000 --> 0:18:24,000
Así que la capitalización de mercado de Facebook

326
0:18:25.4,000 --> 0:18:28,000
se acerca a medio billón de dólares.

327
0:18:28.72,000 --> 0:18:31,000
Es porque funciona muy bien como arquitectura de persuasión.

328
0:18:33.76,000 --> 0:18:35,000
Pero la estructura de esa arquitectura

329
0:18:36.6,000 --> 0:18:39,000
es la misma, sea que vendan zapatos

330
0:18:39.84,000 --> 0:18:41,000
o sea que vendan política.

331
0:18:42.36,000 --> 0:18:45,000
Los algoritmos no ven la diferencia.

332
0:18:46.24,000 --> 0:18:49,000
Los mismos algoritmos utilizados en nosotros

333
0:18:49.56,000 --> 0:18:52,000
para hacernos más receptivos a los anuncios,

334
0:18:52.76,000 --> 0:18:58,000
también organizan nuestros flujos de información política, social y personal

335
0:18:59.52,000 --> 0:19:,000
y eso debe cambiar.

336
0:19:02.24,000 --> 0:19:04,000
No me malinterpreten,

337
0:19:04.56,000 --> 0:19:07,000
usamos plataformas digitales porque nos ofrecen muchísimo.

338
0:19:09.12,000 --> 0:19:12,000
Yo uso Facebook para estar en contacto con amigos y familia alrededor del mundo.

339
0:19:14,000 --> 0:19:19,000
He escrito acerca lo cruciales que son las redes para los movimientos sociales.

340
0:19:19.8,000 --> 0:19:22,000
He estudiado cómo pueden usarse estas tecnologías

341
0:19:22.84,000 --> 0:19:24,000
para eludir la censura en todo el mundo.

342
0:19:27.28,000 --> 0:19:33,000
Pero no es que la gente que administra Facebook o Google

343
0:19:33.72,000 --> 0:19:35,000
esté tratando maliciosa y deliberadamente

344
0:19:36.44,000 --> 0:19:4,000
de polarizar un país o el mundo

345
0:19:40.92,000 --> 0:19:41,000
y promover el extremismo.

346
0:19:43.44,000 --> 0:19:46,000
He leído las muchas y bien intencionadas afirmaciones

347
0:19:47.44,000 --> 0:19:5,000
que esa gente publica.

348
0:19:51.6,000 --> 0:19:57,000
Pero no son sus intenciones ni las afirmaciones lo que importa,

349
0:19:57.68,000 --> 0:20:,000
son las estructuras y modelos de negocio que están construyendo.

350
0:20:02.36,000 --> 0:20:04,000
Y ese es el meollo del problema.

351
0:20:04.48,000 --> 0:20:08,000
O Facebook es una estafa gigante de medio billón de dólares,

352
0:20:10.2,000 --> 0:20:11,000
los avisos en el sitio no funcionan

353
0:20:12.12,000 --> 0:20:14,000
y no sirve como arquitectura de persuasión,

354
0:20:14.84,000 --> 0:20:18,000
o su poder de influencia es altamente preocupante.

355
0:20:20.56,000 --> 0:20:21,000
Debe ser una o la otra.

356
0:20:22.36,000 --> 0:20:23,000
Es similar también para Google.

357
0:20:24.88,000 --> 0:20:26,000
Así que, ¿qué podemos hacer?

358
0:20:27.36,000 --> 0:20:28,000
Esto necesita cambiar.

359
0:20:29.32,000 --> 0:20:31,000
No puedo ofrecerles una receta simple,

360
0:20:31.92,000 --> 0:20:33,000
porque necesitamos reestructurar

361
0:20:34.2,000 --> 0:20:37,000
completamente cómo opera nuestra tecnología digital.

362
0:20:37.24,000 --> 0:20:41,000
Todo, desde la manera en que se desarrolla la tecnología

363
0:20:41.36,000 --> 0:20:44,000
hasta la manera en que los incentivos, económicos y demás,

364
0:20:45.24,000 --> 0:20:47,000
están incorporados al sistema.

365
0:20:48.48,000 --> 0:20:52,000
Tenemos que encarar y tratar de lidiar con la falta de transparencia

366
0:20:53.486,000 --> 0:20:56,000
creada por los algoritmos patentados,

367
0:20:56.64,000 --> 0:20:59,000
el desafío estructural en la opacidad del aprendizaje automático,

368
0:21:00.48,000 --> 0:21:03,000
todos estos datos sobre nosotros recolectados indiscriminadamente.

369
0:21:05,000 --> 0:21:07,000
Tenemos una gran tarea por delante.

370
0:21:08.16,000 --> 0:21:1,000
Tenemos que movilizar nuestra tecnología,

371
0:21:11.76,000 --> 0:21:12,000
nuestra creatividad

372
0:21:13.36,000 --> 0:21:14,000
y sí, nuestra política,

373
0:21:16.24,000 --> 0:21:18,000
para que podamos construir inteligencia artificial

374
0:21:18.92,000 --> 0:21:21,000
que apoye nuestros objetivos humanos,

375
0:21:22.8,000 --> 0:21:25,000
pero que también esté limitada por nuestros valores humanos.

376
0:21:27.6,000 --> 0:21:29,000
Entiendo que esto no será sencillo.

377
0:21:30.36,000 --> 0:21:33,000
Quizá ni siquiera acordemos fácilmente qué significan esos términos.

378
0:21:34.92,000 --> 0:21:36,000
Pero si nos tomamos en serio

379
0:21:38.24,000 --> 0:21:43,000
cómo operan estos sistemas de los que dependemos para tantas cosas,

380
0:21:44.24,000 --> 0:21:48,000
no veo cómo podemos seguir posponiendo esta conversación.

381
0:21:49.2,000 --> 0:21:51,000
Estas estructuras

382
0:21:51.76,000 --> 0:21:55,000
están organizando cómo funcionamos

383
0:21:55.88,000 --> 0:21:57,000
y están controlando

384
0:21:58.2,000 --> 0:22:,000
lo que podemos y no podemos hacer.

385
0:22:00.84,000 --> 0:22:02,000
Muchas de estas plataformas financiadas por anuncios

386
0:22:03.32,000 --> 0:22:04,000
presumen de ser gratuitas.

387
0:22:04.92,000 --> 0:22:08,000
En este contexto, eso significa que nosotros somos el producto de venta.

388
0:22:10.84,000 --> 0:22:12,000
Necesitamos una economía digital

389
0:22:13.6,000 --> 0:22:16,000
donde nuestros datos y nuestra atención

390
0:22:17.12,000 --> 0:22:22,000
no estén a la venta para el autoritario o demagogo que pague más.

391
0:22:23.16,000 --> 0:22:26,000
(Aplausos)

392
0:22:30.48,000 --> 0:22:33,000
Así que, volviendo a la paráfrasis de Hollywood:

393
0:22:33.76,000 --> 0:22:36,000
Sí, queremos que el enorme potencial

394
0:22:37.52,000 --> 0:22:4,000
de la inteligencia artificial y la tecnología digital florezca,

395
0:22:41.4,000 --> 0:22:45,000
pero para eso debemos enfrentar esta enorme amenaza,

396
0:22:46.36,000 --> 0:22:47,000
con los ojos abiertos y ahora.

397
0:22:48.32,000 --> 0:22:49,000
Gracias.

398
0:22:49.56,000 --> 0:22:53,000
(Aplausos)

