1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Claire Ghyselen

2
0:00:13.468,000 --> 0:00:18,000
Le 23 avril 2013,

3
0:00:18.714,000 --> 0:00:23,000
l'Associated Press a publié le tweet suivant sur Twitter.

4
0:00:24.252,000 --> 0:00:26,000
Il disait : « Flash info :

5
0:00:26.673,000 --> 0:00:28,000
deux explosions à la Maison-Blanche

6
0:00:29.268,000 --> 0:00:31,000
et Barack Obama a été blessé. »

7
0:00:32.212,000 --> 0:00:37,000
Ce tweet a été retweeté 4 000 fois en moins de cinq minutes

8
0:00:37.661,000 --> 0:00:39,000
et est ensuite devenu viral.

9
0:00:40.76,000 --> 0:00:44,000
Ce tweet n'était pas une vraie info publiée par l'Associated Press.

10
0:00:45.134,000 --> 0:00:48,000
C'était une fausse info, ou une infox,

11
0:00:48.491,000 --> 0:00:5,000
qui a été propagée par des pirates syriens

12
0:00:51.34,000 --> 0:00:55,000
qui avaient piraté le compte de l'Associated Press sur Twitter.

13
0:00:56.407,000 --> 0:00:58,000
Leur objectif était de perturber la société,

14
0:00:58.45,000 --> 0:00:59,000
mais ils ont perturbé bien plus que cela.

15
0:01:00.42,000 --> 0:01:02,000
Car les algorithmes automatisés de trading

16
0:01:02.82,000 --> 0:01:05,000
ont immédiatement saisi le sentiment de ce tweet

17
0:01:06.204,000 --> 0:01:09,000
et ont commencé à faire des opérations basées sur la possibilité

18
0:01:09.226,000 --> 0:01:12,000
que le président des États-Unis ait été blessé ou tué

19
0:01:12.601,000 --> 0:01:13,000
dans une explosion.

20
0:01:14.188,000 --> 0:01:15,000
Alors qu'ils ont commencé à tweeter,

21
0:01:16.204,000 --> 0:01:19,000
ils ont immédiatement causé l’écroulement du cours de la bourse,

22
0:01:19.577,000 --> 0:01:24,000
anéantissant 140 milliards de dollars de valeur de fonds propres en un jour.

23
0:01:25.062,000 --> 0:01:29,000
Robert Mueller, procureur spécial aux États-Unis,

24
0:01:29.562,000 --> 0:01:32,000
a prononcé des chefs d'accusation contre trois entreprises russes

25
0:01:33.478,000 --> 0:01:35,000
et 13 ressortissants russes

26
0:01:36.121,000 --> 0:01:39,000
au sujet d'un complot pour escroquer les États-Unis

27
0:01:39.312,000 --> 0:01:42,000
via une ingérence durant les élections présidentielles de 2016.

28
0:01:43.855,000 --> 0:01:46,000
L'histoire que raconte ce chef d'accusation

29
0:01:47.443,000 --> 0:01:5,000
est celle de l'Internet Research Agency,

30
0:01:50.609,000 --> 0:01:53,000
le bras armé du Kremlin sur les réseaux sociaux.

31
0:01:54.815,000 --> 0:01:56,000
Durant les élections présidentielles uniquement,

32
0:01:57.616,000 --> 0:01:58,000
les efforts de l'Internet Agency

33
0:01:59.529,000 --> 0:02:04,000
ont touché 126 millions de personnes sur Facebook aux États-Unis,

34
0:02:04.72,000 --> 0:02:07,000
ont publié trois millions de tweets

35
0:02:08.021,000 --> 0:02:11,000
et 43 heures de contenu sur YouTube.

36
0:02:11.887,000 --> 0:02:12,000
Tout cela était faux --

37
0:02:13.563,000 --> 0:02:16,000
de la désinformation conçue pour semer la discorde

38
0:02:17.152,000 --> 0:02:19,000
au sein des élections présidentielles américaines.

39
0:02:20.996,000 --> 0:02:22,000
Une étude récente de l'université d'Oxford

40
0:02:23.67,000 --> 0:02:26,000
a montré que durant les récentes élections suédoises,

41
0:02:26.964,000 --> 0:02:3,000
un tiers de toutes les informations se propageant sur les réseaux sociaux

42
0:02:31.363,000 --> 0:02:32,000
au sujet des élections

43
0:02:32.585,000 --> 0:02:34,000
étaient fausses ou de la désinformation.

44
0:02:35.037,000 --> 0:02:4,000
De plus, ce genre de campagne de désinformation sur les réseaux sociaux

45
0:02:40.139,000 --> 0:02:44,000
peut propager ce qu’on en est venu à appeler de la « propagande génocidaire »,

46
0:02:44.314,000 --> 0:02:47,000
par exemple contre les Rohingyas en Birmanie,

47
0:02:47.449,000 --> 0:02:49,000
ou entraînant des massacres collectifs en Inde.

48
0:02:49.776,000 --> 0:02:5,000
Nous avons étudié l'infox

49
0:02:51.294,000 --> 0:02:54,000
et avons commencé à l'étudier avant que ce ne soit un terme populaire.

50
0:02:55.03,000 --> 0:03:,000
Nous avons récemment publié la plus longue étude longitudinale qui soit

51
0:03:00.094,000 --> 0:03:02,000
sur la propagation de l'infox en ligne

52
0:03:02.404,000 --> 0:03:05,000
en couverture de « Science » en mars cette année.

53
0:03:06.523,000 --> 0:03:1,000
Nous avons étudié toutes les infos confirmées comme étant vraies ou fausses

54
0:03:10.708,000 --> 0:03:11,000
qui se sont propagées sur Twitter,

55
0:03:12.485,000 --> 0:03:15,000
depuis son lancement en 2006 jusqu'à 2017.

56
0:03:16.612,000 --> 0:03:18,000
Quand nous avons étudié ces informations,

57
0:03:18.95,000 --> 0:03:2,000
nous avons étudié des informations

58
0:03:21.85,000 --> 0:03:22,000
qui avaient été vérifiées

59
0:03:23.272,000 --> 0:03:25,000
par six organisations de vérification indépendantes.

60
0:03:25.792,000 --> 0:03:27,000
Nous savions donc quelles informations étaient vraies

61
0:03:28.578,000 --> 0:03:3,000
et quelles informations étaient fausses.

62
0:03:30.728,000 --> 0:03:31,000
Nous pouvons mesurer leur diffusion,

63
0:03:32.625,000 --> 0:03:33,000
la vitesse de leur diffusion,

64
0:03:34.3,000 --> 0:03:36,000
la portée et l'ampleur de leur diffusion,

65
0:03:36.419,000 --> 0:03:4,000
combien de gens se sont empêtrés dans cette cascade d'informations, etc.

66
0:03:40.942,000 --> 0:03:41,000
Dans cette publication,

67
0:03:42.45,000 --> 0:03:45,000
nous avons comparé la propagation d'informations vraies et d'infox.

68
0:03:46.339,000 --> 0:03:47,000
Voici ce que nous avons découvert.

69
0:03:48.046,000 --> 0:03:48,000
Nous avons découvert

70
0:03:49.029,000 --> 0:03:52,000
que l'infox se diffusait plus loin, plus vite, plus profondément

71
0:03:52.049,000 --> 0:03:53,000
et plus largement que la vérité

72
0:03:53.879,000 --> 0:03:56,000
dans chacune des catégories d'informations que nous avons étudiées,

73
0:03:57.026,000 --> 0:03:59,000
la différence étant parfois d'un ordre de grandeur.

74
0:03:59.842,000 --> 0:04:02,000
Les infox politiques étaient les plus virales.

75
0:04:03.39,000 --> 0:04:06,000
Elles se diffusaient plus loin, vite, profondément et largement

76
0:04:06.561,000 --> 0:04:08,000
que n'importe quel autre type d'infox.

77
0:04:09.387,000 --> 0:04:1,000
Quand nous avons vu cela,

78
0:04:10.704,000 --> 0:04:12,000
nous étions à la fois inquiets mais aussi curieux.

79
0:04:13.569,000 --> 0:04:14,000
Pourquoi ?

80
0:04:14.744,000 --> 0:04:17,000
Pourquoi est-ce que l'infox circule tellement plus loin, vite, profondément

81
0:04:18.271,000 --> 0:04:19,000
et largement que la vérité ?

82
0:04:20.339,000 --> 0:04:22,000
Notre première hypothèse était :

83
0:04:23.324,000 --> 0:04:25,000
« Peut-être que les gens diffusant de l'infox

84
0:04:25.55,000 --> 0:04:27,000
ont plus de suiveurs ou suivent plus de monde,

85
0:04:28.14,000 --> 0:04:29,000
tweetent plus souvent,

86
0:04:29.557,000 --> 0:04:32,000
peut-être sont-ils plus souvent des utilisateurs « vérifiés » de Twitter,

87
0:04:32.981,000 --> 0:04:33,000
avec plus de crédibilité,

88
0:04:34.201,000 --> 0:04:35,000
ou sont-ils là depuis plus longtemps. »

89
0:04:36.167,000 --> 0:04:38,000
Nous avons vérifié chacune de ces hypothèses.

90
0:04:38.691,000 --> 0:04:4,000
Nous avons découvert exactement le contraire.

91
0:04:41.635,000 --> 0:04:43,000
Les diffuseurs d'infox avaient moins de suiveurs,

92
0:04:44.095,000 --> 0:04:46,000
suivaient moins de gens, étaient moins actifs,

93
0:04:46.373,000 --> 0:04:47,000
moins souvent « vérifiés »

94
0:04:47.857,000 --> 0:04:49,000
et étaient sur Twitter depuis moins longtemps.

95
0:04:50.841,000 --> 0:04:51,000
Et pourtant,

96
0:04:52.054,000 --> 0:04:54,000
les infox avaient 70% de probabilités de plus

97
0:04:54.945,000 --> 0:04:56,000
d'être retweetées par rapport à la vérité,

98
0:04:57.111,000 --> 0:05:,000
en contrôlant ces facteurs et de nombreux autres.

99
0:05:00.498,000 --> 0:05:02,000
Nous devions trouver d'autres explications.

100
0:05:03.212,000 --> 0:05:04,000
Nous avons formulé

101
0:05:04.258,000 --> 0:05:06,000
ce que nous avons appelé « l'hypothèse de la nouveauté ».

102
0:05:07.038,000 --> 0:05:08,000
Si vous lisez la littérature,

103
0:05:09.022,000 --> 0:05:12,000
il est bien connu que l'attention humaine est attirée par la nouveauté,

104
0:05:12.8,000 --> 0:05:14,000
les choses nouvelles dans l'environnement.

105
0:05:15.343,000 --> 0:05:17,000
Si vous lisez la littérature sociologique,

106
0:05:17.352,000 --> 0:05:21,000
vous savez que nous aimons partager des informations nouvelles.

107
0:05:21.676,000 --> 0:05:24,000
Cela nous donne l'impression d'avoir un accès privilégié à des informations

108
0:05:25.538,000 --> 0:05:28,000
et nous gagnons en statut en diffusant ce genre d'informations.

109
0:05:29.792,000 --> 0:05:35,000
Nous avons alors mesuré la nouveauté d'un tweet, qu'il soit vrai ou faux,

110
0:05:36.268,000 --> 0:05:4,000
l'avons comparé au corpus de ce que l'individu avait vu

111
0:05:40.347,000 --> 0:05:42,000
sur Twitter les 60 derniers jours.

112
0:05:43.323,000 --> 0:05:45,000
Mais cela ne suffisait pas car nous avons pensé :

113
0:05:46.006,000 --> 0:05:48,000
« Peut-être que l'infox représente plus de nouveauté

114
0:05:48.848,000 --> 0:05:49,000
dans un sens théorique,

115
0:05:50.238,000 --> 0:05:53,000
mais peut-être que les gens n'y perçoivent pas plus de nouveauté. »

116
0:05:53.849,000 --> 0:05:56,000
Pour comprendre la perception que les gens avaient des infox,

117
0:05:57.8,000 --> 0:06:,000
nous avons considéré l'information et le sentiment

118
0:06:01.514,000 --> 0:06:05,000
que les réponses des tweets vrais et faux incluaient.

119
0:06:06.022,000 --> 0:06:07,000
Nous avons découvert

120
0:06:07.252,000 --> 0:06:11,000
que parmi tout un tas de mesures différentes des sentiments --

121
0:06:11.49,000 --> 0:06:14,000
la surprise, le dégoût, la peur, la tristesse,

122
0:06:14.815,000 --> 0:06:16,000
l'anticipation, la joie et la confiance --

123
0:06:17.323,000 --> 0:06:22,000
les infox présentaient notablement plus de surprise et de dégoût

124
0:06:23.204,000 --> 0:06:25,000
dans les réponses aux tweets.

125
0:06:26.392,000 --> 0:06:29,000
Les informations avérées présentaient notablement plus d'anticipation,

126
0:06:30.205,000 --> 0:06:31,000
de joie et de confiance

127
0:06:31.776,000 --> 0:06:33,000
dans les réponses aux tweets.

128
0:06:34.347,000 --> 0:06:37,000
La surprise corrobore notre hypothèse de la nouveauté.

129
0:06:38.157,000 --> 0:06:4,000
C'est nouveau et surprenant,

130
0:06:40.342,000 --> 0:06:42,000
nous allons donc plus probablement le partager.

131
0:06:43.092,000 --> 0:06:45,000
En même temps, il y avait un témoignage au Congrès,

132
0:06:46.041,000 --> 0:06:49,000
devant les deux chambres du Congrès des États-Unis,

133
0:06:49.101,000 --> 0:06:52,000
considérant le rôle des robots dans la diffusion de la désinformation.

134
0:06:52.863,000 --> 0:06:53,000
Nous l'avons envisagé --

135
0:06:54.241,000 --> 0:06:54,000
nous avons utilisé

136
0:06:55.193,000 --> 0:06:57,000
de multiples algorithmes sophistiqués de détection de robots

137
0:06:58.033,000 --> 0:07:,000
pour trouver les robots dans nos données et les en extraire.

138
0:07:01.347,000 --> 0:07:03,000
Nous les en avons extraits, les avons inclus à nouveau

139
0:07:04.03,000 --> 0:07:07,000
et avons comparé ce qu'il arrivait à nos mesures.

140
0:07:07.173,000 --> 0:07:09,000
Nous avons découvert que oui, en effet,

141
0:07:09.49,000 --> 0:07:12,000
les robots accéléraient la diffusion d'infox en ligne,

142
0:07:13.196,000 --> 0:07:15,000
mais ils accéléraient la diffusion d'informations avérées

143
0:07:15.871,000 --> 0:07:17,000
approximativement au même rythme.

144
0:07:18.3,000 --> 0:07:2,000
Ce que signifie que les robots ne sont pas responsables

145
0:07:21.182,000 --> 0:07:25,000
pour l'écart dans la diffusion de la vérité et de la fausseté en ligne.

146
0:07:25.919,000 --> 0:07:27,000
Nous ne pouvons pas refuser cette responsabilité

147
0:07:28.792,000 --> 0:07:32,000
car nous, êtres humains, sommes responsables de cette diffusion.

148
0:07:34.472,000 --> 0:07:37,000
Tout ce que je vous ai dit jusqu'ici,

149
0:07:37.83,000 --> 0:07:38,000
malheureusement pour nous tous,

150
0:07:39.608,000 --> 0:07:4,000
ce sont les bonnes nouvelles.

151
0:07:42.67,000 --> 0:07:46,000
La raison en est que cela va bientôt devenir bien pire.

152
0:07:47.85,000 --> 0:07:5,000
Deux technologies en particulier vont rendre cela bien pire.

153
0:07:52.207,000 --> 0:07:57,000
Nous allons voir la montée d'une énorme vague de média synthétiques :

154
0:07:57.403,000 --> 0:08:03,000
de fausses vidéos, de faux enregistrements très convaincants pour l’œil humain.

155
0:08:03.458,000 --> 0:08:05,000
Cela sera propulsé par deux technologies.

156
0:08:06.236,000 --> 0:08:09,000
La première est connue sous le nom de « réseaux adverses génératifs ».

157
0:08:10.093,000 --> 0:08:12,000
C'est un modèle d'apprentissage automatique avec deux réseaux :

158
0:08:13.06,000 --> 0:08:14,000
un discriminateur,

159
0:08:14.251,000 --> 0:08:18,000
dont le rôle est de déterminer si quelque chose est vrai ou faux,

160
0:08:18.475,000 --> 0:08:19,000
et un générateur,

161
0:08:19.666,000 --> 0:08:22,000
dont le rôle est de générer des médias synthétiques.

162
0:08:22.84,000 --> 0:08:27,000
Le générateur synthétique génère une vidéo ou un enregistrement audio synthétique

163
0:08:27.966,000 --> 0:08:31,000
et le discriminateur essaye de dire s'il est réel ou falsifié.

164
0:08:32.665,000 --> 0:08:34,000
La tâche du générateur

165
0:08:35.563,000 --> 0:08:39,000
consiste à maximiser la probabilité de faire croire au discriminateur

166
0:08:40.022,000 --> 0:08:45,000
que la vidéo et l'enregistrement audio qu'il crée sont réels.

167
0:08:45.387,000 --> 0:08:47,000
Imaginez une machine dans une boucle infinie

168
0:08:47.784,000 --> 0:08:49,000
et essayant d'améliorer sa capacité à nous tromper.

169
0:08:51.114,000 --> 0:08:53,000
Cela, en combinaison avec la deuxième technologie,

170
0:08:53.638,000 --> 0:08:58,000
qui est essentiellement la démocratisation de l'intelligence artificielle,

171
0:08:59.384,000 --> 0:09:01,000
la capacité offerte à tous,

172
0:09:01.597,000 --> 0:09:03,000
sans aucune formation en intelligence artificielle

173
0:09:04.427,000 --> 0:09:05,000
ou en apprentissage automatique,

174
0:09:05.967,000 --> 0:09:08,000
de déployer ce genre d'algorithmes pour générer des médias synthétiques

175
0:09:09.784,000 --> 0:09:13,000
rendant cela tellement plus simple de créer des vidéos.

176
0:09:14.355,000 --> 0:09:18,000
La Maison-Blanche a publié une vidéo falsifiée, truquée

177
0:09:18.8,000 --> 0:09:2,000
d'un journaliste interagissant avec une stagiaire

178
0:09:21.707,000 --> 0:09:22,000
et essayant de prendre son micro.

179
0:09:23.427,000 --> 0:09:24,000
Ils ont enlevé des images de cette vidéo

180
0:09:25.45,000 --> 0:09:28,000
afin que ces actes paraissent plus percutants.

181
0:09:29.157,000 --> 0:09:32,000
Quand des vidéastes, des cascadeurs et des cascadeuses

182
0:09:32.566,000 --> 0:09:34,000
ont été interrogés sur ce genre de technique,

183
0:09:35.017,000 --> 0:09:38,000
ils ont dit : « Oui, nous utilisons constamment cela dans les films

184
0:09:38.869,000 --> 0:09:42,000
pour que nos coups de poing et de pied paraissent plus musclés et agressifs. »

185
0:09:44.268,000 --> 0:09:45,000
Ils ont ensuite publié cette vidéo

186
0:09:46.159,000 --> 0:09:48,000
l'ont utilisée comme justification partielle

187
0:09:48.683,000 --> 0:09:51,000
pour révoquer la carte de presse de Jim Acosta, le journaliste,

188
0:09:52.706,000 --> 0:09:53,000
pour la Maison-Blanche.

189
0:09:54.069,000 --> 0:09:58,000
CNN a dû aller en justice pour faire rétablir cette carte de presse.

190
0:10:00.538,000 --> 0:10:02,000
Il y a environ cinq chemins différents

191
0:10:03.109,000 --> 0:10:06,000
que je peux imaginer et que nous pourrions suivre

192
0:10:06.165,000 --> 0:10:09,000
pour essayer de remédier à ces problèmes très difficiles.

193
0:10:10.379,000 --> 0:10:11,000
Chacun est prometteur

194
0:10:12.213,000 --> 0:10:14,000
mais chacun a des défis qui lui sont propres.

195
0:10:15.236,000 --> 0:10:17,000
Le premier est l'étiquetage.

196
0:10:17.268,000 --> 0:10:18,000
Réfléchissez-y :

197
0:10:18.649,000 --> 0:10:21,000
quand vous allez au supermarché pour acheter de la nourriture,

198
0:10:22.284,000 --> 0:10:23,000
elle est amplement étiquetée.

199
0:10:24.212,000 --> 0:10:26,000
Vous savez combien de calories elle contient,

200
0:10:26.318,000 --> 0:10:27,000
combien de graisses --

201
0:10:28.053,000 --> 0:10:3,000
et pourtant, quand nous consommons des informations,

202
0:10:30.665,000 --> 0:10:31,000
nous n'avons pas d'étiquettes.

203
0:10:32.355,000 --> 0:10:33,000
Que contient cette information ?

204
0:10:34.307,000 --> 0:10:35,000
La source est-elle crédible ?

205
0:10:35.784,000 --> 0:10:37,000
Où cette information a-t-elle été recueillie ?

206
0:10:38.125,000 --> 0:10:39,000
Nous n'avons aucune de ces informations

207
0:10:40.034,000 --> 0:10:42,000
quand nous consommons des informations.

208
0:10:42.101,000 --> 0:10:45,000
C'est une solution potentielle, mais elle a ses défis.

209
0:10:45.363,000 --> 0:10:51,000
Par exemple, qui décide, dans la société, ce qui est vrai et ce qui est faux ?

210
0:10:52.387,000 --> 0:10:53,000
Est-ce que ce sont les gouvernements ?

211
0:10:54.213,000 --> 0:10:54,000
Est-ce Facebook ?

212
0:10:55.601,000 --> 0:10:58,000
Est-ce un consortium indépendant de vérificateurs d'informations ?

213
0:10:59.387,000 --> 0:11:01,000
Et qui contrôle ceux qui vérifient les informations ?

214
0:11:02.427,000 --> 0:11:05,000
Une autre option possible, ce sont les incitations.

215
0:11:05.535,000 --> 0:11:08,000
Nous savons que durant les élections présidentielles américaines,

216
0:11:08.583,000 --> 0:11:11,000
il y a eu une vague de désinformation venue de Macédoine du Nord

217
0:11:11.907,000 --> 0:11:13,000
qui n'avait aucun mobile politique

218
0:11:14.268,000 --> 0:11:16,000
mais avait un mobile économique.

219
0:11:16.752,000 --> 0:11:18,000
Ce mobile économique existait

220
0:11:18.924,000 --> 0:11:21,000
car l'infox circule tellement plus loin, plus vite

221
0:11:22.472,000 --> 0:11:24,000
et plus profondément que la vérité

222
0:11:24.506,000 --> 0:11:26,000
et vous pouvez gagner de l'argent dans la publicité

223
0:11:26.94,000 --> 0:11:28,000
alors que vous recueillez les regards et l'attention

224
0:11:29.49,000 --> 0:11:3,000
avec ce genre d'informations.

225
0:11:31.474,000 --> 0:11:34,000
Mais si nous pouvons réduire la diffusion de ces informations,

226
0:11:35.331,000 --> 0:11:4,000
peut-être cela réduirait-il l'incitation économique à les produire.

227
0:11:40.966,000 --> 0:11:42,000
Troisièmement, nous pouvons réfléchir à une réglementation

228
0:11:43.68,000 --> 0:11:45,000
et nous devrions réfléchir à cette option.

229
0:11:45.839,000 --> 0:11:46,000
Actuellement aux États-Unis,

230
0:11:47.474,000 --> 0:11:49,000
nous explorons ce qu'il pourrait se passer

231
0:11:50.166,000 --> 0:11:52,000
si Facebook et les autres étaient réglementés.

232
0:11:52.346,000 --> 0:11:55,000
Si nous devrions considérer des choses comme réglementer le discours politique,

233
0:11:56.171,000 --> 0:11:58,000
l'étiqueter comme étant du discours politique,

234
0:11:58.703,000 --> 0:11:59,000
nous assurer que les acteurs étrangers

235
0:12:00.596,000 --> 0:12:02,000
ne puissent pas financer un discours politique,

236
0:12:02.786,000 --> 0:12:04,000
cela a aussi ses dangers.

237
0:12:05.522,000 --> 0:12:09,000
Par exemple, la Malaisie vient d'instituer une peine de prison de six ans

238
0:12:10.424,000 --> 0:12:12,000
pour quiconque diffusant de la désinformation.

239
0:12:13.696,000 --> 0:12:15,000
Dans les régimes autoritaires,

240
0:12:15.799,000 --> 0:12:17,000
ce genre de politiques peuvent être utilisées

241
0:12:18.169,000 --> 0:12:2,000
pour étouffer les opinions minoritaires

242
0:12:20.489,000 --> 0:12:23,000
et continuer à accroître la répression.

243
0:12:24.68,000 --> 0:12:27,000
La quatrième option possible est la transparence.

244
0:12:28.843,000 --> 0:12:31,000
Nous voulons savoir comment fonctionnent les algorithmes de Facebook.

245
0:12:32.581,000 --> 0:12:34,000
Comment les données se combinent-elles aux algorithmes

246
0:12:35.485,000 --> 0:12:37,000
pour générer les résultats que nous observons ?

247
0:12:38.347,000 --> 0:12:4,000
Nous voulons qu'ils ouvrent le kimono

248
0:12:40.72,000 --> 0:12:44,000
et nous montrent les rouages internes du fonctionnement de Facebook.

249
0:12:44.934,000 --> 0:12:46,000
Pour connaître les effets des réseaux sociaux sur la société,

250
0:12:47.801,000 --> 0:12:49,000
il faut que les scientifiques, les chercheurs

251
0:12:49.931,000 --> 0:12:52,000
et les autres aient accès à ce genre d'informations.

252
0:12:53.038,000 --> 0:12:54,000
Mais en même temps,

253
0:12:54.609,000 --> 0:12:57,000
nous demandons à Facebook de tout verrouiller

254
0:12:58.434,000 --> 0:13:,000
pour assurer la sécurité des données.

255
0:13:00.631,000 --> 0:13:03,000
Facebook et les autres plateformes de réseaux sociaux

256
0:13:03.814,000 --> 0:13:06,000
font face à ce que j'appelle un paradoxe de la transparence.

257
0:13:07.266,000 --> 0:13:09,000
Nous leur demandons à la fois

258
0:13:09.964,000 --> 0:13:13,000
d'être ouverts et transparents et, simultanément, d'être sécurisés.

259
0:13:14.797,000 --> 0:13:16,000
Cela est difficile à accomplir,

260
0:13:17.512,000 --> 0:13:18,000
mais ils devront le faire

261
0:13:19.449,000 --> 0:13:2,000
si nous voulons

262
0:13:20.59,000 --> 0:13:22,000
que les technologies sociales tiennent leurs promesses

263
0:13:23.26,000 --> 0:13:24,000
tout en évitant les risques.

264
0:13:24.926,000 --> 0:13:26,000
Pour finir, nous pourrions réfléchir

265
0:13:27.341,000 --> 0:13:29,000
aux algorithmes et à l'apprentissage automatique.

266
0:13:29.641,000 --> 0:13:34,000
Une technologie conçue pour éradiquer et comprendre l'infox, sa diffusion,

267
0:13:34.942,000 --> 0:13:36,000
et essayer d'atténuer sa circulation.

268
0:13:37.824,000 --> 0:13:39,000
Les êtres humains doivent jouer un rôle dans cette technologie

269
0:13:40.745,000 --> 0:13:42,000
car nous ne pourrons jamais échapper au fait

270
0:13:43.047,000 --> 0:13:47,000
que pour toute solution ou approche technologie,

271
0:13:47.109,000 --> 0:13:51,000
il y a une question éthique et philosophique fondamentale

272
0:13:51.18,000 --> 0:13:54,000
quant à notre définition du vrai et du faux,

273
0:13:54.474,000 --> 0:13:57,000
à qui nous donnons le pouvoir de définir le vrai et le faux

274
0:13:57.678,000 --> 0:13:59,000
et quelles opinions sont légitimes,

275
0:14:00.162,000 --> 0:14:03,000
une rapidité de quel ordre de grandeur devrait être autorisée et ainsi de suite.

276
0:14:03.952,000 --> 0:14:05,000
La technologie n'est pas la solution.

277
0:14:06.244,000 --> 0:14:09,000
L'éthique et la philosophie le sont.

278
0:14:10.95,000 --> 0:14:13,000
Presque toutes les théories sur la prise de décision humaine,

279
0:14:14.292,000 --> 0:14:16,000
la coopération humaine et la coordination humaine

280
0:14:17.077,000 --> 0:14:2,000
ont une part de vérité en elles.

281
0:14:21.347,000 --> 0:14:23,000
Mais avec l'essor des infox,

282
0:14:23.427,000 --> 0:14:24,000
l'essor des vidéos truquées,

283
0:14:24.894,000 --> 0:14:26,000
l'essor des enregistrements audios truqués,

284
0:14:26.9,000 --> 0:14:29,000
nous vacillons au bord de la fin de la réalité,

285
0:14:30.748,000 --> 0:14:33,000
où nous ne pouvons pas discerner ce qui est réel et ce qui est faux.

286
0:14:34.661,000 --> 0:14:37,000
Cela est potentiellement incroyablement dangereux.

287
0:14:38.931,000 --> 0:14:41,000
Nous devons être vigilants dans la défense de la vérité

288
0:14:42.903,000 --> 0:14:43,000
face à la désinformation.

289
0:14:44.919,000 --> 0:14:47,000
Avec nos technologies, avec nos politiques

290
0:14:48.379,000 --> 0:14:49,000
et, peut-être surtout, individuellement,

291
0:14:50.323,000 --> 0:14:53,000
avec nos responsabilités, nos décisions,

292
0:14:53.561,000 --> 0:14:56,000
nos comportements et nos actions.

293
0:14:57.553,000 --> 0:14:58,000
Merci beaucoup.

294
0:14:59.014,000 --> 0:15:02,000
(Applaudissements)

