1
0:00:,000 --> 0:00:07,000
Traductor: Camille Martínez Revisor: Sebastian Betti

2
0:00:12.203,000 --> 0:00:13,000
Greg Gage: La lectura del pensamiento.

3
0:00:14.064,000 --> 0:00:15,000
Se ve en películas de ciencia ficción,

4
0:00:15.916,000 --> 0:00:16,000
las máquinas que nos leen la mente.

5
0:00:17.617,000 --> 0:00:2,000
Pero existen aparatos hoy que pueden leer la actividad eléctrica

6
0:00:20.704,000 --> 0:00:21,000
del cerebro.

7
0:00:21.932,000 --> 0:00:22,000
Lo llamamos el EEG.

8
0:00:23.695,000 --> 0:00:25,000
¿Hay información contenida en estas ondas cerebrales?

9
0:00:26.548,000 --> 0:00:29,000
¿Podríamos programar una computadora para leer los pensamientos?

10
0:00:29.806,000 --> 0:00:31,000
Mi amigo Nathan está intentando hackear el EEG

11
0:00:32.334,000 --> 0:00:34,000
para construir una máquina que pueda leer la mente.

12
0:00:34.765,000 --> 0:00:36,000
[Neurociencia casera]

13
0:00:36.939,000 --> 0:00:37,000
Así funciona el EEG:

14
0:00:38.567,000 --> 0:00:39,000
dentro de la cabeza está el cerebro,

15
0:00:40.392,000 --> 0:00:42,000
y el cerebro se compone de miles de millones de neuronas.

16
0:00:42.974,000 --> 0:00:44,000
Cada neurona transmite un mensaje eléctrico la una a la otra.

17
0:00:45.92,000 --> 0:00:48,000
Estos pequeños mensajes pueden combinarse para formar una onda eléctrica

18
0:00:49.349,000 --> 0:00:5,000
que se detecta en un monitor.

19
0:00:50.775,000 --> 0:00:52,000
Tradicionalmente, el EEG nos puede decir cosas generales,

20
0:00:53.524,000 --> 0:00:55,000
por ejemplo, si uno está dormido o despierto.

21
0:00:55.615,000 --> 0:00:56,000
Pero ¿nos puede decir algo más?

22
0:00:57.177,000 --> 0:00:59,000
¿Puede realmente leer los pensamientos?

23
0:00:59.313,000 --> 0:01:,000
Lo vamos a comprobar.

24
0:01:00.489,000 --> 0:01:03,000
No empezaremos con pensamientos complejos; haremos algo muy sencillo.

25
0:01:03.884,000 --> 0:01:06,000
¿Podemos interpretar lo que alguien ve utilizando solo sus ondas cerebrales?

26
0:01:07.882,000 --> 0:01:1,000
Nathan empezará por pegar electrodos en la cabeza de Christy.

27
0:01:10.978,000 --> 0:01:11,000
Nathan: Mi vida está enredada.

28
0:01:12.642,000 --> 0:01:13,000
(Risas)

29
0:01:14.152,000 --> 0:01:16,000
GG: Luego le mostrará una serie de imágenes

30
0:01:16.315,000 --> 0:01:17,000
de cuatro categorías diferentes.

31
0:01:17.909,000 --> 0:01:2,000
Nathan: Una cara, una casa, un paisaje e imágenes raras.

32
0:01:20.984,000 --> 0:01:22,000
GG: Mientras le mostramos a Christy centenares de estas imágenes,

33
0:01:23.952,000 --> 0:01:26,000
se están registrando las ondas eléctricas en la computadora de Nathan.

34
0:01:27.291,000 --> 0:01:29,000
Queremos ver si podemos detectar información visual sobre las fotos

35
0:01:30.122,000 --> 0:01:31,000
contenida en las ondas cerebrales,

36
0:01:31.83,000 --> 0:01:33,000
así que al terminar, sabremos si el EEG nos puede decir

37
0:01:34.77,000 --> 0:01:35,000
qué tipo de imagen Christy está mirando.

38
0:01:36.738,000 --> 0:01:37,000
Y si este es el caso,

39
0:01:37.914,000 --> 0:01:4,000
cada categoría debe desencadenar una señal diferente del cerebro.

40
0:01:41.015,000 --> 0:01:43,000
Bien, recolectamos todos los datos sin procesar del EEG,

41
0:01:43.206,000 --> 0:01:44,000
y estos son los resultados.

42
0:01:45.389,000 --> 0:01:47,000
Se ven bastante desorganizados, entonces ordenémoslos por imagen.

43
0:01:48.196,000 --> 0:01:51,000
Aún está demasiado caótico para ver alguna diferencia,

44
0:01:51.506,000 --> 0:01:54,000
pero si igualamos el EEG entre todos los tipos de imágenes

45
0:01:54.57,000 --> 0:01:56,000
y las alineamos según el momento en que la imagen apareció

46
0:01:57.278,000 --> 0:01:59,000
podemos normalizar el caos, y dentro de poco,

47
0:01:59.467,000 --> 0:02:02,000
vemos que unas tendencias principales emergen para cada categoría.

48
0:02:02.617,000 --> 0:02:04,000
Las señales todavía se ven bastante similares.

49
0:02:04.797,000 --> 0:02:05,000
Mirémoslas más de cerca.

50
0:02:06.121,000 --> 0:02:08,000
Como a cien milisegundos después de que sale la imagen,

51
0:02:08.763,000 --> 0:02:1,000
vemos un aumento positivo en cada uno de los cuarto casos.

52
0:02:11.521,000 --> 0:02:12,000
Lo llamamos el P100,

53
0:02:13.116,000 --> 0:02:15,000
y creemos que eso es lo que pasa en el cerebro

54
0:02:15.328,000 --> 0:02:16,000
cuando reconoces un objeto.

55
0:02:16.892,000 --> 0:02:18,000
Pero ¡guau!, mira la señal para la cara.

56
0:02:19.259,000 --> 0:02:2,000
Parece diferente de las otras.

57
0:02:20.994,000 --> 0:02:24,000
Hay una baja a unos 170 milisegundos después de que sale la imagen.

58
0:02:25.472,000 --> 0:02:26,000
¿Qué estará pasando aquí?

59
0:02:27.246,000 --> 0:02:3,000
Los estudios demuestran que el cerebro tiene muchas neuronas

60
0:02:30.336,000 --> 0:02:32,000
dedicadas al reconocimiento de caras humanas.

61
0:02:32.51,000 --> 0:02:34,000
Entonces esta caída de N170 podrían ser todas esas neuronas

62
0:02:35.328,000 --> 0:02:37,000
que se activan a la vez en el mismo sitio,

63
0:02:37.423,000 --> 0:02:38,000
Y podemos detectarlo en el EEG.

64
0:02:39.082,000 --> 0:02:4,000
Hay dos puntos claves aquí.

65
0:02:40.927,000 --> 0:02:43,000
Uno: los ojos no pueden detectar diferencias en los patrones

66
0:02:44.036,000 --> 0:02:45,000
sin que se iguale el ruido.

67
0:02:45.631,000 --> 0:02:47,000
Y dos: aun después de que se elimina el ruido,

68
0:02:47.931,000 --> 0:02:5,000
los ojos solo pueden detectar las señales asociadas con las caras.

69
0:02:51.323,000 --> 0:02:53,000
Entonces, recurrimos al aprendizaje automático.

70
0:02:53.556,000 --> 0:02:54,000
Los ojos no son muy buenos

71
0:02:55.071,000 --> 0:02:57,000
para identificar patrones entre datos caóticos,

72
0:02:57.254,000 --> 0:02:59,000
pero los algoritmos de aprendizaje automático

73
0:02:59.411,000 --> 0:03:,000
están precisamente para eso.

74
0:03:00.721,000 --> 0:03:02,000
Entonces, ¿podríamos tomar muchas imágenes y datos

75
0:03:03.333,000 --> 0:03:04,000
y entrarlos a una computadora

76
0:03:05.214,000 --> 0:03:08,000
para programarla a interpretar lo que Christy está viendo en tiempo real?

77
0:03:09.088,000 --> 0:03:13,000
Tratamos de escribir la información que sale de su EEG

78
0:03:13.229,000 --> 0:03:14,000
en tiempo real,

79
0:03:14.428,000 --> 0:03:16,000
y predecir lo que sus ojos están viendo.

80
0:03:16.913,000 --> 0:03:19,000
Y si funciona, entonces cada vez que le toca una imagen del paisaje,

81
0:03:20.77,000 --> 0:03:22,000
debe decir: paisaje, paisaje, paisaje, paisaje.

82
0:03:23.081,000 --> 0:03:25,000
Y cuando es una cara: cara, cara, cara, cara.

83
0:03:25.36,000 --> 0:03:27,000
Pero no está funcionando exactamente así,

84
0:03:27.798,000 --> 0:03:28,000
aparentemente.

85
0:03:34.09,000 --> 0:03:36,000
(Risas)

86
0:03:36.957,000 --> 0:03:37,000
Muy bien.

87
0:03:38.132,000 --> 0:03:39,000
Christy: ¿Qué está pasando aquí?

88
0:03:39.741,000 --> 0:03:4,000
GG: Necesitamos una nueva carrera, creo.

89
0:03:41.687,000 --> 0:03:42,000
(Risas)

90
0:03:42.713,000 --> 0:03:44,000
GG: Bien, eso fue un enorme fracaso.

91
0:03:45.1,000 --> 0:03:46,000
Pero seguimos curiosos:

92
0:03:46.274,000 --> 0:03:48,000
¿Hasta dónde podríamos llevar esta tecnología?

93
0:03:48.436,000 --> 0:03:49,000
Revisamos lo que habíamos hecho.

94
0:03:5,000 --> 0:03:52,000
Notamos que los datos entraban a la computadora muy rápido,

95
0:03:52.836,000 --> 0:03:54,000
sin indicar dónde ocurrían los intervalos entre imágenes.

96
0:03:55.536,000 --> 0:03:57,000
Eso sería el equivalente de leer una oración muy larga

97
0:03:58.16,000 --> 0:03:59,000
sin espacios entre las palabras.

98
0:03:59.738,000 --> 0:04:,000
Algo así sería difícil de leer,

99
0:04:01.513,000 --> 0:04:04,000
pero una vez que insertemos los espacios, aparecen las palabras individuales

100
0:04:05.16,000 --> 0:04:07,000
y es mucho más comprensible.

101
0:04:07.228,000 --> 0:04:08,000
Pero ¿qué pasa si hacemos trampa?

102
0:04:09.072,000 --> 0:04:11,000
Utilizando un sensor, podemos decirle a la computadora

103
0:04:11.626,000 --> 0:04:12,000
el momento en que sale la imagen.

104
0:04:13.256,000 --> 0:04:16,000
De ese modo, la onda cerebral deja de ser un flujo continuo de información,

105
0:04:16.827,000 --> 0:04:18,000
y en cambio se vuelven unidades individuales de significado.

106
0:04:19.587,000 --> 0:04:2,000
Vamos a hacer un poquito más de trampa,

107
0:04:21.413,000 --> 0:04:22,000
usando solo dos de las categorías.

108
0:04:23.249,000 --> 0:04:25,000
Veamos si podemos leer la mente en tiempo real.

109
0:04:25.554,000 --> 0:04:28,000
En este nuevo experimento, lo restringimos un poco más

110
0:04:29.036,000 --> 0:04:31,000
para saber el momento en que sale la imagen,

111
0:04:31.312,000 --> 0:04:34,000
y limitamos las categorías a solo "cara" y "paisaje".

112
0:04:35.097,000 --> 0:04:36,000
Nathan: Cara. Correcto.

113
0:04:37.78,000 --> 0:04:38,000
Paisaje. Correcto.

114
0:04:40.251,000 --> 0:04:42,000
GG: Así que cada vez que aparece la imagen,

115
0:04:42.648,000 --> 0:04:44,000
sacamos una foto del momento en que sale

116
0:04:44.938,000 --> 0:04:45,000
y desciframos las ondas del EEG.

117
0:04:46.657,000 --> 0:04:47,000
Está mejorando.

118
0:04:47.937,000 --> 0:04:48,000
Nathan: Sí. Cara. Correcto.

119
0:04:49.626,000 --> 0:04:51,000
GG: Entonces sí, hay información en la señal del EEG,

120
0:04:52.314,000 --> 0:04:54,000
nada más la tuvimos que alinear con la apariencia de la imagen.

121
0:04:55.307,000 --> 0:04:56,000
Nathan: Paisaje. Correcto.

122
0:04:59.344,000 --> 0:05:,000
Cara. Sí.

123
0:05:00.518,000 --> 0:05:02,000
GG: Esto significa que sí, hay información presente,

124
0:05:03.038,000 --> 0:05:05,000
y si sabemos el momento en que apareció la imagen,

125
0:05:05.767,000 --> 0:05:07,000
podemos determinar qué tipo de imagen era,

126
0:05:07.84,000 --> 0:05:09,000
posiblemente, o por lo menos en promedio,

127
0:05:10.068,000 --> 0:05:12,000
observando estos potenciales activados.

128
0:05:12.91,000 --> 0:05:13,000
Nathan: Exactamente.

129
0:05:14.259,000 --> 0:05:16,000
GG: Si me hubieras dicho al principio que esto era posible,

130
0:05:17.128,000 --> 0:05:18,000
habría dicho que no hay manera.

131
0:05:18.65,000 --> 0:05:19,000
No creí que fuera posible.

132
0:05:20.001,000 --> 0:05:23,000
¿De verdad funcionó nuestro experimento de lectura del pensamiento?

133
0:05:23.193,000 --> 0:05:24,000
Sí, pero tuvimos que hacer mucha trampa.

134
0:05:25.192,000 --> 0:05:28,000
Al final, puedes encontrar algunas cosas interesantes en el EEG,

135
0:05:28.291,000 --> 0:05:3,000
por ejemplo, si estás mirando la cara de alguien.

136
0:05:30.705,000 --> 0:05:31,000
Pero está bastante limitado.

137
0:05:32.616,000 --> 0:05:34,000
Quizás se harán grandes avances en el aprendizaje automático

138
0:05:35.586,000 --> 0:05:38,000
y algún día podremos descifrar lo que pasa en nuestros pensamientos.

139
0:05:39,000 --> 0:05:43,000
Pero por ahora, cuando una compañía dice que pueden emplear tus ondas cerebrales

140
0:05:43.101,000 --> 0:05:44,000
para poder controlar aparatos,

141
0:05:44.992,000 --> 0:05:46,000
es tu derecho --es tu obligación--

142
0:05:47.062,000 --> 0:05:48,000
ser escéptico.

