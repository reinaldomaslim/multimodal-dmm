1
0:00:,000 --> 0:00:07,000
Traductor: Ciro Gomez Revisor: Lidia Cámara de la Fuente

2
0:00:12.151,000 --> 0:00:17,000
Bryn Freedman: Eres un tipo cuya compañía financia estos programas de IA e invierte.

3
0:00:18.111,000 --> 0:00:23,000
¿Por qué deberíamos confiar en que no tienes un sesgo

4
0:00:23.18,000 --> 0:00:26,000
y nos contarás algo realmente útil para el resto de nosotros

5
0:00:26.38,000 --> 0:00:28,000
sobre el futuro del trabajo?

6
0:00:29.046,000 --> 0:00:3,000
Roy Bahat: Sí, lo soy.

7
0:00:30.561,000 --> 0:00:32,000
Y cuando te levantas por la mañana y lees en el periódico:

8
0:00:33.573,000 --> 0:00:36,000
"Los robots están llegando, pueden tomar todos nuestros trabajos",

9
0:00:37.046,000 --> 0:00:39,000
como un inversor centrado en el futuro del trabajo,

10
0:00:39.601,000 --> 0:00:41,000
nuestro fondo fue el primero en decir

11
0:00:41.942,000 --> 0:00:43,000
que la IA debe ser un foco para nosotros.

12
0:00:44.276,000 --> 0:00:46,000
Me levanté una mañana y leí eso y dije:

13
0:00:46.553,000 --> 0:00:49,000
"Oh, Dios mío, están hablando de mí. Soy yo quien hace eso".

14
0:00:50.839,000 --> 0:00:52,000
Y luego pensé: espera un minuto.

15
0:00:52.887,000 --> 0:00:54,000
Si las cosas continúan,

16
0:00:55.323,000 --> 0:01:,000
tal vez no solo lo harán las empresas de nueva creación en las que invertimos

17
0:01:00.601,000 --> 0:01:02,000
porque no habrá gente que tenga trabajo

18
0:01:03.186,000 --> 0:01:05,000
para pagar las cosas que ellas hacen y comprarlas,

19
0:01:05.912,000 --> 0:01:08,000
pero nuestra economía y nuestra sociedad también podrían luchar.

20
0:01:09.013,000 --> 0:01:12,000
Y mira, debería ser el tipo que se sienta aquí y te dice:

21
0:01:12.258,000 --> 0:01:15,000
"Todo va a salir bien. Todo va a funcionar muy bien".

22
0:01:15.413,000 --> 0:01:17,000
Cuando introdujeron el cajero automático,

23
0:01:17.468,000 --> 0:01:19,000
años después, hay más cajeros en los bancos".

24
0:01:19.629,000 --> 0:01:2,000
Es verdad.

25
0:01:20.811,000 --> 0:01:23,000
Y sin embargo, cuando lo miré, pensé: "Esto va a acelerar.

26
0:01:24.011,000 --> 0:01:27,000
Y si se acelera, existe la posibilidad de que el centro no se mantenga".

27
0:01:27.435,000 --> 0:01:29,000
Pero pensé que alguien debía saber la respuesta;

28
0:01:29.792,000 --> 0:01:3,000
hay tantas ideas por ahí.

29
0:01:31.121,000 --> 0:01:35,000
Y leí todos los libros, y fui a las conferencias, y en un punto,

30
0:01:35.819,000 --> 0:01:39,000
contamos más de 100 esfuerzos para estudiar el futuro del trabajo.

31
0:01:40.417,000 --> 0:01:42,000
Y fue una experiencia frustrante,

32
0:01:43.268,000 --> 0:01:47,000
porque escuchaba lo mismo una y otra vez:

33
0:01:47.307,000 --> 0:01:48,000
"¡Los robots están llegando!".

34
0:01:49.109,000 --> 0:01:5,000
Y entonces alguien más diría,

35
0:01:50.736,000 --> 0:01:53,000
"Oh, no te preocupes por eso, siempre lo han dicho y está bien".

36
0:01:53.844,000 --> 0:01:54,000
Entonces alguien más diría,

37
0:01:55.373,000 --> 0:01:57,000
"Bueno, en realidad se trata del significado de tu trabajo".

38
0:01:58.257,000 --> 0:02:,000
Y luego todos se encogían de hombros y se iban a tomar algo.

39
0:02:01.08,000 --> 0:02:04,000
Y se sintió como si hubiera este teatro Kabuki de esta discusión,

40
0:02:04.236,000 --> 0:02:06,000
donde nadie hablaba entre sí.

41
0:02:06.325,000 --> 0:02:09,000
Muchas personas que conocí y con las que trabajé en el mundo de la tecnología.

42
0:02:10.051,000 --> 0:02:12,000
no hablaban con los políticos; los políticos no les hablaban.

43
0:02:12.981,000 --> 0:02:16,000
Entonces, nos asociamos con una ONG de expertos llamada New America

44
0:02:17.863,000 --> 0:02:18,000
para estudiar este tema.

45
0:02:19.19,000 --> 0:02:21,000
Y reunimos a un grupo de personas,

46
0:02:21.562,000 --> 0:02:24,000
incluyendo un zar de IA en una empresa de tecnología.

47
0:02:25.016,000 --> 0:02:26,000
y un diseñador de videojuegos

48
0:02:26.912,000 --> 0:02:27,000
y un conservador tradicional

49
0:02:28.309,000 --> 0:02:29,000
y un inversor de Wall Street

50
0:02:29.687,000 --> 0:02:3,000
y un editor de una revista socialista...

51
0:02:31.631,000 --> 0:02:34,000
literalmente, en la misma habitación; ocasionalmente era incómodo...

52
0:02:34.889,000 --> 0:02:36,000
para tratar de averiguar qué sucederá aquí.

53
0:02:37.268,000 --> 0:02:39,000
La pregunta que hicimos fue simple.

54
0:02:40.704,000 --> 0:02:43,000
Era: ¿Cuál será el efecto de la tecnología en el trabajo?

55
0:02:44.141,000 --> 0:02:45,000
Y miramos de 10 a 20 años,

56
0:02:45.792,000 --> 0:02:48,000
porque queríamos mirar tan lejos que pudiera haber un cambio real,

57
0:02:49.01,000 --> 0:02:53,000
pero tan cerca como para no hablar de teletransportación ni nada de eso.

58
0:02:53.293,000 --> 0:02:54,000
Y reconocimos

59
0:02:54.695,000 --> 0:02:57,000
--y creo que cada año recordamos esto en el mundo--

60
0:02:57.768,000 --> 0:02:59,000
que predecir lo que va a pasar es difícil.

61
0:02:59.999,000 --> 0:03:01,000
En lugar de predecir, hay otras cosas que puedes hacer.

62
0:03:02.879,000 --> 0:03:04,000
Puedes intentar imaginar futuros posibles alternativos,

63
0:03:05.699,000 --> 0:03:06,000
que es lo que hicimos.

64
0:03:06.795,000 --> 0:03:08,000
Hicimos un ejercicio de planificación de escenarios,

65
0:03:09.233,000 --> 0:03:11,000
e imaginamos casos en que ningún trabajo es seguro.

66
0:03:11.747,000 --> 0:03:14,000
Imaginamos casos en que cada trabajo es seguro.

67
0:03:14.874,000 --> 0:03:18,000
E imaginamos todas las posibilidades que pudimos.

68
0:03:18.937,000 --> 0:03:21,000
Y el resultado, que realmente nos sorprendió,

69
0:03:22.244,000 --> 0:03:25,000
fue que, al pensar en esos futuros y qué deberíamos hacer,

70
0:03:25.966,000 --> 0:03:28,000
las respuestas sobre lo que deberíamos hacer en realidad resultan ser las mismas,

71
0:03:29.905,000 --> 0:03:3,000
no importa lo que pase.

72
0:03:31.425,000 --> 0:03:34,000
Y la ironía de mirar hacia el futuro de 10 a 20 años es,

73
0:03:34.661,000 --> 0:03:36,000
que te das cuenta de las cosas sobre las que queremos actuar

74
0:03:37.624,000 --> 0:03:38,000
en realidad están sucediendo ahora mismo.

75
0:03:39.609,000 --> 0:03:41,000
La automatización es ahora, el futuro es ahora.

76
0:03:42.419,000 --> 0:03:44,000
BF: Entonces, ¿qué significa eso y qué nos dice eso?

77
0:03:45.038,000 --> 0:03:47,000
Si el futuro es ahora, ¿qué deberíamos estar haciendo

78
0:03:47.76,000 --> 0:03:48,000
y qué deberíamos estar pensando?

79
0:03:49.654,000 --> 0:03:51,000
RB: Tenemos primero que entender el problema.

80
0:03:51.864,000 --> 0:03:54,000
Los datos son que a medida que la economía se vuelve más productiva

81
0:03:55.191,000 --> 0:03:57,000
y los trabajadores individuales más productivos,

82
0:03:57.83,000 --> 0:03:58,000
sus salarios no han aumentado

83
0:03:59.276,000 --> 0:04:02,000
Si vemos la proporción de hombres en edad cúspide de trabajar,

84
0:04:02.49,000 --> 0:04:03,000
en EE. UU. al menos,

85
0:04:04.006,000 --> 0:04:07,000
que trabajan ahora versus en 1960,

86
0:04:07.779,000 --> 0:04:09,000
tenemos tres veces más hombres que no trabajan.

87
0:04:10.271,000 --> 0:04:11,000
Y luego escuchas historias.

88
0:04:11.739,000 --> 0:04:13,000
Me senté con unos trabajadores de Walmart y dije:

89
0:04:14.202,000 --> 0:04:17,000
"¿Qué piensan sobre este cajero, esta cosa futurista de autopago?".

90
0:04:17.419,000 --> 0:04:19,000
Dijeron: "Está bien, pero ¿sabes del dispensador de efectivo?

91
0:04:20.348,000 --> 0:04:22,000
Es una máquina que está siendo instalada ahora,

92
0:04:22.975,000 --> 0:04:24,000
y está eliminando dos empleos en cada Walmart hoy".

93
0:04:25.576,000 --> 0:04:27,000
Y así pensamos, "Caray. No entendemos el problema".

94
0:04:28.519,000 --> 0:04:31,000
Y así vimos las voces que fueron excluidas,

95
0:04:32.524,000 --> 0:04:34,000
que son todas las personas afectadas por este cambio.

96
0:04:35.149,000 --> 0:04:36,000
Y decidimos escucharlos,

97
0:04:36.76,000 --> 0:04:37,000
tipo "automatización y sus descontentos".

98
0:04:38.768,000 --> 0:04:39,000
He pasado los últimos años haciéndolo.

99
0:04:40.775,000 --> 0:04:42,000
He estado en Flint, Michigan, y Youngstown, Ohio,

100
0:04:43.565,000 --> 0:04:45,000
hablando de emprendedores, intentando de que funcione

101
0:04:46.216,000 --> 0:04:48,000
en un ambiente muy diferente de Nueva York o San Francisco

102
0:04:49.042,000 --> 0:04:5,000
o Londres o Tokio.

103
0:04:50.551,000 --> 0:04:51,000
He estado en prisiones dos veces

104
0:04:52.092,000 --> 0:04:54,000
hablando con los presos sobre sus trabajos luego de que salgan.

105
0:04:55.077,000 --> 0:04:58,000
Les he preguntado a camioneros sobre el camión de conducción automática,

106
0:04:58.698,000 --> 0:05:,000
con personas que, además de su trabajo a tiempo completo,

107
0:05:01.386,000 --> 0:05:02,000
cuidan a un familiar envejecido.

108
0:05:03.134,000 --> 0:05:04,000
Y cuando hablas con la gente,

109
0:05:04.722,000 --> 0:05:06,000
hubo dos temas que salieron en voz alta y clara.

110
0:05:08.285,000 --> 0:05:12,000
El primero fue que la gente busca menos más dinero

111
0:05:13.153,000 --> 0:05:16,000
o salir del miedo de que un robot tome su trabajo,

112
0:05:16.455,000 --> 0:05:17,000
y solo quieren algo estable.

113
0:05:18.375,000 --> 0:05:19,000
Quieren algo predecible.

114
0:05:19.939,000 --> 0:05:22,000
Si encuestas a las personas y les preguntas qué quieren fuera de trabajo,

115
0:05:23.638,000 --> 0:05:26,000
todos los que ganan menos de USD 150 000 al año.

116
0:05:27.142,000 --> 0:05:3,000
tomarán un ingreso más estable y seguro, en promedio,

117
0:05:30.522,000 --> 0:05:31,000
sobre ganar más dinero.

118
0:05:32.411,000 --> 0:05:34,000
Y si piensas en el hecho de que

119
0:05:34.649,000 --> 0:05:37,000
no solo todas las personas de la Tierra que no ganan para vivir,

120
0:05:38.061,000 --> 0:05:39,000
sino los que lo hacen,

121
0:05:39.276,000 --> 0:05:41,000
la gran mayoría gana una cantidad diferente de mes a mes.

122
0:05:42.26,000 --> 0:05:43,000
y tienen inestabilidad,

123
0:05:43.498,000 --> 0:05:44,000
de repente te das cuenta,

124
0:05:44.919,000 --> 0:05:46,000
"Un minuto. Tenemos un problema real entre manos".

125
0:05:47.351,000 --> 0:05:5,000
Y lo segundo que dicen, que nos llevó más tiempo entender,

126
0:05:51.188,000 --> 0:05:53,000
es que dicen que quieren dignidad.

127
0:05:53.894,000 --> 0:05:57,000
Y ese concepto de autoestima a través del trabajo

128
0:05:58.9,000 --> 0:06:,000
surgió una y otra y otra vez en nuestras conversaciones.

129
0:06:01.665,000 --> 0:06:03,000
BF: De verdad aprecio esta respuesta.

130
0:06:04.673,000 --> 0:06:05,000
Pero no se puede comer dignidad,

131
0:06:06.304,000 --> 0:06:08,000
no puedes vestir a tus hijos con autoestima.

132
0:06:09.077,000 --> 0:06:12,000
Y, ¿qué es eso, ¿cómo se reconcilia?

133
0:06:12.589,000 --> 0:06:13,000
¿qué significa dignidad

134
0:06:14.454,000 --> 0:06:17,000
y cuál es la relación entre dignidad y estabilidad?

135
0:06:18.081,000 --> 0:06:2,000
RB: No puedes comer dignidad. Necesitas estabilidad primero.

136
0:06:20.935,000 --> 0:06:2,000
La buena noticia es,

137
0:06:21.911,000 --> 0:06:23,000
que se están dando muchas conversaciones ahora

138
0:06:24.69,000 --> 0:06:25,000
sobre cómo resolvemos eso.

139
0:06:26.276,000 --> 0:06:29,000
Soy un defensor de estudiar los ingresos garantizados,

140
0:06:30.165,000 --> 0:06:31,000
como un ejemplo,

141
0:06:31.466,000 --> 0:06:33,000
conversaciones sobre cómo dar atención médica

142
0:06:33.778,000 --> 0:06:34,000
y otros beneficios.

143
0:06:35.041,000 --> 0:06:36,000
Esas conversaciones se están dando,

144
0:06:36.772,000 --> 0:06:38,000
y estamos en un momento en que debemos resolver eso.

145
0:06:39.262,000 --> 0:06:4,000
Es la crisis de nuestra era.

146
0:06:40.927,000 --> 0:06:42,000
Y mi punto de vista después de hablar con la gente.

147
0:06:43.863,000 --> 0:06:45,000
es que podemos hacer eso,

148
0:06:45.927,000 --> 0:06:46,000
y aún podría no ser suficiente.

149
0:06:47.531,000 --> 0:06:49,000
Porque lo que hay que hacer desde el principio es entender

150
0:06:50.322,000 --> 0:06:52,000
qué es el trabajo que da dignidad a la gente

151
0:06:52.626,000 --> 0:06:55,000
para que puedan vivir las vidas que quieren vivir.

152
0:06:55.99,000 --> 0:06:59,000
Y ese concepto de dignidad es...

153
0:07:00.029,000 --> 0:07:01,000
es difícil de entender,

154
0:07:01.935,000 --> 0:07:04,000
porque cuando la gente lo escucha, en especial, para ser honesto, gente rica,

155
0:07:05.608,000 --> 0:07:06,000
escuchan "sentido".

156
0:07:06.768,000 --> 0:07:08,000
Escuchan: "Mi trabajo es importante para mí".

157
0:07:08.994,000 --> 0:07:1,000
Y de nuevo, si encuestas a las personas y les preguntas,

158
0:07:11.679,000 --> 0:07:15,000
"¿Qué tan importante es para Ud. que su trabajo sea importante para Ud.?"

159
0:07:15.903,000 --> 0:07:18,000
solo personas que ganan USD 150 000 al año o más.

160
0:07:19.228,000 --> 0:07:23,000
dicen que es importante para ellos que su trabajo sea importante.

161
0:07:24.05,000 --> 0:07:25,000
BF: ¿Te refieres a significativo?

162
0:07:25.744,000 --> 0:07:27,000
RB: Se define como "¿Es tu trabajo importante para ti?".

163
0:07:29.87,000 --> 0:07:3,000
Lo que sea que alguien tome en serio.

164
0:07:31.69,000 --> 0:07:32,000
Aunque, claro, la dignidad es esencial.

165
0:07:33.628,000 --> 0:07:34,000
Hablamos con camioneros que dijeron:

166
0:07:35.372,000 --> 0:07:38,000
"Vi a mi primo conducir, y me subí a la carretera y fue increíble".

167
0:07:39.28,000 --> 0:07:42,000
Y empecé a ganar más dinero que los que fueron a la universidad".

168
0:07:42.454,000 --> 0:07:45,000
Luego llegarían al final de su pensamiento y dirían algo como:

169
0:07:45.652,000 --> 0:07:47,000
"La gente necesita sus frutas y verduras en la mañana,

170
0:07:48.231,000 --> 0:07:49,000
y yo soy el que se los lleva".

171
0:07:49.704,000 --> 0:07:52,000
Hablamos con alguien que, además de su trabajo, estaba cuidando a su tía.

172
0:07:53.308,000 --> 0:07:54,000
Estaba haciendo mucho dinero.

173
0:07:54.751,000 --> 0:07:55,000
En un punto preguntamos,

174
0:07:56.192,000 --> 0:08:,000
"¿Por qué cuidar a tu tía? ¿No puedes solo pagarle a alguien para que lo haga?"

175
0:08:00.858,000 --> 0:08:02,000
Dijo: "Mi tía no quiere a alguien a quien le paguemos.

176
0:08:03.451,000 --> 0:08:04,000
Mi tía me quiere a mí".

177
0:08:04.601,000 --> 0:08:07,000
Así que ahí estaba este concepto de ser necesario.

178
0:08:08.292,000 --> 0:08:1,000
Si estudias la palabra "dignidad", es fascinante.

179
0:08:10.773,000 --> 0:08:13,000
Es una de las palabras más antiguas del inglés, desde la antigüedad.

180
0:08:14.022,000 --> 0:08:15,000
Tiene dos significados:

181
0:08:15.248,000 --> 0:08:16,000
uno es autoestima,

182
0:08:16.426,000 --> 0:08:2,000
y el otro es algo que es adecuado, que es apropiado,

183
0:08:20.677,000 --> 0:08:22,000
lo que significa que eres parte de algo más grande que tú,

184
0:08:23.559,000 --> 0:08:24,000
y se conecta a un todo más amplio.

185
0:08:25.406,000 --> 0:08:26,000
En otras palabras, que te necesitan.

186
0:08:27.147,000 --> 0:08:28,000
BF: ¿Cómo respondes a esta pregunta,

187
0:08:28.915,000 --> 0:08:3,000
este concepto de que no pagamos a los maestros,

188
0:08:31.345,000 --> 0:08:33,000
no pagamos a los cuidadores de ancianos.

189
0:08:33.557,000 --> 0:08:35,000
No pagamos a quienes realmente se preocupan por las personas

190
0:08:36.561,000 --> 0:08:38,000
y ellos son necesarios.

191
0:08:38.871,000 --> 0:08:41,000
RB: La buena noticia es que finalmente se está haciendo la pregunta.

192
0:08:42.087,000 --> 0:08:44,000
Como inversores de IA, a menudo recibimos llamadas

193
0:08:44.51,000 --> 0:08:46,000
de fundaciones o CEOs y directorios diciendo,

194
0:08:47.158,000 --> 0:08:48,000
"¿Qué hacemos con esto?".

195
0:08:48.486,000 --> 0:08:48,000
Solían preguntar,

196
0:08:49.44,000 --> 0:08:51,000
"¿Qué hacer con la introducción de la automatización?".

197
0:08:52.042,000 --> 0:08:54,000
Ahora preguntan: "¿Qué hacemos con la autoestima?".

198
0:08:54.678,000 --> 0:08:56,000
Saben que los empleados que trabajan para ellos

199
0:08:57.025,000 --> 0:08:59,000
tienen un cónyuge que se preocupa por alguien,

200
0:08:59.501,000 --> 0:09:02,000
que esa dignidad es esencial para su capacidad de hacer su trabajo.

201
0:09:02.942,000 --> 0:09:03,000
Creo que hay dos tipos de respuestas:

202
0:09:04.742,000 --> 0:09:06,000
está el lado del dinero de hacer que tu vida funcione.

203
0:09:07.292,000 --> 0:09:09,000
Eso es estabilidad. Necesitas comer.

204
0:09:09.641,000 --> 0:09:11,000
Luego piensas en nuestra cultura más ampliamente,

205
0:09:12.002,000 --> 0:09:14,000
y preguntas: ¿A quién hacemos héroes?

206
0:09:14.704,000 --> 0:09:18,000
Lo que quiero es ver en la portada de una revista

207
0:09:19.188,000 --> 0:09:21,000
la persona que es el cuidador heroico.

208
0:09:22.292,000 --> 0:09:24,000
O una serie de Netflix que dramatice a la persona

209
0:09:24.766,000 --> 0:09:27,000
que hace que nuestra vida funcione a fin de poder hacer lo que hacemos.

210
0:09:28.307,000 --> 0:09:29,000
Hagamos héroes a esas personas.

211
0:09:30.154,000 --> 0:09:32,000
Es la serie de Netflix en la que me embriagaría.

212
0:09:32.467,000 --> 0:09:34,000
Hemos tenido cronistas de esto antes...

213
0:09:34.514,000 --> 0:09:35,000
Studs Terkel,

214
0:09:35.752,000 --> 0:09:38,000
la historia oral de la experiencia laboral en EE. UU.

215
0:09:39.482,000 --> 0:09:42,000
Lo que necesitamos es la experiencia de necesitarnos unos a otros

216
0:09:42.658,000 --> 0:09:43,000
y estar conectados entre sí.

217
0:09:44.207,000 --> 0:09:47,000
Tal vez esa sea la respuesta de cómo encajamos todos como sociedad.

218
0:09:47.371,000 --> 0:09:48,000
Y el ejercicio mental, para mí, es:

219
0:09:49.117,000 --> 0:09:51,000
si tuvieras que volver 100 años atrás y la gente

220
0:09:51.719,000 --> 0:09:54,000
--mis abuelos, bisabuelos, un sastre, un trabajaron en una mina--

221
0:09:55.514,000 --> 0:09:58,000
miran lo que todos hacemos para ganarnos la vida y dicen: "Eso no es trabajo".

222
0:09:59.363,000 --> 0:10:02,000
Nos sentamos y tecleamos y hablamos, y no hay peligro de lastimarse.

223
0:10:03.526,000 --> 0:10:06,000
Mi conjetura es que si imaginas dentro de 100 años,

224
0:10:06.831,000 --> 0:10:08,000
seguiremos haciendo cosas el uno por el otro.

225
0:10:08.987,000 --> 0:10:09,000
Aún nos necesitaremos unos a otros.

226
0:10:10.773,000 --> 0:10:11,000
Y lo veremos como un trabajo.

227
0:10:12.538,000 --> 0:10:13,000
Lo que estoy tratando de decir

228
0:10:14.236,000 --> 0:10:16,000
es que la dignidad no debe ser solo tener un trabajo.

229
0:10:17.133,000 --> 0:10:19,000
Porque si dices que necesitas un trabajo para tener dignidad,

230
0:10:20.014,000 --> 0:10:21,000
lo que mucha gente dice,

231
0:10:21.402,000 --> 0:10:23,000
lo segundo que les dices, lo que les dices a todos los padres

232
0:10:24.323,000 --> 0:10:26,000
y a todos los maestros y a todos los cuidadores

233
0:10:26.792,000 --> 0:10:27,000
es que todos de repente,

234
0:10:27.99,000 --> 0:10:29,000
porque no les pagan por lo que hacen,

235
0:10:30.111,000 --> 0:10:32,000
de alguna manera carecen de esta cualidad humana esencial.

236
0:10:32.878,000 --> 0:10:34,000
Para mí, es el gran enigma de nuestro tiempo:

237
0:10:35.006,000 --> 0:10:38,000
Podremos averiguar cómo proporcionar esa estabilidad a lo largo de la vida,

238
0:10:38.637,000 --> 0:10:4,000
y luego descifrar cómo crear algo inclusivo,

239
0:10:40.744,000 --> 0:10:44,000
no solo racial, de género, sino multigeneracionalmente inclusivo

240
0:10:44.989,000 --> 0:10:48,000
---quiero decir, cada experiencia humana diferente incluida--

241
0:10:49.792,000 --> 0:10:52,000
de manera de entender cómo podemos ser necesitados el uno por el otro.

242
0:10:53.119,000 --> 0:10:54,000
BF: Gracias. RB: Gracias.

243
0:10:54.509,000 --> 0:10:56,000
BF: Muchas gracias por tu participación.

244
0:10:56.73,000 --> 0:10:57,000
(Aplausos)

