1
0:00:13.373,000 --> 0:00:14,000
Is it just me,

2
0:00:15.141,000 --> 0:00:17,000
or are there other people here

3
0:00:17.497,000 --> 0:00:19,000
that are a little bit disappointed with democracy?

4
0:00:20.986,000 --> 0:00:22,000
(Applause)

5
0:00:24.141,000 --> 0:00:26,000
So let's look at a few numbers.

6
0:00:26.934,000 --> 0:00:28,000
If we look across the world,

7
0:00:29.131,000 --> 0:00:32,000
the median turnout in presidential elections

8
0:00:33.047,000 --> 0:00:34,000
over the last 30 years

9
0:00:34.722,000 --> 0:00:36,000
has been just 67 percent.

10
0:00:38.329,000 --> 0:00:39,000
Now, if we go to Europe

11
0:00:40.326,000 --> 0:00:44,000
and we look at people that participated in EU parliamentary elections,

12
0:00:44.778,000 --> 0:00:46,000
the median turnout in those elections

13
0:00:46.873,000 --> 0:00:48,000
is just 42 percent.

14
0:00:50.125,000 --> 0:00:51,000
Now let's go to New York,

15
0:00:51.818,000 --> 0:00:55,000
and let's see how many people voted in the last election for mayor.

16
0:00:56.523,000 --> 0:00:59,000
We will find that only 24 percent of people showed up to vote.

17
0:01:01.063,000 --> 0:01:04,000
What that means is that, if "Friends" was still running,

18
0:01:04.182,000 --> 0:01:07,000
Joey and maybe Phoebe would have shown up to vote.

19
0:01:07.554,000 --> 0:01:08,000
(Laughter)

20
0:01:09.434,000 --> 0:01:13,000
And you cannot blame them because people are tired of politicians.

21
0:01:13.884,000 --> 0:01:16,000
And people are tired of other people using the data that they have generated

22
0:01:17.795,000 --> 0:01:19,000
to communicate with their friends and family,

23
0:01:20.017,000 --> 0:01:22,000
to target political propaganda at them.

24
0:01:22.519,000 --> 0:01:24,000
But the thing about this is that this is not new.

25
0:01:25.271,000 --> 0:01:28,000
Nowadays, people use likes to target propaganda at you

26
0:01:28.52,000 --> 0:01:31,000
before they use your zip code or your gender or your age,

27
0:01:31.917,000 --> 0:01:34,000
because the idea of targeting people with propaganda for political purposes

28
0:01:35.511,000 --> 0:01:36,000
is as old as politics.

29
0:01:37.53,000 --> 0:01:39,000
And the reason why that idea is there

30
0:01:39.832,000 --> 0:01:42,000
is because democracy has a basic vulnerability.

31
0:01:43.71,000 --> 0:01:44,000
This is the idea of a representative.

32
0:01:46.049,000 --> 0:01:49,000
In principle, democracy is the ability of people to exert power.

33
0:01:50.017,000 --> 0:01:53,000
But in practice, we have to delegate that power to a representative

34
0:01:53.859,000 --> 0:01:55,000
that can exert that power for us.

35
0:01:56.561,000 --> 0:01:57,000
That representative is a bottleneck,

36
0:01:58.41,000 --> 0:01:59,000
or a weak spot.

37
0:01:59.731,000 --> 0:02:02,000
It is the place that you want to target if you want to attack democracy

38
0:02:03.704,000 --> 0:02:06,000
because you can capture democracy by either capturing that representative

39
0:02:07.217,000 --> 0:02:09,000
or capturing the way that people choose it.

40
0:02:10.065,000 --> 0:02:11,000
So the big question is:

41
0:02:11.505,000 --> 0:02:12,000
Is this the end of history?

42
0:02:13.989,000 --> 0:02:16,000
Is this the best that we can do

43
0:02:17.878,000 --> 0:02:2,000
or, actually, are there alternatives?

44
0:02:22.13,000 --> 0:02:24,000
Some people have been thinking about alternatives,

45
0:02:24.508,000 --> 0:02:27,000
and one of the ideas that is out there is the idea of direct democracy.

46
0:02:28.79,000 --> 0:02:3,000
This is the idea of bypassing politicians completely

47
0:02:31.293,000 --> 0:02:33,000
and having people vote directly on issues,

48
0:02:33.72,000 --> 0:02:35,000
having people vote directly on bills.

49
0:02:36.415,000 --> 0:02:37,000
But this idea is naive

50
0:02:37.775,000 --> 0:02:4,000
because there's too many things that we would need to choose.

51
0:02:40.97,000 --> 0:02:42,000
If you look at the 114th US Congress,

52
0:02:43.776,000 --> 0:02:45,000
you will have seen that the House of Representatives

53
0:02:46.287,000 --> 0:02:48,000
considered more than 6,000 bills,

54
0:02:49.2,000 --> 0:02:51,000
the Senate considered more than 3,000 bills

55
0:02:51.88,000 --> 0:02:53,000
and they approved more than 300 laws.

56
0:02:54.712,000 --> 0:02:55,000
Those would be many decisions

57
0:02:56.315,000 --> 0:02:58,000
that each person would have to make a week

58
0:02:58.526,000 --> 0:03:,000
on topics that they know little about.

59
0:03:01.229,000 --> 0:03:03,000
So there's a big cognitive bandwidth problem

60
0:03:03.534,000 --> 0:03:06,000
if we're going to try to think about direct democracy as a viable alternative.

61
0:03:08.205,000 --> 0:03:12,000
So some people think about the idea of liquid democracy, or fluid democracy,

62
0:03:12.664,000 --> 0:03:15,000
which is the idea that you endorse your political power to someone,

63
0:03:16.464,000 --> 0:03:17,000
who can endorse it to someone else,

64
0:03:18.188,000 --> 0:03:2,000
and, eventually, you create a large follower network

65
0:03:20.753,000 --> 0:03:23,000
in which, at the end, there's a few people that are making decisions

66
0:03:24.071,000 --> 0:03:27,000
on behalf of all of their followers and their followers.

67
0:03:28.326,000 --> 0:03:32,000
But this idea also doesn't solve the problem of the cognitive bandwidth

68
0:03:32.479,000 --> 0:03:35,000
and, to be honest, it's also quite similar to the idea of having a representative.

69
0:03:36.795,000 --> 0:03:39,000
So what I'm going to do today is I'm going to be a little bit provocative,

70
0:03:40.277,000 --> 0:03:42,000
and I'm going to ask you, well:

71
0:03:42.601,000 --> 0:03:48,000
What if, instead of trying to bypass politicians,

72
0:03:49.187,000 --> 0:03:51,000
we tried to automate them?

73
0:03:57.871,000 --> 0:03:59,000
The idea of automation is not new.

74
0:04:00.821,000 --> 0:04:02,000
It was started more than 300 years ago,

75
0:04:02.925,000 --> 0:04:05,000
when French weavers decided to automate the loom.

76
0:04:06.82,000 --> 0:04:1,000
The winner of that industrial war was Joseph-Marie Jacquard.

77
0:04:11.204,000 --> 0:04:12,000
He was a French weaver and merchant

78
0:04:12.979,000 --> 0:04:14,000
that married the loom with the steam engine

79
0:04:15.443,000 --> 0:04:17,000
to create autonomous looms.

80
0:04:17.657,000 --> 0:04:19,000
And in those autonomous looms, he gained control.

81
0:04:20.434,000 --> 0:04:23,000
He could now make fabrics that were more complex and more sophisticated

82
0:04:24.343,000 --> 0:04:26,000
than the ones they were able to do by hand.

83
0:04:27.193,000 --> 0:04:29,000
But also, by winning that industrial war,

84
0:04:29.849,000 --> 0:04:32,000
he laid out what has become the blueprint of automation.

85
0:04:34.135,000 --> 0:04:36,000
The way that we automate things for the last 300 years

86
0:04:37.029,000 --> 0:04:38,000
has always been the same:

87
0:04:39.006,000 --> 0:04:41,000
we first identify a need,

88
0:04:41.539,000 --> 0:04:44,000
then we create a tool to satisfy that need,

89
0:04:44.747,000 --> 0:04:46,000
like the loom, in this case,

90
0:04:46.811,000 --> 0:04:48,000
and then we study how people use that tool

91
0:04:49.226,000 --> 0:04:5,000
to automate that user.

92
0:04:51.242,000 --> 0:04:54,000
That's how we came from the mechanical loom

93
0:04:54.327,000 --> 0:04:55,000
to the autonomous loom,

94
0:04:56.247,000 --> 0:04:58,000
and that took us a thousand years.

95
0:04:58.391,000 --> 0:05:,000
Now, it's taken us only a hundred years

96
0:05:00.486,000 --> 0:05:03,000
to use the same script to automate the car.

97
0:05:05.286,000 --> 0:05:07,000
But the thing is that, this time around,

98
0:05:07.762,000 --> 0:05:09,000
automation is kind of for real.

99
0:05:09.915,000 --> 0:05:12,000
This is a video that a colleague of mine from Toshiba shared with me

100
0:05:13.26,000 --> 0:05:16,000
that shows the factory that manufactures solid state drives.

101
0:05:16.543,000 --> 0:05:18,000
The entire factory is a robot.

102
0:05:18.585,000 --> 0:05:19,000
There are no humans in that factory.

103
0:05:21.033,000 --> 0:05:23,000
And the robots are soon to leave the factories

104
0:05:23.278,000 --> 0:05:25,000
and become part of our world,

105
0:05:25.324,000 --> 0:05:26,000
become part of our workforce.

106
0:05:27.183,000 --> 0:05:28,000
So what I do in my day job

107
0:05:28.98,000 --> 0:05:31,000
is actually create tools that integrate data for entire countries

108
0:05:32.996,000 --> 0:05:35,000
so that we can ultimately have the foundations that we need

109
0:05:36.486,000 --> 0:05:39,000
for a future in which we need to also manage those machines.

110
0:05:41.195,000 --> 0:05:43,000
But today, I'm not here to talk to you about these tools

111
0:05:44.125,000 --> 0:05:45,000
that integrate data for countries.

112
0:05:46.463,000 --> 0:05:48,000
But I'm here to talk to you about another idea

113
0:05:49.109,000 --> 0:05:53,000
that might help us think about how to use artificial intelligence in democracy.

114
0:05:53.998,000 --> 0:05:57,000
Because the tools that I build are designed for executive decisions.

115
0:05:58.755,000 --> 0:06:01,000
These are decisions that can be cast in some sort of term of objectivity --

116
0:06:02.621,000 --> 0:06:03,000
public investment decisions.

117
0:06:04.885,000 --> 0:06:06,000
But there are decisions that are legislative,

118
0:06:07.54,000 --> 0:06:1,000
and these decisions that are legislative require communication among people

119
0:06:11.351,000 --> 0:06:12,000
that have different points of view,

120
0:06:13.075,000 --> 0:06:15,000
require participation, require debate,

121
0:06:15.712,000 --> 0:06:16,000
require deliberation.

122
0:06:18.241,000 --> 0:06:2,000
And for a long time, we have thought that, well,

123
0:06:21.069,000 --> 0:06:24,000
what we need to improve democracy is actually more communication.

124
0:06:24.553,000 --> 0:06:27,000
So all of the technologies that we have advanced in the context of democracy,

125
0:06:28.286,000 --> 0:06:3,000
whether they are newspapers or whether it is social media,

126
0:06:31.088,000 --> 0:06:33,000
have tried to provide us with more communication.

127
0:06:34.103,000 --> 0:06:35,000
But we've been down that rabbit hole,

128
0:06:35.949,000 --> 0:06:37,000
and we know that's not what's going to solve the problem.

129
0:06:38.721,000 --> 0:06:39,000
Because it's not a communication problem,

130
0:06:40.741,000 --> 0:06:41,000
it's a cognitive bandwidth problem.

131
0:06:42.513,000 --> 0:06:44,000
So if the problem is one of cognitive bandwidth,

132
0:06:44.903,000 --> 0:06:46,000
well, adding more communication to people

133
0:06:47.514,000 --> 0:06:49,000
is not going to be what's going to solve it.

134
0:06:50.282,000 --> 0:06:53,000
What we are going to need instead is to have other technologies

135
0:06:53.419,000 --> 0:06:56,000
that help us deal with some of the communication

136
0:06:56.489,000 --> 0:06:58,000
that we are overloaded with.

137
0:06:58.755,000 --> 0:06:59,000
Think of, like, a little avatar,

138
0:07:00.478,000 --> 0:07:01,000
a software agent,

139
0:07:01.841,000 --> 0:07:02,000
a digital Jiminy Cricket --

140
0:07:03.743,000 --> 0:07:04,000
(Laughter)

141
0:07:05.005,000 --> 0:07:09,000
that basically is able to answer things on your behalf.

142
0:07:09.759,000 --> 0:07:1,000
And if we had that technology,

143
0:07:11.57,000 --> 0:07:13,000
we would be able to offload some of the communication

144
0:07:14.072,000 --> 0:07:18,000
and help, maybe, make better decisions or decisions at a larger scale.

145
0:07:18.86,000 --> 0:07:21,000
And the thing is that the idea of software agents is also not new.

146
0:07:22.603,000 --> 0:07:24,000
We already use them all the time.

147
0:07:25.216,000 --> 0:07:26,000
We use software agents

148
0:07:26.761,000 --> 0:07:29,000
to choose the way that we're going to drive to a certain location,

149
0:07:31.07,000 --> 0:07:33,000
the music that we're going to listen to

150
0:07:33.758,000 --> 0:07:36,000
or to get suggestions for the next books that we should read.

151
0:07:37.994,000 --> 0:07:39,000
So there is an obvious idea in the 21st century

152
0:07:40.592,000 --> 0:07:42,000
that was as obvious as the idea

153
0:07:43.259,000 --> 0:07:48,000
of putting together a steam engine with a loom at the time of Jacquard.

154
0:07:49.538,000 --> 0:07:53,000
And that idea is combining direct democracy with software agents.

155
0:07:54.849,000 --> 0:07:56,000
Imagine, for a second, a world

156
0:07:56.994,000 --> 0:07:59,000
in which, instead of having a representative that represents you

157
0:08:00.184,000 --> 0:08:01,000
and millions of other people,

158
0:08:01.782,000 --> 0:08:04,000
you can have a representative that represents only you,

159
0:08:05.504,000 --> 0:08:07,000
with your nuanced political views --

160
0:08:07.782,000 --> 0:08:1,000
that weird combination of libertarian and liberal

161
0:08:11.15,000 --> 0:08:13,000
and maybe a little bit conservative on some issues

162
0:08:13.566,000 --> 0:08:15,000
and maybe very progressive on others.

163
0:08:15.698,000 --> 0:08:18,000
Politicians nowadays are packages, and they're full of compromises.

164
0:08:18.989,000 --> 0:08:21,000
But you might have someone that can represent only you,

165
0:08:22.647,000 --> 0:08:23,000
if you are willing to give up the idea

166
0:08:24.523,000 --> 0:08:26,000
that that representative is a human.

167
0:08:27.229,000 --> 0:08:29,000
If that representative is a software agent,

168
0:08:29.335,000 --> 0:08:33,000
we could have a senate that has as many senators as we have citizens.

169
0:08:33.529,000 --> 0:08:35,000
And those senators are going to be able to read every bill

170
0:08:36.411,000 --> 0:08:38,000
and they're going to be able to vote on each one of them.

171
0:08:39.822,000 --> 0:08:41,000
So there's an obvious idea that maybe we want to consider.

172
0:08:42.802,000 --> 0:08:44,000
But I understand that in this day and age,

173
0:08:45.248,000 --> 0:08:46,000
this idea might be quite scary.

174
0:08:48.391,000 --> 0:08:51,000
In fact, thinking of a robot coming from the future

175
0:08:51.855,000 --> 0:08:52,000
to help us run our governments

176
0:08:53.552,000 --> 0:08:54,000
sounds terrifying.

177
0:08:56.223,000 --> 0:08:57,000
But we've been there before.

178
0:08:57.898,000 --> 0:08:58,000
(Laughter)

179
0:08:59.195,000 --> 0:09:01,000
And actually he was quite a nice guy.

180
0:09:03.677,000 --> 0:09:09,000
So what would the Jacquard loom version of this idea look like?

181
0:09:10.135,000 --> 0:09:11,000
It would be a very simple system.

182
0:09:12.06,000 --> 0:09:15,000
Imagine a system that you log in and you create your avatar,

183
0:09:15.542,000 --> 0:09:17,000
and then you're going to start training your avatar.

184
0:09:18.022,000 --> 0:09:2,000
So you can provide your avatar with your reading habits,

185
0:09:20.728,000 --> 0:09:21,000
or connect it to your social media,

186
0:09:22.613,000 --> 0:09:24,000
or you can connect it to other data,

187
0:09:25.045,000 --> 0:09:27,000
for example by taking psychological tests.

188
0:09:27.341,000 --> 0:09:29,000
And the nice thing about this is that there's no deception.

189
0:09:30.333,000 --> 0:09:33,000
You are not providing data to communicate with your friends and family

190
0:09:33.696,000 --> 0:09:36,000
that then gets used in a political system.

191
0:09:36.871,000 --> 0:09:39,000
You are providing data to a system that is designed to be used

192
0:09:40.599,000 --> 0:09:42,000
to make political decisions on your behalf.

193
0:09:43.264,000 --> 0:09:46,000
Then you take that data and you choose a training algorithm,

194
0:09:47.268,000 --> 0:09:48,000
because it's an open marketplace

195
0:09:48.855,000 --> 0:09:5,000
in which different people can submit different algorithms

196
0:09:51.665,000 --> 0:09:55,000
to predict how you're going to vote, based on the data you have provided.

197
0:09:56.083,000 --> 0:09:59,000
And the system is open, so nobody controls the algorithms;

198
0:09:59.562,000 --> 0:10:01,000
there are algorithms that become more popular

199
0:10:01.698,000 --> 0:10:02,000
and others that become less popular.

200
0:10:03.445,000 --> 0:10:04,000
Eventually, you can audit the system.

201
0:10:05.276,000 --> 0:10:06,000
You can see how your avatar is working.

202
0:10:07.181,000 --> 0:10:09,000
If you like it, you can leave it on autopilot.

203
0:10:09.357,000 --> 0:10:11,000
If you want to be a little more controlling,

204
0:10:11.443,000 --> 0:10:12,000
you can actually choose that they ask you

205
0:10:13.435,000 --> 0:10:15,000
every time they're going to make a decision,

206
0:10:15.527,000 --> 0:10:16,000
or you can be anywhere in between.

207
0:10:17.186,000 --> 0:10:19,000
One of the reasons why we use democracy so little

208
0:10:19.615,000 --> 0:10:22,000
may be because democracy has a very bad user interface.

209
0:10:23.207,000 --> 0:10:25,000
And if we improve the user interface of democracy,

210
0:10:25.714,000 --> 0:10:27,000
we might be able to use it more.

211
0:10:28.452,000 --> 0:10:31,000
Of course, there's a lot of questions that you might have.

212
0:10:32.473,000 --> 0:10:34,000
Well, how do you train these avatars?

213
0:10:34.658,000 --> 0:10:35,000
How do you keep the data secure?

214
0:10:36.576,000 --> 0:10:39,000
How do you keep the systems distributed and auditable?

215
0:10:39.848,000 --> 0:10:41,000
How about my grandmother, who's 80 years old

216
0:10:41.946,000 --> 0:10:42,000
and doesn't know how to use the internet?

217
0:10:44.262,000 --> 0:10:46,000
Trust me, I've heard them all.

218
0:10:46.507,000 --> 0:10:5,000
So when you think about an idea like this, you have to beware of pessimists

219
0:10:51.091,000 --> 0:10:55,000
because they are known to have a problem for every solution.

220
0:10:55.434,000 --> 0:10:56,000
(Laughter)

221
0:10:57.283,000 --> 0:11:,000
So I want to invite you to think about the bigger ideas.

222
0:11:00.347,000 --> 0:11:03,000
The questions I just showed you are little ideas

223
0:11:03.997,000 --> 0:11:05,000
because they are questions about how this would not work.

224
0:11:07.502,000 --> 0:11:08,000
The big ideas are ideas of:

225
0:11:09.507,000 --> 0:11:1,000
What else can you do with this

226
0:11:11.338,000 --> 0:11:12,000
if this would happen to work?

227
0:11:13.774,000 --> 0:11:16,000
And one of those ideas is, well, who writes the laws?

228
0:11:17.854,000 --> 0:11:21,000
In the beginning, we could have the avatars that we already have,

229
0:11:22.101,000 --> 0:11:25,000
voting on laws that are written by the senators or politicians

230
0:11:25.622,000 --> 0:11:26,000
that we already have.

231
0:11:27.491,000 --> 0:11:28,000
But if this were to work,

232
0:11:29.902,000 --> 0:11:31,000
you could write an algorithm

233
0:11:32.276,000 --> 0:11:34,000
that could try to write a law

234
0:11:34.45,000 --> 0:11:36,000
that would get a certain percentage of approval,

235
0:11:36.895,000 --> 0:11:37,000
and you could reverse the process.

236
0:11:38.701,000 --> 0:11:41,000
Now, you might think that this idea is ludicrous and we should not do it,

237
0:11:42.237,000 --> 0:11:44,000
but you cannot deny that it's an idea that is only possible

238
0:11:45.047,000 --> 0:11:48,000
in a world in which direct democracy and software agents

239
0:11:48.091,000 --> 0:11:5,000
are a viable form of participation.

240
0:11:52.596,000 --> 0:11:54,000
So how do we start the revolution?

241
0:11:56.238,000 --> 0:11:59,000
We don't start this revolution with picket fences or protests

242
0:11:59.572,000 --> 0:12:03,000
or by demanding our current politicians to be changed into robots.

243
0:12:03.786,000 --> 0:12:04,000
That's not going to work.

244
0:12:05.359,000 --> 0:12:06,000
This is much more simple,

245
0:12:06.995,000 --> 0:12:07,000
much slower

246
0:12:08.178,000 --> 0:12:09,000
and much more humble.

247
0:12:09.616,000 --> 0:12:13,000
We start this revolution by creating simple systems like this in grad schools,

248
0:12:13.989,000 --> 0:12:15,000
in libraries, in nonprofits.

249
0:12:16.107,000 --> 0:12:18,000
And we try to figure out all of those little questions

250
0:12:18.785,000 --> 0:12:19,000
and those little problems

251
0:12:20.03,000 --> 0:12:23,000
that we're going to have to figure out to make this idea something viable,

252
0:12:23.955,000 --> 0:12:25,000
to make this idea something that we can trust.

253
0:12:26.33,000 --> 0:12:29,000
And as we create those systems that have a hundred people, a thousand people,

254
0:12:29.989,000 --> 0:12:32,000
a hundred thousand people voting in ways that are not politically binding,

255
0:12:33.783,000 --> 0:12:35,000
we're going to develop trust in this idea,

256
0:12:35.825,000 --> 0:12:36,000
the world is going to change,

257
0:12:37.368,000 --> 0:12:39,000
and those that are as little as my daughter is right now

258
0:12:40.267,000 --> 0:12:41,000
are going to grow up.

259
0:12:42.58,000 --> 0:12:44,000
And by the time my daughter is my age,

260
0:12:44.973,000 --> 0:12:48,000
maybe this idea, that I know today is very crazy,

261
0:12:49.433,000 --> 0:12:53,000
might not be crazy to her and to her friends.

262
0:12:53.956,000 --> 0:12:54,000
And at that point,

263
0:12:55.817,000 --> 0:12:57,000
we will be at the end of our history,

264
0:12:58.444,000 --> 0:13:,000
but they will be at the beginning of theirs.

265
0:13:01.646,000 --> 0:13:02,000
Thank you.

266
0:13:02.853,000 --> 0:13:05,000
(Applause)

