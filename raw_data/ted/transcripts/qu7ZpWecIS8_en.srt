1
0:00:16.26,000 --> 0:00:18,000
The story starts in Kenya

2
0:00:18.26,000 --> 0:00:2,000
in December of 2007,

3
0:00:20.26,000 --> 0:00:22,000
when there was a disputed presidential election,

4
0:00:22.26,000 --> 0:00:25,000
and in the immediate aftermath of that election,

5
0:00:25.26,000 --> 0:00:27,000
there was an outbreak of ethnic violence.

6
0:00:27.26,000 --> 0:00:3,000
And there was a lawyer in Nairobi, Ory Okolloh --

7
0:00:30.26,000 --> 0:00:32,000
who some of you may know from her TEDTalk --

8
0:00:32.26,000 --> 0:00:34,000
who began blogging about it on her site,

9
0:00:34.26,000 --> 0:00:36,000
Kenyan Pundit.

10
0:00:36.26,000 --> 0:00:39,000
And shortly after the election and the outbreak of violence,

11
0:00:39.26,000 --> 0:00:41,000
the government suddenly imposed

12
0:00:41.26,000 --> 0:00:43,000
a significant media blackout.

13
0:00:43.26,000 --> 0:00:45,000
And so weblogs went from being

14
0:00:45.26,000 --> 0:00:47,000
commentary as part of the media landscape

15
0:00:47.26,000 --> 0:00:5,000
to being a critical part of the media landscape

16
0:00:50.26,000 --> 0:00:53,000
in trying to understand where the violence was.

17
0:00:53.26,000 --> 0:00:55,000
And Okolloh solicited

18
0:00:55.26,000 --> 0:00:57,000
from her commenters

19
0:00:57.26,000 --> 0:00:59,000
more information about what was going on.

20
0:00:59.26,000 --> 0:01:01,000
The comments began pouring in,

21
0:01:01.26,000 --> 0:01:03,000
and Okolloh would collate them. She would post them.

22
0:01:03.26,000 --> 0:01:05,000
And she quickly said, "It's too much.

23
0:01:05.26,000 --> 0:01:07,000
I could do this all day every day

24
0:01:07.26,000 --> 0:01:09,000
and I can't keep up.

25
0:01:09.26,000 --> 0:01:11,000
There is more information

26
0:01:11.26,000 --> 0:01:13,000
about what's going on in Kenya right now

27
0:01:13.26,000 --> 0:01:15,000
than any one person can manage.

28
0:01:15.26,000 --> 0:01:17,000
If only there was a way to automate this."

29
0:01:17.26,000 --> 0:01:19,000
And two programmers who read her blog

30
0:01:19.26,000 --> 0:01:22,000
held their hands up and said, "We could do that,"

31
0:01:22.26,000 --> 0:01:25,000
and in 72 hours, they launched Ushahidi.

32
0:01:25.26,000 --> 0:01:27,000
Ushahidi -- the name means "witness"

33
0:01:27.26,000 --> 0:01:29,000
or "testimony" in Swahili --

34
0:01:29.26,000 --> 0:01:32,000
is a very simple way of taking reports from the field,

35
0:01:32.26,000 --> 0:01:35,000
whether it's from the web or, critically,

36
0:01:35.26,000 --> 0:01:37,000
via mobile phones and SMS,

37
0:01:37.26,000 --> 0:01:4,000
aggregating it and putting it on a map.

38
0:01:40.26,000 --> 0:01:42,000
That's all it is, but that's all that's needed

39
0:01:42.26,000 --> 0:01:45,000
because what it does is it takes the tacit information

40
0:01:45.26,000 --> 0:01:47,000
available to the whole population --

41
0:01:47.26,000 --> 0:01:49,000
everybody knows where the violence is,

42
0:01:49.26,000 --> 0:01:52,000
but no one person knows what everyone knows --

43
0:01:52.26,000 --> 0:01:54,000
and it takes that tacit information

44
0:01:54.26,000 --> 0:01:56,000
and it aggregates it,

45
0:01:56.26,000 --> 0:01:58,000
and it maps it and it makes it public.

46
0:01:58.26,000 --> 0:02:,000
And that, that maneuver

47
0:02:00.26,000 --> 0:02:02,000
called "crisis mapping,"

48
0:02:02.26,000 --> 0:02:05,000
was kicked off in Kenya

49
0:02:05.26,000 --> 0:02:07,000
in January of 2008.

50
0:02:07.26,000 --> 0:02:1,000
And enough people looked at it and found it valuable enough

51
0:02:10.26,000 --> 0:02:12,000
that the programmers who created Ushahidi

52
0:02:12.26,000 --> 0:02:14,000
decided they were going to make it open source

53
0:02:14.26,000 --> 0:02:16,000
and turn it into a platform.

54
0:02:16.26,000 --> 0:02:18,000
It's since been deployed in Mexico

55
0:02:18.26,000 --> 0:02:2,000
to track electoral fraud.

56
0:02:20.26,000 --> 0:02:23,000
It's been deployed in Washington D.C. to track snow cleanup.

57
0:02:23.26,000 --> 0:02:25,000
And it's been used most famously in Haiti

58
0:02:25.26,000 --> 0:02:28,000
in the aftermath of the earthquake.

59
0:02:28.26,000 --> 0:02:3,000
And when you look at the map

60
0:02:30.26,000 --> 0:02:32,000
now posted on the Ushahidi front page,

61
0:02:32.26,000 --> 0:02:34,000
you can see that the number of deployments in Ushahidi

62
0:02:34.26,000 --> 0:02:37,000
has gone worldwide, all right?

63
0:02:37.26,000 --> 0:02:39,000
This went from a single idea

64
0:02:39.26,000 --> 0:02:41,000
and a single implementation

65
0:02:41.26,000 --> 0:02:44,000
in East Africa in the beginning of 2008

66
0:02:44.26,000 --> 0:02:46,000
to a global deployment

67
0:02:46.26,000 --> 0:02:49,000
in less than three years.

68
0:02:49.26,000 --> 0:02:52,000
Now what Okolloh did

69
0:02:52.26,000 --> 0:02:54,000
would not have been possible

70
0:02:54.26,000 --> 0:02:57,000
without digital technology.

71
0:02:57.26,000 --> 0:03:,000
What Okolloh did would not have been possible

72
0:03:00.26,000 --> 0:03:02,000
without human generosity.

73
0:03:02.26,000 --> 0:03:04,000
And the interesting moment now,

74
0:03:04.26,000 --> 0:03:06,000
the number of environments

75
0:03:06.26,000 --> 0:03:08,000
where the social design challenge

76
0:03:08.26,000 --> 0:03:11,000
relies on both of those things being true.

77
0:03:11.26,000 --> 0:03:14,000
That is the resource that I'm talking about.

78
0:03:14.26,000 --> 0:03:16,000
I call it cognitive surplus.

79
0:03:16.26,000 --> 0:03:18,000
And it represents the ability

80
0:03:18.26,000 --> 0:03:2,000
of the world's population

81
0:03:20.26,000 --> 0:03:23,000
to volunteer and to contribute and collaborate

82
0:03:23.26,000 --> 0:03:26,000
on large, sometimes global, projects.

83
0:03:26.26,000 --> 0:03:28,000
Cognitive surplus is made up of two things.

84
0:03:28.26,000 --> 0:03:31,000
The first, obviously, is the world's free time and talents.

85
0:03:31.26,000 --> 0:03:33,000
The world has over

86
0:03:33.26,000 --> 0:03:36,000
a trillion hours a year

87
0:03:36.26,000 --> 0:03:38,000
of free time

88
0:03:38.26,000 --> 0:03:4,000
to commit to shared projects.

89
0:03:40.26,000 --> 0:03:42,000
Now, that free time existed in the 20th century,

90
0:03:42.26,000 --> 0:03:45,000
but we didn't get Ushahidi in the 20th century.

91
0:03:45.26,000 --> 0:03:47,000
That's the second half of cognitive surplus.

92
0:03:47.26,000 --> 0:03:49,000
The media landscape in the 20th century

93
0:03:49.26,000 --> 0:03:52,000
was very good at helping people consume,

94
0:03:52.26,000 --> 0:03:54,000
and we got, as a result,

95
0:03:54.26,000 --> 0:03:56,000
very good at consuming.

96
0:03:56.26,000 --> 0:03:58,000
But now that we've been given media tools --

97
0:03:58.26,000 --> 0:04:01,000
the Internet, mobile phones -- that let us do more than consume,

98
0:04:01.26,000 --> 0:04:04,000
what we're seeing is that people weren't couch potatoes

99
0:04:04.26,000 --> 0:04:06,000
because we liked to be.

100
0:04:06.26,000 --> 0:04:08,000
We were couch potatoes because that was

101
0:04:08.26,000 --> 0:04:1,000
the only opportunity given to us.

102
0:04:10.26,000 --> 0:04:12,000
We still like to consume, of course.

103
0:04:12.26,000 --> 0:04:14,000
But it turns out we also like to create,

104
0:04:14.26,000 --> 0:04:17,000
and we like to share.

105
0:04:17.26,000 --> 0:04:19,000
And it's those two things together --

106
0:04:19.26,000 --> 0:04:21,000
ancient human motivation

107
0:04:21.26,000 --> 0:04:23,000
and the modern tools to allow that motivation

108
0:04:23.26,000 --> 0:04:26,000
to be joined up in large-scale efforts --

109
0:04:26.26,000 --> 0:04:29,000
that are the new design resource.

110
0:04:29.26,000 --> 0:04:31,000
And using cognitive surplus,

111
0:04:31.26,000 --> 0:04:34,000
we're starting to see truly incredible experiments

112
0:04:34.26,000 --> 0:04:36,000
in scientific, literary,

113
0:04:36.26,000 --> 0:04:39,000
artistic, political efforts.

114
0:04:39.26,000 --> 0:04:41,000
Designing.

115
0:04:41.26,000 --> 0:04:44,000
We're also getting, of course, a lot of LOLcats.

116
0:04:44.26,000 --> 0:04:46,000
LOLcats are cute pictures of cats

117
0:04:46.26,000 --> 0:04:49,000
made cuter with the addition of cute captions.

118
0:04:49.26,000 --> 0:04:51,000
And they are also

119
0:04:51.26,000 --> 0:04:54,000
part of the abundant media landscape we're getting now.

120
0:04:54.26,000 --> 0:04:56,000
This is one of the participatory --

121
0:04:56.26,000 --> 0:04:58,000
one of the participatory models

122
0:04:58.26,000 --> 0:05:01,000
we see coming out of that, along with Ushahidi.

123
0:05:01.26,000 --> 0:05:03,000
Now I want to stipulate, as the lawyers say,

124
0:05:03.26,000 --> 0:05:05,000
that LOLcats are the stupidest possible

125
0:05:05.26,000 --> 0:05:07,000
creative act.

126
0:05:07.26,000 --> 0:05:09,000
There are other candidates of course,

127
0:05:09.26,000 --> 0:05:12,000
but LOLcats will do as a general case.

128
0:05:12.26,000 --> 0:05:14,000
But here's the thing:

129
0:05:14.26,000 --> 0:05:16,000
The stupidest possible creative act

130
0:05:16.26,000 --> 0:05:19,000
is still a creative act.

131
0:05:19.26,000 --> 0:05:22,000
Someone who has done something like this,

132
0:05:22.26,000 --> 0:05:25,000
however mediocre and throwaway,

133
0:05:25.26,000 --> 0:05:28,000
has tried something, has put something forward in public.

134
0:05:28.26,000 --> 0:05:31,000
And once they've done it, they can do it again,

135
0:05:31.26,000 --> 0:05:33,000
and they could work on getting it better.

136
0:05:33.26,000 --> 0:05:36,000
There is a spectrum between mediocre work and good work,

137
0:05:36.26,000 --> 0:05:39,000
and as anybody who's worked as an artist or a creator knows,

138
0:05:39.26,000 --> 0:05:41,000
it's a spectrum you're constantly

139
0:05:41.26,000 --> 0:05:43,000
struggling to get on top of.

140
0:05:43.26,000 --> 0:05:45,000
The gap is between

141
0:05:45.26,000 --> 0:05:48,000
doing anything and doing nothing.

142
0:05:48.26,000 --> 0:05:5,000
And someone who makes a LOLcat

143
0:05:50.26,000 --> 0:05:53,000
has already crossed over that gap.

144
0:05:53.26,000 --> 0:05:55,000
Now it's tempting to want to get the Ushahidis

145
0:05:55.26,000 --> 0:05:57,000
without the LOLcats, right,

146
0:05:57.26,000 --> 0:06:,000
to get the serious stuff without the throwaway stuff.

147
0:06:00.26,000 --> 0:06:03,000
But media abundance never works that way.

148
0:06:03.26,000 --> 0:06:06,000
Freedom to experiment means freedom to experiment with anything.

149
0:06:06.26,000 --> 0:06:08,000
Even with the sacred printing press,

150
0:06:08.26,000 --> 0:06:1,000
we got erotic novels 150 years

151
0:06:10.26,000 --> 0:06:13,000
before we got scientific journals.

152
0:06:14.26,000 --> 0:06:17,000
So before I talk about

153
0:06:17.26,000 --> 0:06:19,000
what is, I think, the critical difference

154
0:06:19.26,000 --> 0:06:21,000
between LOLcats and Ushahidi,

155
0:06:21.26,000 --> 0:06:23,000
I want to talk about

156
0:06:23.26,000 --> 0:06:25,000
their shared source.

157
0:06:25.26,000 --> 0:06:28,000
And that source is design for generosity.

158
0:06:28.26,000 --> 0:06:31,000
It is one of the curiosities of our historical era

159
0:06:31.26,000 --> 0:06:33,000
that even as cognitive surplus

160
0:06:33.26,000 --> 0:06:35,000
is becoming a resource we can design around,

161
0:06:35.26,000 --> 0:06:38,000
social sciences are also starting to explain

162
0:06:38.26,000 --> 0:06:4,000
how important

163
0:06:40.26,000 --> 0:06:42,000
our intrinsic motivations are to us,

164
0:06:42.26,000 --> 0:06:45,000
how much we do things because we like to do them

165
0:06:45.26,000 --> 0:06:47,000
rather than because our boss told us to do them,

166
0:06:47.26,000 --> 0:06:5,000
or because we're being paid to do them.

167
0:06:50.26,000 --> 0:06:53,000
This is a graph from a paper

168
0:06:53.26,000 --> 0:06:55,000
by Uri Gneezy and Aldo Rustichini,

169
0:06:55.26,000 --> 0:06:58,000
who set out to test, at the beginning of this decade,

170
0:06:58.26,000 --> 0:07:,000
what they called "deterrence theory."

171
0:07:00.26,000 --> 0:07:02,000
And deterrence theory is a very simple theory of human behavior:

172
0:07:02.26,000 --> 0:07:04,000
If you want somebody to do less of something,

173
0:07:04.26,000 --> 0:07:06,000
add a punishment and they'll do less of it.

174
0:07:06.26,000 --> 0:07:09,000
Simple, straightforward, commonsensical --

175
0:07:09.26,000 --> 0:07:11,000
also, largely untested.

176
0:07:11.26,000 --> 0:07:13,000
And so they went and studied

177
0:07:13.26,000 --> 0:07:15,000
10 daycare centers in Haifa, Israel.

178
0:07:15.26,000 --> 0:07:17,000
They studied those daycare centers

179
0:07:17.26,000 --> 0:07:19,000
at the time of highest tension,

180
0:07:19.26,000 --> 0:07:21,000
which is pick-up time.

181
0:07:21.26,000 --> 0:07:23,000
At pick-up time the teachers,

182
0:07:23.26,000 --> 0:07:25,000
who have been with your children all day,

183
0:07:25.26,000 --> 0:07:28,000
would like you to be there at the appointed hour to take your children back.

184
0:07:28.26,000 --> 0:07:31,000
Meanwhile, the parents -- perhaps a little busy at work, running late, running errands --

185
0:07:31.26,000 --> 0:07:34,000
want a little slack to pick the kids up late.

186
0:07:34.26,000 --> 0:07:36,000
So Gneezy and Rustichini said,

187
0:07:36.26,000 --> 0:07:38,000
"How many instances of late pick-ups

188
0:07:38.26,000 --> 0:07:4,000
are there at these 10 daycare centers?"

189
0:07:40.26,000 --> 0:07:42,000
Now they saw -- and this is what the graph is,

190
0:07:42.26,000 --> 0:07:45,000
these are the number of weeks and these are the number of late arrivals --

191
0:07:45.26,000 --> 0:07:47,000
that there were between six and 10

192
0:07:47.26,000 --> 0:07:49,000
instances of late pick-ups

193
0:07:49.26,000 --> 0:07:51,000
on average in these 10 daycare centers.

194
0:07:51.26,000 --> 0:07:54,000
So they divided the daycare centers into two groups.

195
0:07:54.26,000 --> 0:07:56,000
The white group there

196
0:07:56.26,000 --> 0:07:59,000
is the control group; they change nothing.

197
0:07:59.26,000 --> 0:08:02,000
But the group of daycare centers represented by the black line,

198
0:08:02.26,000 --> 0:08:04,000
they said, "We are changing this bargain

199
0:08:04.26,000 --> 0:08:06,000
as of right now.

200
0:08:06.26,000 --> 0:08:08,000
If you pick your kid up more than 10 minutes late,

201
0:08:08.26,000 --> 0:08:1,000
we're going to add a 10 shekel fine to your bill.

202
0:08:10.26,000 --> 0:08:13,000
Boom. No ifs, ands or buts."

203
0:08:13.26,000 --> 0:08:15,000
And the minute they did that,

204
0:08:15.26,000 --> 0:08:17,000
the behavior in those daycare centers changed.

205
0:08:17.26,000 --> 0:08:19,000
Late pick-ups went up

206
0:08:19.26,000 --> 0:08:22,000
every week for the next four weeks

207
0:08:22.26,000 --> 0:08:25,000
until they topped out at triple the pre-fine average,

208
0:08:25.26,000 --> 0:08:27,000
and then they fluctuated

209
0:08:27.26,000 --> 0:08:29,000
at between double and triple the pre-fine average

210
0:08:29.26,000 --> 0:08:31,000
for the life of the fine.

211
0:08:31.26,000 --> 0:08:34,000
And you can see immediately what happened, right?

212
0:08:35.26,000 --> 0:08:37,000
The fine broke the culture

213
0:08:37.26,000 --> 0:08:39,000
of the daycare center.

214
0:08:39.26,000 --> 0:08:41,000
By adding a fine,

215
0:08:41.26,000 --> 0:08:43,000
what they did was communicate to the parents

216
0:08:43.26,000 --> 0:08:45,000
that their entire debt to the teachers

217
0:08:45.26,000 --> 0:08:47,000
had been discharged

218
0:08:47.26,000 --> 0:08:49,000
with the payment of 10 shekels,

219
0:08:49.26,000 --> 0:08:52,000
and that there was no residue of guilt or social concern

220
0:08:52.26,000 --> 0:08:54,000
that the parents owed the teachers.

221
0:08:54.26,000 --> 0:08:56,000
And so the parents, quite sensibly, said,

222
0:08:56.26,000 --> 0:08:58,000
"10 shekels to pick my kid up late?

223
0:08:58.26,000 --> 0:09:,000
What could be bad?"

224
0:09:00.26,000 --> 0:09:02,000
(Laughter)

225
0:09:04.26,000 --> 0:09:06,000
The explanation of human behavior

226
0:09:06.26,000 --> 0:09:09,000
that we inherited in the 20th century

227
0:09:09.26,000 --> 0:09:12,000
was that we are all rational, self-maximizing actors,

228
0:09:12.26,000 --> 0:09:14,000
and in that explanation --

229
0:09:14.26,000 --> 0:09:17,000
the daycare center had no contract --

230
0:09:17.26,000 --> 0:09:2,000
should have been operating without any constraints.

231
0:09:20.26,000 --> 0:09:22,000
But that's not right.

232
0:09:22.26,000 --> 0:09:24,000
They were operating with social constraints

233
0:09:24.26,000 --> 0:09:26,000
rather than contractual ones.

234
0:09:26.26,000 --> 0:09:28,000
And critically, the social constraints

235
0:09:28.26,000 --> 0:09:31,000
created a culture that was more generous

236
0:09:31.26,000 --> 0:09:33,000
than the contractual constraints did.

237
0:09:33.26,000 --> 0:09:36,000
So Gneezy and Rustichini run this experiment for a dozen weeks --

238
0:09:36.26,000 --> 0:09:38,000
run the fine for a dozen weeks --

239
0:09:38.26,000 --> 0:09:41,000
and then they say, "Okay, that's it. All done; fine."

240
0:09:41.26,000 --> 0:09:43,000
And then a really interesting thing happens:

241
0:09:43.26,000 --> 0:09:46,000
Nothing changes.

242
0:09:46.26,000 --> 0:09:49,000
The culture that got broken by the fine

243
0:09:49.26,000 --> 0:09:52,000
stayed broken when the fine was removed.

244
0:09:52.26,000 --> 0:09:55,000
Not only are economic motivations

245
0:09:55.26,000 --> 0:09:57,000
and intrinsic motivations

246
0:09:57.26,000 --> 0:09:59,000
incompatible,

247
0:09:59.26,000 --> 0:10:01,000
that incompatibility

248
0:10:01.26,000 --> 0:10:04,000
can persist over long periods.

249
0:10:04.26,000 --> 0:10:06,000
So the trick

250
0:10:06.26,000 --> 0:10:08,000
in designing these kinds of situations

251
0:10:08.26,000 --> 0:10:11,000
is to understand where you're relying on

252
0:10:11.26,000 --> 0:10:14,000
the economic part of the bargain -- as with the parents paying the teachers --

253
0:10:14.26,000 --> 0:10:17,000
and when you're relying on the social part of the bargain,

254
0:10:17.26,000 --> 0:10:2,000
when you're really designing for generosity.

255
0:10:20.26,000 --> 0:10:23,000
This brings me back to the LOLcats

256
0:10:23.26,000 --> 0:10:25,000
and to Ushahidi.

257
0:10:25.26,000 --> 0:10:27,000
This is, I think, the range that matters.

258
0:10:27.26,000 --> 0:10:29,000
Both of these rely on cognitive surplus.

259
0:10:29.26,000 --> 0:10:31,000
Both of these design for the assumption

260
0:10:31.26,000 --> 0:10:34,000
that people like to create and we want to share.

261
0:10:34.26,000 --> 0:10:37,000
Here is the critical difference between these:

262
0:10:39.26,000 --> 0:10:42,000
LOLcats is communal value.

263
0:10:42.26,000 --> 0:10:44,000
It's value created by the participants

264
0:10:44.26,000 --> 0:10:46,000
for each other.

265
0:10:46.26,000 --> 0:10:49,000
Communal value on the networks we have

266
0:10:49.26,000 --> 0:10:51,000
is everywhere --

267
0:10:51.26,000 --> 0:10:53,000
every time you see a large aggregate

268
0:10:53.26,000 --> 0:10:56,000
of shared, publicly available data,

269
0:10:56.26,000 --> 0:10:58,000
whether it's photos on Flickr

270
0:10:58.26,000 --> 0:11:,000
or videos on Youtube or whatever.

271
0:11:00.26,000 --> 0:11:02,000
This is good. I like LOLcats as much as the next guy,

272
0:11:02.26,000 --> 0:11:04,000
maybe a little more even,

273
0:11:04.26,000 --> 0:11:07,000
but this is also

274
0:11:07.26,000 --> 0:11:09,000
a largely solved problem.

275
0:11:09.26,000 --> 0:11:11,000
I have a hard time envisioning a future

276
0:11:11.26,000 --> 0:11:13,000
in which someone is saying,

277
0:11:13.26,000 --> 0:11:15,000
"Where, oh where, can I find a picture

278
0:11:15.26,000 --> 0:11:17,000
of a cute cat?"

279
0:11:17.26,000 --> 0:11:19,000
Ushahidi, by contrast,

280
0:11:19.26,000 --> 0:11:21,000
is civic value.

281
0:11:21.26,000 --> 0:11:23,000
It's value created by the participants

282
0:11:23.26,000 --> 0:11:25,000
but enjoyed by society as a whole.

283
0:11:25.26,000 --> 0:11:27,000
The goals set out by Ushahidi

284
0:11:27.26,000 --> 0:11:29,000
are not just to make life better

285
0:11:29.26,000 --> 0:11:31,000
for the participants,

286
0:11:31.26,000 --> 0:11:34,000
but to make life better for everyone in the society

287
0:11:34.26,000 --> 0:11:36,000
in which Ushahidi is operating.

288
0:11:36.26,000 --> 0:11:39,000
And that kind of civic value

289
0:11:39.26,000 --> 0:11:41,000
is not just a side effect

290
0:11:41.26,000 --> 0:11:44,000
of opening up to human motivation.

291
0:11:44.26,000 --> 0:11:46,000
It really is going to be a side effect

292
0:11:46.26,000 --> 0:11:48,000
of what we, collectively,

293
0:11:48.26,000 --> 0:11:51,000
make of these kinds of efforts.

294
0:11:51.26,000 --> 0:11:53,000
There are a trillion

295
0:11:53.26,000 --> 0:11:55,000
hours a year

296
0:11:55.26,000 --> 0:11:57,000
of participatory value

297
0:11:57.26,000 --> 0:11:59,000
up for grabs.

298
0:11:59.26,000 --> 0:12:02,000
That will be true year-in and year-out.

299
0:12:02.26,000 --> 0:12:04,000
The number of people who are going to be able

300
0:12:04.26,000 --> 0:12:06,000
to participate in these kinds of projects

301
0:12:06.26,000 --> 0:12:08,000
is going to grow,

302
0:12:08.26,000 --> 0:12:11,000
and we can see that organizations

303
0:12:11.26,000 --> 0:12:13,000
designed around a culture of generosity

304
0:12:13.26,000 --> 0:12:15,000
can achieve incredible effects

305
0:12:15.26,000 --> 0:12:18,000
without an enormous amount of contractual overhead --

306
0:12:18.26,000 --> 0:12:2,000
a very different model

307
0:12:20.26,000 --> 0:12:23,000
than our default model for large-scale group action in the 20th century.

308
0:12:24.26,000 --> 0:12:27,000
What's going to make the difference here

309
0:12:27.26,000 --> 0:12:3,000
is what Dean Kamen said,

310
0:12:30.26,000 --> 0:12:32,000
the inventor and entrepreneur.

311
0:12:32.26,000 --> 0:12:35,000
Kamen said, "Free cultures get what they celebrate."

312
0:12:36.26,000 --> 0:12:39,000
We've got a choice before us.

313
0:12:39.26,000 --> 0:12:41,000
We've got this trillion hours a year.

314
0:12:41.26,000 --> 0:12:44,000
We can use it to crack each other up, and we're going to do that.

315
0:12:44.26,000 --> 0:12:46,000
That, we get for free.

316
0:12:46.26,000 --> 0:12:48,000
But we can also celebrate

317
0:12:48.26,000 --> 0:12:5,000
and support and reward the people

318
0:12:50.26,000 --> 0:12:52,000
trying to use cognitive surplus

319
0:12:52.26,000 --> 0:12:54,000
to create civic value.

320
0:12:54.26,000 --> 0:12:57,000
And to the degree we're going to do that, to the degree we're able to do that,

321
0:12:57.26,000 --> 0:12:59,000
we'll be able to change society.

322
0:12:59.26,000 --> 0:13:01,000
Thank you very much.

