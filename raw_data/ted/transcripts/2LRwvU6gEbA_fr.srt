1
0:00:,000 --> 0:00:07,000
Traducteur: Karine Gantin Relecteur: Hélène Vernet

2
0:00:12.76,000 --> 0:00:16,000
Après 13,8 milliards d’années d’histoire cosmique,

3
0:00:17.22,000 --> 0:00:2,000
notre univers s’est éveillé et a pris conscience de lui-même.

4
0:00:21.41,000 --> 0:00:22,000
Sur une petite planète bleue,

5
0:00:23.38,000 --> 0:00:25,000
des petites parts conscientes de notre univers

6
0:00:25.54,000 --> 0:00:28,000
ont commencé à observer le cosmos avec des télescopes,

7
0:00:28.876,000 --> 0:00:3,000
et ont découvert avec humilité

8
0:00:31.276,000 --> 0:00:35,000
que notre univers est beaucoup plus grand que ne l'imaginaient nos ancêtres,

9
0:00:35.466,000 --> 0:00:39,000
et que la vie semble être une perturbation mineure, presque imperceptible,

10
0:00:39.816,000 --> 0:00:41,000
dans un univers complètement mort.

11
0:00:42.32,000 --> 0:00:45,000
Mais nous avons aussi découvert, avec inspiration,

12
0:00:45.406,000 --> 0:00:46,000
que les technologies que nous développons

13
0:00:47.356,000 --> 0:00:5,000
ont la capacité d'aider la vie à s'épanouir comme jamais auparavant,

14
0:00:51.24,000 --> 0:00:54,000
non seulement pendant des siècles, mais des milliards d’années,

15
0:00:54.406,000 --> 0:00:58,000
et non seulement sur Terre, mais dans une bonne partie de ce formidable cosmos.

16
0:00:59.68,000 --> 0:01:02,000
Les premières formes de vie sont pour moi « La vie 1.0 »,

17
0:01:03.086,000 --> 0:01:04,000
parce qu'elles étaient vraiment ignorantes,

18
0:01:05.012,000 --> 0:01:08,000
comme les bactéries incapables d’apprendre quoi que ce soit dans leur vie.

19
0:01:08.84,000 --> 0:01:11,000
Nous, humains, représentons « La vie 2.0 » car nous pouvons apprendre,

20
0:01:12.146,000 --> 0:01:13,000
ce qui pour nous, en language de geek,

21
0:01:13.986,000 --> 0:01:16,000
est un peu comme télécharger de nouveaux logiciels dans notre cerveau,

22
0:01:17.292,000 --> 0:01:19,000
comme des langues ou des compétences professionnelles.

23
0:01:19.82,000 --> 0:01:23,000
« La vie 3.0. » qui peut concevoir son programme mais aussi son équipement

24
0:01:23.84,000 --> 0:01:24,000
n'existe pas encore, bien sûr.

25
0:01:25.576,000 --> 0:01:28,000
Mais peut-être que notre technologie a déjà fait de nous « La vie 2.1 »,

26
0:01:29.5,000 --> 0:01:33,000
considérant nos genoux artificiels, pacemakers et implants cochléaires.

27
0:01:33.846,000 --> 0:01:37,000
Donc, observons de plus près notre relation à la technologie, d'accord ?

28
0:01:38.8,000 --> 0:01:41,000
Par exemple, la mission lunaire Apollo 11

29
0:01:42.136,000 --> 0:01:45,000
a été à la fois un succès et une source d'inspiration.

30
0:01:45.336,000 --> 0:01:48,000
Elle a montré que quand nous utilisons la technologie intelligemment,

31
0:01:48.602,000 --> 0:01:51,000
nous pouvons accomplir des choses que nos ancêtres ne pouvaient que rêver.

32
0:01:52.246,000 --> 0:01:54,000
Mais il existe un voyage encore plus inspirant,

33
0:01:55.186,000 --> 0:01:58,000
propulsé par une chose encore plus puissante que les moteurs de fusée,

34
0:01:59.07,000 --> 0:02:01,000
et dont les voyageurs ne sont pas juste trois astronautes,

35
0:02:01.806,000 --> 0:02:02,000
mais l’humanité toute entière.

36
0:02:03.3,000 --> 0:02:05,000
Parlons de notre voyage collectif dans le futur

37
0:02:06.296,000 --> 0:02:08,000
avec l’intelligence artificielle.

38
0:02:08.96,000 --> 0:02:12,000
Mon ami Jaan Tallinn aime signaler que, comme pour les fusées,

39
0:02:13.536,000 --> 0:02:16,000
il ne suffit pas de rendre notre technologie efficace.

40
0:02:17.54,000 --> 0:02:2,000
Nous devons aussi, si nous voulons être vraiment ambitieux,

41
0:02:20.769,000 --> 0:02:23,000
trouver le moyen de la guider et définir ce que nous voulons en faire.

42
0:02:24.84,000 --> 0:02:27,000
Donc, parlons de l'intelligence artificielle sous ces trois aspects :

43
0:02:28.44,000 --> 0:02:3,000
sa puissance, son guidage, et sa destination.

44
0:02:31.48,000 --> 0:02:32,000
Commençons par sa puissance.

45
0:02:33.6,000 --> 0:02:36,000
Je définis « l’intelligence » de manière très inclusive

46
0:02:36.766,000 --> 0:02:4,000
comme étant notre capacité à atteindre des objectifs complexes,

47
0:02:41.01,000 --> 0:02:44,000
car je veux y inclure les deux intelligences, biologique et artificielle.

48
0:02:45.026,000 --> 0:02:48,000
Et je souhaite éviter l'idée absurde du chauvinisme carbonique

49
0:02:48.88,000 --> 0:02:51,000
qui dit, vous ne pouvez être intelligent que si vous êtes fait de viande.

50
0:02:52.88,000 --> 0:02:56,000
Récemment, le pouvoir de l’I.A. s’est développé de façon incroyable.

51
0:02:57.08,000 --> 0:02:58,000
Réfléchissez un instant.

52
0:02:58.39,000 --> 0:03:01,000
Il n'y a pas si longtemps, les robots ne pouvaient pas marcher.

53
0:03:02.94,000 --> 0:03:04,000
Aujourd’hui, ils font des saltos arrières.

54
0:03:06.12,000 --> 0:03:07,000
Il n'y a pas si longtemps,

55
0:03:07.896,000 --> 0:03:1,000
nous n’avions pas de voiture totalement autonome.

56
0:03:10.97,000 --> 0:03:13,000
Aujourd’hui, nous avons des fusées autonomes.

57
0:03:15.96,000 --> 0:03:16,000
Il n'y a pas si longtemps,

58
0:03:17.406,000 --> 0:03:19,000
l’I.A. était incapable de reconnaître nos visages.

59
0:03:20.056,000 --> 0:03:22,000
Aujourd’hui, elle peut générer de faux visages

60
0:03:23.02,000 --> 0:03:27,000
et simuler le vôtre en train de dire des trucs que vous n’avez jamais dits.

61
0:03:28.4,000 --> 0:03:31,000
Il n'y a pas si longtemps, l’I.A. ne pouvait pas nous battre au jeu de go.

62
0:03:32.33,000 --> 0:03:34,000
Puis l'I.A. de Google DeepMind, AlphaZero,

63
0:03:34.89,000 --> 0:03:39,000
a pris 3 000 ans de parties et la sagacité du jeu Go, pour ensuite tout ignorer,

64
0:03:39.906,000 --> 0:03:41,000
et elle est devenue la meilleure joueuse mondiale

65
0:03:42.206,000 --> 0:03:43,000
et en jouant seulement contre elle-même.

66
0:03:44.126,000 --> 0:03:47,000
Le plus impressionnant, ce n'est pas sa victoire contre des joueurs humains

67
0:03:47.67,000 --> 0:03:49,000
mais sa victoire contre les chercheurs en I.A.

68
0:03:50.09,000 --> 0:03:53,000
qui avaient passé des décennies à concevoir leur logiciel de jeu.

69
0:03:54.2,000 --> 0:03:58,000
AlphaZero a battu les chercheurs en I.A. au jeu de go, mais aussi au jeu d’échecs,

70
0:03:58.86,000 --> 0:04:,000
un jeu sur lequel nous avons travaillé depuis 1950.

71
0:04:02,000 --> 0:04:06,000
Donc, tous ces progrès récents incroyables de l’l.A. soulèvent vraiment la question :

72
0:04:07.28,000 --> 0:04:09,000
« Jusqu’où cela va-t-il aller ? »

73
0:04:09.8,000 --> 0:04:1,000
J’aime imaginer cette question

74
0:04:11.24,000 --> 0:04:14,000
sous la forme d'un paysage abstrait constitué de tâches,

75
0:04:14.516,000 --> 0:04:16,000
où l’altitude représente le degré de difficulté qu'a l'I.A.

76
0:04:17.33,000 --> 0:04:19,000
à exécuter chaque tâche comme un être humain,

77
0:04:19.696,000 --> 0:04:22,000
et le niveau de la mer, les capacités de l'I.A. aujourd’hui.

78
0:04:23.12,000 --> 0:04:25,000
Son niveau monte en fonction des progrès de l’I.A.,

79
0:04:25.586,000 --> 0:04:28,000
donc on peut voir une sorte de réchauffement climatique ici.

80
0:04:28.67,000 --> 0:04:29,000
(Rires)

81
0:04:3,000 --> 0:04:33,000
Evidemment, la première conclusion, c'est éviter les métiers en bord de mer,

82
0:04:33.615,000 --> 0:04:34,000
(Rires)

83
0:04:34.75,000 --> 0:04:36,000
parce qu'ils vont bientôt être automatisés.

84
0:04:37.53,000 --> 0:04:4,000
Mais il y a une autre question bien plus importante :

85
0:04:40.54,000 --> 0:04:42,000
« Jusqu’où l’eau va-t-elle monter ? »

86
0:04:43.44,000 --> 0:04:46,000
Est-ce qu'elle va tout inonder

87
0:04:47.76,000 --> 0:04:49,000
et être comparable à l'intelligence humaine partout ?

88
0:04:50.446,000 --> 0:04:53,000
Voici la définition de l'Intelligence Artificielle Générale,

89
0:04:54.12,000 --> 0:04:55,000
I.A.G.

90
0:04:55.426,000 --> 0:04:58,000
le Saint Graal de la recherche en I.A. depuis son lancement.

91
0:04:58.91,000 --> 0:04:59,000
Selon cette définition, ceux qui disent

92
0:05:00.886,000 --> 0:05:03,000
« Il y aura toujours des métiers où l’homme fait mieux que la machine »,

93
0:05:04.27,000 --> 0:05:07,000
disent simplement que nous n’aurons jamais d'I.A.G.

94
0:05:07.66,000 --> 0:05:1,000
Bien sûr, on pourrait choisir de garder certains emplois humains,

95
0:05:11.196,000 --> 0:05:14,000
ou offrir, par nos emplois, un revenu et une vocation,

96
0:05:14.646,000 --> 0:05:17,000
mais l’I.A.G. va de toute façon modifier la vie telle que nous la connaissons

97
0:05:18.3,000 --> 0:05:2,000
quand les humains ne seront plus les plus intelligents.

98
0:05:20.896,000 --> 0:05:23,000
Maintenant, si le niveau de la mer atteint effectivement l’I.A.G.,

99
0:05:24.586,000 --> 0:05:27,000
les progrès futurs de l’I.A. seront alors gérés principalement

100
0:05:27.75,000 --> 0:05:29,000
non plus par les hommes, mais par l’I.A.,

101
0:05:29.96,000 --> 0:05:33,000
ce qui pourrait impliquer de nouveaux progrès bien plus rapides

102
0:05:34.086,000 --> 0:05:37,000
que les années de recherches nécessaires à l'homme pour aboutir.

103
0:05:37.662,000 --> 0:05:4,000
Cela évoque la possibilité contestée d’un essor gigantesque de l’I.A.,

104
0:05:41.65,000 --> 0:05:43,000
où l'I.A. qui s'auto-améliore récursivement

105
0:05:43.936,000 --> 0:05:46,000
devance très vite l’intelligence humaine,

106
0:05:47.37,000 --> 0:05:49,000
en créant ce qu’on appelle une « superintelligence ».

107
0:05:51.8,000 --> 0:05:53,000
Bon, revenons à la réalité.

108
0:05:55.12,000 --> 0:05:57,000
Allons-nous obtenir l'I.A.G. dans un futur proche ?

109
0:05:58.36,000 --> 0:06:,000
Des chercheurs célèbres en I.A., comme Rodney Brooks,

110
0:06:00.936,000 --> 0:06:02,000
pensent que cela ne va pas arriver avant des centaines d’années.

111
0:06:03.822,000 --> 0:06:06,000
Mais d’autres, comme Demis Hassabis, le fondateur de Google DeepMind,

112
0:06:07.54,000 --> 0:06:1,000
sont plus optimistes et travaillent à ce qu'elle se produise beaucoup plus tôt.

113
0:06:11.386,000 --> 0:06:14,000
Des enquêtes récentes ont montré que la plupart des chercheurs en I.A.

114
0:06:14.702,000 --> 0:06:16,000
partagent l’optimisme de Demis, en fait,

115
0:06:17.66,000 --> 0:06:2,000
et s’attendent à voir l'I.A.G. émerger d’ici quelques dizaines d'années,

116
0:06:21.61,000 --> 0:06:23,000
donc du vivant de beaucoup d'entre nous ;

117
0:06:23.906,000 --> 0:06:25,000
ce qui nous amène à nous demander « Et ensuite ? »

118
0:06:27.04,000 --> 0:06:29,000
Quel rôle voulons-nous donner aux hommes

119
0:06:29.27,000 --> 0:06:32,000
quand les machines pourront tout faire mieux que nous et moins cher ?

120
0:06:35,000 --> 0:06:37,000
D'après moi, nous faisons face à un choix.

121
0:06:37.93,000 --> 0:06:39,000
Première option, nous pouvons dire avec complaisance,

122
0:06:40.46,000 --> 0:06:43,000
« Oh ! Fabriquons des machines qui font tout à notre place

123
0:06:43.47,000 --> 0:06:44,000
sans nous soucier des conséquences.

124
0:06:45.21,000 --> 0:06:48,000
Quoi ! Si nous créons une technologie qui rend l’humain obsolète,

125
0:06:48.45,000 --> 0:06:5,000
que pourrait-il arriver ? »,

126
0:06:50.83,000 --> 0:06:51,000
(Rires)

127
0:06:52.3,000 --> 0:06:55,000
mais, je pense que ce serait inadapté.

128
0:06:56.08,000 --> 0:06:59,000
Je pense que nous devrions être plus ambitieux, dans l’esprit de TED.

129
0:06:59.486,000 --> 0:07:02,000
Imaginons un avenir de haute technologie, qui nous inspire vraiment,

130
0:07:03.11,000 --> 0:07:05,000
et essayons d'y parvenir.

131
0:07:05.61,000 --> 0:07:08,000
Ceci nous conduit à la deuxième partie de notre métaphore de fusée : le guidage.

132
0:07:09.52,000 --> 0:07:11,000
Nous rendons l’I.A. plus performante

133
0:07:12.18,000 --> 0:07:14,000
mais comment la guider vers un avenir

134
0:07:15.14,000 --> 0:07:18,000
où elle contribue à la prospérité, non à la ruine des hommes ?

135
0:07:18.69,000 --> 0:07:21,000
Pour aider dans ce sens, j’ai cofondé l'Institut « Future of Life »,

136
0:07:21.89,000 --> 0:07:24,000
une petite ONG encourageant l'utilisation bénéfique de la technologie.

137
0:07:25.19,000 --> 0:07:27,000
Notre but, c'est l'existence d'un avenir pour la vie

138
0:07:27.68,000 --> 0:07:29,000
qui soit aussi enrichissant que possible.

139
0:07:29.806,000 --> 0:07:31,000
Vous savez, j'aime la technologie.

140
0:07:32.85,000 --> 0:07:35,000
C'est elle qui fait qu'aujourd'hui est mieux que l’Âge de pierre.

141
0:07:36.6,000 --> 0:07:37,000
Et je suis optimiste, nous pouvons créer

142
0:07:38.54,000 --> 0:07:41,000
un futur de haute-technologie vraiment exaltant,

143
0:07:41.68,000 --> 0:07:44,000
si, et seulement si, nous gagnons la course à la sagesse,

144
0:07:45.486,000 --> 0:07:47,000
la course entre le pouvoir grandissant de notre technologie

145
0:07:48.42,000 --> 0:07:5,000
et la sagesse grandissante avec laquelle nous la gérons.

146
0:07:51.13,000 --> 0:07:53,000
Mais pour cela, il va falloir un changement de stratégie,

147
0:07:53.826,000 --> 0:07:56,000
car notre stratégie a toujours été de tirer des leçons de nos erreurs.

148
0:07:57.29,000 --> 0:08:,000
On a inventé le feu ; on s'est planté pas mal de fois ;

149
0:08:00.34,000 --> 0:08:01,000
alors on a inventé l’extincteur.

150
0:08:02.16,000 --> 0:08:03,000
(Rires)

151
0:08:03.616,000 --> 0:08:05,000
On a inventé la voiture ; on s'est planté ;

152
0:08:05.912,000 --> 0:08:07,000
on a créé le feu rouge, la ceinture de sécurité et l’airbag.

153
0:08:08.717,000 --> 0:08:11,000
Mais avec des technologies plus puissantes comme l’arme nucléaire ou l'I.A.G.,

154
0:08:12.576,000 --> 0:08:15,000
tirer parti des erreurs est une stratégie plutôt mauvaise, qu'en pensez-vous ?

155
0:08:16.57,000 --> 0:08:17,000
(Rires)

156
0:08:17.7,000 --> 0:08:2,000
Mieux vaut être proactif plutôt que réactif,

157
0:08:20.83,000 --> 0:08:22,000
et planifier et réussir la première fois

158
0:08:23.21,000 --> 0:08:25,000
car il se peut qu'il n'y ait pas de seconde fois.

159
0:08:25.696,000 --> 0:08:27,000
C’est drôle, parfois les gens me disent

160
0:08:27.742,000 --> 0:08:3,000
« Max, chuut ! Ne parle pas comme ça !

161
0:08:30.84,000 --> 0:08:32,000
C'est de l'alarmisme luddiste. »

162
0:08:34.01,000 --> 0:08:35,000
Mais ce n’est pas de l’alarmisme.

163
0:08:35.726,000 --> 0:08:38,000
Au M.I.T., on appelle ça « l’ingénierie de la sécurité ».

164
0:08:39.2,000 --> 0:08:4,000
Réfléchissez.

165
0:08:40.396,000 --> 0:08:42,000
Avant de lancer sa mission Apollo 11,

166
0:08:42.572,000 --> 0:08:45,000
la NASA a réfléchi à tout ce qui pourrait mal tourner

167
0:08:45.626,000 --> 0:08:48,000
si on plaçait des gens sur des réservoirs d'essence explosifs

168
0:08:48.696,000 --> 0:08:5,000
et on les envoyait là où personne ne peut les aider.

169
0:08:51.155,000 --> 0:08:54,000
Beaucoup de choses pouvaient mal tourner ! Était-ce de l’alarmisme ?

170
0:08:55.129,000 --> 0:08:57,000
Non, et c'est précisément cette ingénierie de la sécurité

171
0:08:57.866,000 --> 0:08:58,000
qui a assuré le succès de la mission.

172
0:08:59.856,000 --> 0:09:03,000
Et c’est précisément, à mon avis, la stratégie à adopter avec l'I.A.G. :

173
0:09:04.49,000 --> 0:09:08,000
réfléchir à tout ce qui peut mal tourner pour s’assurer que tout se passe bien.

174
0:09:08.632,000 --> 0:09:1,000
Donc dans cet esprit, nous avons organisé des conférences,

175
0:09:11.35,000 --> 0:09:14,000
réuni d'éminents chercheurs et penseurs pour réfléchir à des façons de développer

176
0:09:15.18,000 --> 0:09:17,000
cette sagesse nécessaire au maintien d'une I.A. bénéfique.

177
0:09:17.906,000 --> 0:09:2,000
Notre dernière conférence, c'était l’an dernier, à Asilomar-Californie.

178
0:09:21.26,000 --> 0:09:23,000
Elle a débouché sur une liste de 23 principes

179
0:09:24.08,000 --> 0:09:28,000
qui ont reçu plus de mille signatures de chercheurs et industriels de l’I.A.

180
0:09:28.465,000 --> 0:09:3,000
Laissez-moi vous expliquer trois de ces principes.

181
0:09:31.63,000 --> 0:09:32,000
Le premier,

182
0:09:33.155,000 --> 0:09:37,000
c'est éviter la course à l’armement et aux armes létales autonomes.

183
0:09:37.44,000 --> 0:09:39,000
L'idée c'est que toute science peut être utilisée

184
0:09:39.756,000 --> 0:09:41,000
soit pour aider les gens, soit pour leur nuire.

185
0:09:42.59,000 --> 0:09:46,000
Par exemple, la biologie et la chimie ont plus de chances de servir

186
0:09:46.61,000 --> 0:09:48,000
à inventer de nouveaux médicaments et traitements

187
0:09:49.09,000 --> 0:09:51,000
que de nouvelles façons de tuer,

188
0:09:51.5,000 --> 0:09:54,000
car les biologistes et les chimistes exercent une pression efficace

189
0:09:54.78,000 --> 0:09:56,000
pour l'interdiction des armes biologiques et chimiques.

190
0:09:57.386,000 --> 0:09:59,000
De même, la plupart des chercheurs en I.A.

191
0:09:59.416,000 --> 0:10:02,000
veulent stigmatiser et proscrire les armes létales autonomes.

192
0:10:03.58,000 --> 0:10:04,000
Le second principe issu d'Asilomar,

193
0:10:05.56,000 --> 0:10:08,000
c'est atténuer les inégalités de revenus que l’I.A. entraîne.

194
0:10:09.17,000 --> 0:10:13,000
Je pense que si nous pouvons générer des revenus fabuleux avec l’I.A.,

195
0:10:13.68,000 --> 0:10:15,000
sans trouver de solution pour les répartir

196
0:10:15.95,000 --> 0:10:16,000
afin d'améliorer la vie de chacun,

197
0:10:17.68,000 --> 0:10:18,000
alors vraiment, c'est une honte pour nous !

198
0:10:19.676,000 --> 0:10:21,000
(Applaudissements)

199
0:10:23.12,000 --> 0:10:26,000
Bon, maintenant, levez la main ceux dont l’ordinateur est déjà tombé en panne.

200
0:10:27.09,000 --> 0:10:28,000
(Rires)

201
0:10:28.67,000 --> 0:10:29,000
Houah ! C'est beaucoup de mains levées !

202
0:10:30.6,000 --> 0:10:32,000
Alors, le troisième principe va vous plaire.

203
0:10:32.685,000 --> 0:10:35,000
C'est d'investir davantage dans la recherche sur la sécurité,

204
0:10:35.87,000 --> 0:10:38,000
car si nous mettons l’I.A. à la tête de plus de décisions et d'infrastructures,

205
0:10:39.62,000 --> 0:10:42,000
nous devons transformer les ordinateurs d'aujourd'hui, buggés et piratables,

206
0:10:43.243,000 --> 0:10:45,000
en systèmes d’I.A. robustes et vraiment fiables.

207
0:10:45.933,000 --> 0:10:48,000
Sinon, toute cette superbe technologie pourrait faillir et nous nuire,

208
0:10:49.42,000 --> 0:10:51,000
ou être piratée et retournée contre nous.

209
0:10:51.636,000 --> 0:10:53,000
Ce travail de sécurisation de l’I.A.

210
0:10:54.133,000 --> 0:10:57,000
passe nécessairement par l’alignement de ses valeurs,

211
0:10:57.33,000 --> 0:10:59,000
car la véritable menace de l'I.A.G. n’est pas la malveillance,

212
0:11:00.286,000 --> 0:11:03,000
décrite dans les films bébêtes hollywoodien, mais la compétence.

213
0:11:03.626,000 --> 0:11:06,000
C'est une I.A.G. qui réalise des objectifs ne concordant pas avec les nôtres.

214
0:11:07.25,000 --> 0:11:11,000
Ainsi, quand nous avons provoqué l’extinction du rhino noir ouest-africain,

215
0:11:11.91,000 --> 0:11:14,000
ce n'est pas parce que nous sommes un groupe malfaisant anti-rhino, non ?

216
0:11:15.708,000 --> 0:11:19,000
Mais parce nous sommes plus intelligents et avons des buts différents des leurs.

217
0:11:20.484,000 --> 0:11:22,000
Or l'I.A.G. est par définition plus intelligente que nous.

218
0:11:23.23,000 --> 0:11:26,000
Donc, si nous ne voulons pas connaître le même sort que ces rhinos,

219
0:11:26.65,000 --> 0:11:28,000
en créant l'I.A.G.,

220
0:11:28.68,000 --> 0:11:32,000
nous devons découvrir le moyen d'obliger les machines à comprendre,

221
0:11:32.89,000 --> 0:11:35,000
adopter et préserver nos objectifs.

222
0:11:37.32,000 --> 0:11:39,000
Et au fait, les objectifs de qui ?

223
0:11:40.206,000 --> 0:11:41,000
Et quels objectifs ?

224
0:11:42.146,000 --> 0:11:44,000
Cela nous conduit à la troisième partie de notre métaphore :

225
0:11:44.96,000 --> 0:11:45,000
la destination.

226
0:11:47.16,000 --> 0:11:48,000
Nous rendons l’I.A. plus puissante,

227
0:11:49.02,000 --> 0:11:51,000
en essayant de trouver un moyen de la guider,

228
0:11:51.126,000 --> 0:11:53,000
mais où voulons-nous qu'elle nous emmène ?

229
0:11:53.76,000 --> 0:11:56,000
C’est l'épine dans le pied dont presque personne ne parle,

230
0:11:57.46,000 --> 0:11:58,000
pas même ici à TED,

231
0:11:59.3,000 --> 0:12:03,000
parce que nous sommes obnubilés par les défis à court terme de l’I.A.

232
0:12:03.93,000 --> 0:12:07,000
Ecoutez, notre espèce essaie de construire l’I.A.G.,

233
0:12:08.73,000 --> 0:12:11,000
motivée par la curiosité et l'intérêt financier,

234
0:12:12.24,000 --> 0:12:16,000
mais quelle sorte de société espérons-nous avoir si nous y parvenons ?

235
0:12:16.6,000 --> 0:12:19,000
Nous avons fait un sondage récemment, et j'ai été surpris de voir

236
0:12:19.66,000 --> 0:12:22,000
que la plupart des gens veulent la création d'une super intelligence,

237
0:12:22.906,000 --> 0:12:25,000
d'une I.A. largement supérieure à nous sur tous les plans.

238
0:12:27.13,000 --> 0:12:3,000
Et presque tous étaient d'accord que nous devrions être ambitieux

239
0:12:30.466,000 --> 0:12:32,000
et essayer de répandre la vie dans l'univers.

240
0:12:32.61,000 --> 0:12:36,000
Mais il y avait moins de consensus quant à qui ou quoi devrait être en charge,

241
0:12:37.05,000 --> 0:12:39,000
et j'ai été plutôt amusé de constater

242
0:12:39.25,000 --> 0:12:42,000
que quelques-uns voulaient que ce soit seulement les machines.

243
0:12:42.383,000 --> 0:12:43,000
(Rires)

244
0:12:44.08,000 --> 0:12:47,000
Il y avait aussi un désaccord total quant au rôle des hommes,

245
0:12:47.91,000 --> 0:12:49,000
même au niveau le plus élémentaire.

246
0:12:50.086,000 --> 0:12:52,000
Donc voyons un peu les futurs possibles

247
0:12:52.71,000 --> 0:12:54,000
que nous pourrions choisir pour nous guider, d'accord ?

248
0:12:55.58,000 --> 0:12:58,000
Comprenez bien que je ne parle pas de voyage dans l’espace,

249
0:12:59.016,000 --> 0:13:02,000
mais simplement du voyage métaphorique de l’humanité vers le futur.

250
0:13:02.9,000 --> 0:13:05,000
Donc l'option appréciée de certains de mes collègues en I.A.,

251
0:13:06.406,000 --> 0:13:09,000
c'est de créer une super intelligence mais sous le contrôle des hommes,

252
0:13:10.13,000 --> 0:13:13,000
comme un dieu réduit en esclavage, déconnectée de l’Internet,

253
0:13:13.33,000 --> 0:13:16,000
et utilisée pour créer une technologie et une richesse inimaginables

254
0:13:16.68,000 --> 0:13:18,000
au profit de quiconque la contrôle.

255
0:13:18.8,000 --> 0:13:2,000
Mais Lord Acton nous a prévenus que le pouvoir corrompt,

256
0:13:21.555,000 --> 0:13:23,000
et qu'un pouvoir absolu corrompt absolument.

257
0:13:23.86,000 --> 0:13:27,000
Il est donc à craindre que les humains ne soient pas assez intelligents,

258
0:13:27.97,000 --> 0:13:3,000
ou plutôt doués de sagesse suffisante pour gérer un tel pouvoir.

259
0:13:31.64,000 --> 0:13:33,000
A côté des scrupules moraux que nous pourrions avoir

260
0:13:34.1,000 --> 0:13:36,000
concernant l'asservissement d'esprits supérieurs,

261
0:13:36.466,000 --> 0:13:4,000
il serait à craindre que cette superintelligence soit plus maline,

262
0:13:40.5,000 --> 0:13:42,000
se libère et prenne le pouvoir.

263
0:13:43.51,000 --> 0:13:46,000
J’ai aussi des collègues qui approuvent une prise de pouvoir de l'I.A.

264
0:13:46.97,000 --> 0:13:48,000
même si cela mènerait à l’extinction des humains,

265
0:13:49.426,000 --> 0:13:52,000
tant que nous estimons que les I.A. sont nos dignes descendants,

266
0:13:52.92,000 --> 0:13:53,000
à l'instar de nos enfants.

267
0:13:54.66,000 --> 0:13:56,000
Mais comment allons-nous savoir

268
0:13:57.29,000 --> 0:13:59,000
que les I.A. ont adopté nos meilleures valeurs,

269
0:14:00.27,000 --> 0:14:02,000
qu'elles ne sont pas des zombies sans conscience

270
0:14:02.548,000 --> 0:14:04,000
nous attirant dans le piège de l'anthropomorphisme ?

271
0:14:04.998,000 --> 0:14:06,000
Et ceux qui ne veulent pas l'extinction de l'homme,

272
0:14:07.646,000 --> 0:14:09,000
n’ont-ils pas aussi leur mot à dire ?

273
0:14:10.24,000 --> 0:14:13,000
Si vous n’avez aimé aucune de ces deux options haute-technologie,

274
0:14:13.41,000 --> 0:14:15,000
rappelez-vous qu'opter pour une technologie bas de gamme

275
0:14:16.106,000 --> 0:14:17,000
est un suicide du point de vue cosmique,

276
0:14:18.03,000 --> 0:14:2,000
car si nous ne dépassons pas la technologie d'aujourd'hui,

277
0:14:20.739,000 --> 0:14:22,000
la question n'est plus si l’humanité va être exterminée

278
0:14:23.323,000 --> 0:14:26,000
mais comment elle va l'être par le prochain astéroïde meurtrier,

279
0:14:26.806,000 --> 0:14:27,000
un super volcan,

280
0:14:28.053,000 --> 0:14:31,000
ou tout autre problème qu’une technologie meilleure aurait pu résoudre.

281
0:14:31.406,000 --> 0:14:34,000
Donc, pourquoi ne pas avoir le beurre et l’argent du beurre,

282
0:14:34.416,000 --> 0:14:37,000
avec une I.A.G. non asservie qui nous traite bien

283
0:14:38.03,000 --> 0:14:4,000
car ses valeurs épousent les nôtres ?

284
0:14:40.306,000 --> 0:14:44,000
C'est en substance ce qu’Eliezer Yudkowsky a appelé « l’I.A. Amicale »,

285
0:14:44.566,000 --> 0:14:47,000
et si nous pouvons fabriquer ça, ça pourrait être formidable.

286
0:14:47.84,000 --> 0:14:49,000
Elle pourrait non seulement éliminer les mauvaises expériences

287
0:14:50.76,000 --> 0:14:53,000
comme la maladie, la pauvreté, le crime et autres souffrances,

288
0:14:54.166,000 --> 0:14:56,000
mais elle pourrait nous donner la liberté de choisir

289
0:14:56.62,000 --> 0:15:,000
parmi une variété nouvelle et fantastique d’expériences positives,

290
0:15:01.086,000 --> 0:15:04,000
nous rendant, au fond, maîtres de notre propre destin.

291
0:15:06.29,000 --> 0:15:07,000
Donc en résumé,

292
0:15:07.65,000 --> 0:15:1,000
notre situation en matière de technologie est compliquée,

293
0:15:10.73,000 --> 0:15:12,000
mais la vue d’ensemble est assez simple.

294
0:15:13.11,000 --> 0:15:15,000
La majorité des chercheurs prévoit l'arrivée de l'I.A.G.

295
0:15:15.806,000 --> 0:15:19,000
d'ici quelques décennies, et si nous plongeons la-dedans sans préparation,

296
0:15:19.86,000 --> 0:15:22,000
ça pourrait être la pire des erreurs de toute l'Histoire de l’humanité.

297
0:15:23.32,000 --> 0:15:24,000
Regardons les choses en face.

298
0:15:24.706,000 --> 0:15:26,000
Elle rend possible une dictature brutale à l'échelle mondiale,

299
0:15:27.62,000 --> 0:15:3,000
des inégalités, une surveillance et une souffrance sans précédents,

300
0:15:30.8,000 --> 0:15:32,000
voire même l’extinction de l'espèce humaine.

301
0:15:32.88,000 --> 0:15:35,000
Par contre, si nous manoeuvrons prudemment,

302
0:15:36.04,000 --> 0:15:39,000
nous pourrions avoir un futur fantastique où tout le monde est mieux loti -

303
0:15:39.57,000 --> 0:15:41,000
les pauvres plus riches et les riches plus riches -

304
0:15:42.23,000 --> 0:15:46,000
et où tout le monde est bonne santé et libre de poursuivre ses rêves.

305
0:15:46.92,000 --> 0:15:47,000
Mais, attendez !

306
0:15:48.656,000 --> 0:15:52,000
Voulez-vous l'avenir politiquement de droite ou de gauche ?

307
0:15:53.046,000 --> 0:15:56,000
Voulez-vous la société pieuse avec des règles morales strictes,

308
0:15:56.046,000 --> 0:16:,000
ou la société libertaire hédoniste comme un festival continu de Burning Man ?

309
0:16:00.076,000 --> 0:16:02,000
Voulez-vous des belles plages, des forêts et des lacs ?

310
0:16:02.696,000 --> 0:16:04,000
Ou préférez-vous réagencer leurs atomes dans des ordinateurs

311
0:16:05.5,000 --> 0:16:06,000
et générer des expériences virtuelles ?

312
0:16:07.369,000 --> 0:16:1,000
Avec l'I.A. Amicale, nous pourrions créer toutes ces sociétés

313
0:16:10.89,000 --> 0:16:12,000
et offrir aux gens la liberté de choisir la leur,

314
0:16:13.56,000 --> 0:16:16,000
car nous ne serions plus limités par notre intelligence

315
0:16:17.21,000 --> 0:16:18,000
mais par les seules lois de la physique,

316
0:16:19.136,000 --> 0:16:23,000
et donc, la quantité de ressources et d’espace serait astronomique.

317
0:16:23.456,000 --> 0:16:24,000
Littéralement !

318
0:16:25.32,000 --> 0:16:27,000
Donc, nous avons le choix :

319
0:16:27.88,000 --> 0:16:3,000
nous pouvons être complaisant concernant notre futur

320
0:16:31.47,000 --> 0:16:33,000
en prenant aveuglément pour article de foi

321
0:16:34.12,000 --> 0:16:38,000
que toute nouvelle technologie est forcément bénéfique

322
0:16:38.19,000 --> 0:16:42,000
et en nous répétant ça, encore et encore, comme un mantra,

323
0:16:42.21,000 --> 0:16:44,000
tandis que nous glissons, comme un bateau à la dérive,

324
0:16:44.77,000 --> 0:16:46,000
vers notre propre obsolescence ;

325
0:16:46.92,000 --> 0:16:49,000
ou bien nous choisissons d'être ambitieux,

326
0:16:49.96,000 --> 0:16:51,000
en réfléchissant à la façon de guider notre technologie

327
0:16:52.526,000 --> 0:16:55,000
vers la destination désirée, pour créer l'âge de l'émerveillement.

328
0:16:56.89,000 --> 0:16:58,000
Nous sommes tous ici pour célébrer l’âge de l’émerveillement,

329
0:16:59.846,000 --> 0:17:,000
et j’ai l'impression qu'au fond,

330
0:17:01.78,000 --> 0:17:04,000
il consisterait à ne pas permettre à notre technologie de prendre le dessus

331
0:17:05.33,000 --> 0:17:07,000
mais plutôt, à nous enrichir.

332
0:17:07.88,000 --> 0:17:08,000
Merci.

333
0:17:09.28,000 --> 0:17:12,000
(Applaudissements)

