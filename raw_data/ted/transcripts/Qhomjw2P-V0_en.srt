1
0:00:12.58,000 --> 0:00:15,000
How many companies have you interacted with today?

2
0:00:17.06,000 --> 0:00:18,000
Well, you got up in the morning,

3
0:00:18.74,000 --> 0:00:19,000
took a shower,

4
0:00:19.98,000 --> 0:00:2,000
washed your hair,

5
0:00:21.26,000 --> 0:00:22,000
used a hair dryer,

6
0:00:22.82,000 --> 0:00:23,000
ate breakfast --

7
0:00:24.06,000 --> 0:00:25,000
ate cereals, fruit, yogurt, whatever --

8
0:00:25.942,000 --> 0:00:26,000
had coffee --

9
0:00:27.18,000 --> 0:00:28,000
tea.

10
0:00:28.58,000 --> 0:00:29,000
You took public transport to come here,

11
0:00:30.58,000 --> 0:00:31,000
or maybe used your private car.

12
0:00:33.34,000 --> 0:00:36,000
You interacted with the company that you work for or that you own.

13
0:00:37.98,000 --> 0:00:38,000
You interacted with your clients,

14
0:00:40.58,000 --> 0:00:41,000
your customers,

15
0:00:42.46,000 --> 0:00:43,000
and so on and so forth.

16
0:00:43.74,000 --> 0:00:46,000
I'm pretty sure there are at least seven companies

17
0:00:47.38,000 --> 0:00:48,000
you've interacted with today.

18
0:00:49.78,000 --> 0:00:51,000
Let me tell you a stunning statistic.

19
0:00:52.66,000 --> 0:00:56,000
One out of seven large, public corporations

20
0:00:57.06,000 --> 0:00:59,000
commit fraud every year.

21
0:01:00.22,000 --> 0:01:03,000
This is a US academic study that looks at US companies --

22
0:01:03.66,000 --> 0:01:06,000
I have no reason to believe that it's different in Europe.

23
0:01:07.3,000 --> 0:01:11,000
This is a study that looks at both detected and undetected fraud

24
0:01:11.54,000 --> 0:01:12,000
using statistical methods.

25
0:01:13.3,000 --> 0:01:14,000
This is not petty fraud.

26
0:01:15.94,000 --> 0:01:17,000
These frauds cost the shareholders of these companies,

27
0:01:18.82,000 --> 0:01:19,000
and therefore society,

28
0:01:20.1,000 --> 0:01:23,000
on the order of 380 billion dollars per year.

29
0:01:24.78,000 --> 0:01:26,000
We can all think of some examples, right?

30
0:01:27.02,000 --> 0:01:3,000
The car industry's secrets aren't quite so secret anymore.

31
0:01:31.62,000 --> 0:01:34,000
Fraud has become a feature,

32
0:01:34.94,000 --> 0:01:35,000
not a bug,

33
0:01:36.18,000 --> 0:01:37,000
of the financial services industry.

34
0:01:38.14,000 --> 0:01:4,000
That's not me who's claiming that,

35
0:01:40.38,000 --> 0:01:43,000
that's the president of the American Finance Association

36
0:01:43.66,000 --> 0:01:45,000
who stated that in his presidential address.

37
0:01:46.62,000 --> 0:01:48,000
That's a huge problem if you think about, especially,

38
0:01:49.38,000 --> 0:01:5,000
an economy like Switzerland,

39
0:01:51.1,000 --> 0:01:55,000
which relies so much on the trust put into its financial industry.

40
0:01:56.78,000 --> 0:01:57,000
On the other hand,

41
0:01:58.02,000 --> 0:02:01,000
there are six out of seven companies who actually remain honest

42
0:02:01.58,000 --> 0:02:04,000
despite all temptations to start engaging in fraud.

43
0:02:06.06,000 --> 0:02:08,000
There are whistle-blowers like Michael Woodford,

44
0:02:08.38,000 --> 0:02:1,000
who blew the whistle on Olympus.

45
0:02:10.74,000 --> 0:02:12,000
These whistle-blowers risk their careers,

46
0:02:13.46,000 --> 0:02:14,000
their friendships,

47
0:02:14.7,000 --> 0:02:16,000
to bring out the truth about their companies.

48
0:02:16.86,000 --> 0:02:18,000
There are journalists like Anna Politkovskaya

49
0:02:19.5,000 --> 0:02:22,000
who risk even their lives to report human rights violations.

50
0:02:23.38,000 --> 0:02:24,000
She got killed --

51
0:02:24.62,000 --> 0:02:25,000
every year,

52
0:02:25.86,000 --> 0:02:26,000
around 100 journalists get killed

53
0:02:27.54,000 --> 0:02:29,000
because of their conviction to bring out the truth.

54
0:02:31.86,000 --> 0:02:32,000
So in my talk today,

55
0:02:33.14,000 --> 0:02:36,000
I want to share with you some insights I've obtained and learned

56
0:02:36.66,000 --> 0:02:39,000
in the last 10 years of conducting research in this.

57
0:02:39.98,000 --> 0:02:42,000
I'm a researcher, a scientist working with economists,

58
0:02:43.5,000 --> 0:02:44,000
financial economists,

59
0:02:44.86,000 --> 0:02:46,000
ethicists, neuroscientists,

60
0:02:46.94,000 --> 0:02:47,000
lawyers and others

61
0:02:48.3,000 --> 0:02:5,000
trying to understand what makes humans tick,

62
0:02:50.42,000 --> 0:02:54,000
and how can we address this issue of fraud in corporations

63
0:02:55.22,000 --> 0:02:58,000
and therefore contribute to the improvement of the world.

64
0:02:59.1,000 --> 0:03:02,000
I want to start by sharing with you two very distinct visions

65
0:03:02.66,000 --> 0:03:03,000
of how people behave.

66
0:03:04.5,000 --> 0:03:05,000
First, meet Adam Smith,

67
0:03:07.02,000 --> 0:03:08,000
founding father of modern economics.

68
0:03:10.1,000 --> 0:03:14,000
His basic idea was that if everybody behaves in their own self-interests,

69
0:03:14.42,000 --> 0:03:16,000
that's good for everybody in the end.

70
0:03:17.9,000 --> 0:03:2,000
Self-interest isn't a narrowly defined concept

71
0:03:20.98,000 --> 0:03:21,000
just for your immediate utility.

72
0:03:22.94,000 --> 0:03:23,000
It has a long-run implication.

73
0:03:24.9,000 --> 0:03:25,000
Let's think about that.

74
0:03:26.9,000 --> 0:03:28,000
Think about this dog here.

75
0:03:28.94,000 --> 0:03:29,000
That might be us.

76
0:03:31.26,000 --> 0:03:32,000
There's this temptation --

77
0:03:32.54,000 --> 0:03:34,000
I apologize to all vegetarians, but --

78
0:03:34.94,000 --> 0:03:35,000
(Laughter)

79
0:03:35.98,000 --> 0:03:36,000
Dogs do like the bratwurst.

80
0:03:37.7,000 --> 0:03:39,000
(Laughter)

81
0:03:40.1,000 --> 0:03:43,000
Now, the straight-up, self-interested move here

82
0:03:43.22,000 --> 0:03:44,000
is to go for that.

83
0:03:44.82,000 --> 0:03:46,000
So my friend Adam here might jump up,

84
0:03:47.78,000 --> 0:03:5,000
get the sausage and thereby ruin all this beautiful tableware.

85
0:03:51.82,000 --> 0:03:52,000
But that's not what Adam Smith meant.

86
0:03:53.66,000 --> 0:03:55,000
He didn't mean disregard all consequences --

87
0:03:56.34,000 --> 0:03:57,000
to the contrary.

88
0:03:57.58,000 --> 0:03:58,000
He would have thought,

89
0:03:58.86,000 --> 0:04:,000
well, there may be negative consequences,

90
0:04:00.9,000 --> 0:04:01,000
for example,

91
0:04:02.14,000 --> 0:04:05,000
the owner might be angry with the dog

92
0:04:05.26,000 --> 0:04:08,000
and the dog, anticipating that, might not behave in this way.

93
0:04:09.66,000 --> 0:04:1,000
That might be us,

94
0:04:10.94,000 --> 0:04:13,000
weighing the benefits and costs of our actions.

95
0:04:14.02,000 --> 0:04:15,000
How does that play out?

96
0:04:15.78,000 --> 0:04:16,000
Well, many of you, I'm sure,

97
0:04:17.78,000 --> 0:04:18,000
have in your companies,

98
0:04:19.34,000 --> 0:04:2,000
especially if it's a large company,

99
0:04:21.18,000 --> 0:04:22,000
a code of conduct.

100
0:04:22.86,000 --> 0:04:25,000
And then if you behave according to that code of conduct,

101
0:04:26.3,000 --> 0:04:29,000
that improves your chances of getting a bonus payment.

102
0:04:29.5,000 --> 0:04:31,000
And on the other hand, if you disregard it,

103
0:04:31.659,000 --> 0:04:33,000
then there are higher chances of not getting your bonus

104
0:04:34.42,000 --> 0:04:35,000
or its being diminished.

105
0:04:35.98,000 --> 0:04:36,000
In other words,

106
0:04:37.26,000 --> 0:04:38,000
this is a very economic motivation

107
0:04:39.1,000 --> 0:04:41,000
of trying to get people to be more honest,

108
0:04:41.9,000 --> 0:04:44,000
or more aligned with the corporation's principles.

109
0:04:46.06,000 --> 0:04:51,000
Similarly, reputation is a very powerful economic force, right?

110
0:04:51.34,000 --> 0:04:52,000
We try to build a reputation,

111
0:04:52.9,000 --> 0:04:53,000
maybe for being honest,

112
0:04:54.34,000 --> 0:04:56,000
because then people trust us more in the future.

113
0:04:57.78,000 --> 0:04:58,000
Right?

114
0:04:59.02,000 --> 0:05:01,000
Adam Smith talked about the baker

115
0:05:01.14,000 --> 0:05:04,000
who's not producing good bread out of his benevolence

116
0:05:04.94,000 --> 0:05:07,000
for those people who consume the bread,

117
0:05:07.98,000 --> 0:05:1,000
but because he wants to sell more future bread.

118
0:05:11.98,000 --> 0:05:13,000
In my research, we find, for example,

119
0:05:14.22,000 --> 0:05:15,000
at the University of Zurich,

120
0:05:15.62,000 --> 0:05:19,000
that Swiss banks who get caught up in media,

121
0:05:20.54,000 --> 0:05:21,000
and in the context, for example,

122
0:05:22.34,000 --> 0:05:23,000
of tax evasion, of tax fraud,

123
0:05:23.9,000 --> 0:05:24,000
have bad media coverage.

124
0:05:25.66,000 --> 0:05:27,000
They lose net new money in the future

125
0:05:28.42,000 --> 0:05:29,000
and therefore make lower profits.

126
0:05:30.06,000 --> 0:05:32,000
That's a very powerful reputational force.

127
0:05:34.02,000 --> 0:05:35,000
Benefits and costs.

128
0:05:36.94,000 --> 0:05:38,000
Here's another viewpoint of the world.

129
0:05:39.54,000 --> 0:05:4,000
Meet Immanuel Kant,

130
0:05:41.1,000 --> 0:05:43,000
18th-century German philosopher superstar.

131
0:05:44.74,000 --> 0:05:45,000
He developed this notion

132
0:05:46.38,000 --> 0:05:49,000
that independent of the consequences,

133
0:05:49.54,000 --> 0:05:51,000
some actions are just right

134
0:05:52.54,000 --> 0:05:53,000
and some are just wrong.

135
0:05:54.26,000 --> 0:05:57,000
It's just wrong to lie, for example.

136
0:05:57.5,000 --> 0:06:,000
So, meet my friend Immanuel here.

137
0:06:00.66,000 --> 0:06:02,000
He knows that the sausage is very tasty,

138
0:06:03.5,000 --> 0:06:05,000
but he's going to turn away because he's a good dog.

139
0:06:05.98,000 --> 0:06:07,000
He knows it's wrong to jump up

140
0:06:08.7,000 --> 0:06:1,000
and risk ruining all this beautiful tableware.

141
0:06:12.34,000 --> 0:06:14,000
If you believe that people are motivated like that,

142
0:06:14.78,000 --> 0:06:16,000
then all the stuff about incentives,

143
0:06:16.98,000 --> 0:06:19,000
all the stuff about code of conduct and bonus systems and so on,

144
0:06:20.78,000 --> 0:06:22,000
doesn't make a whole lot of sense.

145
0:06:22.98,000 --> 0:06:26,000
People are motivated by different values perhaps.

146
0:06:27.18,000 --> 0:06:3,000
So, what are people actually motivated by?

147
0:06:30.58,000 --> 0:06:32,000
These two gentlemen here have perfect hairdos,

148
0:06:32.78,000 --> 0:06:36,000
but they give us very different views of the world.

149
0:06:37.66,000 --> 0:06:38,000
What do we do with this?

150
0:06:38.94,000 --> 0:06:39,000
Well, I'm an economist

151
0:06:40.62,000 --> 0:06:44,000
and we conduct so-called experiments to address this issue.

152
0:06:44.82,000 --> 0:06:47,000
We strip away facts which are confusing in reality.

153
0:06:48.14,000 --> 0:06:5,000
Reality is so rich, there is so much going on,

154
0:06:50.9,000 --> 0:06:53,000
it's almost impossible to know what drives people's behavior really.

155
0:06:55.34,000 --> 0:06:57,000
So let's do a little experiment together.

156
0:06:58.5,000 --> 0:07:,000
Imagine the following situation.

157
0:07:02.22,000 --> 0:07:04,000
You're in a room alone,

158
0:07:04.66,000 --> 0:07:05,000
not like here.

159
0:07:06.22,000 --> 0:07:09,000
There's a five-franc coin like the one I'm holding up right now

160
0:07:10.38,000 --> 0:07:11,000
in front of you.

161
0:07:11.98,000 --> 0:07:12,000
Here are your instructions:

162
0:07:13.58,000 --> 0:07:15,000
toss the coin four times,

163
0:07:17.62,000 --> 0:07:19,000
and then on a computer terminal in front of you,

164
0:07:20.06,000 --> 0:07:23,000
enter the number of times tails came up.

165
0:07:23.74,000 --> 0:07:24,000
This is the situation.

166
0:07:25.54,000 --> 0:07:26,000
Here's the rub.

167
0:07:26.78,000 --> 0:07:29,000
For every time that you announce that you had a tails throw,

168
0:07:30.18,000 --> 0:07:31,000
you get paid five francs.

169
0:07:31.7,000 --> 0:07:33,000
So if you say I had two tails throws,

170
0:07:34.26,000 --> 0:07:36,000
you get paid 10 francs.

171
0:07:36.5,000 --> 0:07:38,000
If you say you had zero, you get paid zero francs.

172
0:07:39.46,000 --> 0:07:41,000
If you say, "I had four tails throws,"

173
0:07:41.94,000 --> 0:07:43,000
then you get paid 20 francs.

174
0:07:43.98,000 --> 0:07:44,000
It's anonymous,

175
0:07:45.26,000 --> 0:07:46,000
nobody's watching what you're doing,

176
0:07:47.18,000 --> 0:07:49,000
and you get paid that money anonymously.

177
0:07:49.54,000 --> 0:07:5,000
I've got two questions for you.

178
0:07:51.58,000 --> 0:07:52,000
(Laughter)

179
0:07:53.22,000 --> 0:07:54,000
You know what's coming now, right?

180
0:07:55.82,000 --> 0:07:58,000
First, how would you behave in that situation?

181
0:08:00.06,000 --> 0:08:02,000
The second, look to your left and look to your right --

182
0:08:03.02,000 --> 0:08:04,000
(Laughter)

183
0:08:04.06,000 --> 0:08:06,000
and think about how the person sitting next to you

184
0:08:06.46,000 --> 0:08:07,000
might behave in that situation.

185
0:08:08.14,000 --> 0:08:1,000
We did this experiment for real.

186
0:08:10.3,000 --> 0:08:12,000
We did it at the Manifesta art exhibition

187
0:08:13.02,000 --> 0:08:15,000
that took place here in Zurich recently,

188
0:08:15.5,000 --> 0:08:17,000
not with students in the lab at the university

189
0:08:18.38,000 --> 0:08:19,000
but with the real population,

190
0:08:20.18,000 --> 0:08:21,000
like you guys.

191
0:08:21.9,000 --> 0:08:23,000
First, a quick reminder of stats.

192
0:08:24.06,000 --> 0:08:27,000
If I throw the coin four times and it's a fair coin,

193
0:08:27.66,000 --> 0:08:31,000
then the probability that it comes up four times tails

194
0:08:31.78,000 --> 0:08:33,000
is 6.25 percent.

195
0:08:34.9,000 --> 0:08:35,000
And I hope you can intuitively see

196
0:08:36.58,000 --> 0:08:39,000
that the probability that all four of them are tails is much lower

197
0:08:39.98,000 --> 0:08:41,000
than if two of them are tails, right?

198
0:08:42.58,000 --> 0:08:43,000
Here are the specific numbers.

199
0:08:45.859,000 --> 0:08:46,000
Here's what happened.

200
0:08:47.379,000 --> 0:08:49,000
People did this experiment for real.

201
0:08:50.619,000 --> 0:08:53,000
Around 30 to 35 percent of people said,

202
0:08:53.979,000 --> 0:08:55,000
"Well, I had four tails throws."

203
0:08:57.46,000 --> 0:08:58,000
That's extremely unlikely.

204
0:08:59.3,000 --> 0:09:,000
(Laughter)

205
0:09:01.26,000 --> 0:09:04,000
But the really amazing thing here,

206
0:09:04.42,000 --> 0:09:05,000
perhaps to an economist,

207
0:09:05.74,000 --> 0:09:11,000
is there are around 65 percent of people who did not say I had four tails throws,

208
0:09:12.3,000 --> 0:09:14,000
even though in that situation,

209
0:09:14.5,000 --> 0:09:16,000
nobody's watching you,

210
0:09:16.62,000 --> 0:09:17,000
the only consequence that's in place

211
0:09:18.58,000 --> 0:09:21,000
is you get more money if you say four than less.

212
0:09:21.94,000 --> 0:09:24,000
You leave 20 francs on the table by announcing zero.

213
0:09:25.86,000 --> 0:09:27,000
I don't know whether the other people all were honest

214
0:09:28.46,000 --> 0:09:31,000
or whether they also said a little bit higher or lower than what they did

215
0:09:31.94,000 --> 0:09:32,000
because it's anonymous.

216
0:09:33.18,000 --> 0:09:34,000
We only observed the distribution.

217
0:09:34.86,000 --> 0:09:36,000
But what I can tell you -- and here's another coin toss.

218
0:09:37.54,000 --> 0:09:38,000
There you go, it's tails.

219
0:09:39.06,000 --> 0:09:4,000
(Laughter)

220
0:09:40.58,000 --> 0:09:41,000
Don't check, OK?

221
0:09:42.06,000 --> 0:09:44,000
(Laughter)

222
0:09:44.9,000 --> 0:09:45,000
What I can tell you

223
0:09:46.22,000 --> 0:09:5,000
is that not everybody behaved like Adam Smith would have predicted.

224
0:09:52.66,000 --> 0:09:53,000
So what does that leave us with?

225
0:09:54.26,000 --> 0:09:58,000
Well, it seems people are motivated by certain intrinsic values

226
0:09:58.78,000 --> 0:09:59,000
and in our research, we look at this.

227
0:10:01.26,000 --> 0:10:05,000
We look at the idea that people have so-called protected values.

228
0:10:06.58,000 --> 0:10:08,000
A protected value isn't just any value.

229
0:10:09.42,000 --> 0:10:14,000
A protected value is a value where you're willing to pay a price

230
0:10:15.26,000 --> 0:10:16,000
to uphold that value.

231
0:10:16.54,000 --> 0:10:2,000
You're willing to pay a price to withstand the temptation to give in.

232
0:10:22.02,000 --> 0:10:24,000
And the consequence is you feel better

233
0:10:24.7,000 --> 0:10:28,000
if you earn money in a way that's consistent with your values.

234
0:10:29.02,000 --> 0:10:33,000
Let me show you this again in the metaphor of our beloved dog here.

235
0:10:34.42,000 --> 0:10:38,000
If we succeed in getting the sausage without violating our values,

236
0:10:38.5,000 --> 0:10:39,000
then the sausage tastes better.

237
0:10:40.5,000 --> 0:10:41,000
That's what our research shows.

238
0:10:42.54,000 --> 0:10:43,000
If, on the other hand,

239
0:10:43.82,000 --> 0:10:44,000
we do so --

240
0:10:45.1,000 --> 0:10:46,000
if we get the sausage

241
0:10:46.54,000 --> 0:10:49,000
and in doing so we actually violate values,

242
0:10:50.02,000 --> 0:10:52,000
we value the sausage less.

243
0:10:53.02,000 --> 0:10:55,000
Quantitatively, that's quite powerful.

244
0:10:55.5,000 --> 0:10:57,000
We can measure these protected values,

245
0:10:57.98,000 --> 0:10:58,000
for example,

246
0:10:59.22,000 --> 0:11:,000
by a survey measure.

247
0:11:02.18,000 --> 0:11:07,000
Simple, nine-item survey that's quite predictive in these experiments.

248
0:11:08.18,000 --> 0:11:1,000
If you think about the average of the population

249
0:11:10.54,000 --> 0:11:12,000
and then there's a distribution around it --

250
0:11:12.66,000 --> 0:11:14,000
people are different, we all are different.

251
0:11:15.3,000 --> 0:11:17,000
People who have a set of protected values

252
0:11:18.3,000 --> 0:11:22,000
that's one standard deviation above the average,

253
0:11:22.5,000 --> 0:11:27,000
they discount money they receive by lying by about 25 percent.

254
0:11:27.58,000 --> 0:11:3,000
That means a dollar received when lying

255
0:11:31.22,000 --> 0:11:33,000
is worth to them only 75 cents

256
0:11:33.38,000 --> 0:11:36,000
without any incentives you put in place for them to behave honestly.

257
0:11:37.1,000 --> 0:11:38,000
It's their intrinsic motivation.

258
0:11:38.86,000 --> 0:11:39,000
By the way, I'm not a moral authority.

259
0:11:40.74,000 --> 0:11:42,000
I'm not saying I have all these beautiful values, right?

260
0:11:44.26,000 --> 0:11:45,000
But I'm interested in how people behave

261
0:11:46.22,000 --> 0:11:49,000
and how we can leverage that richness in human nature

262
0:11:49.62,000 --> 0:11:52,000
to actually improve the workings of our organizations.

263
0:11:54.22,000 --> 0:11:57,000
So there are two very, very different visions here.

264
0:11:57.42,000 --> 0:11:58,000
On the one hand,

265
0:11:58.78,000 --> 0:12:01,000
you can appeal to benefits and costs

266
0:12:01.82,000 --> 0:12:03,000
and try to get people to behave according to them.

267
0:12:04.5,000 --> 0:12:05,000
On the other hand,

268
0:12:06.14,000 --> 0:12:1,000
you can select people who have the values

269
0:12:10.18,000 --> 0:12:12,000
and the desirable characteristics, of course --

270
0:12:12.42,000 --> 0:12:15,000
competencies that go in line with your organization.

271
0:12:16.02,000 --> 0:12:2,000
I do not yet know where these protected values really come from.

272
0:12:20.26,000 --> 0:12:23,000
Is it nurture or is it nature?

273
0:12:23.66,000 --> 0:12:24,000
What I can tell you

274
0:12:25.06,000 --> 0:12:3,000
is that the distribution looks pretty similar for men and women.

275
0:12:30.18,000 --> 0:12:33,000
It looks pretty similar for those who had studied economics

276
0:12:33.98,000 --> 0:12:35,000
or those who had studied psychology.

277
0:12:37.82,000 --> 0:12:4,000
It looks even pretty similar around different age categories

278
0:12:41.22,000 --> 0:12:42,000
among adults.

279
0:12:42.46,000 --> 0:12:44,000
But I don't know yet how this develops over a lifetime.

280
0:12:45.14,000 --> 0:12:48,000
That will be the subject of future research.

281
0:12:49.46,000 --> 0:12:5,000
The idea I want to leave you with

282
0:12:51.14,000 --> 0:12:53,000
is it's all right to appeal to incentives.

283
0:12:53.94,000 --> 0:12:54,000
I'm an economist;

284
0:12:55.18,000 --> 0:12:57,000
I certainly believe in the fact that incentives work.

285
0:12:59.22,000 --> 0:13:03,000
But do think about selecting the right people

286
0:13:03.26,000 --> 0:13:06,000
rather than having people and then putting incentives in place.

287
0:13:06.78,000 --> 0:13:08,000
Selecting the right people with the right values

288
0:13:09.06,000 --> 0:13:12,000
may go a long way to saving a lot of trouble

289
0:13:13.02,000 --> 0:13:14,000
and a lot of money

290
0:13:14.42,000 --> 0:13:15,000
in your organizations.

291
0:13:16.18,000 --> 0:13:17,000
In other words,

292
0:13:17.46,000 --> 0:13:2,000
it will pay off to put people first.

293
0:13:21.86,000 --> 0:13:22,000
Thank you.

294
0:13:23.1,000 --> 0:13:26,000
(Applause)

