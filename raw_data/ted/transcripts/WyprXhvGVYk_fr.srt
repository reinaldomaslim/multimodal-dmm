1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Jérémie Pinguet

2
0:00:12.857,000 --> 0:00:13,000
Voici un graphique

3
0:00:14.36,000 --> 0:00:17,000
qui représente l'histoire économique de la civilisation humaine.

4
0:00:18.043,000 --> 0:00:21,000
[PIB mondial par habitant durant les 200 000 dernières années]

5
0:00:23.757,000 --> 0:00:25,000
On n'y voit pas grand chose, n'est-ce pas ?

6
0:00:26.751,000 --> 0:00:28,000
Durant la grande majorité de l'histoire humaine,

7
0:00:29.835,000 --> 0:00:33,000
presque tout le monde a vécu avec l'équivalent d'un dollar par jour

8
0:00:33.907,000 --> 0:00:34,000
et peu de choses changeaient.

9
0:00:36.757,000 --> 0:00:38,000
Mais il s'est ensuite produit quelque chose d'extraordinaire :

10
0:00:40.677,000 --> 0:00:42,000
les révolutions scientifique et industrielle.

11
0:00:43.512,000 --> 0:00:45,000
Et le graphique franchement plat que vous venez de voir

12
0:00:46.321,000 --> 0:00:48,000
s'est transformé en ceci.

13
0:00:50.612,000 --> 0:00:54,000
Ce graphique signifie qu'en termes de capacité à changer le monde,

14
0:00:55.271,000 --> 0:00:58,000
nous vivons une ère sans précédent dans l'histoire de l'humanité

15
0:00:58.733,000 --> 0:01:01,000
et, selon moi, notre compréhension éthique n'a pas encore rattrapé son retard.

16
0:01:03.716,000 --> 0:01:05,000
Les révolutions scientifique et industrielle

17
0:01:05.77,000 --> 0:01:07,000
ont transformé notre compréhension du monde

18
0:01:08.657,000 --> 0:01:09,000
et notre capacité à le changer.

19
0:01:11.505,000 --> 0:01:14,000
Nous avons besoin d'une révolution éthique

20
0:01:15.196,000 --> 0:01:16,000
afin de pouvoir déterminer

21
0:01:16.768,000 --> 0:01:19,000
comment utiliser cette extraordinaire abondance de ressources

22
0:01:19.944,000 --> 0:01:2,000
pour améliorer le monde.

23
0:01:22.249,000 --> 0:01:23,000
Pendant ces 10 dernières années,

24
0:01:23.864,000 --> 0:01:24,000
mes collègues et moi avons développé

25
0:01:25.721,000 --> 0:01:27,000
un programme de philosophie et de recherche

26
0:01:27.721,000 --> 0:01:28,000
que nous appelons altruisme efficace.

27
0:01:30.366,000 --> 0:01:33,000
Il essaie de répondre à ces changements radicaux de notre monde

28
0:01:33.985,000 --> 0:01:35,000
en utilisant des indices et un raisonnement minutieux

29
0:01:36.637,000 --> 0:01:38,000
pour essayer de répondre à cette question :

30
0:01:40.173,000 --> 0:01:42,000
comment pouvons-nous faire le plus de bien ?

31
0:01:44.265,000 --> 0:01:47,000
Il y a beaucoup de questions à aborder

32
0:01:47.51,000 --> 0:01:49,000
si vous voulez vous attaquer à ce problème :

33
0:01:49.797,000 --> 0:01:51,000
faire du bien de façon caritative,

34
0:01:51.852,000 --> 0:01:53,000
par votre carrière ou votre engagement politique,

35
0:01:54.154,000 --> 0:01:56,000
sur quels programmes se concentrer, avec qui travailler.

36
0:01:57.624,000 --> 0:01:58,000
Mais ce dont je veux parler

37
0:01:59.124,000 --> 0:02:01,000
est ce qui est, à mon avis, le problème le plus fondamental.

38
0:02:02.02,000 --> 0:02:04,000
De tous les problèmes auxquels le monde fait face,

39
0:02:05.962,000 --> 0:02:07,000
sur la résolution duquel devrions-nous nous concentrer ?

40
0:02:10.668,000 --> 0:02:13,000
Je vais vous donner un cadre pour réfléchir à cette question

41
0:02:14.16,000 --> 0:02:15,000
et ce cadre est très simple.

42
0:02:16.952,000 --> 0:02:18,000
La priorité d'un problème se définit en fonction

43
0:02:19.416,000 --> 0:02:21,000
de son importance, de sa facilité de résolution,

44
0:02:22.316,000 --> 0:02:23,000
et de la manière dont il est négligé.

45
0:02:24.706,000 --> 0:02:25,000
Plus il est important, mieux c'est,

46
0:02:26.386,000 --> 0:02:28,000
car nous avons plus à gagner à résoudre le problème.

47
0:02:30.221,000 --> 0:02:32,000
Plus sa résolution est simple, mieux c'est,

48
0:02:32.27,000 --> 0:02:35,000
car je peux résoudre le problème avec moins de temps et d'argent.

49
0:02:35.737,000 --> 0:02:37,000
Et de façon plus subtile,

50
0:02:38.681,000 --> 0:02:41,000
moins on s'y intéresse, mieux c'est, du fait des rendements décroissants.

51
0:02:42.285,000 --> 0:02:45,000
Plus il y a eu de ressources investies dans la résolution d'un problème,

52
0:02:46.023,000 --> 0:02:48,000
plus il sera difficile de progresser.

53
0:02:50.56,000 --> 0:02:54,000
Ce que je veux que vous reteniez c'est ce cadre,

54
0:02:54.643,000 --> 0:02:56,000
afin que vous réfléchissiez par vous-même

55
0:02:56.667,000 --> 0:02:58,000
aux priorités mondiales.

56
0:02:59.954,000 --> 0:03:02,000
Mais des gens de la communauté de l'altruisme efficace et moi-même

57
0:03:03.106,000 --> 0:03:05,000
sommes tombés d'accord sur trois questions morales

58
0:03:05.473,000 --> 0:03:08,000
que nous pensons particulièrement importantes,

59
0:03:08.573,000 --> 0:03:1,000
qui obtiennent un score très élevé dans ce cadre.

60
0:03:11.151,000 --> 0:03:13,000
Tout d'abord, la santé mondiale.

61
0:03:13.988,000 --> 0:03:15,000
Cela peut tout à fait être résolu.

62
0:03:16.423,000 --> 0:03:19,000
Nous avons un excellent palmarès en ce qui concerne la santé mondiale.

63
0:03:19.844,000 --> 0:03:24,000
Les taux de mortalité liés à la rougeole, à la malaria, aux maladies diarrhéiques

64
0:03:25.288,000 --> 0:03:27,000
ont chuté de plus de 70%.

65
0:03:29.534,000 --> 0:03:31,000
En 1980, nous avons éradiqué la variole.

66
0:03:33.815,000 --> 0:03:36,000
J'estime que nous avons ainsi sauvé plus de 60 millions de vies.

67
0:03:37.506,000 --> 0:03:4,000
Ce sont plus de vies sauvées qu'avec la paix dans le monde

68
0:03:40.594,000 --> 0:03:41,000
obtenue durant la même période.

69
0:03:43.893,000 --> 0:03:45,000
D'après nos meilleures estimations,

70
0:03:46.242,000 --> 0:03:5,000
nous pouvons sauver une vie en distribuant des moustiquaires traitées durablement

71
0:03:50.394,000 --> 0:03:52,000
pour seulement quelques milliers de dollars.

72
0:03:52.911,000 --> 0:03:53,000
C'est une super opportunité.

73
0:03:55.594,000 --> 0:03:57,000
La deuxième priorité est l'élevage industriel.

74
0:03:58.681,000 --> 0:03:59,000
Personne ne s'y intéresse.

75
0:04:00.768,000 --> 0:04:04,000
Cinquante milliards d'animaux terrestres sont utilisés par an pour l'alimentation

76
0:04:05.475,000 --> 0:04:07,000
et la majorité d'entre eux sont élevés industriellement,

77
0:04:08.197,000 --> 0:04:1,000
en vivant dans d'atroces souffrances.

78
0:04:10.601,000 --> 0:04:13,000
Ils font probablement partie des créatures les plus mal loties

79
0:04:13.776,000 --> 0:04:15,000
et souvent, nous pourrions grandement améliorer leur vie

80
0:04:16.658,000 --> 0:04:17,000
pour quelques centimes par animal.

81
0:04:19.123,000 --> 0:04:21,000
Mais très peu de gens s'y intéressent.

82
0:04:21.229,000 --> 0:04:24,000
Il y a 3 000 fois plus d'animaux en élevage industriel

83
0:04:25.063,000 --> 0:04:26,000
qu'il n'y a d'animaux errants

84
0:04:28.6,000 --> 0:04:31,000
mais l'élevage industriel n'obtient qu'un cinquantième

85
0:04:31.712,000 --> 0:04:32,000
du financement philanthrope.

86
0:04:34.211,000 --> 0:04:36,000
Des ressources supplémentaires dans ce domaine

87
0:04:36.379,000 --> 0:04:38,000
pourraient avoir un effet transformateur.

88
0:04:39.458,000 --> 0:04:42,000
Le troisième domaine est celui sur lequel je veux me concentrer,

89
0:04:42.467,000 --> 0:04:44,000
et c'est la catégorie des risques existentiels :

90
0:04:45.475,000 --> 0:04:47,000
des événements tels qu'une guerre nucléaire

91
0:04:48.106,000 --> 0:04:49,000
ou une pandémie mondiale,

92
0:04:50.824,000 --> 0:04:52,000
qui pourraient faire altérer définitivement la civilisation

93
0:04:54.156,000 --> 0:04:56,000
ou mener à l'extinction de la race humaine.

94
0:04:57.882,000 --> 0:04:59,000
Laissez-moi vous expliquer pourquoi c'est une telle priorité

95
0:05:00.692,000 --> 0:05:01,000
dans ce cadre.

96
0:05:02.992,000 --> 0:05:03,000
Un : son importance.

97
0:05:05.341,000 --> 0:05:08,000
Quel serait vraiment l'impact s'il y avait une catastrophe existentielle ?

98
0:05:10.92,000 --> 0:05:16,000
Cela impliquerait la mort des sept milliards de personnes sur Terre,

99
0:05:17.286,000 --> 0:05:2,000
cela signifie vous et tous ceux que vous connaissez et aimez.

100
0:05:21.214,000 --> 0:05:23,000
C'est une tragédie dont l'importance est inimaginable.

101
0:05:25.684,000 --> 0:05:26,000
En plus de ça,

102
0:05:27.684,000 --> 0:05:3,000
cela signifierait aussi une réduction du futur potentiel humain

103
0:05:31.313,000 --> 0:05:33,000
et je crois que le potentiel de l'humanité est vaste.

104
0:05:35.551,000 --> 0:05:38,000
La race humaine existe depuis environ 200 000 ans

105
0:05:39.026,000 --> 0:05:41,000
et si elle vivait aussi longtemps qu'un mammifère en général,

106
0:05:41.933,000 --> 0:05:43,000
elle subsisterait environ deux millions d'années.

107
0:05:46.884,000 --> 0:05:48,000
Si la race humaine était un seul individu,

108
0:05:49.599,000 --> 0:05:51,000
elle aurait environ 10 ans aujourd'hui.

109
0:05:53.526,000 --> 0:05:57,000
Et, en réalité, l'Humain n'est pas un mammifère comme les autres.

110
0:05:58.95,000 --> 0:06:,000
Si nous faisons attention, il n'y a pas de raison

111
0:06:01.246,000 --> 0:06:03,000
que nous nous éteignions après deux millions d'années.

112
0:06:03.839,000 --> 0:06:07,000
La Terre restera encore habitable pendant 500 millions d'années.

113
0:06:08.696,000 --> 0:06:09,000
Et si nous allions dans les étoiles,

114
0:06:11.64,000 --> 0:06:13,000
la civilisation pourrait continuer des milliards d'années.

115
0:06:16.193,000 --> 0:06:18,000
Je pense que l'avenir sera très long,

116
0:06:19.669,000 --> 0:06:2,000
mais sera-t-il un bel avenir ?

117
0:06:21.495,000 --> 0:06:23,000
La race humaine mérite-t-elle d'être sauvée ?

118
0:06:26.54,000 --> 0:06:29,000
Nous entendons dire constamment combien les choses empirent,

119
0:06:31.459,000 --> 0:06:33,000
mais quand nous considérons le long terme,

120
0:06:34.176,000 --> 0:06:36,000
les choses se sont radicalement améliorées.

121
0:06:37.453,000 --> 0:06:39,000
Voici, par exemple, l'espérance de vie au cours du temps.

122
0:06:40.892,000 --> 0:06:43,000
Voici la proportion de la population ne vivant pas dans l'extrême pauvreté.

123
0:06:45.106,000 --> 0:06:49,000
Voici le nombre de pays qui ont décriminalisé l'homosexualité.

124
0:06:50.848,000 --> 0:06:53,000
Voici le nombre de pays qui sont devenus démocratiques.

125
0:06:55.015,000 --> 0:06:59,000
En nous tournant vers l'avenir, il pourrait y avoir tant à gagner.

126
0:06:59.218,000 --> 0:07:,000
Nous serons tellement plus riches,

127
0:07:00.91,000 --> 0:07:03,000
nous pourrons résoudre tant de problèmes qui sont insolubles aujourd'hui.

128
0:07:05.389,000 --> 0:07:09,000
Voici un graphique représentant la manière dont l'humanité a progressé

129
0:07:09.858,000 --> 0:07:11,000
en termes d'épanouissement humain au fil du temps.

130
0:07:12.772,000 --> 0:07:15,000
Voici ce à quoi nous espérons que les progrès futurs ressembleront.

131
0:07:16.881,000 --> 0:07:17,000
C'est immense !

132
0:07:18.953,000 --> 0:07:19,000
Ici, par exemple,

133
0:07:20.175,000 --> 0:07:23,000
nous nous attendons à ce que personne ne vive dans une extrême pauvreté.

134
0:07:25.93,000 --> 0:07:28,000
Ici, nous nous attendons à ce que tout le monde soit mieux loti

135
0:07:29.156,000 --> 0:07:31,000
que la personne la plus riche actuellement en vie.

136
0:07:32.081,000 --> 0:07:35,000
Peut-être qu'ici nous découvrirons les lois fondamentales de la nature

137
0:07:35.443,000 --> 0:07:36,000
qui régissent notre monde.

138
0:07:37.516,000 --> 0:07:4,000
Peut-être qu'ici nous découvrirons une toute nouvelle forme d'art,

139
0:07:41.245,000 --> 0:07:44,000
une forme de musique que nos oreilles n'entendent pas.

140
0:07:45.072,000 --> 0:07:47,000
Ce ne sont que les quelques prochains milliers d'années.

141
0:07:47.827,000 --> 0:07:49,000
Si nous voyons au-delà,

142
0:07:50.056,000 --> 0:07:54,000
nous ne pouvons pas imaginer les sommets atteints par les exploits de l'humanité.

143
0:07:54.247,000 --> 0:07:57,000
L'avenir pourrait être long et très beau,

144
0:07:57.311,000 --> 0:07:59,000
mais pourrions-nous perdre cette qualité ?

145
0:08:00.366,000 --> 0:08:01,000
Malheureusement, je crois que oui.

146
0:08:02.216,000 --> 0:08:06,000
Les deux derniers siècles ont apporté des progrès technologiques énormes

147
0:08:06.293,000 --> 0:08:08,000
mais aussi les risques mondiaux d'une guerre nucléaire

148
0:08:08.939,000 --> 0:08:1,000
et la possibilité d'un changement climatique extrême.

149
0:08:11.725,000 --> 0:08:12,000
Quand nous pensons aux siècles à venir,

150
0:08:13.582,000 --> 0:08:15,000
nous devrions nous attendre à revoir cette tendance.

151
0:08:16.187,000 --> 0:08:19,000
Nous pouvons voir les technologies extrêmement puissantes à venir.

152
0:08:20.132,000 --> 0:08:22,000
La biologie synthétique nous permettra de créer des virus

153
0:08:23.005,000 --> 0:08:26,000
d'une capacité à contaminer et à tuer sans précédent.

154
0:08:27.131,000 --> 0:08:28,000
La géo-ingénierie nous permettra

155
0:08:28.828,000 --> 0:08:3,000
de changer drastiquement le climat de la Terre.

156
0:08:31.798,000 --> 0:08:35,000
L'intelligence artificielle nous permettra de créer des agents intelligents

157
0:08:36.021,000 --> 0:08:38,000
aux capacités bien supérieures aux nôtres.

158
0:08:40.222,000 --> 0:08:43,000
Je ne dis pas qu'un de ces risques est particulièrement probable,

159
0:08:44.134,000 --> 0:08:45,000
mais il y a tant de choses en jeu

160
0:08:45.802,000 --> 0:08:47,000
que même les plus faibles probabilités comptent beaucoup.

161
0:08:49.568,000 --> 0:08:52,000
Imaginez que vous preniez l'avion, que vous soyez stressé

162
0:08:52.593,000 --> 0:08:55,000
et que le pilote vous rassure en disant :

163
0:08:56.061,000 --> 0:09:,000
« Il n'y a qu'une chance sur 1 000 de s'écraser, pas d'inquiétude. »

164
0:09:02.157,000 --> 0:09:03,000
Seriez-vous rassuré ?

165
0:09:04.509,000 --> 0:09:08,000
C'est pour ces raisons que je crois que protéger l'avenir de l'humanité

166
0:09:08.621,000 --> 0:09:1,000
fait partie des problèmes les plus importants

167
0:09:10.726,000 --> 0:09:11,000
auxquels nous faisons face.

168
0:09:12.546,000 --> 0:09:14,000
Mais continuons à utiliser ce cadre.

169
0:09:14.72,000 --> 0:09:15,000
S'intéresse-t-on à ce problème ?

170
0:09:18.085,000 --> 0:09:2,000
Je crois que la réponse est non

171
0:09:20.391,000 --> 0:09:23,000
et c'est parce que les problèmes qui touchent les générations futures

172
0:09:23.74,000 --> 0:09:24,000
sont souvent énormément négligés.

173
0:09:26.93,000 --> 0:09:27,000
Pourquoi ?

174
0:09:28.36,000 --> 0:09:31,000
Les futures générations ne participent pas aux marchés d'aujourd'hui.

175
0:09:31.862,000 --> 0:09:32,000
Ils n'ont pas le droit de vote.

176
0:09:33.931,000 --> 0:09:35,000
Il n'y a pas de lobby représentant les intérêts

177
0:09:36.628,000 --> 0:09:38,000
de ceux qui naîtront en 2 300 après J.-C.

178
0:09:40.313,000 --> 0:09:43,000
Ils ne peuvent pas influencer les décisions prises aujourd'hui.

179
0:09:43.995,000 --> 0:09:44,000
Ils n'ont pas de voix.

180
0:09:46.49,000 --> 0:09:49,000
Cela signifie que nous dépensons une somme dérisoire pour ces questions :

181
0:09:49.959,000 --> 0:09:51,000
la non prolifération des armes nucléaires,

182
0:09:51.968,000 --> 0:09:53,000
la géo-ingénierie, les risques biologiques,

183
0:09:55.414,000 --> 0:09:57,000
la sûreté de l'intelligence artificielle.

184
0:09:57.923,000 --> 0:09:59,000
Ils ne reçoivent que quelques dizaines de millions de dollars

185
0:10:00.821,000 --> 0:10:01,000
de financement philantropique par an.

186
0:10:04.044,000 --> 0:10:07,000
C'est très faible, comparé aux 390 milliards de dollars

187
0:10:08.79,000 --> 0:10:1,000
investis par les philantropes américains.

188
0:10:13.885,000 --> 0:10:15,000
Le dernier aspect de notre cadre, donc :

189
0:10:17.083,000 --> 0:10:18,000
cela peut-il être résolu ?

190
0:10:19.289,000 --> 0:10:2,000
Je crois que oui.

191
0:10:21.014,000 --> 0:10:24,000
Vous pouvez contribuer par votre argent,

192
0:10:24.085,000 --> 0:10:26,000
par votre carrière ou votre engagement politique.

193
0:10:28.225,000 --> 0:10:3,000
Votre argent peut soutenir des organisations

194
0:10:30.424,000 --> 0:10:31,000
qui se concentrent sur ces risques

195
0:10:32.056,000 --> 0:10:34,000
telles que l'Initiative contre la menace nucléaire,

196
0:10:34.479,000 --> 0:10:36,000
militant pour que les armes nucléaires ne soient plus

197
0:10:36.969,000 --> 0:10:37,000
en état d'alerte instantanée,

198
0:10:38.369,000 --> 0:10:41,000
ou le comité d'experts qui élabore des politiques pour minimiser les dégâts

199
0:10:41.944,000 --> 0:10:43,000
des pandémies naturelles et anthropiques,

200
0:10:45.158,000 --> 0:10:47,000
ou le Centre pour une IA compatible avec l'Homme

201
0:10:47.402,000 --> 0:10:48,000
réalisant des recherches

202
0:10:48.632,000 --> 0:10:5,000
pour s'assurer que les systèmes d'IA sont sûrs et fiables.

203
0:10:52.652,000 --> 0:10:53,000
Avec votre engagement politique,

204
0:10:54.192,000 --> 0:10:57,000
vous pouvez voter pour des candidats à qui ces risques importent

205
0:10:57.312,000 --> 0:10:59,000
et soutenir une meilleure coopération internationale.

206
0:11:01.767,000 --> 0:11:04,000
Avec votre carrière, vous pouvez faire tant de choses.

207
0:11:05.333,000 --> 0:11:08,000
Bien sûr, nous avons besoin de scientifiques, de décisionnaires,

208
0:11:08.345,000 --> 0:11:09,000
de dirigeants,

209
0:11:09.865,000 --> 0:11:1,000
mais aussi, tout autant,

210
0:11:11.041,000 --> 0:11:15,000
de comptables, de gestionnaires, d'assistants

211
0:11:16.691,000 --> 0:11:19,000
pour travailler dans ces organisations qui abordent ces problèmes.

212
0:11:20.469,000 --> 0:11:23,000
Le programme de recherche sur l'altruisme efficace

213
0:11:25.191,000 --> 0:11:26,000
en est à ses débuts

214
0:11:27.262,000 --> 0:11:29,000
et nous ignorons encore beaucoup de choses.

215
0:11:31.173,000 --> 0:11:33,000
Mais même avec ce que nous avons appris jusqu'ici,

216
0:11:34.748,000 --> 0:11:36,000
nous voyons qu'en réfléchissant minutieusement

217
0:11:37.494,000 --> 0:11:39,000
et en nous concentrant sur ces problèmes

218
0:11:40.202,000 --> 0:11:42,000
qui sont importants, résolubles et négligés,

219
0:11:43.152,000 --> 0:11:45,000
nous pouvons faire changer les choses dans le monde

220
0:11:45.884,000 --> 0:11:46,000
pour les milliers d'années à venir.

221
0:11:47.963,000 --> 0:11:48,000
Merci.

222
0:11:49.138,000 --> 0:11:53,000
(Applaudissements)

