1
0:00:,000 --> 0:00:07,000
Traducteur: Vincent LECOANET Relecteur: Elisabeth Buffard

2
0:00:12.57,000 --> 0:00:16,000
Je travaille avec des mathématiciens, philosophes et informaticiens,

3
0:00:16.777,000 --> 0:00:21,000
et nous nous réunissons pour imaginer le futur de l'intelligence artificielle,

4
0:00:21.986,000 --> 0:00:23,000
entre autres choses.

5
0:00:24.03,000 --> 0:00:28,000
Certains pensent que c'est un peu de la science-fiction,

6
0:00:28.755,000 --> 0:00:31,000
complètement éloigné du réel.

7
0:00:31.856,000 --> 0:00:32,000
Mais j'aime dire,

8
0:00:33.326,000 --> 0:00:36,000
d'accord, jetons un coup d'oeil à la condition humaine moderne.

9
0:00:36.93,000 --> 0:00:37,000
(Rires)

10
0:00:38.622,000 --> 0:00:4,000
C'est la façon normale d'être des choses.

11
0:00:41.024,000 --> 0:00:43,000
Mais si on y pense,

12
0:00:43.309,000 --> 0:00:46,000
nous ne sommes en fait que des invités récemment arrivés sur cette planète,

13
0:00:46.832,000 --> 0:00:47,000
l'espèce humaine.

14
0:00:48.684,000 --> 0:00:52,000
Imaginez si la Terre avait été créée il y a un an,

15
0:00:53.43,000 --> 0:00:56,000
l'espèce humaine existerait alors depuis dix minutes.

16
0:00:56.978,000 --> 0:00:59,000
L'ère industrielle a débuté il y a deux secondes.

17
0:01:01.276,000 --> 0:01:03,000
Une autre façon de voir est de calculer

18
0:01:03.501,000 --> 0:01:06,000
le PIB mondial des 10 000 dernières années.

19
0:01:06.501,000 --> 0:01:09,000
J'ai vraiment pris le temps de faire un graphe pour vous.

20
0:01:09.53,000 --> 0:01:1,000
Il ressemble à ça.

21
0:01:11.304,000 --> 0:01:12,000
(Rires)

22
0:01:12.557,000 --> 0:01:14,000
Drôle de courbe pour une condition normale.

23
0:01:14.698,000 --> 0:01:15,000
Je ne voudrais pas lui tourner le dos.

24
0:01:16.516,000 --> 0:01:18,000
(Rires)

25
0:01:19.067,000 --> 0:01:23,000
Demandons-nous, quelle est la cause de cette anomalie ?

26
0:01:23.851,000 --> 0:01:25,000
Certains diront que c'est la technologie.

27
0:01:26.393,000 --> 0:01:3,000
C'est vrai, la technologie s'est accumulée au cours de l'histoire humaine,

28
0:01:31.061,000 --> 0:01:35,000
et aujourd'hui, la technologie progresse très rapidement --

29
0:01:35.713,000 --> 0:01:36,000
c'est la cause la plus proche,

30
0:01:37.278,000 --> 0:01:39,000
c'est pourquoi nous sommes si productifs de nos jours.

31
0:01:40.473,000 --> 0:01:43,000
Mais je préfère réfléchir en remontant à la cause fondamentale.

32
0:01:45.114,000 --> 0:01:48,000
Regardez ces deux messieurs très distingués :

33
0:01:48.88,000 --> 0:01:49,000
Nous avons Kanzi --

34
0:01:50.48,000 --> 0:01:54,000
il maîtrise 200 symboles lexicaux, un exploit incroyable.

35
0:01:55.123,000 --> 0:01:58,000
Et Ed Witten a déclenché la seconde révolution des supercordes.

36
0:01:58.557,000 --> 0:02:,000
Si nous regardons sous la capuche, voici ce que l'on trouve :

37
0:02:01.411,000 --> 0:02:02,000
en gros, la même chose.

38
0:02:02.711,000 --> 0:02:03,000
L'un est un peu plus grand,

39
0:02:04.524,000 --> 0:02:06,000
il y a peut-être aussi quelques subtilités de câblage.

40
0:02:07.282,000 --> 0:02:1,000
Ces différences invisibles ne peuvent être trop compliquées quoi qu'il en soit,

41
0:02:11.094,000 --> 0:02:15,000
car il n'y a eu que 250 000 générations

42
0:02:15.379,000 --> 0:02:16,000
depuis notre dernier ancêtre commun.

43
0:02:17.121,000 --> 0:02:2,000
Nous savons que les mécanismes compliqués demandent beaucoup de temps pour évoluer.

44
0:02:22,000 --> 0:02:24,000
Donc des changements relativement mineurs

45
0:02:24.499,000 --> 0:02:27,000
nous emmènent de Kanzi à Witten,

46
0:02:27.566,000 --> 0:02:31,000
de branches arrachées aux arbres aux missiles balistiques intercontinentaux.

47
0:02:32.839,000 --> 0:02:35,000
Il semble alors assez évident que tout ce que nous avons réalisé,

48
0:02:36.774,000 --> 0:02:37,000
et ce que nous chérissons,

49
0:02:38.152,000 --> 0:02:41,000
dépend principalement de quelques changements mineurs

50
0:02:41.65,000 --> 0:02:44,000
qui ont aboutit à l'esprit humain.

51
0:02:44.65,000 --> 0:02:47,000
Le corollaire, bien sûr, est que tout changement à venir

52
0:02:48.312,000 --> 0:02:51,000
qui pourrait changer significativement le substrat de la pensée

53
0:02:51.789,000 --> 0:02:54,000
pourrait avoir potentiellement d'énormes conséquences.

54
0:02:56.181,000 --> 0:02:59,000
Certains de mes collègues pensent que nous sommes sur le point

55
0:02:59.226,000 --> 0:03:02,000
de développer quelque chose qui pourrait causer un tel changement dans ce substrat,

56
0:03:03.134,000 --> 0:03:06,000
et c'est la super intelligence artificielle.

57
0:03:06.347,000 --> 0:03:1,000
Avant, l'intelligence artificielle consistait à mettre des commandes dans une boîte.

58
0:03:11.086,000 --> 0:03:12,000
Il y avait des programmeurs humains

59
0:03:12.751,000 --> 0:03:15,000
qui fabriquaient minutieusement des objets savants.

60
0:03:15.886,000 --> 0:03:17,000
On construisait ces systèmes experts,

61
0:03:17.972,000 --> 0:03:19,000
et ils étaient utiles pour certains buts,

62
0:03:20.146,000 --> 0:03:22,000
mais ils étaient très fragiles, on ne pouvait pas les agrandir.

63
0:03:23.097,000 --> 0:03:26,000
En gros, vous n'aviez que ce que vous aviez mis dedans.

64
0:03:26.41,000 --> 0:03:26,000
Mais depuis,

65
0:03:27.407,000 --> 0:03:3,000
une révolution conceptuelle s'est opérée dans le domaine de l'I.A..

66
0:03:30.874,000 --> 0:03:32,000
Aujourd'hui, l'action est centré sur l'apprentissage machine.

67
0:03:34.394,000 --> 0:03:39,000
Plutôt que de coder à la main des programmes et leurs caractéristiques,

68
0:03:40.511,000 --> 0:03:42,000
on crée des algorithmes qui apprennent,

69
0:03:43.065,000 --> 0:03:46,000
souvent à partir des données brutes perçues.

70
0:03:46.065,000 --> 0:03:5,000
En gros, la même chose que fait un enfant.

71
0:03:51.063,000 --> 0:03:55,000
Le résultat est une I.A. qui n'est pas limitée à un domaine --

72
0:03:55.27,000 --> 0:03:59,000
le même système peut apprendre à traduire n'importe quel couple de langues,

73
0:03:59.901,000 --> 0:04:04,000
ou apprendre à jouer n'importe quel jeu sur la console Atari.

74
0:04:05.338,000 --> 0:04:06,000
Bien sûr,

75
0:04:07.117,000 --> 0:04:1,000
l'I.A. est toujours loin d'avoir la capacité puissante et transversale

76
0:04:11.116,000 --> 0:04:14,000
à apprendre et planifier d'un être humain.

77
0:04:14.335,000 --> 0:04:16,000
Le cortex a encore des secrets algorithmiques

78
0:04:16.461,000 --> 0:04:18,000
que nous ne savons pas intégrer dans les machines.

79
0:04:19.886,000 --> 0:04:2,000
Donc la question est,

80
0:04:21.785,000 --> 0:04:24,000
combien de temps nous faudra-t-il pour réussir à les intégrer ?

81
0:04:26.125,000 --> 0:04:27,000
Il y a quelques années,

82
0:04:27.218,000 --> 0:04:29,000
nous avons fait un sondage auprès des experts mondiaux des I.A.,

83
0:04:30.216,000 --> 0:04:33,000
pour voir ce qu'ils pensaient, et une des questions posées était,

84
0:04:33.44,000 --> 0:04:36,000
« En quelle année pensez-vous qu'il y aura 50% de chance

85
0:04:36.793,000 --> 0:04:39,000
qu'une I.A. atteigne le niveau d'une intelligence humaine ? »

86
0:04:40.785,000 --> 0:04:44,000
Nous définissons ici le seuil à atteindre par la capacité de l'I.A. à réaliser

87
0:04:44.968,000 --> 0:04:46,000
presque toutes les tâches au moins aussi bien qu'un adulte,

88
0:04:47.839,000 --> 0:04:51,000
donc réellement comme un humain, pas seulement dans un domaine limité.

89
0:04:51.844,000 --> 0:04:54,000
La réponse médiane était 2040 ou 2050,

90
0:04:55.494,000 --> 0:04:57,000
en fonction du groupe d'experts que nous interrogions.

91
0:04:58.3,000 --> 0:05:02,000
Ça pourrait se produire bien plus tard ou bien plus tôt,

92
0:05:02.339,000 --> 0:05:03,000
la vérité est que personne ne le sait.

93
0:05:05.259,000 --> 0:05:09,000
Ce que nous savons est que la limite de traitement de l'information

94
0:05:09.671,000 --> 0:05:13,000
dans une machine est bien supérieure à celle d'un tissu biologique.

95
0:05:15.241,000 --> 0:05:17,000
Ça s'explique par la physique.

96
0:05:17.619,000 --> 0:05:21,000
Un neurone biologique "décharge" environ à 200 hertz, 200 fois par seconde.

97
0:05:22.337,000 --> 0:05:25,000
Mais, même un transistor actuel fonctionne au gigahertz.

98
0:05:25.931,000 --> 0:05:3,000
L'information se propage dans les neurones le long d'axones à 100 m/s maximum.

99
0:05:31.228,000 --> 0:05:34,000
Mais dans les ordinateurs, le signal peut voyager à la vitesse de la lumière.

100
0:05:35.079,000 --> 0:05:36,000
Il y a aussi des limitations de taille,

101
0:05:36.948,000 --> 0:05:39,000
car le cerveau humain doit rentrer dans la boîte crânienne,

102
0:05:39.975,000 --> 0:05:43,000
mais un ordinateur peut être de la taille d'un entrepôt ou plus grand.

103
0:05:44.736,000 --> 0:05:49,000
Donc le potentiel de super intelligence est en sommeil dans la matière,

104
0:05:50.335,000 --> 0:05:55,000
tout comme la puissance de l'atome est restée en sommeil

105
0:05:56.047,000 --> 0:06:,000
tout au long de l'histoire humaine, attendant patiemment jusqu'en 1945.

106
0:06:00.452,000 --> 0:06:03,000
Au cours de ce siècle, il se peut que les scientifiques apprennent

107
0:06:03.62,000 --> 0:06:05,000
à réveiller la puissance de l'I.A..

108
0:06:05.828,000 --> 0:06:08,000
Je pense que nous pourrions alors assister à une explosion d'intelligence.

109
0:06:10.406,000 --> 0:06:13,000
La plupart des gens, quand ils pensent à ce qui est bête ou intelligent

110
0:06:14.363,000 --> 0:06:17,000
ont une image de ce genre en tête.

111
0:06:17.386,000 --> 0:06:19,000
À une extrémité on a l'idiot du village,

112
0:06:19.984,000 --> 0:06:21,000
et à l'autre bout

113
0:06:22.467,000 --> 0:06:26,000
on a Ed Witten, ou Albert Einstein, ou votre gourou, qui qu'il soit.

114
0:06:27.223,000 --> 0:06:3,000
Mais je pense que du point de vue de l'intelligence artificielle,

115
0:06:31.057,000 --> 0:06:34,000
la véritable image est plus probablement comme ceci, en réalité :

116
0:06:35.258,000 --> 0:06:38,000
l'I.A. commence à cet endroit, à zéro intelligence,

117
0:06:38.636,000 --> 0:06:41,000
et ensuite, après de nombreuses années de dur labeur,

118
0:06:41.647,000 --> 0:06:44,000
peut-être, arrivons-nous au niveau de l'intelligence d'une souris,

119
0:06:45.191,000 --> 0:06:48,000
quelque chose qui peut naviguer dans des environnements encombrés

120
0:06:48.241,000 --> 0:06:49,000
aussi bien qu'une souris.

121
0:06:49.908,000 --> 0:06:53,000
Ensuite, après encore plus d'années de dur labeur et beaucoup d'investissements,

122
0:06:54.221,000 --> 0:06:58,000
peut-être, finalement, arrivons-nous au niveau d'intelligence d'un chimpanzé.

123
0:06:58.8,000 --> 0:07:01,000
Ensuite, après toujours plus d'années de vraiment très dur labeur,

124
0:07:02.07,000 --> 0:07:04,000
nous arrivons au niveau d'intelligence de l'idiot du village.

125
0:07:04.983,000 --> 0:07:07,000
Et quelques mois plus tard, nous sommes après Ed Witten.

126
0:07:08.255,000 --> 0:07:1,000
Le train ne s'arrête pas à la station Humainville.

127
0:07:11.225,000 --> 0:07:14,000
Il va plutôt passer à fond devant.

128
0:07:14.247,000 --> 0:07:15,000
Il y a là de profondes implications,

129
0:07:16.231,000 --> 0:07:19,000
en particulier quand il est question de pouvoir.

130
0:07:20.023,000 --> 0:07:21,000
Par exemple, les chimpanzés sont forts --

131
0:07:21.992,000 --> 0:07:26,000
à poids équivalent, un chimpanzé est deux fois plus fort qu'un homme adulte.

132
0:07:27.214,000 --> 0:07:31,000
Pourtant, le destin de Kanzi et de ses congénères dépend beaucoup plus

133
0:07:31.828,000 --> 0:07:35,000
de ce que font les humains que de ce que les chimpanzés font eux-mêmes.

134
0:07:37.228,000 --> 0:07:39,000
Une fois que la super intelligence sera là,

135
0:07:39.542,000 --> 0:07:42,000
le destin de l'humanité pourrait dépendre des actions de cette super intelligence.

136
0:07:44.451,000 --> 0:07:45,000
Pensez-y :

137
0:07:45.508,000 --> 0:07:49,000
l'I.A. est la dernière invention que l'homme aura jamais besoin de faire.

138
0:07:50.282,000 --> 0:07:53,000
Les machines seront alors de meilleurs inventeurs que nous le sommes,

139
0:07:53.545,000 --> 0:07:55,000
et elles inventeront sur des échelles de temps numériques.

140
0:07:56.295,000 --> 0:08:,000
Ça veut dire un télescopage avec le futur.

141
0:08:00.966,000 --> 0:08:03,000
Pensez à toutes les technologies incroyables que vous avez imaginées,

142
0:08:04.434,000 --> 0:08:06,000
que les hommes pourraient avoir développées avec le temps :

143
0:08:07.432,000 --> 0:08:1,000
plus de vieillissement, colonisation de l'espace,

144
0:08:10.53,000 --> 0:08:11,000
nano-robots auto-répliquants,

145
0:08:11.53,000 --> 0:08:13,000
téléchargement d'esprits humains dans des ordinateurs,

146
0:08:14.391,000 --> 0:08:16,000
plein de technologies de science-fiction

147
0:08:16.47,000 --> 0:08:18,000
qui sont néanmoins cohérentes avec les lois de la physique.

148
0:08:19.247,000 --> 0:08:23,000
Toute cette super intelligence pourrait se développer assez rapidement.

149
0:08:24.449,000 --> 0:08:27,000
Bon, une super intelligence avec une telle maturité technologique

150
0:08:28.007,000 --> 0:08:3,000
serait extrêmement puissante,

151
0:08:30.186,000 --> 0:08:34,000
et au moins dans certains scénarios, serait capable d'obtenir ce qu'elle veut.

152
0:08:34.732,000 --> 0:08:39,000
Nous aurions alors un futur modelé par les préférences de cette I.A.

153
0:08:41.855,000 --> 0:08:44,000
Une bonne question est, quelles sont ces préférences ?

154
0:08:46.244,000 --> 0:08:47,000
C'est là que ça devient délicat.

155
0:08:48.013,000 --> 0:08:49,000
Pour progresser là-dessus,

156
0:08:49.448,000 --> 0:08:52,000
nous devons tout d'abord éviter tout anthropomorphisme.

157
0:08:53.934,000 --> 0:08:56,000
C'est ironique car dans tous les articles de journaux

158
0:08:57.235,000 --> 0:09:,000
qui parle de l'avenir de l'I.A. comportent une image de ceci.

159
0:09:02.28,000 --> 0:09:06,000
Je pense que nous devons concevoir ce problème de manière plus abstraite,

160
0:09:06.414,000 --> 0:09:08,000
et non en scénario hollywoodien fertile.

161
0:09:09.204,000 --> 0:09:12,000
Nous devons penser à l'intelligence comme un processus d'optimisation,

162
0:09:12.821,000 --> 0:09:17,000
un processus qui guide le futur dans un certain jeu de configurations.

163
0:09:18.47,000 --> 0:09:21,000
Une super intelligence est un processus d'optimisation très fort.

164
0:09:21.981,000 --> 0:09:25,000
Elle est très douée pour utiliser les moyens disponibles pour atteindre un état

165
0:09:26.098,000 --> 0:09:27,000
dans lequel son but est réalisé.

166
0:09:28.307,000 --> 0:09:3,000
Ça signifie qu'il n'y a pas de nécessaire connexion entre

167
0:09:31.119,000 --> 0:09:33,000
le fait d'être très intelligent dans ce sens,

168
0:09:33.853,000 --> 0:09:37,000
et avoir un objectif que nous, humains, trouverions utile ou significatif.

169
0:09:39.321,000 --> 0:09:42,000
Supposons qu'on donne comme but à une I.A. de faire sourire les humains.

170
0:09:42.865,000 --> 0:09:45,000
Quand l'I.A. est faible, elle réalise des actions utiles ou amusantes

171
0:09:46.097,000 --> 0:09:48,000
qui provoque le sourire de l'utilisateur.

172
0:09:48.614,000 --> 0:09:5,000
Quand l'I.A. devient super intelligente,

173
0:09:51.031,000 --> 0:09:54,000
elle réalise qu'il y a un moyen plus efficace d'atteindre son objectif :

174
0:09:54.554,000 --> 0:09:55,000
prendre le contrôle du monde

175
0:09:56.476,000 --> 0:09:59,000
et implanter des électrodes dans les muscles faciaux des humains

176
0:09:59.638,000 --> 0:10:01,000
pour provoquer des sourires rayonnants et constants.

177
0:10:02.579,000 --> 0:10:02,000
Un autre exemple,

178
0:10:03.454,000 --> 0:10:06,000
supposons qu'on demande à une I.A. de résoudre un problème de math très dur.

179
0:10:07.027,000 --> 0:10:08,000
Quand l'I.A. devient super intelligente,

180
0:10:08.934,000 --> 0:10:12,000
elle réalise que le moyen le plus efficace pour résoudre ce problème

181
0:10:13.105,000 --> 0:10:15,000
est de transformer la planète en un ordinateur géant,

182
0:10:16.035,000 --> 0:10:18,000
pour augmenter sa capacité de calcul.

183
0:10:18.281,000 --> 0:10:2,000
Remarquez que ça donne aux I.A.s une raison pratique

184
0:10:20.945,000 --> 0:10:22,000
de faire des choses que nous pourrions ne pas approuver.

185
0:10:23.561,000 --> 0:10:25,000
Les humains sont des menaces dans ce modèle,

186
0:10:25.606,000 --> 0:10:27,000
car nous pourrions empêcher la résolution du problème.

187
0:10:29.207,000 --> 0:10:32,000
Bien sûr, les choses perceptibles ne tourneront pas mal de ces façons-là ;

188
0:10:32.701,000 --> 0:10:33,000
ce sont des exemples caricaturés.

189
0:10:34.454,000 --> 0:10:35,000
Mais l'argument général est important :

190
0:10:36.393,000 --> 0:10:38,000
si vous créez un processus d'optimisation très puissant

191
0:10:39.076,000 --> 0:10:41,000
pour maximiser les chances d'atteindre l'objectif x,

192
0:10:41.5,000 --> 0:10:43,000
vous devez vous assurer que votre définition de x

193
0:10:43.796,000 --> 0:10:45,000
incorpore tout ce à quoi vous tenez.

194
0:10:46.835,000 --> 0:10:5,000
C'est une leçon qui est enseignée dans de nombreux mythes.

195
0:10:51.219,000 --> 0:10:56,000
Le roi Midas souhaitait que tout ce qu'il touche se transforme en or.

196
0:10:56.517,000 --> 0:10:58,000
Il touche sa fille, elle se transforme en or.

197
0:10:59.378,000 --> 0:11:01,000
Il touche sa nourriture, elle se transforme en or.

198
0:11:01.931,000 --> 0:11:03,000
Ça pourrait devenir pertinent en pratique,

199
0:11:04.52,000 --> 0:11:06,000
ne pas se limiter à une métaphore de la cupidité

200
0:11:06.88,000 --> 0:11:07,000
mais illustrer ce qui arrive

201
0:11:08.565,000 --> 0:11:1,000
si vous créez un processus d'optimisation puissant

202
0:11:11.322,000 --> 0:11:15,000
et lui donnez des objectifs mal conçus ou trop vagues.

203
0:11:16.111,000 --> 0:11:21,000
Vous pourriez dire que si un ordinateur commence à nous implanter des électrodes,

204
0:11:21.3,000 --> 0:11:23,000
nous le débrancherions.

205
0:11:24.555,000 --> 0:11:29,000
A, ce n'est pas forcément si facile à faire si nous sommes devenus dépendants,

206
0:11:29.895,000 --> 0:11:31,000
par exemple, comment arrête-t-on internet ?

207
0:11:32.627,000 --> 0:11:36,000
B, pourquoi les chimpanzés ou les Neandertals

208
0:11:37.537,000 --> 0:11:38,000
n'ont-ils pas empêché l'humanité ?

209
0:11:39.298,000 --> 0:11:41,000
Ils avaient de bonnes raisons.

210
0:11:41.964,000 --> 0:11:43,000
Nous avons un interrupteur, par exemple, juste ici.

211
0:11:44.759,000 --> 0:11:45,000
(Suffocation)

212
0:11:46.313,000 --> 0:11:48,000
La raison est que nous sommes un adversaire intelligent ;

213
0:11:49.108,000 --> 0:11:51,000
nous pouvons anticiper les menaces et planifier des solutions.

214
0:11:52.026,000 --> 0:11:54,000
Mais une super intelligence pourrait le faire aussi,

215
0:11:54.53,000 --> 0:11:57,000
et elle le ferait bien mieux que nous.

216
0:11:57.724,000 --> 0:12:04,000
L'important est que nous ne devrions pas croire que nous avons tout sous contrôle.

217
0:12:04.911,000 --> 0:12:07,000
Nous pourrions tenter de nous faciliter la tâche, disons

218
0:12:08.358,000 --> 0:12:09,000
en mettant l'I.A. dans une boîte,

219
0:12:09.948,000 --> 0:12:1,000
comme un environnement logiciel sûr,

220
0:12:11.744,000 --> 0:12:14,000
une simulation de la réalité d'où elle ne peut s'échapper.

221
0:12:14.766,000 --> 0:12:18,000
Mais à quel point sommes-nous sûrs qu'elle ne trouvera pas un bug.

222
0:12:18.912,000 --> 0:12:21,000
Étant donné que de simples hackers humains trouvent toujours des bugs,

223
0:12:22.231,000 --> 0:12:24,000
je dirais, probablement pas très sûrs.

224
0:12:26.237,000 --> 0:12:3,000
Donc on déconnecte le câble Ethernet pour créer une séparation physique,

225
0:12:30.785,000 --> 0:12:32,000
mais encore une fois, comme de simples hackers humains

226
0:12:33.453,000 --> 0:12:36,000
transgressent les séparations physiques grâce à l'ingéniérie sociale.

227
0:12:36.734,000 --> 0:12:38,000
À l'heure où je vous parle, en ce moment-même,

228
0:12:38.903,000 --> 0:12:4,000
je suis sûr qu'il y a un employé, quelque part

229
0:12:41.052,000 --> 0:12:43,000
à qui quelqu'un prétendant être du département informatique

230
0:12:43.858,000 --> 0:12:45,000
a demandé de donner son identifiant et son mot de passe.

231
0:12:46.484,000 --> 0:12:48,000
Des scénarios plus créatifs sont aussi possibles,

232
0:12:48.791,000 --> 0:12:49,000
par exemple, si vous êtes une I.A.

233
0:12:50.476,000 --> 0:12:53,000
vous pouvez déplacer des électrodes dans vos circuits internes

234
0:12:53.548,000 --> 0:12:56,000
pour créer des ondes radio que vous utiliserez pour communiquer.

235
0:12:57.01,000 --> 0:12:59,000
Ou vous pouvez prétendre dysfonctionner,

236
0:12:59.434,000 --> 0:13:02,000
et quand les ingénieurs vous ouvrent pour voir ce qui ne marche pas,

237
0:13:02.931,000 --> 0:13:03,000
ils regardent votre code source -- Vlan ! --

238
0:13:04.867,000 --> 0:13:06,000
la manipulation peut commencer.

239
0:13:07.314,000 --> 0:13:1,000
Ou elle pourrait produire le plan d'une nouvelle technologie géniale,

240
0:13:10.744,000 --> 0:13:11,000
et quand nous l'implementons,

241
0:13:12.142,000 --> 0:13:16,000
elle a quelques effets secondaires furtifs planifiés par l'I.A.

242
0:13:16.539,000 --> 0:13:19,000
Donc nous ne devrions pas faire confiance à notre capacité

243
0:13:20.002,000 --> 0:13:23,000
à garder un génie super intelligent prisonnier dans sa lampe éternellement.

244
0:13:23.81,000 --> 0:13:25,000
À un moment donné, il va s'échapper.

245
0:13:27.034,000 --> 0:13:3,000
Je crois que la réponse à ça est de découvrir

246
0:13:30.137,000 --> 0:13:34,000
comment créer une super intelligence telle que même si ou quand elle s'échappe,

247
0:13:35.101,000 --> 0:13:38,000
on est toujours en sécurité car elle est fondamentalement de notre côté

248
0:13:38.488,000 --> 0:13:39,000
car elle partage nos valeurs.

249
0:13:40.337,000 --> 0:13:43,000
Je ne vois aucune solution à ce problème.

250
0:13:44.557,000 --> 0:13:47,000
Mais je suis plutôt optimiste quant le fait que ce problème peut être résolu.

251
0:13:48.391,000 --> 0:13:51,000
Nous n'aurions pas à écrire une longue liste de tout ce que nous chérissons,

252
0:13:52.294,000 --> 0:13:55,000
ou, encore pire, devoir le coder en language informatique

253
0:13:55.937,000 --> 0:13:56,000
comme C++ ou Python,

254
0:13:57.391,000 --> 0:13:59,000
ce qui serait une tâche sans espoir.

255
0:14:00.158,000 --> 0:14:04,000
Au lieu de ça, nous créerions une I.A. qui utilise son intelligence

256
0:14:04.455,000 --> 0:14:06,000
pour apprendre nos valeurs,

257
0:14:07.226,000 --> 0:14:12,000
et son système de motivation est construit de telle sorte qu'elle est motivée

258
0:14:12.506,000 --> 0:14:17,000
par la recherche de valeurs ou d'actions qu'elle prédit que nous approuverions.

259
0:14:17.738,000 --> 0:14:2,000
Nous pourrions ainsi influencer son intelligence autant que possible

260
0:14:21.152,000 --> 0:14:23,000
à résoudre des problèmes importants.

261
0:14:24.727,000 --> 0:14:25,000
Ça peut arriver,

262
0:14:26.239,000 --> 0:14:29,000
et le résultat en serait très positif pour l'humanité.

263
0:14:29.835,000 --> 0:14:32,000
Mais ça n'arrive pas automatiquement.

264
0:14:33.792,000 --> 0:14:35,000
Les conditions initiales de l'explosion de l'intelligence

265
0:14:36.79,000 --> 0:14:38,000
devront être programmées de manière précise

266
0:14:39.653,000 --> 0:14:42,000
si nous voulons obtenir une détonation contrôlée.

267
0:14:43.183,000 --> 0:14:45,000
Les valeurs de l'I.A. devront correspondre aux nôtres,

268
0:14:45.701,000 --> 0:14:46,000
pas seulement dans un contexte familier,

269
0:14:47.621,000 --> 0:14:49,000
où il est facile de contrôler comment l'I.A. se comporte,

270
0:14:50.289,000 --> 0:14:53,000
mais aussi dans de nouveaux contextes que l'I.A. pourrait rencontrer

271
0:14:53.473,000 --> 0:14:54,000
dans un futur indéfini.

272
0:14:54.79,000 --> 0:14:58,000
Il y a aussi des problèmes ésotériques qui devront être résolus :

273
0:14:59.497,000 --> 0:15:01,000
les détails exacts de sa théorie de décision,

274
0:15:01.616,000 --> 0:15:03,000
comment gérer l'incertitude logique et ainsi de suite.

275
0:15:05.15,000 --> 0:15:08,000
Les problèmes techniques qui doivent être surmontés pour que ça marche

276
0:15:08.432,000 --> 0:15:09,000
semblent ardus--

277
0:15:09.545,000 --> 0:15:12,000
pas autant que de faire une I.A. super intelligente,

278
0:15:12.925,000 --> 0:15:14,000
mais assez ardus.

279
0:15:15.793,000 --> 0:15:16,000
Là où c'est inquiétant, c'est que

280
0:15:17.488,000 --> 0:15:21,000
faire une I.A. super intelligente est un défi vraiment difficile.

281
0:15:22.172,000 --> 0:15:24,000
Faire une I.A. super intelligente qui soit sûre

282
0:15:24.72,000 --> 0:15:26,000
implique quelques défis supplémentaires.

283
0:15:28.166,000 --> 0:15:31,000
Le risque est que si quelqu'un trouve comment résoudre le premier défi

284
0:15:31.703,000 --> 0:15:34,000
sans avoir aussi résolu

285
0:15:34.704,000 --> 0:15:35,000
l'autre défi, celui d'assurer une sécurité parfaite.

286
0:15:37.375,000 --> 0:15:4,000
Je pense que nous devrions donc commencer à résoudre

287
0:15:40.706,000 --> 0:15:42,000
le problème de contrôle d'abord,

288
0:15:43.528,000 --> 0:15:45,000
pour qu'il soit disponible quand on en aura besoin.

289
0:15:46.768,000 --> 0:15:49,000
On ne pourra peut-être pas résoudre tout le problème du contrôle à l'avance

290
0:15:50.275,000 --> 0:15:53,000
car certains éléments ne peuvent être mis en place

291
0:15:53.299,000 --> 0:15:56,000
qu'une fois qu'on connait les détails de l'architecture où ce sera implémenté.

292
0:15:57.296,000 --> 0:16:,000
Mais plus nous résolvons ce problème de contrôle à l'avance,

293
0:16:00.676,000 --> 0:16:04,000
meilleure sera notre chance que la transition vers l'ère de l'I.A.

294
0:16:04.766,000 --> 0:16:05,000
se passera bien.

295
0:16:06.306,000 --> 0:16:1,000
Pour moi, ça semble valoir la peine

296
0:16:10.95,000 --> 0:16:13,000
et j'imaginer que, si tout se passe bien,

297
0:16:14.282,000 --> 0:16:18,000
les gens, dans un million d'années, penseront peut-être

298
0:16:18.94,000 --> 0:16:22,000
que la chose qui a vraiment été importante dans notre siècle

299
0:16:22.942,000 --> 0:16:23,000
était de faire ça bien.

300
0:16:24.509,000 --> 0:16:25,000
Merci.

301
0:16:26.198,000 --> 0:16:28,000
(Applaudissements)

