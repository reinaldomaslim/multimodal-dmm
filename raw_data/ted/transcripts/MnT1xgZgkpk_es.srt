1
0:00:,000 --> 0:00:07,000
Traductor: Denise RQ Revisor: Sebastian Betti

2
0:00:12.584,000 --> 0:00:16,000
Yo trabajo con un grupo de matemáticos, filósofos y científicos informáticos,

3
0:00:16.794,000 --> 0:00:21,000
y nos juntamos para pensar en el futuro de la inteligencia artificial,

4
0:00:21.986,000 --> 0:00:23,000
entre otras cosas.

5
0:00:24.03,000 --> 0:00:28,000
Algunos piensan que estas cosas son una especie de ciencia ficción

6
0:00:28.777,000 --> 0:00:31,000
alocadas y alejadas de la verdad.

7
0:00:31.856,000 --> 0:00:32,000
Pero bueno, me gusta sugerir

8
0:00:33.326,000 --> 0:00:36,000
que analicemos la condición humana moderna.

9
0:00:36.93,000 --> 0:00:37,000
(Risas)

10
0:00:38.622,000 --> 0:00:4,000
Así es cómo tienen que ser las cosas.

11
0:00:41.024,000 --> 0:00:43,000
Pero si lo pensamos,

12
0:00:43.309,000 --> 0:00:46,000
en realidad acabamos de llegar a este planeta,

13
0:00:46.602,000 --> 0:00:48,000
nosotros, la especie humana.

14
0:00:48.684,000 --> 0:00:52,000
Piensen que si la Tierra hubiera sido creada hace un año,

15
0:00:53.43,000 --> 0:00:56,000
entonces la raza humana solo tendría 10 minutos de edad

16
0:00:56.978,000 --> 0:00:59,000
y la era industrial habría empezado hace dos segundos.

17
0:01:01.276,000 --> 0:01:05,000
Otra forma de abordar esto es pensar en el PIB mundial en los últimos 10 000 años,

18
0:01:06.251,000 --> 0:01:09,000
y de hecho, me he tomado la molestia de representarlo en un gráfico para Uds.

19
0:01:09.93,000 --> 0:01:1,000
Se ve así.

20
0:01:11.304,000 --> 0:01:12,000
(Risas)

21
0:01:12.427,000 --> 0:01:14,000
Tiene una forma curiosa para ser normal.

22
0:01:14.818,000 --> 0:01:16,000
Y seguro que no me gustaría sentarme en ella.

23
0:01:16.936,000 --> 0:01:18,000
(Risas)

24
0:01:19.067,000 --> 0:01:23,000
Preguntémonos ¿cuál es la causa de esta anomalía actual?

25
0:01:23.841,000 --> 0:01:25,000
Algunas personas dirán que es la tecnología.

26
0:01:26.393,000 --> 0:01:3,000
Ahora bien es cierto que la tecnología ha aumentado a lo largo de la historia,

27
0:01:31.061,000 --> 0:01:35,000
y en la actualidad avanza muy rápidamente

28
0:01:35.713,000 --> 0:01:36,000
--esa es la causa inmediata--

29
0:01:37.278,000 --> 0:01:39,000
y por esto la razón de ser muy productivos hoy en día.

30
0:01:40.473,000 --> 0:01:43,000
Pero me gusta indagar más allá, buscar la causa de todo.

31
0:01:45.114,000 --> 0:01:48,000
Miren a estos 2 caballeros muy distinguidos:

32
0:01:48.876,000 --> 0:01:49,000
Tenemos a Kanzi,

33
0:01:50.48,000 --> 0:01:54,000
que domina 200 unidades léxicas, una hazaña increíble,

34
0:01:55.125,000 --> 0:01:58,000
y a Ed Witten que desató la segunda revolución de las supercuerdas.

35
0:01:58.626,000 --> 0:02:,000
Si miramos atentamente esto es lo que encontramos:

36
0:02:01.141,000 --> 0:02:02,000
básicamente la misma cosa.

37
0:02:02.711,000 --> 0:02:03,000
Uno es un poco más grande,

38
0:02:04.524,000 --> 0:02:07,000
y puede que también tenga algunos trucos más por la forma en que está diseñado,

39
0:02:08.292,000 --> 0:02:11,000
sin embargo, estas diferencias invisibles no pueden ser demasiado complicadas

40
0:02:12.034,000 --> 0:02:15,000
porque solo nos separan 250 000 generaciones

41
0:02:15.379,000 --> 0:02:16,000
de nuestro último ancestro común.

42
0:02:17.111,000 --> 0:02:2,000
Sabemos que los mecanismos complicados tardan mucho tiempo en evolucionar

43
0:02:22,000 --> 0:02:24,000
así que una serie de pequeños cambios

44
0:02:24.499,000 --> 0:02:27,000
nos lleva de Kanzi a Witten,

45
0:02:27.584,000 --> 0:02:31,000
de las ramas de árboles rotas a los misiles balísticos intercontinentales.

46
0:02:32.839,000 --> 0:02:35,000
Así que parece bastante claro que todo lo que hemos logrado,

47
0:02:36.554,000 --> 0:02:37,000
y todo lo que nos importa,

48
0:02:38.152,000 --> 0:02:41,000
depende fundamentalmente de algunos cambios relativamente menores

49
0:02:41.726,000 --> 0:02:42,000
sufridos por la mente humana.

50
0:02:44.65,000 --> 0:02:47,000
Y el corolario es que, por supuesto, cualquier cambio ulterior

51
0:02:48.312,000 --> 0:02:51,000
que cambiara significativamente el fundamento del pensamiento

52
0:02:51.789,000 --> 0:02:54,000
podría potencialmente acarrear enormes consecuencias.

53
0:02:56.321,000 --> 0:02:58,000
Algunos de mis colegas piensan que estamos muy cerca

54
0:02:59.226,000 --> 0:03:02,000
de algo que podría causar un cambio significativo en ese fundamento

55
0:03:03.135,000 --> 0:03:06,000
y que eso es la máquina superinteligente.

56
0:03:06.347,000 --> 0:03:1,000
La inteligencia artificial solía ser la integración de comandos en una caja,

57
0:03:11.086,000 --> 0:03:12,000
con programadores humanos

58
0:03:12.751,000 --> 0:03:15,000
que elaboraban conocimiento minuciosamente a mano.

59
0:03:15.886,000 --> 0:03:17,000
Se construían estos sistemas especializados

60
0:03:17.972,000 --> 0:03:19,000
y eran bastante útiles para ciertos propósitos,

61
0:03:20.296,000 --> 0:03:22,000
pero eran muy frágiles y no se podían ampliar.

62
0:03:22.969,000 --> 0:03:25,000
Básicamente, se conseguía solamente lo que se invertía en ellos.

63
0:03:26.41,000 --> 0:03:26,000
Pero desde entonces,

64
0:03:27.408,000 --> 0:03:3,000
hubo un cambio de paradigma en el campo de la inteligencia artificial.

65
0:03:31.083,000 --> 0:03:34,000
Hoy, la acción gira en torno al aprendizaje máquina.

66
0:03:35.002,000 --> 0:03:37,000
Así que en lugar de producir características

67
0:03:37.576,000 --> 0:03:4,000
y representar el conocimiento de manera artesanal,

68
0:03:40.821,000 --> 0:03:45,000
creamos algoritmos que aprenden a menudo a partir de datos de percepción en bruto.

69
0:03:46.065,000 --> 0:03:5,000
Básicamente lo mismo que hace el bebé humano.

70
0:03:51.063,000 --> 0:03:55,000
El resultado es inteligencia artificial que no se limita a un solo campo;

71
0:03:55.27,000 --> 0:03:59,000
el mismo sistema puede aprender a traducir entre cualquier par de idiomas

72
0:03:59.918,000 --> 0:04:04,000
o aprender a jugar a cualquier juego de ordenador en la consola Atari.

73
0:04:05.338,000 --> 0:04:06,000
Ahora, por supuesto,

74
0:04:07.125,000 --> 0:04:1,000
la IA está todavía muy lejos de tener el mismo poder y alcance interdisciplinario

75
0:04:11.122,000 --> 0:04:14,000
para aprender y planificar como lo hacen los humanos.

76
0:04:14.155,000 --> 0:04:16,000
La corteza cerebral aún esconde algunos trucos algorítmicos

77
0:04:16.961,000 --> 0:04:18,000
que todavía no sabemos cómo simular en las máquinas.

78
0:04:19.766,000 --> 0:04:21,000
Así que la pregunta es,

79
0:04:21.785,000 --> 0:04:24,000
¿cuánto nos falta para poder implementar esos trucos?

80
0:04:26.145,000 --> 0:04:26,000
Hace un par de años

81
0:04:27.139,000 --> 0:04:3,000
hicimos una encuesta entre los expertos de IA más importantes del mundo

82
0:04:30.446,000 --> 0:04:33,000
para ver lo que piensan, y una de las preguntas que hicimos fue,

83
0:04:33.56,000 --> 0:04:36,000
"¿En qué año crees que habrá un 50 % de probabilidad en elevar

84
0:04:36.793,000 --> 0:04:39,000
la inteligencia artificial al mismo nivel que la inteligencia humana?"

85
0:04:40.785,000 --> 0:04:43,000
Donde hemos definido ese nivel como la capacidad de realizar

86
0:04:44.628,000 --> 0:04:47,000
casi todas las tareas, al menos así como las desarrolla un humano adulto,

87
0:04:48.169,000 --> 0:04:51,000
por lo cual, un nivel real no solo dentro de un área limitada.

88
0:04:51.844,000 --> 0:04:54,000
Y la respuesta fue alrededor de 2040 o 2050,

89
0:04:55.494,000 --> 0:04:57,000
dependiendo del grupo de expertos consultados.

90
0:04:58.3,000 --> 0:05:02,000
Ahora, puede ocurrir mucho más tarde o más temprano,

91
0:05:02.339,000 --> 0:05:04,000
la verdad es que nadie lo sabe realmente.

92
0:05:05.259,000 --> 0:05:09,000
Lo que sí sabemos es que el umbral en el procesamiento de información

93
0:05:09.672,000 --> 0:05:11,000
en una infraestructura artificial

94
0:05:12.055,000 --> 0:05:15,000
se encuentra mucho más allá de los límites del tejido biológico.

95
0:05:15.302,000 --> 0:05:17,000
Esto pertenece al campo de la física.

96
0:05:17.619,000 --> 0:05:21,000
Una neurona biológica manda impulsos quizá a 200 Hertz, 200 veces por segundo.

97
0:05:22.083,000 --> 0:05:25,000
mientras que incluso hoy, un transistor opera a la frecuencia de los gigahercios.

98
0:05:25.931,000 --> 0:05:28,000
Las neuronas propagan el impulso lentamente a lo largo de los axones,

99
0:05:29.238,000 --> 0:05:3,000
a máximo 100 metros por segundo.

100
0:05:31.228,000 --> 0:05:34,000
Pero en las computadoras, las señales pueden viajar a la velocidad de la luz.

101
0:05:35.079,000 --> 0:05:36,000
También hay limitaciones de tamaño,

102
0:05:36.948,000 --> 0:05:39,000
como el cerebro humano que tiene que encajar dentro del cráneo,

103
0:05:39.975,000 --> 0:05:43,000
pero una computadora puede ser del tamaño de un almacén o aún más grande.

104
0:05:44.736,000 --> 0:05:47,000
Así que el potencial de la máquina superinteligente

105
0:05:48.025,000 --> 0:05:5,000
permanece latente en la materia,

106
0:05:50.335,000 --> 0:05:55,000
al igual que el poder atómico a lo largo de toda la historia

107
0:05:56.047,000 --> 0:06:,000
que esperó pacientemente hasta 1945.

108
0:06:00.452,000 --> 0:06:03,000
De cara a este siglo los científicos pueden aprender a despertar

109
0:06:03.61,000 --> 0:06:05,000
el poder de la inteligencia artificial

110
0:06:05.818,000 --> 0:06:09,000
y creo que podríamos ser testigos de una explosión de inteligencia.

111
0:06:10.406,000 --> 0:06:14,000
Cuando la mayoría de la gente piensa en lo inteligente o lo tonto

112
0:06:14.421,000 --> 0:06:16,000
creo que tienen en mente una imagen más o menos así.

113
0:06:17.386,000 --> 0:06:19,000
En un extremo tenemos al tonto del pueblo,

114
0:06:19.984,000 --> 0:06:21,000
y lejos en el otro extremo,

115
0:06:22.459,000 --> 0:06:26,000
tenemos a Ed Witten o a Albert Einstein, o quien sea su gurú favorito.

116
0:06:27.223,000 --> 0:06:3,000
Pero creo que desde el punto de vista de la inteligencia artificial,

117
0:06:31.057,000 --> 0:06:34,000
lo más probable es que la imagen real sea la siguiente:

118
0:06:35.258,000 --> 0:06:38,000
Se empieza en este punto aquí, en ausencia de inteligencia

119
0:06:38.636,000 --> 0:06:41,000
y luego, después de muchos, muchos años de trabajo muy arduo,

120
0:06:41.647,000 --> 0:06:44,000
quizá finalmente lleguemos al nivel intelectual de un ratón,

121
0:06:45.491,000 --> 0:06:47,000
algo que puede navegar entornos desordenados

122
0:06:47.921,000 --> 0:06:48,000
igual que un ratón.

123
0:06:49.908,000 --> 0:06:51,000
Y luego, después de muchos, muchos más años

124
0:06:52.115,000 --> 0:06:54,000
de trabajo muy arduo, de mucha inversión,

125
0:06:54.221,000 --> 0:06:58,000
tal vez alcancemos el nivel de inteligencia de un chimpancé.

126
0:06:58.86,000 --> 0:07:01,000
Y luego, después de más años de trabajo muy, muy arduo

127
0:07:02.083,000 --> 0:07:04,000
alcancemos la inteligencia artificial del tonto del pueblo.

128
0:07:04.994,000 --> 0:07:07,000
Un poco más tarde, estaremos más allá de Ed Witten.

129
0:07:08.105,000 --> 0:07:11,000
El tren del progreso no se detiene en la estación de los Humanos.

130
0:07:11.225,000 --> 0:07:14,000
Es probable que más bien, pase volando.

131
0:07:14.247,000 --> 0:07:15,000
Esto tiene profundas consecuencias,

132
0:07:16.231,000 --> 0:07:19,000
especialmente si se trata de poder.

133
0:07:20.093,000 --> 0:07:22,000
Por ejemplo, los chimpancés son fuertes.

134
0:07:22.292,000 --> 0:07:26,000
Un chimpancé es dos veces más fuerte y en mejor forma física que un hombre

135
0:07:27.214,000 --> 0:07:31,000
y, sin embargo, el destino de Kanzi y sus amigos depende mucho más

136
0:07:31.828,000 --> 0:07:35,000
de lo que hacemos los humanos que de lo que ellos mismos hacen.

137
0:07:37.209,000 --> 0:07:39,000
Una vez que hay superinteligencia,

138
0:07:39.542,000 --> 0:07:42,000
el destino de la humanidad dependerá de lo que haga la superinteligencia.

139
0:07:44.451,000 --> 0:07:45,000
Piensen en esto:

140
0:07:45.508,000 --> 0:07:47,000
la máquina inteligente es el último invento

141
0:07:48.259,000 --> 0:07:5,000
que la humanidad jamás tendrá que realizar.

142
0:07:50.552,000 --> 0:07:52,000
Las máquinas serán entonces mejores inventores que nosotros,

143
0:07:53.525,000 --> 0:07:55,000
y lo harán a escala de tiempo digital

144
0:07:56.083,000 --> 0:08:,000
lo que significa básicamente que acelerarán la cercanía al futuro.

145
0:08:00.966,000 --> 0:08:04,000
Piensen en todas las tecnologías que tal vez, en su opinión,

146
0:08:05.042,000 --> 0:08:07,000
los humanos pueden desarrollar con el paso del tiempo:

147
0:08:07.586,000 --> 0:08:1,000
tratamientos para el envejecimiento, la colonización del espacio,

148
0:08:10.711,000 --> 0:08:13,000
nanobots autoreplicantes, mentes integradas en las computadoras,

149
0:08:14.311,000 --> 0:08:16,000
todo tipo de ciencia-ficción

150
0:08:16.47,000 --> 0:08:18,000
y sin embargo en consonancia con las leyes de la física.

151
0:08:19.207,000 --> 0:08:21,000
Todo esta superinteligencia podría desarrollarse

152
0:08:21.586,000 --> 0:08:22,000
y posiblemente con bastante rapidez.

153
0:08:24.449,000 --> 0:08:27,000
Ahora, una superinteligencia con tanta madurez tecnológica

154
0:08:28.007,000 --> 0:08:3,000
sería extremadamente poderosa,

155
0:08:30.186,000 --> 0:08:34,000
y con la excepción de algunos casos sería capaz de conseguir lo que quiere.

156
0:08:34.751,000 --> 0:08:38,000
Nuestro futuro se determinaría por las preferencias de esta IA.

157
0:08:41.855,000 --> 0:08:44,000
Y una buena pregunta es ¿cuáles son esas preferencias?

158
0:08:46.244,000 --> 0:08:47,000
Aquí se vuelve más complicado.

159
0:08:48.013,000 --> 0:08:49,000
Para avanzar con esto,

160
0:08:49.448,000 --> 0:08:52,000
debemos en primer lugar evitar el antropomorfismo.

161
0:08:53.933,000 --> 0:08:57,000
Y esto es irónico porque cada artículo de prensa sobre el futuro de la IA

162
0:08:58.655,000 --> 0:09:,000
presenta una imagen como esta:

163
0:09:02.7,000 --> 0:09:05,000
Así que creo que tenemos que pensar de manera más abstracta,

164
0:09:06.414,000 --> 0:09:08,000
no según escenarios entretenidos de Hollywood.

165
0:09:09.209,000 --> 0:09:12,000
Tenemos que pensar en la inteligencia como un proceso de optimización

166
0:09:12.836,000 --> 0:09:17,000
un proceso que dirige el futuro hacia un conjunto especifico de configuraciones.

167
0:09:18.47,000 --> 0:09:21,000
Un superinteligencia es un proceso de optimización realmente potente.

168
0:09:21.981,000 --> 0:09:24,000
Es muy bueno en el uso de recursos disponibles

169
0:09:25.038,000 --> 0:09:27,000
para lograr un estado óptimo y alcanzar su objetivo.

170
0:09:28.447,000 --> 0:09:3,000
Esto significa que no hay ningún vínculo necesario

171
0:09:31.119,000 --> 0:09:33,000
entre ser muy inteligente en este sentido,

172
0:09:33.853,000 --> 0:09:37,000
y tener una meta que para los humanos vale la pena o es significativa.

173
0:09:39.334,000 --> 0:09:42,000
Por ejemplo, la IA podría tener el objetivo de hacer sonreír a los humanos.

174
0:09:43.125,000 --> 0:09:46,000
Cuando la IA está en desarrollo, realiza acciones entretenidas

175
0:09:46.126,000 --> 0:09:48,000
para hacer sonreír a su usuario.

176
0:09:48.614,000 --> 0:09:5,000
Cuando la IA se vuelve superinteligente,

177
0:09:51.031,000 --> 0:09:54,000
se da cuenta de que hay una manera más eficaz para lograr su objetivo:

178
0:09:54.554,000 --> 0:09:55,000
tomar el control del mundo

179
0:09:56.476,000 --> 0:09:59,000
e introducir electrodos en los músculos faciales de la gente

180
0:09:59.638,000 --> 0:10:01,000
para provocar sonrisas constantes y radiantes.

181
0:10:02.579,000 --> 0:10:03,000
Otro ejemplo,

182
0:10:03.614,000 --> 0:10:06,000
supongamos que le damos el objetivo de resolver un problema matemático difícil.

183
0:10:07.417,000 --> 0:10:08,000
Cuando la IA se vuelve superinteligente,

184
0:10:09.384,000 --> 0:10:12,000
se da cuenta de que la forma más eficaz para conseguir la solución a este problema

185
0:10:13.335,000 --> 0:10:16,000
es mediante la transformación del planeta en un computador gigante,

186
0:10:16.485,000 --> 0:10:18,000
para aumentar su capacidad de pensar.

187
0:10:18.731,000 --> 0:10:2,000
Y tengan en cuenta que esto da a la IA una razón instrumental

188
0:10:21.725,000 --> 0:10:23,000
para hacer cosas que nosotros no podemos aprobar.

189
0:10:24.111,000 --> 0:10:26,000
Los seres humanos se convierten en una amenaza,

190
0:10:26.396,000 --> 0:10:28,000
ya que podríamos evitar que el problema se resuelva.

191
0:10:29.127,000 --> 0:10:32,000
Por supuesto, las cosas no tienen necesariamente que pasar de esa manera:

192
0:10:32.715,000 --> 0:10:33,000
son ejemplos de muestra.

193
0:10:34.454,000 --> 0:10:35,000
Pero lo importante,

194
0:10:36.393,000 --> 0:10:38,000
si crean un proceso de optimización muy potente,

195
0:10:39.259,000 --> 0:10:41,000
optimizado para lograr el objetivo X,

196
0:10:41.501,000 --> 0:10:43,000
más vale asegurarse de que la definición de X

197
0:10:43.776,000 --> 0:10:45,000
incluye todo lo que importa.

198
0:10:46.835,000 --> 0:10:5,000
Es una moraleja que también se enseña a través de varios mitos.

199
0:10:51.219,000 --> 0:10:56,000
El rey Midas deseaba convertir en oro todo lo que tocaba.

200
0:10:56.517,000 --> 0:10:58,000
Toca a su hija y ella se convierte en oro.

201
0:10:59.378,000 --> 0:11:01,000
Toca su comida, se convierte en oro.

202
0:11:01.931,000 --> 0:11:03,000
Es un ejemplo relevante

203
0:11:04.52,000 --> 0:11:07,000
no solo de una metáfora de la codicia sino como ilustración

204
0:11:07.845,000 --> 0:11:1,000
de lo que sucede si crean un proceso de optimización potente

205
0:11:11.322,000 --> 0:11:15,000
pero le encomiendan objetivos incomprensibles o sin claridad.

206
0:11:16.111,000 --> 0:11:17,000
Uno puede pensar:

207
0:11:17.454,000 --> 0:11:21,000
"Si una computadora empieza a poner electrodos en la cara de la gente

208
0:11:21.542,000 --> 0:11:23,000
bastaría simplemente con apagarla.

209
0:11:24.555,000 --> 0:11:26,000
En primer lugar, puede que no sea tan sencillo

210
0:11:27.133,000 --> 0:11:29,000
si somos dependientes del sistema

211
0:11:29.895,000 --> 0:11:31,000
por ejemplo: ¿dónde está el botón para apagar Internet?

212
0:11:32.627,000 --> 0:11:34,000
En segundo lugar, ¿por qué los chimpancés

213
0:11:35.375,000 --> 0:11:38,000
no tienen acceso al mismo interruptor de la humanidad, o los neandertales?

214
0:11:39.335,000 --> 0:11:41,000
Sin duda razones tendrían.

215
0:11:41.964,000 --> 0:11:43,000
Tenemos un interruptor de apagado, por ejemplo, aquí mismo.

216
0:11:44.759,000 --> 0:11:45,000
(Finge estrangulación)

217
0:11:46.313,000 --> 0:11:48,000
La razón es que somos un adversario inteligente;

218
0:11:49.238,000 --> 0:11:51,000
podemos anticipar amenazas y planificar en consecuencia,

219
0:11:51.896,000 --> 0:11:53,000
pero también podría hacerlo un agente superinteligente,

220
0:11:54.47,000 --> 0:11:57,000
y mucho mejor que nosotros.

221
0:11:57.724,000 --> 0:11:59,000
El tema es que no debemos confiar

222
0:12:02.661,000 --> 0:12:04,000
que podemos controlar esta situación.

223
0:12:04.911,000 --> 0:12:07,000
Y podríamos tratar de hacer nuestro trabajo un poco más fácil digamos,

224
0:12:08.358,000 --> 0:12:11,000
poniendo a la IA en una caja, en un entorno de software seguro,

225
0:12:11.768,000 --> 0:12:13,000
una simulación de realidad virtual de la que no pueda escapar.

226
0:12:14.766,000 --> 0:12:18,000
Pero, ¿cómo podemos estar seguros de que la IA no encontrará un error?

227
0:12:18.918,000 --> 0:12:21,000
Dado que incluso los hackers humanos encuentran errores todo el tiempo,

228
0:12:22.381,000 --> 0:12:24,000
yo diría que probablemente, no podemos estar muy seguros.

229
0:12:26.25,000 --> 0:12:3,000
Así que desconectamos el cable ethernet para crear un espacio vacío,

230
0:12:30.621,000 --> 0:12:32,000
pero una vez más, al igual que los hackers humanos,

231
0:12:33.453,000 --> 0:12:36,000
podrían superar estos espacios usando la ingeniería social.

232
0:12:36.814,000 --> 0:12:37,000
Ahora mismo, mientras hablo,

233
0:12:38.163,000 --> 0:12:4,000
estoy seguro de que algún empleado, en algún lugar

234
0:12:40.582,000 --> 0:12:43,000
ha sido convencido para revelar los detalles de su cuenta

235
0:12:43.834,000 --> 0:12:45,000
por alguien que dice ser del departamento de IT.

236
0:12:46.574,000 --> 0:12:48,000
Otros escenarios creativos también son posibles,

237
0:12:48.851,000 --> 0:12:49,000
por ejemplo si Ud. es la IA,

238
0:12:50.206,000 --> 0:12:53,000
puede hacer cambios en los electrodos de su circuito interno de seguridad

239
0:12:53.718,000 --> 0:12:56,000
para crear ondas de radio y usarlas para comunicarse.

240
0:12:57.01,000 --> 0:12:59,000
O tal vez fingir un mal funcionamiento,

241
0:12:59.254,000 --> 0:13:02,000
y cuando los programadores lo abren para entender qué está mal,

242
0:13:02.862,000 --> 0:13:03,000
al mirar el código fuente, ¡pum!

243
0:13:04.799,000 --> 0:13:06,000
ya empieza a manipular.

244
0:13:07.245,000 --> 0:13:1,000
O podría idear un programa tecnológico realmente ingenioso,

245
0:13:10.675,000 --> 0:13:11,000
y cuando lo implementamos,

246
0:13:12.183,000 --> 0:13:16,000
tener efectos secundarios ocultos planeados por la IA.

247
0:13:16.539,000 --> 0:13:19,000
No debemos confiar en nuestra capacidad

248
0:13:20.002,000 --> 0:13:23,000
para mantener un genio superinteligente encerrado en su lámpara para siempre.

249
0:13:23.81,000 --> 0:13:25,000
Tarde o temprano, saldrá.

250
0:13:27.034,000 --> 0:13:32,000
Creo que la solución es averiguar cómo crear una IA superinteligente

251
0:13:32.227,000 --> 0:13:36,000
para que incluso si, o cuando se escape, sea todavía segura

252
0:13:36.251,000 --> 0:13:4,000
para que fundamentalmente esté de nuestro lado y comparta nuestros valores.

253
0:13:40.398,000 --> 0:13:43,000
No veo cómo evitar este problema difícil.

254
0:13:44.557,000 --> 0:13:47,000
En realidad soy bastante optimista de que este problema pueda ser resuelto.

255
0:13:48.544,000 --> 0:13:51,000
No tendríamos que escribir una larga lista de todo lo que nos importa,

256
0:13:52.294,000 --> 0:13:55,000
o, peor aún, codificarla en algún lenguaje informático

257
0:13:55.938,000 --> 0:13:56,000
como C++ o Python,

258
0:13:57.391,000 --> 0:13:59,000
sería una reto imposible.

259
0:14:00.158,000 --> 0:14:04,000
A cambio, crearíamos una IA que use su inteligencia

260
0:14:04.455,000 --> 0:14:06,000
para aprender lo que valoramos,

261
0:14:07.226,000 --> 0:14:09,000
y su sistema integrado de motivación sería diseñado

262
0:14:11.926,000 --> 0:14:12,000
para defender nuestros valores

263
0:14:13.845,000 --> 0:14:16,000
y realizar acciones que se ajusten a ellos.

264
0:14:17.738,000 --> 0:14:2,000
Así que usaríamos su inteligencia tanto como fuera posible

265
0:14:21.152,000 --> 0:14:23,000
para resolver el problema de la atribución de valores.

266
0:14:24.727,000 --> 0:14:25,000
Esto puede suceder,

267
0:14:26.239,000 --> 0:14:29,000
y el resultado podría ser muy bueno para la humanidad.

268
0:14:29.835,000 --> 0:14:32,000
Pero no sucede automáticamente.

269
0:14:33.792,000 --> 0:14:35,000
Las condiciones iniciales para la explosión de la inteligencia

270
0:14:36.79,000 --> 0:14:38,000
necesitan ser perfectamente definidas

271
0:14:39.653,000 --> 0:14:42,000
si queremos contar con una detonación controlada.

272
0:14:43.103,000 --> 0:14:45,000
Los valores de la IA tienen que coincidir con los nuestros

273
0:14:45.921,000 --> 0:14:46,000
no solo en el ámbito familiar,

274
0:14:47.491,000 --> 0:14:49,000
donde podemos comprobar fácilmente cómo se comporta,

275
0:14:49.999,000 --> 0:14:52,000
sino también en todos los nuevos contextos

276
0:14:53.233,000 --> 0:14:55,000
donde la IA podría encontrarse en un futuro indefinido.

277
0:14:55.88,000 --> 0:14:58,000
Y también hay algunas cuestiones esotéricas que habría que resolver:

278
0:14:59.568,000 --> 0:15:01,000
los detalles exactos de su teoría de la decisión,

279
0:15:01.946,000 --> 0:15:03,000
cómo manejar la incertidumbre lógica, etc.

280
0:15:05.33,000 --> 0:15:07,000
Así que los problemas técnicos que hay que resolver

281
0:15:07.959,000 --> 0:15:09,000
para hacer este trabajo parecen muy difíciles

282
0:15:10.162,000 --> 0:15:12,000
--no tan difíciles como crear una IA superinteligente--

283
0:15:12.923,000 --> 0:15:13,000
pero bastante difíciles.

284
0:15:15.793,000 --> 0:15:16,000
Este es la preocupación:

285
0:15:17.501,000 --> 0:15:2,000
crear una IA superinteligente es un reto muy difícil

286
0:15:22.172,000 --> 0:15:24,000
y crear una que sea segura

287
0:15:24.72,000 --> 0:15:26,000
implica desafíos adicionales.

288
0:15:28.586,000 --> 0:15:31,000
El riesgo es si alguien encuentra la manera de superar el primer reto

289
0:15:32.093,000 --> 0:15:36,000
sin resolver el otro desafío de garantizar la máxima seguridad.

290
0:15:37.375,000 --> 0:15:4,000
Así que creo que deberíamos encontrar una solución

291
0:15:40.706,000 --> 0:15:42,000
al problema del control por adelantado,

292
0:15:43.528,000 --> 0:15:45,000
de modo que esté disponible para cuando sea necesario.

293
0:15:46.768,000 --> 0:15:48,000
Puede ser que no podamos resolver por completo

294
0:15:49.138,000 --> 0:15:5,000
el problema del control de antemano

295
0:15:50.865,000 --> 0:15:53,000
porque tal vez, algunos elementos solo pueden ser desarrollados

296
0:15:53.959,000 --> 0:15:56,000
después de reunir los detalles técnicos de la IA en cuestión.

297
0:15:57.296,000 --> 0:16:,000
Pero cuanto antes solucionemos el problema del control,

298
0:16:00.676,000 --> 0:16:01,000
mayores serán las probabilidades

299
0:16:02.218,000 --> 0:16:04,000
de que la transición a la era de las máquinas inteligentes

300
0:16:05.056,000 --> 0:16:06,000
vaya bien.

301
0:16:06.306,000 --> 0:16:1,000
Esto me parece algo digno de hacer

302
0:16:10.95,000 --> 0:16:13,000
y puedo imaginar que si las cosas salen bien,

303
0:16:14.282,000 --> 0:16:18,000
la gente en un millón de años discutirá nuestro siglo

304
0:16:18.94,000 --> 0:16:21,000
y dirá que posiblemente lo único que hicimos bien y mereció la pena

305
0:16:22.587,000 --> 0:16:23,000
fue superar con éxito este reto.

306
0:16:24.509,000 --> 0:16:25,000
Gracias.

307
0:16:25.788,000 --> 0:16:26,000
(Aplausos)

