1
0:00:,000 --> 0:00:07,000
Traducteur: Julien Souliers Relecteur: eric vautier

2
0:00:12.87,000 --> 0:00:15,000
Combien de décisions ont été prises sur vous aujourd'hui,

3
0:00:16.667,000 --> 0:00:18,000
cette semaine ou cette année,

4
0:00:19.292,000 --> 0:00:2,000
par intelligence artificielle ?

5
0:00:22.958,000 --> 0:00:23,000
Je gagne ma vie grâce à l'IA

6
0:00:24.667,000 --> 0:00:27,000
alors, spoiler, je suis un peu une geek.

7
0:00:27.708,000 --> 0:00:29,000
Et parce que je suis un peu une geek,

8
0:00:30.125,000 --> 0:00:32,000
chaque fois qu'il y a un nouvel article

9
0:00:32.5,000 --> 0:00:35,000
sur comment l'intelligence artificielle vole nos emplois,

10
0:00:35.958,000 --> 0:00:39,000
ou sur les robots obtenant la citoyenneté dans un pays,

11
0:00:40.167,000 --> 0:00:43,000
Je suis celle à qui mes amis et abonnés envoient un message

12
0:00:43.333,000 --> 0:00:44,000
paniqués par l'avenir.

13
0:00:45.833,000 --> 0:00:47,000
Nous voyons ça partout.

14
0:00:47.958,000 --> 0:00:51,000
Cette peur des médias que les robots nous dominent.

15
0:00:52.875,000 --> 0:00:53,000
Nous pourrions blâmer Hollywood pour ça.

16
0:00:56.125,000 --> 0:01:,000
Mais ce n'est pas le problème sur lequel nous devrions nous concentrer.

17
0:01:01.25,000 --> 0:01:04,000
Il y a un danger plus pressant, un plus grand risque avec l'IA,

18
0:01:04.917,000 --> 0:01:05,000
que nous devons résoudre d'abord.

19
0:01:07.417,000 --> 0:01:09,000
Nous revenons donc à cette question :

20
0:01:09.75,000 --> 0:01:13,000
Combien de décisions ont été prises sur vous aujourd'hui par l'IA ?

21
0:01:15.792,000 --> 0:01:16,000
Et combien de ces décisions

22
0:01:17.792,000 --> 0:01:21,000
étaient basées sur votre sexe, votre origine ou votre milieu ?

23
0:01:24.5,000 --> 0:01:26,000
Les algorithmes sont utilisés tout le temps

24
0:01:27.292,000 --> 0:01:3,000
pour décider de qui nous sommes et ce que nous voulons.

25
0:01:32.208,000 --> 0:01:35,000
Quelques femmes dans cette salle savent de quoi je parle

26
0:01:35.875,000 --> 0:01:38,000
si vous avez vu passer des pubs pour des tests de grossesse sur Youtube

27
0:01:39.667,000 --> 0:01:41,000
environ 1 000 fois.

28
0:01:41.75,000 --> 0:01:43,000
Ou vous avez vu des annonces pour des cliniques de fertilité

29
0:01:44.625,000 --> 0:01:46,000
sur votre mur Facebook.

30
0:01:47.625,000 --> 0:01:49,000
Ou comme moi, des agences de mariage indiennes.

31
0:01:50.042,000 --> 0:01:51,000
(Rires)

32
0:01:51.333,000 --> 0:01:53,000
Mais l'IA n'est pas juste utilisée pour prendre des décisions

33
0:01:54.333,000 --> 0:01:56,000
sur les produits que nous consommons

34
0:01:56.958,000 --> 0:01:58,000
ou indiquer ce que nous voulons binge watcher.

35
0:02:01.042,000 --> 0:02:06,000
Que penseriez-vous de quelqu'un qui penserait :

36
0:02:06.25,000 --> 0:02:07,000
« Il y a moins de chances qu'une personne

37
0:02:08.214,000 --> 0:02:12,000
noire ou latino rembourse son prêt à temps par rapport à une personne blanche. »

38
0:02:13.542,000 --> 0:02:15,000
«John est un meilleur programmeur

39
0:02:16.375,000 --> 0:02:17,000
que Mary. »

40
0:02:19.25,000 --> 0:02:24,000
« Un homme noir est plus susceptible de récidiver qu'un homme blanc. »

41
0:02:26.958,000 --> 0:02:27,000
Vous pensez probablement :

42
0:02:28.25,000 --> 0:02:31,000
« Waouh, cette personne est sexiste et raciste, » n'est-ce pas ?

43
0:02:33,000 --> 0:02:37,000
Pourtant ce sont des décisions que des IA ont prises très récemment,

44
0:02:37.875,000 --> 0:02:39,000
basées sur les biais qu'elle a appris de nous,

45
0:02:40.833,000 --> 0:02:41,000
des humains.

46
0:02:43.75,000 --> 0:02:47,000
L'IA est utilisée pour décider de si vous obtenez cet entretien d'embauche,

47
0:02:48.583,000 --> 0:02:5,000
de combien vous payez votre assurance auto,

48
0:02:51,000 --> 0:02:52,000
de votre cote de solvabilité,

49
0:02:52.917,000 --> 0:02:55,000
et même de l'appréciation de votre performance annuelle.

50
0:02:57.083,000 --> 0:03:,000
Mais ces décisions sont prises en faisant

51
0:03:00.25,000 --> 0:03:05,000
des hypothèses basées sur notre nom, notre ethnie, notre sexe et notre âge.

52
0:03:08.25,000 --> 0:03:1,000
Comment ça se passe?

53
0:03:10.542,000 --> 0:03:13,000
Prenez une IA aidant un DRH

54
0:03:14.083,000 --> 0:03:16,000
à trouver le prochain leader high-tech de la société.

55
0:03:16.958,000 --> 0:03:19,000
Jusqu'à présent, le gérant n'a embauché que des hommes.

56
0:03:20.083,000 --> 0:03:24,000
L'IA apprend donc que les hommes sont plus souvent programmeurs que les femmes.

57
0:03:25.542,000 --> 0:03:27,000
Et de là il n'y a qu'un pas vers :

58
0:03:28.458,000 --> 0:03:3,000
« Les hommes programment mieux que les femmes. »

59
0:03:31.417,000 --> 0:03:34,000
Nous avons introduit nos propres biais dans l'IA.

60
0:03:35.167,000 --> 0:03:38,000
Et ensuite, elle élimine les candidats féminins.

61
0:03:40.917,000 --> 0:03:43,000
Si un DRH humain faisait ça,

62
0:03:43.958,000 --> 0:03:45,000
nous serions scandalisés, ça ne passerait pas.

63
0:03:46.333,000 --> 0:03:49,000
Les discriminations sexistes ne sont pas acceptables.

64
0:03:49.833,000 --> 0:03:53,000
Et pourtant, l'IA est au-dessus des lois,

65
0:03:54.375,000 --> 0:03:56,000
parce que la décision est prise par une machine.

66
0:03:57.833,000 --> 0:03:58,000
Ce n'est pas tout.

67
0:03:59.375,000 --> 0:04:03,000
Nous renforçons également nos biais à travers notre interaction avec l'IA.

68
0:04:04.917,000 --> 0:04:09,000
Combien de fois avez-vous utilisé un assistant vocal comme Siri ou Alexa ?

69
0:04:10.917,000 --> 0:04:12,000
Ils ont tous deux choses en commun :

70
0:04:13.5,000 --> 0:04:16,000
ils ne prononcent jamais correctement mon nom,

71
0:04:16.625,000 --> 0:04:18,000
et deuxièmement, ce sont tous des femmes.

72
0:04:20.417,000 --> 0:04:22,000
Ils ont été créés pour être d'obéissants serviteurs,

73
0:04:23.208,000 --> 0:04:26,000
allumant et éteignant la lumière, passant des commandes.

74
0:04:27.125,000 --> 0:04:3,000
Il y a aussi des IA homme mais ils ont généralement plus de pouvoirs,

75
0:04:30.458,000 --> 0:04:33,000
par exemple, IBM Watson qui prend des décisions commerciales,

76
0:04:33.541,000 --> 0:04:36,000
Salesforce Einstein ou ROSS, le robot avocat.

77
0:04:38.208,000 --> 0:04:42,000
Ainsi, même ces pauvres robots souffrent du sexisme dans le milieu du travail.

78
0:04:42.292,000 --> 0:04:43,000
(Rires)

79
0:04:44.542,000 --> 0:04:46,000
Pensez à comment ces deux points combinés

80
0:04:47.417,000 --> 0:04:52,000
affectent un enfant grandissant dans un monde baignant dans l'IA.

81
0:04:52.75,000 --> 0:04:54,000
Imaginez qu'il fasse une recherche pour un projet à l'école

82
0:04:55.708,000 --> 0:04:58,000
et qu'il tape sur Google, « images de PDG ».

83
0:04:58.75,000 --> 0:05:,000
L'algorithme ne lui montre que des hommes.

84
0:05:01.667,000 --> 0:05:03,000
Ensuite, il tape « assistant personnel ».

85
0:05:04.25,000 --> 0:05:07,000
Comme vous pouvez l'imaginez, il n'y a pratiquement que des femmes.

86
0:05:07.708,000 --> 0:05:1,000
Ensuite, il veut mettre de la musique ou commander à manger,

87
0:05:11.333,000 --> 0:05:17,000
et pour cela il aboie des ordres à une obéissante voix féminine.

88
0:05:19.542,000 --> 0:05:24,000
Ces technologies sont créées par certains de nos plus brillants cerveaux.

89
0:05:24.875,000 --> 0:05:28,000
Ils pouvaient créer ces technologies exactement comme ils le souhaitaient.

90
0:05:29.083,000 --> 0:05:34,000
Et ils ont choisi de les créer dans le style secrétaire des « Mad Men ».

91
0:05:34.792,000 --> 0:05:35,000
Super !

92
0:05:36.958,000 --> 0:05:37,000
Ne vous inquiétez pas,

93
0:05:38.292,000 --> 0:05:4,000
je ne vais pas finir en vous disant que

94
0:05:40.375,000 --> 0:05:43,000
nous allons vers un monde dirigé par des machines sexistes et racistes.

95
0:05:44.792,000 --> 0:05:49,000
La bonne nouvelle est que les IA sont entièrement sous notre contrôle.

96
0:05:51.333,000 --> 0:05:55,000
Nous pouvons leur enseigner les bonnes valeurs, l'éthique.

97
0:05:56.167,000 --> 0:05:58,000
Il y a trois choses que nous pouvons faire.

98
0:05:58.375,000 --> 0:06:01,000
Premièrement, nous pouvons avoir conscience de nos propres biais

99
0:06:01.75,000 --> 0:06:03,000
et des biais des machines qui nous entourent.

100
0:06:04.5,000 --> 0:06:08,000
Ensuite, nous pouvons nous assurer que des équipes mixtes construisent les IA.

101
0:06:09.042,000 --> 0:06:13,000
Et enfin, les IA doivent apprendre à partir d'expériences variées.

102
0:06:14.875,000 --> 0:06:17,000
Je peux parler des deux premiers points par expérience personnelle.

103
0:06:18.208,000 --> 0:06:2,000
Quand vous travaillez dans la technologie

104
0:06:20.233,000 --> 0:06:22,000
sans ressembler à Mark Zuckerberg ou Elon Musk,

105
0:06:23.083,000 --> 0:06:26,000
votre vie est un peu difficile, vos compétences sont remises en question.

106
0:06:27.875,000 --> 0:06:28,000
Voici un exemple.

107
0:06:29.292,000 --> 0:06:32,000
Comme la plupart des programmeurs, je participe sur des forums tech en ligne

108
0:06:33.042,000 --> 0:06:36,000
et partage mes connaissances pour aider les autres.

109
0:06:36.292,000 --> 0:06:37,000
Et j'ai découvert

110
0:06:37.625,000 --> 0:06:4,000
qu'en me connectant avec ma vraie photo et mon vrai nom,

111
0:06:41.625,000 --> 0:06:45,000
j'ai tendance à recevoir des questions ou commentaires comme ceux-ci :

112
0:06:46.25,000 --> 0:06:49,000
« Tu crois être qualifiée pour parler d'IA ? »

113
0:06:50.458,000 --> 0:06:53,000
« Qu'est-ce qui te fait penser que tu t'y connais en machine learning ? »

114
0:06:53.958,000 --> 0:06:56,000
Je me suis donc créé un nouveau profil,

115
0:06:57.417,000 --> 0:07:01,000
avec à la place de ma photo, un chat avec un jet pack.

116
0:07:02.292,000 --> 0:07:04,000
J'ai choisi un nom qui ne révélait pas mon genre.

117
0:07:05.917,000 --> 0:07:07,000
Vous voyez où je veux en venir, non ?

118
0:07:08.667,000 --> 0:07:14,000
Cette foi, je n'ai reçu aucun commentaire condescendant sur mes capacités

119
0:07:15.083,000 --> 0:07:18,000
et je pouvais travailler correctement.

120
0:07:19.5,000 --> 0:07:2,000
Ça craint, les gars.

121
0:07:21.375,000 --> 0:07:23,000
Je construis des robots depuis mes 15 ans,

122
0:07:23.875,000 --> 0:07:25,000
j'ai plusieurs diplômes en informatique,

123
0:07:26.167,000 --> 0:07:28,000
et pourtant, je dois cacher mon genre

124
0:07:28.625,000 --> 0:07:3,000
pour que mon travail soit pris au sérieux.

125
0:07:31.875,000 --> 0:07:32,000
Que se passe-t-il ?

126
0:07:33.792,000 --> 0:07:36,000
Les hommes sont-ils juste meilleurs que les femmes en technologie ?

127
0:07:37.917,000 --> 0:07:38,000
Une autre étude a trouvé

128
0:07:39.5,000 --> 0:07:43,000
que quand les codeurs femmes cachaient, comme moi, leur genre sur une plateforme,

129
0:07:44.458,000 --> 0:07:47,000
leur code était accepté 4% de plus que celui des hommes.

130
0:07:48.542,000 --> 0:07:5,000
Donc ce n'est pas un problème de talent.

131
0:07:51.958,000 --> 0:07:53,000
C'est un problème d'élitisme dans l'IA qui dit

132
0:07:54.875,000 --> 0:07:56,000
qu'un développeur doit ressembler à une certaine personne.

133
0:07:59.375,000 --> 0:08:02,000
Ce que nous devons faire pour rendre l'IA meilleure,

134
0:08:02.5,000 --> 0:08:05,000
c'est rassembler des gens de toutes origines.

135
0:08:06.542,000 --> 0:08:08,000
Nous avons besoin de gens pouvant écrire des histoires

136
0:08:09.125,000 --> 0:08:11,000
pour créer des personnalités aux IA,

137
0:08:12.208,000 --> 0:08:14,000
pouvant résoudre des problèmes,

138
0:08:15.125,000 --> 0:08:18,000
pouvant faire face à différents défis

139
0:08:18.917,000 --> 0:08:23,000
et de personnes pouvant nous indiquer les réels problèmes à résoudre

140
0:08:24.292,000 --> 0:08:27,000
et les stratégies que les technologies peuvent mettre en place.

141
0:08:29.833,000 --> 0:08:32,000
Quand des personnes de différentes origines se rassemblent,

142
0:08:33.583,000 --> 0:08:35,000
quand nous construisons correctement,

143
0:08:35.75,000 --> 0:08:37,000
les possibilités sont infinies.

144
0:08:38.75,000 --> 0:08:41,000
C'est là-dessus que je veux finir.

145
0:08:42.083,000 --> 0:08:46,000
Moins de robots racistes, de machines qui vont nous voler nos emplois,

146
0:08:46.332,000 --> 0:08:49,000
et plus de ce que les technologies peuvent réellement faire.

147
0:08:50.292,000 --> 0:08:53,000
Alors oui, une partie de l'énergie dans le monde de l'IA,

148
0:08:53.75,000 --> 0:08:54,000
des technologies,

149
0:08:55.167,000 --> 0:08:59,000
est dirigée pour décider de quelles publicités vous sont proposées.

150
0:08:59.458,000 --> 0:09:04,000
Mais une grosse partie est orientée pour rendre le monde meilleur.

151
0:09:05.5,000 --> 0:09:08,000
Imaginez une femme enceinte en République Démocratique du Congo,

152
0:09:09.292,000 --> 0:09:13,000
qui doit marcher 17 heures pour atteindre la clinique prénatale la plus proche

153
0:09:13.5,000 --> 0:09:14,000
et avoir une consultation.

154
0:09:15.375,000 --> 0:09:17,000
Et si elle pouvait avoir un diagnostic sur son téléphone ?

155
0:09:19.75,000 --> 0:09:2,000
Imaginez ce que l'IA pourrait faire

156
0:09:21.583,000 --> 0:09:23,000
pour ces 1 femmes sur 3 en Afrique du Sud

157
0:09:24.333,000 --> 0:09:26,000
qui subissent des violences domestiques.

158
0:09:27.083,000 --> 0:09:29,000
S'il est dangereux de dénoncer à voix haute,

159
0:09:29.833,000 --> 0:09:31,000
elles pourraient lancer une alarme grâce à une IA,

160
0:09:32.333,000 --> 0:09:34,000
ou avoir une aide financière et judiciaire.

161
0:09:35.958,000 --> 0:09:4,000
Ce sont de vrais projets sur lesquels des gens, moi y compris,

162
0:09:41,000 --> 0:09:43,000
travaillent en ce moment, avec l'IA.

163
0:09:45.542,000 --> 0:09:47,000
Je suis certaine qu'il y aura dans les prochains jours,

164
0:09:48.257,000 --> 0:09:51,000
une histoire sur le risque pour notre existence,

165
0:09:51.875,000 --> 0:09:53,000
sur les robots nous dominant et volant nos emplois.

166
0:09:54.333,000 --> 0:09:55,000
(Rires)

167
0:09:55.375,000 --> 0:09:57,000
Quand quelque chose comme ça se produira,

168
0:09:57.708,000 --> 0:10:,000
je sais que je recevrai les mêmes messages s'inquiétant pour l'avenir.

169
0:10:01.333,000 --> 0:10:04,000
Mais je suis très optimiste à propos de cette technologie.

170
0:10:07.458,000 --> 0:10:12,000
C'est notre chance pour refaire de ce monde, une place égale.

171
0:10:14.458,000 --> 0:10:18,000
Pour faire ça, nous devons la construire correctement dès le départ.

172
0:10:19.667,000 --> 0:10:24,000
Nous avons besoin de personnes de divers genres, ethnies, sexualités et origines.

173
0:10:26.458,000 --> 0:10:28,000
Nous avons de femmes développeurs

174
0:10:28.958,000 --> 0:10:31,000
et pas seulement de machines embauchant les développeurs.

175
0:10:33.875,000 --> 0:10:36,000
Il nous faut penser très soigneusement à ce que nous enseignons aux machines,

176
0:10:37.667,000 --> 0:10:38,000
aux données que nous leur donnons,

177
0:10:39.333,000 --> 0:10:42,000
pour qu'elles ne répètent pas nos propres erreurs.

178
0:10:44.125,000 --> 0:10:47,000
J'espère que je vous laisse avec deux idées.

179
0:10:48.542,000 --> 0:10:52,000
Premièrement, j'espère que vous repartirez en pensant aux biais.

180
0:10:53.125,000 --> 0:10:56,000
Et que la prochaine fois que vous tomberez sur une publicité

181
0:10:56.333,000 --> 0:10:59,000
supposant que vous êtes intéressé par une clinique de fertilité

182
0:10:59.343,000 --> 0:11:01,000
ou un site de paris en ligne,

183
0:11:02.042,000 --> 0:11:04,000
vous vous souviendrez

184
0:11:04.083,000 --> 0:11:08,000
que ces mêmes technologies assument qu'un homme noir récidivera.

185
0:11:09.833,000 --> 0:11:13,000
Ou qu'une femme a plus de chances d'être assistant personnel que PDG.

186
0:11:14.958,000 --> 0:11:17,000
Et j'espère que ça vous rappellera que nous devons faire quelque chose.

187
0:11:20.917,000 --> 0:11:21,000
Deuxièmement,

188
0:11:22.792,000 --> 0:11:23,000
j'espère que vous vous direz

189
0:11:24.708,000 --> 0:11:25,000
qu'il ne faut pas avoir une apparence

190
0:11:26.708,000 --> 0:11:29,000
ou un diplôme en particulier en ingénierie ou technologie

191
0:11:30.583,000 --> 0:11:31,000
pour créer des IA,

192
0:11:31.875,000 --> 0:11:33,000
qui seront une force phénoménale pour l'avenir.

193
0:11:36.166,000 --> 0:11:38,000
Inutile de ressembler à Mark Zuckerberg,

194
0:11:38.333,000 --> 0:11:39,000
vous pouvez me ressembler.

195
0:11:41.25,000 --> 0:11:43,000
Et c'est à vous tous, qui êtes dans cette pièce,

196
0:11:44.167,000 --> 0:11:46,000
de convaincre les gouvernements et les sociétés

197
0:11:46.917,000 --> 0:11:48,000
de construire des IA inclusives,

198
0:11:49.833,000 --> 0:11:51,000
prenant en compte les minorités.

199
0:11:52.25,000 --> 0:11:54,000
Il faut aussi que nous nous éduquions tous

200
0:11:54.333,000 --> 0:11:56,000
sur cette technologie phénoménale.

201
0:11:58.167,000 --> 0:12:,000
Parce que si nous le faisons,

202
0:12:00.208,000 --> 0:12:04,000
nous aurons à peine effleuré la surface de ce qui peut être accompli avec l'IA.

203
0:12:05.125,000 --> 0:12:06,000
Merci.

204
0:12:06.417,000 --> 0:12:08,000
(Applaudissements)

