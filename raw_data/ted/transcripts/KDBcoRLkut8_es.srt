1
0:00:,000 --> 0:00:07,000
Traductor: Sebastian Betti Revisor: Ciro Gomez

2
0:00:12.844,000 --> 0:00:14,000
Al hablar de prejuicios y sesgos,

3
0:00:15.222,000 --> 0:00:17,000
solemos pensar en personas tontas y malvadas,

4
0:00:17.366,000 --> 0:00:19,000
que hacen tonterías y maldades.

5
0:00:19.82,000 --> 0:00:21,000
Resumió muy bien esta idea

6
0:00:21.89,000 --> 0:00:23,000
el crítico británico William Hazlitt,

7
0:00:24.358,000 --> 0:00:26,000
al escribir: "El prejuicio es hijo de la ignorancia".

8
0:00:27.293,000 --> 0:00:29,000
Trataré de convencerlos aquí

9
0:00:29.405,000 --> 0:00:3,000
de que esto es un error.

10
0:00:31.04,000 --> 0:00:32,000
Quiero convencerlos

11
0:00:32.772,000 --> 0:00:33,000
de que el prejuicio y el sesgo

12
0:00:34.495,000 --> 0:00:37,000
son naturales, a menudo racionales,

13
0:00:37.783,000 --> 0:00:38,000
y con frecuencia incluso algo moral,

14
0:00:39.614,000 --> 0:00:41,000
y pienso que una vez que lo entendemos,

15
0:00:41.866,000 --> 0:00:43,000
estamos mejor preparados para darles sentido

16
0:00:44.375,000 --> 0:00:45,000
si se salen de cauce,

17
0:00:45.432,000 --> 0:00:46,000
si tienen consecuencias horribles,

18
0:00:47.2,000 --> 0:00:49,000
estamos mejor preparados para saber qué hacer

19
0:00:49.525,000 --> 0:00:5,000
cuando esto suceda.

20
0:00:51.207,000 --> 0:00:54,000
Empecemos con los estereotipos. Me miran,

21
0:00:54.234,000 --> 0:00:56,000
saben mi nombre, conocen algo de mí,

22
0:00:56.48,000 --> 0:00:57,000
y podrían emitir ciertos juicios.

23
0:00:58.309,000 --> 0:01:,000
Podrían conjeturar sobre mi origen étnico,

24
0:01:01.162,000 --> 0:01:04,000
mi afiliación política, mis creencias religiosas.

25
0:01:04.443,000 --> 0:01:06,000
Lo cierto es que estos juicios suelen ser exactos.

26
0:01:06.542,000 --> 0:01:08,000
Somos muy buenos para esas cosas.

27
0:01:08.724,000 --> 0:01:09,000
Y somos muy buenos para esas cosas

28
0:01:10.207,000 --> 0:01:12,000
porque la capacidad para estereotipar personas

29
0:01:12.94,000 --> 0:01:15,000
no es una peculiaridad de la mente

30
0:01:16.195,000 --> 0:01:18,000
sino un caso específico

31
0:01:18.511,000 --> 0:01:19,000
de un proceso más general,

32
0:01:20.166,000 --> 0:01:21,000
y es que tenemos experiencia

33
0:01:21.785,000 --> 0:01:22,000
de cosas y personas del mundo real

34
0:01:23.326,000 --> 0:01:24,000
que caen en categorías

35
0:01:24.575,000 --> 0:01:26,000
y podemos usar esa experiencia para hacer generalizaciones

36
0:01:27.031,000 --> 0:01:29,000
sobre nuevos casos de estas categorías.

37
0:01:29.39,000 --> 0:01:31,000
Todos aquí tenemos mucha experiencia

38
0:01:31.757,000 --> 0:01:33,000
con sillas, manzanas y perros.

39
0:01:34.01,000 --> 0:01:35,000
Con base en esto uno podría ver

40
0:01:35.646,000 --> 0:01:37,000
ejemplos poco familiares y conjeturar:

41
0:01:37.998,000 --> 0:01:38,000
uno podría sentarse en la silla,

42
0:01:39.314,000 --> 0:01:41,000
podría comer la manzana, el perro ladrará.

43
0:01:41.879,000 --> 0:01:42,000
Podríamos equivocarnos.

44
0:01:43.643,000 --> 0:01:44,000
La silla podría romperse si uno se sentara,

45
0:01:45.443,000 --> 0:01:47,000
la manzana podría estar envenenada, el perro podría no ladrar.

46
0:01:47.665,000 --> 0:01:49,000
De hecho, este es mi perro Tessie, y no ladra.

47
0:01:50.535,000 --> 0:01:52,000
Pero en su mayor parte, somos buenos para esto.

48
0:01:53.294,000 --> 0:01:54,000
En general, hacemos buenas conjeturas

49
0:01:55.21,000 --> 0:01:56,000
tanto en esferas sociales como no sociales,

50
0:01:57.024,000 --> 0:01:58,000
y, de no ser así,

51
0:01:58.973,000 --> 0:02:01,000
de no poder hacer conjeturas sobre nuevos casos que encontramos,

52
0:02:02.189,000 --> 0:02:03,000
no habríamos sobrevivido.

53
0:02:03.64,000 --> 0:02:05,000
De hecho, Hazlitt más adelante en su maravilloso ensayo

54
0:02:06.509,000 --> 0:02:07,000
concede esto.

55
0:02:07.994,000 --> 0:02:09,000
Escribe: "Sin la ayuda de los prejuicios y las costumbres,

56
0:02:10.536,000 --> 0:02:12,000
ni siquiera podría abrirme camino por la sala;

57
0:02:12.876,000 --> 0:02:14,000
ni saber cómo manejarme en cualquier circunstancia,

58
0:02:15.328,000 --> 0:02:19,000
ni qué sentir en cualquier relación de la vida".

59
0:02:19.531,000 --> 0:02:2,000
Hablemos del sesgo.

60
0:02:21.04,000 --> 0:02:22,000
A veces dividimos el mundo

61
0:02:22.748,000 --> 0:02:25,000
en nosotros versus ellos, nuestro grupo versus el resto,

62
0:02:25.749,000 --> 0:02:26,000
y a veces al hacerlo,

63
0:02:26.91,000 --> 0:02:27,000
sabemos que estamos haciendo algo errado,

64
0:02:28.467,000 --> 0:02:29,000
y nos avergüenza un poco.

65
0:02:30.14,000 --> 0:02:31,000
Pero otras veces estamos orgullosos de eso.

66
0:02:31.623,000 --> 0:02:32,000
Lo reconocemos abiertamente.

67
0:02:33.436,000 --> 0:02:34,000
Mi ejemplo favorito de esto

68
0:02:34.718,000 --> 0:02:36,000
es una pregunta que vino del público

69
0:02:37.12,000 --> 0:02:39,000
en el debate republicano anterior a la última elección.

70
0:02:39.837,000 --> 0:02:41,000
(Vídeo) Anderson Cooper: Vamos a su pregunta,

71
0:02:42.129,000 --> 0:02:46,000
la pregunta en la sala sobre ayuda exterior. Sí, señora.

72
0:02:46.31,000 --> 0:02:48,000
Mujer: Los estadounidenses estamos sufriendo

73
0:02:48.546,000 --> 0:02:5,000
en nuestro propio país.

74
0:02:51.183,000 --> 0:02:54,000
¿Por qué seguimos enviando ayuda al exterior,

75
0:02:54.531,000 --> 0:02:55,000
a otros países,

76
0:02:55.847,000 --> 0:02:59,000
cuando necesitamos toda la ayuda que podamos tener para nosotros?

77
0:02:59.95,000 --> 0:03:,000
AC: Gobernador Perry, ¿qué dice?

78
0:03:01.645,000 --> 0:03:02,000
(Aplausos)

79
0:03:03.012,000 --> 0:03:05,000
Rick Perry: Totalmente, pienso que...

80
0:03:05.35,000 --> 0:03:06,000
Paul Bloom: Todos en el escenario

81
0:03:07.01,000 --> 0:03:08,000
están de acuerdo con la premisa de su pregunta,

82
0:03:08.981,000 --> 0:03:1,000
que es, como estadounidenses, deberíamos ocuparnos más

83
0:03:11.1,000 --> 0:03:13,000
de los estadounidenses que de otras personas.

84
0:03:13.226,000 --> 0:03:15,000
De hecho, en general, las personas se rigen a menudo

85
0:03:16.091,000 --> 0:03:19,000
por sentimientos de solidaridad, lealtad, orgullo, patriotismo,

86
0:03:19.599,000 --> 0:03:21,000
hacia su país o hacia su grupo étnico.

87
0:03:22.315,000 --> 0:03:25,000
Más allá de la posición política, muchos están orgullosos de ser estadounidense

88
0:03:25.4,000 --> 0:03:27,000
y favorecen a los estadounidenses sobre otras nacionalidades.

89
0:03:27.462,000 --> 0:03:29,000
Los residentes de otros países sienten lo mismo sobre su nación,

90
0:03:30.312,000 --> 0:03:32,000
y sentimos lo mismo sobre nuestras etnias.

91
0:03:32.798,000 --> 0:03:33,000
Algunos puede que rechacen esto.

92
0:03:34.482,000 --> 0:03:35,000
Alguno de Uds. puede que sea tan cosmopolita

93
0:03:36.213,000 --> 0:03:38,000
que piense que la etnia y la nacionalidad

94
0:03:38.547,000 --> 0:03:4,000
no debería tener influencia moral.

95
0:03:40.7,000 --> 0:03:42,000
Pero aún sofisticándolo aceptemos

96
0:03:43.462,000 --> 0:03:44,000
que debería haber cierta tendencia

97
0:03:45.296,000 --> 0:03:47,000
hacia el grupo de pertenencia, de familiares y amigos,

98
0:03:47.997,000 --> 0:03:48,000
de los seres íntimos,

99
0:03:49.418,000 --> 0:03:5,000
y así uno hace la distinción

100
0:03:50.979,000 --> 0:03:51,000
entre nosotros y ellos.

101
0:03:52.954,000 --> 0:03:54,000
Esta distinción es bastante natural

102
0:03:55.557,000 --> 0:03:57,000
y a menudo algo moral, pero puede salir mal,

103
0:03:58.481,000 --> 0:03:59,000
y esto fue parte de la investigación

104
0:04:00.21,000 --> 0:04:02,000
del gran psicólogo social Henri Tajfel.

105
0:04:02.969,000 --> 0:04:04,000
Tajfel nació en Polonia en 1919.

106
0:04:05.574,000 --> 0:04:07,000
Partió de allí para hacer la universidad en Francia,

107
0:04:07.713,000 --> 0:04:09,000
porque como judío no podía ir a la universidad en Polonia,

108
0:04:10.268,000 --> 0:04:12,000
y luego se alistó en el Ejército francés

109
0:04:12.778,000 --> 0:04:13,000
en la Segunda Guerra Mundial.

110
0:04:14.061,000 --> 0:04:15,000
Fue capturado y terminó

111
0:04:15.83,000 --> 0:04:16,000
en un campo de prisioneros de guerra,

112
0:04:17.361,000 --> 0:04:19,000
y fue un momento aterrador para él,

113
0:04:19.628,000 --> 0:04:2,000
porque si se descubría que era judío,

114
0:04:21.316,000 --> 0:04:23,000
podría haber sido trasladado a un campo de concentración,

115
0:04:23.408,000 --> 0:04:24,000
donde muy probablemente no habría sobrevivido.

116
0:04:25.4,000 --> 0:04:27,000
Y, de hecho, cuando la guerra terminó y quedó en libertad,

117
0:04:27.987,000 --> 0:04:29,000
la mayoría de sus amigos y familiares estaban muertos.

118
0:04:30.492,000 --> 0:04:31,000
Participó en diferentes actividades.

119
0:04:32.329,000 --> 0:04:33,000
Ayudó a los huérfanos de guerra.

120
0:04:33.86,000 --> 0:04:34,000
Pero tenía un interés de larga data

121
0:04:35.591,000 --> 0:04:36,000
en la ciencia del prejuicio.

122
0:04:37.136,000 --> 0:04:39,000
Entonces cuando surgió una beca prestigiosa británica

123
0:04:39.796,000 --> 0:04:4,000
sobre estereotipos, se postuló,

124
0:04:41.641,000 --> 0:04:42,000
y ganó,

125
0:04:42.998,000 --> 0:04:44,000
y empezó esta carrera increíble.

126
0:04:45.188,000 --> 0:04:47,000
Empezó su carrera con la idea

127
0:04:47.937,000 --> 0:04:48,000
de que lo que pensaba la mayoría

128
0:04:49.777,000 --> 0:04:51,000
sobre el Holocausto era erróneo.

129
0:04:51.893,000 --> 0:04:53,000
Mucha gente, la mayoría en esa época,

130
0:04:54.299,000 --> 0:04:55,000
veía al Holocausto como la representación

131
0:04:56.2,000 --> 0:04:59,000
de un defecto trágico alemán,

132
0:04:59.204,000 --> 0:05:02,000
de alguna mancha genética, de una personalidad autoritaria.

133
0:05:03.038,000 --> 0:05:05,000
Y Tajfel rechazaba esto.

134
0:05:05.096,000 --> 0:05:07,000
Tajfel decía que el Holocausto

135
0:05:07.639,000 --> 0:05:09,000
es solo una exageración

136
0:05:09.95,000 --> 0:05:1,000
de los procesos psicológicos normales

137
0:05:11.728,000 --> 0:05:12,000
que existen en todos nosotros.

138
0:05:13.489,000 --> 0:05:15,000
Y para explorar la idea hizo una serie de estudios clásicos

139
0:05:16.174,000 --> 0:05:17,000
con adolescentes británicos.

140
0:05:17.918,000 --> 0:05:18,000
Y en uno de sus estudios le hizo

141
0:05:19.467,000 --> 0:05:21,000
a los adolescentes británicos todo tipo de preguntas,

142
0:05:22.019,000 --> 0:05:23,000
y con base en sus respuestas dijo:

143
0:05:23.903,000 --> 0:05:25,000
"Mirando tus respuestas, con base en ellas,

144
0:05:26.26,000 --> 0:05:28,000
determiné que te encanta",

145
0:05:28.357,000 --> 0:05:29,000
les dijo,

146
0:05:29.363,000 --> 0:05:31,000
"Kandinsky, te encanta la obra de Kandinsky,

147
0:05:32.32,000 --> 0:05:34,000
o te encanta la obra de Klee".

148
0:05:35.298,000 --> 0:05:36,000
Era totalmente falso.

149
0:05:37.114,000 --> 0:05:39,000
Sus respuestas no tenían nada que ver con Kandinsly ni Klee.

150
0:05:39.404,000 --> 0:05:41,000
Probablemente no conocían a esos artistas.

151
0:05:42.132,000 --> 0:05:44,000
Simplemente los dividió en forma arbitraria.

152
0:05:44.872,000 --> 0:05:47,000
Pero encontró que estas categorías importaban,

153
0:05:48.143,000 --> 0:05:5,000
por eso luego al darles dinero a los sujetos,

154
0:05:50.654,000 --> 0:05:51,000
ellos preferían dar el dinero

155
0:05:52.33,000 --> 0:05:53,000
a los miembros de su propio grupo

156
0:05:54.13,000 --> 0:05:55,000
que a los miembros de otro grupo.

157
0:05:55.963,000 --> 0:05:57,000
Peor aún, les interesaba más

158
0:05:58.29,000 --> 0:06:,000
establecer una diferencia

159
0:06:00.296,000 --> 0:06:02,000
entre su grupo y otros grupos,

160
0:06:02.862,000 --> 0:06:03,000
y les darían más dinero a su propio grupo

161
0:06:04.77,000 --> 0:06:09,000
si con eso pudieran darle al otro grupo aún menos.

162
0:06:10.018,000 --> 0:06:12,000
Este sesgo parece surgir muy pronto.

163
0:06:12.236,000 --> 0:06:14,000
Mi colega y esposa, Karen Wynn, en Yale

164
0:06:14.536,000 --> 0:06:15,000
hizo una serie de estudios con bebés.

165
0:06:16.147,000 --> 0:06:18,000
Les dio títeres a los bebés,

166
0:06:18.979,000 --> 0:06:2,000
y los títeres tenían ciertas preferencias alimenticias.

167
0:06:21.244,000 --> 0:06:23,000
A uno de los títeres les gustaban las judías verdes.

168
0:06:23.426,000 --> 0:06:25,000
Al otro títere le gustaban las galletas.

169
0:06:26.001,000 --> 0:06:28,000
Estudiaban las preferencias alimenticias de los bebés,

170
0:06:28.37,000 --> 0:06:3,000
y los bebés por lo general preferían las galletas.

171
0:06:31.06,000 --> 0:06:33,000
La pregunta es: ¿Influía en los bebés

172
0:06:33.672,000 --> 0:06:36,000
la forma de tratar a los títeres? Influía mucho.

173
0:06:36.788,000 --> 0:06:37,000
Preferían al títere

174
0:06:38.307,000 --> 0:06:41,000
que tenía los mismos gustos que ellos,

175
0:06:41.786,000 --> 0:06:43,000
y, peor aún, preferían a los títeres

176
0:06:44.342,000 --> 0:06:46,000
que castigaban a los que tenían distintas preferencias alimenticias.

177
0:06:47.327,000 --> 0:06:49,000
(Risas)

178
0:06:49.604,000 --> 0:06:52,000
Vemos esta psicología de pertenencia a grupos todo el tiempo.

179
0:06:53.236,000 --> 0:06:54,000
Lo vemos en los enfrentamientos políticos

180
0:06:54.9,000 --> 0:06:56,000
en grupos con distintas ideologías.

181
0:06:57.314,000 --> 0:07:,000
Lo vemos al extremo en casos de guerra,

182
0:07:00.94,000 --> 0:07:03,000
donde a quienes no pertenecen al grupo no solo se les da menos

183
0:07:04.157,000 --> 0:07:05,000
sino que se los deshumaniza,

184
0:07:05.745,000 --> 0:07:07,000
como en la mirada nazi de los judíos

185
0:07:07.985,000 --> 0:07:09,000
como alimañas o piojos,

186
0:07:10.07,000 --> 0:07:14,000
o la mirada de EE.UU. a los japoneses como ratas.

187
0:07:14.306,000 --> 0:07:16,000
Los estereotipos también pueden salir mal.

188
0:07:16.52,000 --> 0:07:18,000
A menudo son racionales y útiles,

189
0:07:18.781,000 --> 0:07:19,000
pero a veces son irracionales,

190
0:07:20.355,000 --> 0:07:21,000
dan la respuesta errónea,

191
0:07:21.581,000 --> 0:07:22,000
y otras veces

192
0:07:22.798,000 --> 0:07:24,000
llevan llanamente a consecuencias inmorales.

193
0:07:24.973,000 --> 0:07:26,000
Y el caso más estudiado

194
0:07:27.781,000 --> 0:07:28,000
es el de la raza.

195
0:07:29.448,000 --> 0:07:3,000
Hubo un estudio fascinante

196
0:07:30.855,000 --> 0:07:32,000
antes de las elecciones de 2008

197
0:07:32.929,000 --> 0:07:35,000
en el que unos psicólogos sociales analizaron el grado

198
0:07:35.955,000 --> 0:07:38,000
de asociación de los candidatos con EE.UU.

199
0:07:39.397,000 --> 0:07:42,000
y la asociación inconsciente con la bandera de EE.UU.

200
0:07:43.002,000 --> 0:07:44,000
En uno de sus estudios compararon

201
0:07:44.358,000 --> 0:07:46,000
a Obama y McCain, y hallaron que se piensa

202
0:07:46.372,000 --> 0:07:49,000
que McCain es más estadounidense que Obama,

203
0:07:49.766,000 --> 0:07:51,000
y en cierto punto no es una sorpresa.

204
0:07:52.339,000 --> 0:07:53,000
McCain es un conocido héroe de guerra,

205
0:07:54.257,000 --> 0:07:55,000
y mucha gente diría explícitamente

206
0:07:55.916,000 --> 0:07:57,000
que tiene una historia más afín a EE.UU. que Obama.

207
0:07:58.616,000 --> 0:07:59,000
Pero también compararon a Obama

208
0:08:00.553,000 --> 0:08:02,000
con el PM británico Tony Blair,

209
0:08:03.069,000 --> 0:08:05,000
y hallaron que se piensa que Blair también

210
0:08:05.33,000 --> 0:08:07,000
es más estadounidense que Obama,

211
0:08:07.837,000 --> 0:08:09,000
aunque queda claro que entendían

212
0:08:09.91,000 --> 0:08:11,000
que Blair no es estadunidense.

213
0:08:12.9,000 --> 0:08:13,000
Respondían, como está claro,

214
0:08:14.324,000 --> 0:08:17,000
por el color de la piel.

215
0:08:17.375,000 --> 0:08:19,000
Estos estereotipos y sesgos

216
0:08:19.426,000 --> 0:08:2,000
tienen consecuencias en el mundo real,

217
0:08:20.876,000 --> 0:08:22,000
son sutiles y muy importantes.

218
0:08:23.748,000 --> 0:08:25,000
En un estudio reciente, unos investigadores

219
0:08:26.41,000 --> 0:08:29,000
colocaron anuncios en eBay para vender tarjetas de baloncesto.

220
0:08:29.679,000 --> 0:08:31,000
Algunas eran sostenidas por manos blancas

221
0:08:32.413,000 --> 0:08:33,000
y otras por manos negras.

222
0:08:33.631,000 --> 0:08:34,000
Eran las mismas tarjetas de baloncesto.

223
0:08:35.21,000 --> 0:08:36,000
Las sostenidas por manos negras

224
0:08:36.454,000 --> 0:08:38,000
tuvieron muchas menos ofertas

225
0:08:38.521,000 --> 0:08:4,000
que las sostenidas por manos blancas.

226
0:08:41.005,000 --> 0:08:43,000
En una investigación de Stanford,

227
0:08:43.367,000 --> 0:08:47,000
unos psicólogos exploraron un caso de personas

228
0:08:47.597,000 --> 0:08:5,000
condenadas por el asesinato de una persona blanca.

229
0:08:51.166,000 --> 0:08:53,000
Resulta que, manteniendo todo lo demás constante,

230
0:08:53.97,000 --> 0:08:55,000
es mucho más probable ser ejecutado

231
0:08:56.34,000 --> 0:08:57,000
si uno se parece al hombre de la derecha

232
0:08:58.117,000 --> 0:08:59,000
que al hombre de la izquierda,

233
0:09:00.09,000 --> 0:09:02,000
y esto se debe en gran parte

234
0:09:02.119,000 --> 0:09:04,000
a que el hombre de la derecha se parece más al arquetipo negro

235
0:09:04.653,000 --> 0:09:06,000
que al arquetipo afro-estadounidense,

236
0:09:07.283,000 --> 0:09:09,000
y esto supuestamente influye en las decisiones de las personas

237
0:09:09.332,000 --> 0:09:1,000
sobre qué hacer con él.

238
0:09:11.103,000 --> 0:09:12,000
Y ahora que sabemos esto,

239
0:09:12.65,000 --> 0:09:13,000
¿cómo combatirlo?

240
0:09:14.307,000 --> 0:09:15,000
Hay dos vías diferentes.

241
0:09:15.929,000 --> 0:09:16,000
Una es recurrir

242
0:09:17.363,000 --> 0:09:19,000
a la respuesta emocional de las personas,

243
0:09:19.409,000 --> 0:09:21,000
apelar a la empatía de las personas,

244
0:09:21.542,000 --> 0:09:22,000
y con frecuencia lo hacemos con historias.

245
0:09:23.415,000 --> 0:09:25,000
Si uno es un padre progresista

246
0:09:25.98,000 --> 0:09:26,000
y quiere animar a sus hijos

247
0:09:27.852,000 --> 0:09:29,000
a creer en los méritos de las familias no tradicionales, les da

248
0:09:30.226,000 --> 0:09:32,000
el libro ["Heather tiene dos mamás"].

249
0:09:32.499,000 --> 0:09:33,000
Si uno es conservador y tiene otra actitud,

250
0:09:34.225,000 --> 0:09:35,000
les da el libro

251
0:09:35.422,000 --> 0:09:37,000
["¡Socorro, mamá! ¡Hay progresistas bajo mi cama!"] (Risas)

252
0:09:37.905,000 --> 0:09:4,000
En general, las historias pueden convertir

253
0:09:41.241,000 --> 0:09:43,000
a extraños anónimos en personas que importan,

254
0:09:43.473,000 --> 0:09:45,000
y la idea de ocuparnos de las personas

255
0:09:46.158,000 --> 0:09:47,000
cuando nos centramos en ellos como individuos

256
0:09:47.86,000 --> 0:09:49,000
es una idea repetida a lo largo de la historia.

257
0:09:50.139,000 --> 0:09:52,000
Stalin dijo en forma apócrifa:

258
0:09:52.722,000 --> 0:09:53,000
"Una sola muerte es una tragedia,

259
0:09:54.339,000 --> 0:09:56,000
un millón de muertes es una estadística".

260
0:09:56.379,000 --> 0:09:57,000
Y la Madre Teresa dijo:

261
0:09:57.83,000 --> 0:09:58,000
"Si miro a la masa, nunca voy a actuar.

262
0:09:59.371,000 --> 0:10:01,000
Si miro a la persona, lo haré".

263
0:10:01.696,000 --> 0:10:03,000
Los psicólogos han explorado esto.

264
0:10:03.766,000 --> 0:10:04,000
Por ejemplo, en un estudio,

265
0:10:05.067,000 --> 0:10:07,000
a unas personas se les dio una lista de hechos sobre una crisis,

266
0:10:07.85,000 --> 0:10:11,000
y se analizaba cuánto donaban

267
0:10:12.106,000 --> 0:10:13,000
para resolver la crisis,

268
0:10:13.69,000 --> 0:10:14,000
y a otro grupo no se le dieron hechos

269
0:10:15.527,000 --> 0:10:17,000
sino que se les habló de una persona

270
0:10:17.625,000 --> 0:10:19,000
se les dio un nombre y un rostro,

271
0:10:20.065,000 --> 0:10:23,000
y resultó que donaron mucho más en el último caso.

272
0:10:23.284,000 --> 0:10:24,000
Pienso que nada de esto es un secreto

273
0:10:25.145,000 --> 0:10:27,000
para quienes se dedican a la caridad.

274
0:10:27.256,000 --> 0:10:29,000
No se suele abrumar a la gente

275
0:10:29.904,000 --> 0:10:3,000
con datos y estadísticas.

276
0:10:31.227,000 --> 0:10:32,000
En cambio, se le muestran rostros,

277
0:10:32.249,000 --> 0:10:33,000
se le muestran personas.

278
0:10:33.985,000 --> 0:10:36,000
Es posible que al extender nuestras simpatías

279
0:10:37.212,000 --> 0:10:38,000
a una persona, pueda difundirse

280
0:10:39.183,000 --> 0:10:41,000
al grupo al que pertenece la persona.

281
0:10:42.061,000 --> 0:10:44,000
Esta es Harriet Beecher Stowe.

282
0:10:44.527,000 --> 0:10:46,000
La historia, quizá apócrifa,

283
0:10:46.97,000 --> 0:10:48,000
dice que el presidente Lincoln la invitó

284
0:10:49.044,000 --> 0:10:5,000
a la Casa Blanca en medio de la Guerra Civil

285
0:10:51.042,000 --> 0:10:52,000
y le dijo:

286
0:10:52.626,000 --> 0:10:54,000
"Así que tú eres la damita que empezó esta gran guerra".

287
0:10:55.29,000 --> 0:10:56,000
Y se refería a "La cabaña del tío Tom".

288
0:10:57.175,000 --> 0:10:59,000
"La cabaña del tío Tom" no es un gran libro de filosofía

289
0:10:59.706,000 --> 0:11:02,000
ni de teología, quizá ni siquiera de literatura,

290
0:11:02.85,000 --> 0:11:04,000
pero hace un gran trabajo

291
0:11:05.365,000 --> 0:11:07,000
en hacer que la gente se ponga en los zapatos

292
0:11:07.863,000 --> 0:11:09,000
de personas que de otro modo no sería posible,

293
0:11:10.196,000 --> 0:11:12,000
que se pongan en el lugar de los esclavos.

294
0:11:12.598,000 --> 0:11:13,000
Y eso bien podría haber sido un catalizador

295
0:11:14.379,000 --> 0:11:15,000
de gran cambio social.

296
0:11:15.983,000 --> 0:11:17,000
Más recientemente, mirando a EE.UU.

297
0:11:18.345,000 --> 0:11:21,000
en las últimas décadas,

298
0:11:21.414,000 --> 0:11:24,000
hay alguna razón para creer que programas como "El show de Cosby"

299
0:11:24.563,000 --> 0:11:26,000
cambiaron drásticamente las actitudes de EE.UU. hacia los afro-estadounidenses

300
0:11:27.251,000 --> 0:11:29,000
mientras que "Will y Grace" y "Familia moderna"

301
0:11:30.234,000 --> 0:11:31,000
cambiaron la actitud estadounidense

302
0:11:31.597,000 --> 0:11:32,000
hacia hombres y mujeres homosexuales.

303
0:11:32.897,000 --> 0:11:34,000
Pienso que no es una exageración decir

304
0:11:35.352,000 --> 0:11:37,000
que el principal catalizador del cambio moral en EE.UU.

305
0:11:38.013,000 --> 0:11:4,000
ha sido una comedia de situación.

306
0:11:40.906,000 --> 0:11:41,000
Pero no es todo emociones,

307
0:11:42.322,000 --> 0:11:43,000
y quiero terminar apelando

308
0:11:43.598,000 --> 0:11:45,000
al poder de la razón.

309
0:11:45.833,000 --> 0:11:47,000
En un momento de su maravilloso libro

310
0:11:47.989,000 --> 0:11:48,000
"Los mejores ángeles de nuestra naturaleza"

311
0:11:49.212,000 --> 0:11:51,000
Steven Pinker dice:

312
0:11:51.228,000 --> 0:11:53,000
el Antiguo Testamento dice "ama a tu prójimo",

313
0:11:53.81,000 --> 0:11:55,000
y el Nuevo Testamento dice "ama a tu enemigo",

314
0:11:56.532,000 --> 0:11:58,000
pero yo no amo a ninguno de los dos, realmente no,

315
0:11:59.218,000 --> 0:12:,000
pero no quiero matarlos.

316
0:12:00.885,000 --> 0:12:01,000
Sé que tengo obligaciones para con ellos,

317
0:12:02.751,000 --> 0:12:05,000
pero mis sentimientos morales hacia ellos, mis creencias morales

318
0:12:06.221,000 --> 0:12:07,000
sobre cómo debería comportarme con ellos,

319
0:12:07.934,000 --> 0:12:09,000
no tienen fundamento en el amor.

320
0:12:09.981,000 --> 0:12:1,000
Tienen un anclaje en la comprensión de los Derechos Humanos,

321
0:12:11.92,000 --> 0:12:13,000
en la creencia de que sus vidas son tan valiosos para ellos

322
0:12:14.143,000 --> 0:12:16,000
como la mía para mí,

323
0:12:16.499,000 --> 0:12:17,000
y para respaldar esto, cuenta una historia

324
0:12:18.431,000 --> 0:12:19,000
del gran filósofo Adam Smith,

325
0:12:20.279,000 --> 0:12:21,000
y quiero contar esa historia también,

326
0:12:21.965,000 --> 0:12:22,000
aunque la adecuaré un poco

327
0:12:23.261,000 --> 0:12:24,000
a los tiempos modernos.

328
0:12:24.939,000 --> 0:12:25,000
Adam Smith empieza pidiendo que imaginemos

329
0:12:26.84,000 --> 0:12:27,000
la muerte de miles de personas,

330
0:12:28.741,000 --> 0:12:3,000
y que imaginemos que las miles de personas

331
0:12:30.781,000 --> 0:12:32,000
están en un país que no nos resulta familiar.

332
0:12:33.02,000 --> 0:12:36,000
Podría ser China, o India, o un país de África.

333
0:12:36.574,000 --> 0:12:38,000
Y Smith pregunta: ¿cómo responderíamos?

334
0:12:39.058,000 --> 0:12:41,000
Uno diría, bueno está muy mal,

335
0:12:41.365,000 --> 0:12:42,000
y seguiría con la vida habitual.

336
0:12:43.241,000 --> 0:12:45,000
Si uno leyera The New York Times en línea o algo así,

337
0:12:45.46,000 --> 0:12:47,000
y descubriera esto, de hecho nos pasa todo el tiempo,

338
0:12:48.42,000 --> 0:12:49,000
seguiríamos con nuestras vidas.

339
0:12:49.941,000 --> 0:12:51,000
Pero imaginemos en cambio, dice Smith,

340
0:12:52.135,000 --> 0:12:53,000
que supiéramos que mañana

341
0:12:53.389,000 --> 0:12:55,000
nos amputarían el dedo meñique.

342
0:12:55.928,000 --> 0:12:57,000
Smith dice que eso nos afectaría mucho.

343
0:12:58.097,000 --> 0:12:59,000
No podríamos dormir esa noche

344
0:12:59.508,000 --> 0:13:,000
pensando en eso.

345
0:13:00.861,000 --> 0:13:02,000
Esto plantea la pregunta:

346
0:13:02.88,000 --> 0:13:04,000
¿Sacrificaríamos miles de vidas

347
0:13:05.346,000 --> 0:13:06,000
para salvar nuestro meñique?

348
0:13:07.315,000 --> 0:13:09,000
Respondan esto en su fuero íntimo,

349
0:13:09.633,000 --> 0:13:11,000
pero Smith dice, absolutamente no,

350
0:13:12.552,000 --> 0:13:13,000
qué pensamiento tan horrible.

351
0:13:14.244,000 --> 0:13:16,000
Y esto plantea la pregunta,

352
0:13:16.275,000 --> 0:13:17,000
y, como dice Smith,

353
0:13:17.649,000 --> 0:13:19,000
"Si nuestros sentimientos pasivos son casi siempre

354
0:13:19.867,000 --> 0:13:2,000
tan sórdidos y egoístas,

355
0:13:21.315,000 --> 0:13:22,000
¿cómo es que nuestros principios activos

356
0:13:22.78,000 --> 0:13:24,000
a menudo deberían ser tan generosos y nobles?"

357
0:13:25.313,000 --> 0:13:27,000
Y la respuesta de Smith es: "Existe la razón,

358
0:13:27.363,000 --> 0:13:28,000
los principios, la conciencia

359
0:13:29.138,000 --> 0:13:3,000
"que nos dice

360
0:13:30.679,000 --> 0:13:33,000
con una voz capaz de asombrar a la más presuntuosa de nuestras pasiones,

361
0:13:34.104,000 --> 0:13:35,000
que no somos más que uno en la multitud,

362
0:13:35.781,000 --> 0:13:37,000
en nada mejores que cualquier persona de la misma".

363
0:13:38.222,000 --> 0:13:4,000
Y esta última parte es lo que a menudo se describe

364
0:13:40.347,000 --> 0:13:43,000
como el principio de imparcialidad.

365
0:13:43.555,000 --> 0:13:45,000
Y este principio de imparcialidad se manifiesta

366
0:13:46.184,000 --> 0:13:47,000
en todas las religiones del mundo,

367
0:13:47.951,000 --> 0:13:49,000
en todas las diferentes versiones de la regla dorada,

368
0:13:50.209,000 --> 0:13:52,000
y en todas las filosofías morales del mundo,

369
0:13:52.663,000 --> 0:13:53,000
que difieren en muchos sentidos

370
0:13:53.97,000 --> 0:13:55,000
pero comparten la suposición previa de que debemos juzgar la moralidad

371
0:13:56.964,000 --> 0:13:58,000
desde un punto de vista imparcial.

372
0:13:59.949,000 --> 0:14:,000
La mejor articulación de esta mirada

373
0:14:01.771,000 --> 0:14:04,000
para mí no viene de un teólogo ni de un filósofo,

374
0:14:04.856,000 --> 0:14:05,000
sino de Humphrey Bogart

375
0:14:06.213,000 --> 0:14:07,000
al final de "Casablanca".

376
0:14:07.76,000 --> 0:14:1,000
Les arruinaré el final, le dice a su amada

377
0:14:11.536,000 --> 0:14:12,000
que tienen que separarse

378
0:14:12.676,000 --> 0:14:13,000
por un bien más general,

379
0:14:14.269,000 --> 0:14:15,000
y le dice, no podré hablar con su acento,

380
0:14:16.133,000 --> 0:14:17,000
pero le dice: "No se requiere mucho para entender

381
0:14:17.915,000 --> 0:14:18,000
que los problemas de tres pequeñas personas

382
0:14:19.274,000 --> 0:14:22,000
no equivalen a una colina de frijoles en este mundo loco".

383
0:14:22.385,000 --> 0:14:25,000
Nuestra razón podría anular nuestras pasiones.

384
0:14:25.665,000 --> 0:14:26,000
Nuestra razón podría motivarnos

385
0:14:27.381,000 --> 0:14:28,000
a extender nuestra empatía,

386
0:14:28.602,000 --> 0:14:3,000
podría motivarnos a escribir un libro titulado "La cabaña del tío Tom",

387
0:14:30.929,000 --> 0:14:31,000
o a leer un libro como "La cabaña del tío Tom",

388
0:14:32.652,000 --> 0:14:34,000
y nuestra razón puede motivarnos a crear

389
0:14:35.346,000 --> 0:14:36,000
costumbres, tabúes y leyes

390
0:14:37.308,000 --> 0:14:38,000
que nos impidan

391
0:14:39.118,000 --> 0:14:4,000
actuar siguiendo nuestros impulsos

392
0:14:40.794,000 --> 0:14:41,000
cuando, como seres racionales, sentimos

393
0:14:42.383,000 --> 0:14:43,000
que deberíamos limitarnos.

394
0:14:43.778,000 --> 0:14:45,000
Esto es lo que es una constitución.

395
0:14:45.791,000 --> 0:14:47,000
Una constitución es algo establecido en el pasado

396
0:14:48.712,000 --> 0:14:49,000
que se aplica en el presente,

397
0:14:50.019,000 --> 0:14:5,000
y que nos dice

398
0:14:51.004,000 --> 0:14:53,000
que sin importar cuánto queramos reelegir

399
0:14:53.231,000 --> 0:14:55,000
a un presidente popular a un tercer término,

400
0:14:55.834,000 --> 0:14:57,000
sin importar cuánto los estadounidenses blancos sientan

401
0:14:57.929,000 --> 0:15:01,000
que quieren reinstaurar la institución de la esclavitud, no pueden hacerlo.

402
0:15:01.997,000 --> 0:15:02,000
Nos hemos autolimitado.

403
0:15:03.673,000 --> 0:15:05,000
Y nos autolimitamos en otras formas también.

404
0:15:06.09,000 --> 0:15:08,000
Sabemos que si se trata de elegir a alguien

405
0:15:08.848,000 --> 0:15:1,000
para un empleo, para un premio,

406
0:15:11.799,000 --> 0:15:13,000
tenemos el fuerte sesgo de la raza,

407
0:15:14.757,000 --> 0:15:16,000
tenemos el sesgo del género,

408
0:15:17.053,000 --> 0:15:19,000
tenemos el sesgo del atractivo,

409
0:15:19.268,000 --> 0:15:21,000
y a veces podríamos decir: "Bueno, debería ser de esta forma".

410
0:15:21.919,000 --> 0:15:23,000
Pero otras veces decimos: "Esto está mal".

411
0:15:24.226,000 --> 0:15:25,000
Y para combatirlo

412
0:15:26.115,000 --> 0:15:28,000
no solo ponemos empeño,

413
0:15:28.366,000 --> 0:15:31,000
sino que en cambio establecemos situaciones

414
0:15:31.367,000 --> 0:15:34,000
en las que estas otras fuentes de información no puedan sesgarnos.

415
0:15:34.406,000 --> 0:15:35,000
Por eso muchas orquestas

416
0:15:35.721,000 --> 0:15:37,000
hacen audiciones de músicos detrás de pantallas,

417
0:15:38.366,000 --> 0:15:39,000
para que la única información que tengan

418
0:15:39.61,000 --> 0:15:41,000
sea la información que creen que debería importar.

419
0:15:42.303,000 --> 0:15:44,000
Pienso que el prejuicio y el sesgo

420
0:15:44.626,000 --> 0:15:47,000
ilustran una dualidad esencial de la naturaleza humana.

421
0:15:47.72,000 --> 0:15:5,000
Tenemos intuiciones, instintos, emociones,

422
0:15:51.496,000 --> 0:15:53,000
que afectan nuestros juicios y acciones

423
0:15:53.657,000 --> 0:15:55,000
para bien y para mal,

424
0:15:55.988,000 --> 0:15:58,000
pero también podemos razonar

425
0:15:59.61,000 --> 0:16:,000
y planificar con inteligencia,

426
0:16:01.045,000 --> 0:16:03,000
y usar esto para, en algunos casos,

427
0:16:03.862,000 --> 0:16:04,000
acelerar y nutrir nuestras emociones,

428
0:16:05.805,000 --> 0:16:07,000
y en otros casos para aplacarlas.

429
0:16:08.573,000 --> 0:16:09,000
Y de esta forma

430
0:16:09.807,000 --> 0:16:11,000
la razón nos ayuda a crear un mundo mejor.

431
0:16:12.574,000 --> 0:16:14,000
Gracias.

432
0:16:14.918,000 --> 0:16:17,000
(Aplausos)

