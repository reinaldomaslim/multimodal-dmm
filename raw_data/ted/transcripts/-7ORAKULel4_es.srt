1
0:00:,000 --> 0:00:07,000
Traductor: Jose Manuel Miana Borau Revisor: Paula Motter

2
0:00:13.468,000 --> 0:00:18,000
El 23 de abril de 2013,

3
0:00:18.714,000 --> 0:00:23,000
Associated Press puso el siguiente tuit en Twitter.

4
0:00:24.252,000 --> 0:00:26,000
Decía: "Noticia de última hora:

5
0:00:26.673,000 --> 0:00:28,000
dos explosiones en la Casa Blanca.

6
0:00:29.268,000 --> 0:00:31,000
Barack Obama ha resultado herido".

7
0:00:32.212,000 --> 0:00:37,000
Este tuit fue retuiteado 4000 veces en menos de cinco minutos,

8
0:00:37.661,000 --> 0:00:39,000
y se hizo viral a partir de ese instante.

9
0:00:40.71,000 --> 0:00:42,000
Ahora bien, este tuit no fue una noticia verdadera

10
0:00:43.104,000 --> 0:00:45,000
difundida por la agencia Associated Press.

11
0:00:45.134,000 --> 0:00:48,000
En realidad, fue una noticia falsa,

12
0:00:48.491,000 --> 0:00:5,000
propagada por 'hackers' sirios

13
0:00:51.34,000 --> 0:00:55,000
que se habían hecho con el control del Twitter de Associated Press.

14
0:00:56.407,000 --> 0:00:59,000
Su objetivo era alterar a la sociedad, pero alteraron mucho más,

15
0:01:00.32,000 --> 0:01:02,000
porque los algoritmos automatizados de negociación

16
0:01:02.82,000 --> 0:01:05,000
inmediatamente interpretaron la sensibilidad de este tuit,

17
0:01:06.204,000 --> 0:01:08,000
y comenzaron a operar en base a la posibilidad

18
0:01:09.196,000 --> 0:01:12,000
de que el presidente de los EE. UU. hubiese sido herido o asesinado

19
0:01:12.514,000 --> 0:01:13,000
en esa explosión.

20
0:01:14.188,000 --> 0:01:15,000
Y cuando empezaron a tuitear,

21
0:01:16.204,000 --> 0:01:19,000
hicieron que el mercado de valores se desplomara al instante,

22
0:01:19.577,000 --> 0:01:24,000
y se perdieron 140 mil millones de dólares en valor bursátil en un solo día.

23
0:01:25.062,000 --> 0:01:29,000
Robert Mueller, consejero y fiscal especial de los Estados Unidos,

24
0:01:29.562,000 --> 0:01:32,000
acusó penalmente a tres compañías rusas

25
0:01:33.478,000 --> 0:01:35,000
y a 13 individuos rusos

26
0:01:36.121,000 --> 0:01:39,000
de conspirar para cometer fraude contra los Estados Unidos

27
0:01:39.312,000 --> 0:01:42,000
al entrometerse en las elecciones presidenciales de 2016.

28
0:01:43.855,000 --> 0:01:46,000
Lo que esta acusación deja al descubierto

29
0:01:47.443,000 --> 0:01:5,000
es la historia de la Agencia de Investigación de Internet,

30
0:01:50.609,000 --> 0:01:53,000
el oscuro brazo del Kremlin en las redes sociales.

31
0:01:54.815,000 --> 0:01:56,000
Solo en las elecciones presidenciales,

32
0:01:57.616,000 --> 0:01:58,000
los intentos de la Agencia de Internet

33
0:01:59.529,000 --> 0:02:04,000
llegaron a 126 millones de personas en Facebook en los Estados Unidos,

34
0:02:04.72,000 --> 0:02:07,000
emitieron tres millones de tuits individuales

35
0:02:08.021,000 --> 0:02:11,000
y 43 horas de contenido de YouTube.

36
0:02:11.887,000 --> 0:02:12,000
Todo lo cual era falso,

37
0:02:13.563,000 --> 0:02:19,000
desinformación diseñada para meter cizaña en la elección presidencial de EE. UU.

38
0:02:20.936,000 --> 0:02:22,000
Un estudio reciente realizado por la Universidad de Oxford

39
0:02:23.67,000 --> 0:02:26,000
mostró que en las últimas elecciones suecas,

40
0:02:26.964,000 --> 0:02:3,000
un tercio de toda la información que se difundió en las redes sociales

41
0:02:31.363,000 --> 0:02:32,000
sobre las elecciones

42
0:02:32.585,000 --> 0:02:34,000
era falsa o incorrecta.

43
0:02:35.037,000 --> 0:02:4,000
Además, este tipo de campañas de desinformación en redes sociales

44
0:02:40.139,000 --> 0:02:44,000
pueden difundir lo que se ha llamado "propaganda genocida",

45
0:02:44.314,000 --> 0:02:47,000
por ejemplo contra los rohingya en Burma,

46
0:02:47.449,000 --> 0:02:49,000
que desencadenó linchamientos en la India.

47
0:02:49.776,000 --> 0:02:5,000
Estudiamos las noticias falsas

48
0:02:51.294,000 --> 0:02:54,000
y comenzamos a hacerlo antes de que fuera un término popular.

49
0:02:55.03,000 --> 0:03:,000
Y hemos publicado recientemente el estudio longitudinal más grande jamás realizado

50
0:03:00.094,000 --> 0:03:02,000
sobre la difusión de noticias falsas en línea

51
0:03:02.404,000 --> 0:03:05,000
en la portada de la revista "Science" en marzo de este año.

52
0:03:06.523,000 --> 0:03:1,000
Estudiamos todas las noticias verificadas como verdaderas y falsas

53
0:03:10.708,000 --> 0:03:11,000
que se propagaron por Twitter,

54
0:03:12.485,000 --> 0:03:15,000
desde su creación en 2006 hasta 2017.

55
0:03:16.612,000 --> 0:03:18,000
Y cuando estudiamos esta información,

56
0:03:18.95,000 --> 0:03:2,000
tomamos las noticias verificadas y revisadas

57
0:03:21.85,000 --> 0:03:24,000
por seis organizaciones independientes de comprobación de datos.

58
0:03:25.792,000 --> 0:03:27,000
Así que sabíamos cuáles eran ciertas

59
0:03:28.578,000 --> 0:03:29,000
y cuáles falsas.

60
0:03:30.728,000 --> 0:03:31,000
Podemos medir su difusión,

61
0:03:32.625,000 --> 0:03:33,000
la velocidad de su difusión,

62
0:03:34.3,000 --> 0:03:36,000
el alcance de su difusión,

63
0:03:36.419,000 --> 0:03:4,000
cuántas personas se enredan en esta cascada de información, etc.

64
0:03:40.942,000 --> 0:03:41,000
Y lo que hicimos en esta investigación

65
0:03:42.84,000 --> 0:03:45,000
fue comparar la propagación de noticias verdaderas con las falsas.

66
0:03:46.339,000 --> 0:03:47,000
Y estos son los resultados.

67
0:03:48.046,000 --> 0:03:51,000
Hallamos que una noticia falsa llega más lejos, más rápido

68
0:03:52.049,000 --> 0:03:53,000
y tiene más alcance que la verdadera

69
0:03:53.879,000 --> 0:03:56,000
en todas las categorías de información que hemos estudiado,

70
0:03:56.906,000 --> 0:03:58,000
a veces en un orden de magnitud.

71
0:03:59.842,000 --> 0:04:02,000
Y las noticias falsas en el ámbito de la política fueron las más virales.

72
0:04:03.39,000 --> 0:04:06,000
Se difunden más lejos, más rápido, y tienen mayor alcance

73
0:04:06.561,000 --> 0:04:08,000
que cualquier otro tipo de noticias falsas.

74
0:04:09.387,000 --> 0:04:1,000
Cuando vimos esto,

75
0:04:10.704,000 --> 0:04:12,000
sentimos a la vez preocupación y curiosidad.

76
0:04:13.569,000 --> 0:04:14,000
¿Por qué?

77
0:04:14.744,000 --> 0:04:17,000
¿Por qué las noticias falsas llegan más lejos, más rápido,

78
0:04:18.141,000 --> 0:04:19,000
y tienen mayor alcance que la verdad?

79
0:04:20.339,000 --> 0:04:22,000
La primera hipótesis que se nos ocurrió fue:

80
0:04:23.184,000 --> 0:04:25,000
"Bueno, tal vez quienes difunden noticias falsas

81
0:04:25.676,000 --> 0:04:27,000
tienen más seguidores o siguen a más gente,

82
0:04:28.14,000 --> 0:04:29,000
o tuitean con más frecuencia,

83
0:04:29.721,000 --> 0:04:33,000
o tal vez son más usuarios 'verificados' de Twitter, con más credibilidad,

84
0:04:33.871,000 --> 0:04:35,000
o tal vez han estado en Twitter más tiempo".

85
0:04:36.077,000 --> 0:04:38,000
Así que inspeccionamos cada uno de estos casos.

86
0:04:38.691,000 --> 0:04:4,000
Y lo que encontramos fue exactamente lo contrario.

87
0:04:41.535,000 --> 0:04:43,000
Quienes difundían noticias falsas tenían menos seguidores,

88
0:04:44.265,000 --> 0:04:47,000
seguían a menos gente, eran menos activos, eran usuarios poco "verificados"

89
0:04:47.847,000 --> 0:04:49,000
y habían estado en Twitter por un período de tiempo más corto.

90
0:04:50.841,000 --> 0:04:51,000
Y sin embargo,

91
0:04:52.054,000 --> 0:04:57,000
las noticias falsas eran un 70 % más propensas a ser retuiteadas,

92
0:04:57.111,000 --> 0:05:,000
teniendo en cuenta estos y muchos otros factores.

93
0:05:00.498,000 --> 0:05:02,000
Así que tuvimos que buscar otras explicaciones.

94
0:05:02.93,000 --> 0:05:05,000
E ideamos lo que llamamos "hipótesis de la novedad".

95
0:05:07.038,000 --> 0:05:08,000
Si leemos documentaciones sobre el tema,

96
0:05:09.022,000 --> 0:05:12,000
es bien sabido que la atención humana se siente atraída por la novedad,

97
0:05:12.8,000 --> 0:05:14,000
cosas que son nuevas en el entorno.

98
0:05:15.343,000 --> 0:05:17,000
Y si leemos la literatura sociológica,

99
0:05:17.352,000 --> 0:05:21,000
veremos que nos gusta compartir información novedosa.

100
0:05:21.676,000 --> 0:05:24,000
Sentimos que tenemos acceso a información privilegiada,

101
0:05:25.538,000 --> 0:05:28,000
y ganamos estatus mediante la difusión de este tipo de información.

102
0:05:29.792,000 --> 0:05:35,000
Decidimos entonces medir la novedad de un tuit verdadero o falso,

103
0:05:36.268,000 --> 0:05:4,000
en comparación con el corpus de lo que esa persona había visto

104
0:05:40.347,000 --> 0:05:42,000
en Twitter los 60 días anteriores.

105
0:05:43.323,000 --> 0:05:45,000
Pero no fue suficiente, porque pensamos:

106
0:05:46.006,000 --> 0:05:5,000
"Bueno, quizá las noticias falsas son más novedosas en un sentido teórico,

107
0:05:50.238,000 --> 0:05:53,000
pero tal vez la gente no las percibe como más novedosas".

108
0:05:53.849,000 --> 0:05:56,000
Así que para entender cómo la gente percibe las noticias falsas,

109
0:05:57.8,000 --> 0:06:,000
nos fijamos en la información y el componente afectivo

110
0:06:01.514,000 --> 0:06:05,000
de las respuestas a los tuits verdaderos y falsos.

111
0:06:06.022,000 --> 0:06:07,000
Y lo que detectamos

112
0:06:07.252,000 --> 0:06:11,000
fue que, teniendo en cuenta un montón de sentimientos diferentes,

113
0:06:11.49,000 --> 0:06:14,000
como sorpresa, disgusto, miedo, tristeza,

114
0:06:14.815,000 --> 0:06:16,000
expectativa, alegría y confianza,

115
0:06:17.323,000 --> 0:06:22,000
las noticias falsas generaron significativamente más sorpresa y disgusto

116
0:06:23.204,000 --> 0:06:25,000
en las respuestas a los falsos tuits.

117
0:06:26.392,000 --> 0:06:29,000
Y las noticias verdaderas mostraron significativamente más expectativas,

118
0:06:30.205,000 --> 0:06:31,000
alegría y confianza

119
0:06:31.776,000 --> 0:06:33,000
en respuesta a los tuits verdaderos.

120
0:06:34.347,000 --> 0:06:37,000
La sorpresa corrobora nuestra hipótesis de la novedad.

121
0:06:38.157,000 --> 0:06:42,000
Esto es nuevo y sorprendente, por lo que es más fácil que se comparta.

122
0:06:43.092,000 --> 0:06:45,000
Al mismo tiempo, hubo testimonios ante el Congreso

123
0:06:46.041,000 --> 0:06:49,000
en las dos cámaras parlamentarias de los Estados Unidos

124
0:06:49.101,000 --> 0:06:52,000
sobre el papel de los robots en la propagación de información errónea.

125
0:06:52.743,000 --> 0:06:53,000
Así que consideramos esto también.

126
0:06:54.381,000 --> 0:06:57,000
Utilizamos múltiples algoritmos complejos de rastreo

127
0:06:57.863,000 --> 0:07:,000
para encontrar los robots en nuestros datos y sacarlos.

128
0:07:01.347,000 --> 0:07:03,000
Los sacamos, los volvimos a poner

129
0:07:04.03,000 --> 0:07:07,000
y comparamos lo que sucede con nuestras mediciones.

130
0:07:07.173,000 --> 0:07:09,000
Descubrimos que, efectivamente,

131
0:07:09.49,000 --> 0:07:12,000
los robots aceleraban la propagación de noticias falsas en línea,

132
0:07:13.196,000 --> 0:07:15,000
pero aceleraban la propagación de las verdaderas

133
0:07:15.871,000 --> 0:07:17,000
aproximadamente a la misma velocidad.

134
0:07:18.3,000 --> 0:07:2,000
Lo que significa que los robots no son los responsables

135
0:07:21.182,000 --> 0:07:25,000
de la difusión diferencial de la verdad y la mentira en línea.

136
0:07:25.919,000 --> 0:07:27,000
No podemos renunciar a esa responsabilidad,

137
0:07:28.792,000 --> 0:07:32,000
porque nosotros, los seres humanos, somos responsables de esa propagación.

138
0:07:34.472,000 --> 0:07:37,000
Ahora bien, todo lo que les he dicho hasta el momento,

139
0:07:37.83,000 --> 0:07:38,000
por desgracia para todos nosotros,

140
0:07:39.608,000 --> 0:07:4,000
es la buena noticia.

141
0:07:42.67,000 --> 0:07:46,000
La razón es que está a punto de ponerse mucho peor.

142
0:07:47.85,000 --> 0:07:5,000
Y dos tecnologías específicas van a empeorar la situación.

143
0:07:52.207,000 --> 0:07:57,000
Vamos a presenciar el aumento de una tremenda ola de medios sintéticos.

144
0:07:57.403,000 --> 0:08:03,000
Video falso, audio falso, muy convincentes para el ojo humano.

145
0:08:03.458,000 --> 0:08:05,000
Y esto será impulsado por dos tecnologías.

146
0:08:06.236,000 --> 0:08:09,000
La primera es conocida como "redes de confrontación generativas".

147
0:08:10.093,000 --> 0:08:12,000
Es un modelo de aprendizaje automático con dos redes:

148
0:08:12.68,000 --> 0:08:13,000
un discriminador,

149
0:08:14.251,000 --> 0:08:18,000
cuyo trabajo es determinar si algo es verdadero o falso,

150
0:08:18.475,000 --> 0:08:19,000
y un generador,

151
0:08:19.666,000 --> 0:08:21,000
cuyo trabajo es generar medios sintéticos.

152
0:08:22.84,000 --> 0:08:27,000
El generador sintético genera un video o audio sintético,

153
0:08:27.966,000 --> 0:08:31,000
y el discriminador trata de distinguir si es verdadero o falso.

154
0:08:32.665,000 --> 0:08:34,000
Y, de hecho, el trabajo del generador

155
0:08:35.563,000 --> 0:08:39,000
es maximizar la probabilidad de engañar al discriminador

156
0:08:40.022,000 --> 0:08:43,000
para que crea que el video y el audio sintéticos que está creando

157
0:08:43.633,000 --> 0:08:44,000
son realmente ciertos.

158
0:08:45.387,000 --> 0:08:47,000
Imaginen una máquina en un Hyperloop,

159
0:08:47.784,000 --> 0:08:49,000
que se perfecciona más y más con el fin de engañarnos.

160
0:08:51.114,000 --> 0:08:53,000
Esto, combinado con la segunda tecnología,

161
0:08:53.638,000 --> 0:08:58,000
que es esencialmente la democratización de la inteligencia artificial,

162
0:08:59.384,000 --> 0:09:01,000
la capacidad de cualquier persona,

163
0:09:01.597,000 --> 0:09:03,000
sin ningún tipo de experiencia en inteligencia artificial

164
0:09:04.381,000 --> 0:09:05,000
o aprendizaje automático,

165
0:09:05.657,000 --> 0:09:09,000
de implementar este tipo de algoritmos para generar los medios sintéticos

166
0:09:09.784,000 --> 0:09:13,000
hace que, en última instancia, sea mucho más fácil crear videos.

167
0:09:14.355,000 --> 0:09:18,000
La Casa Blanca emitió el video falso y adulterado

168
0:09:18.8,000 --> 0:09:22,000
de una pasante que intentaba sacarle el micrófono a un periodista.

169
0:09:23.427,000 --> 0:09:24,000
Eliminaron fotogramas de este video

170
0:09:25.45,000 --> 0:09:28,000
para que las acciones del periodista pareciesen más violentas.

171
0:09:29.157,000 --> 0:09:32,000
Y cuando camarógrafos y dobles

172
0:09:32.566,000 --> 0:09:34,000
fueron consultados acerca de este tipo de técnica,

173
0:09:35.017,000 --> 0:09:38,000
dijeron: "Sí, siempre lo hacemos en las películas

174
0:09:38.869,000 --> 0:09:42,000
para que nuestros puñetazos y patadas parezcan más rápidos y agresivos".

175
0:09:44.268,000 --> 0:09:45,000
Entonces mostraron este video

176
0:09:46.159,000 --> 0:09:48,000
y lo utilizaron parcialmente como excusa

177
0:09:48.683,000 --> 0:09:53,000
para denegar el acceso de Jim Acosta como periodista a la Casa Blanca.

178
0:09:54.069,000 --> 0:09:58,000
Y la CNN tuvo que demandarlos para regresarle su pase de prensa.

179
0:10:00.538,000 --> 0:10:05,000
Hay unos cinco modos diferentes que se me ocurren

180
0:10:06.165,000 --> 0:10:09,000
para tratar de abordar algunos de estos problemas difíciles hoy en día.

181
0:10:10.379,000 --> 0:10:11,000
Cada uno es prometedor,

182
0:10:12.213,000 --> 0:10:14,000
pero tiene sus propios desafíos.

183
0:10:15.236,000 --> 0:10:16,000
El primero es el etiquetado.

184
0:10:17.268,000 --> 0:10:18,000
Piénsenlo de esta manera:

185
0:10:18.649,000 --> 0:10:21,000
cuando van a la tienda para comprar alimentos,

186
0:10:22.284,000 --> 0:10:23,000
está todo etiquetado.

187
0:10:24.212,000 --> 0:10:25,000
Saben la cantidad de calorías que tiene,

188
0:10:26.228,000 --> 0:10:27,000
la cantidad de grasa que contiene,

189
0:10:28.053,000 --> 0:10:32,000
pero, cuando consumimos información, no tenemos etiquetas de ningún tipo.

190
0:10:32.355,000 --> 0:10:33,000
¿Qué contiene esta información?

191
0:10:34.307,000 --> 0:10:35,000
¿Es creíble la fuente?

192
0:10:35.784,000 --> 0:10:37,000
¿De dónde se obtuvo esta información?

193
0:10:38.125,000 --> 0:10:41,000
No tenemos ninguno de esos datos cuando consumimos información.

194
0:10:42.101,000 --> 0:10:45,000
Esa es una vía potencial, pero viene con sus desafíos.

195
0:10:45.363,000 --> 0:10:51,000
Por ejemplo, ¿quién decide en la sociedad lo que es cierto y lo que es falso?

196
0:10:52.387,000 --> 0:10:53,000
¿Son los gobiernos?

197
0:10:54.053,000 --> 0:10:55,000
¿Es Facebook?

198
0:10:55.601,000 --> 0:10:58,000
¿Es un consorcio independiente de verificadores?

199
0:10:59.387,000 --> 0:11:01,000
¿Y quién controla a los verificadores?

200
0:11:02.427,000 --> 0:11:05,000
Otra vía potencial son los incentivos.

201
0:11:05.535,000 --> 0:11:07,000
Sabemos que durante la elección presidencial de EE. UU.

202
0:11:08.193,000 --> 0:11:11,000
se produjo una oleada de información falsa que procedía de Macedonia.

203
0:11:11.907,000 --> 0:11:13,000
No tenía ningún fin político

204
0:11:14.268,000 --> 0:11:16,000
pero sí un fin económico.

205
0:11:16.752,000 --> 0:11:18,000
Y este fin económico existió

206
0:11:18.924,000 --> 0:11:21,000
porque las noticias falsas viajan mucho más lejos, más rápido,

207
0:11:22.472,000 --> 0:11:24,000
y tienen mayor alcance que la verdad,

208
0:11:24.506,000 --> 0:11:28,000
y se puede ganar dinero con la publicidad mientras se atrae la atención

209
0:11:29.49,000 --> 0:11:3,000
con este tipo de información.

210
0:11:31.474,000 --> 0:11:34,000
Pero si podemos reducir la difusión de esta información,

211
0:11:35.331,000 --> 0:11:37,000
tal vez se reduciría el incentivo económico

212
0:11:38.252,000 --> 0:11:4,000
para producirla.

213
0:11:40.966,000 --> 0:11:42,000
En tercer lugar, pensemos en la regulación

214
0:11:43.49,000 --> 0:11:45,000
y, desde luego, debemos pensar en esta opción.

215
0:11:45.839,000 --> 0:11:46,000
En EE. UU., en la actualidad,

216
0:11:47.474,000 --> 0:11:51,000
estamos explorando lo que podría suceder si Facebook y otros medios se regularan.

217
0:11:52.346,000 --> 0:11:55,000
Aunque debemos tener en cuenta cosas como la regulación del discurso político,

218
0:11:56.171,000 --> 0:11:58,000
es decir, etiquetarlo como discurso político,

219
0:11:58.589,000 --> 0:12:01,000
asegurarse de que los actores extranjeros no puedan financiar el discurso político,

220
0:12:02.546,000 --> 0:12:04,000
también tiene sus propios peligros.

221
0:12:05.522,000 --> 0:12:09,000
Por ejemplo, Malasia acaba de instituir una condena de seis años de prisión

222
0:12:10.424,000 --> 0:12:13,000
para cualquier persona que sea sorprendida difundiendo datos falsos.

223
0:12:13.696,000 --> 0:12:15,000
Y en los regímenes autoritarios,

224
0:12:15.799,000 --> 0:12:19,000
este tipo de políticas se pueden utilizar para suprimir las opiniones minoritarias

225
0:12:20.489,000 --> 0:12:23,000
y para seguir ampliando la represión.

226
0:12:24.68,000 --> 0:12:27,000
La cuarta opción posible es la transparencia.

227
0:12:28.843,000 --> 0:12:31,000
Queremos saber cómo funcionan los algoritmos de Facebook.

228
0:12:32.581,000 --> 0:12:34,000
¿De qué manera los datos se combinan con los algoritmos

229
0:12:35.485,000 --> 0:12:37,000
para producir los resultados que vemos?

230
0:12:38.347,000 --> 0:12:4,000
Queremos que abran el kimono

231
0:12:40.72,000 --> 0:12:44,000
y nos muestren exactamente el funcionamiento interno de Facebook.

232
0:12:44.838,000 --> 0:12:47,000
Y si queremos conocer el efecto de las redes sociales en la sociedad,

233
0:12:48.091,000 --> 0:12:5,000
necesitamos que científicos, investigadores y otras personas

234
0:12:51.001,000 --> 0:12:53,000
tengan acceso a este tipo de información.

235
0:12:53.038,000 --> 0:12:54,000
Pero al mismo tiempo,

236
0:12:54.609,000 --> 0:12:57,000
estamos pidiendo a Facebook poner todo bajo llave

237
0:12:58.434,000 --> 0:13:,000
para mantener los datos seguros.

238
0:13:00.631,000 --> 0:13:03,000
Así, Facebook y las otras plataformas de medios sociales

239
0:13:03.814,000 --> 0:13:06,000
se enfrentan a lo que llamo "la paradoja de la transparencia".

240
0:13:07.266,000 --> 0:13:09,000
Les estamos pidiendo

241
0:13:09.964,000 --> 0:13:13,000
que sean abiertas, transparentes y, al mismo tiempo, seguras.

242
0:13:14.797,000 --> 0:13:16,000
Esta es una aguja muy difícil enhebrar,

243
0:13:17.512,000 --> 0:13:18,000
pero deberán enhebrar esta aguja

244
0:13:19.449,000 --> 0:13:22,000
si queremos alcanzar la promesa de las tecnologías sociales

245
0:13:23.26,000 --> 0:13:24,000
y, a la vez, evitar sus riesgos.

246
0:13:24.926,000 --> 0:13:26,000
La última opción posible

247
0:13:27.225,000 --> 0:13:29,000
son los algoritmos y el aprendizaje automático,

248
0:13:29.641,000 --> 0:13:33,000
tecnología ideada para erradicar y entender las noticias falsas,

249
0:13:33.891,000 --> 0:13:36,000
cómo se transmiten, y tratar de reducir su difusión.

250
0:13:37.824,000 --> 0:13:39,000
La humanidad tiene que estar en el bucle de esta tecnología,

251
0:13:40.745,000 --> 0:13:42,000
porque nunca podremos negar

252
0:13:43.047,000 --> 0:13:47,000
que detrás de cualquier solución o enfoque tecnológico

253
0:13:47.109,000 --> 0:13:51,000
hay una pregunta ética y filosófica fundamental

254
0:13:51.18,000 --> 0:13:54,000
acerca de cómo definimos la verdad y la falsedad,

255
0:13:54.474,000 --> 0:13:57,000
a quién le damos el poder de definir la verdad y la mentira,

256
0:13:57.678,000 --> 0:13:59,000
y qué opiniones son legítimas,

257
0:14:00.162,000 --> 0:14:03,000
qué tipo de discurso debe permitirse y así sucesivamente.

258
0:14:03.851,000 --> 0:14:05,000
La tecnología no es una solución en este caso.

259
0:14:06.244,000 --> 0:14:09,000
La ética y la filosofía son la solución.

260
0:14:10.95,000 --> 0:14:13,000
Casi todas las teorías de la toma de decisiones humanas,

261
0:14:14.292,000 --> 0:14:16,000
la cooperación humana y la coordinación humana

262
0:14:17.077,000 --> 0:14:2,000
tienen un cierto sentido de la verdad en su esencia.

263
0:14:21.347,000 --> 0:14:23,000
Pero con el aumento de noticias falsas,

264
0:14:23.427,000 --> 0:14:24,000
de videos falsos,

265
0:14:24.894,000 --> 0:14:25,000
de audios falsos,

266
0:14:26.8,000 --> 0:14:29,000
estamos al borde del precipicio del fin de la realidad,

267
0:14:30.748,000 --> 0:14:33,000
donde no podemos diferenciar lo que es real de lo que es falso.

268
0:14:34.661,000 --> 0:14:37,000
Y eso es potencialmente muy peligroso.

269
0:14:38.931,000 --> 0:14:41,000
Tenemos que estar vigilantes en la defensa de la verdad

270
0:14:42.903,000 --> 0:14:43,000
contra la información errónea,

271
0:14:44.919,000 --> 0:14:47,000
con nuestras tecnologías, con nuestras políticas

272
0:14:48.379,000 --> 0:14:49,000
y, quizás lo más importante,

273
0:14:50.323,000 --> 0:14:53,000
con nuestras propias responsabilidades,

274
0:14:53.561,000 --> 0:14:56,000
decisiones, comportamientos y acciones individuales.

275
0:14:57.553,000 --> 0:14:58,000
Muchas gracias.

276
0:14:59.014,000 --> 0:15:02,000
(Aplausos)

