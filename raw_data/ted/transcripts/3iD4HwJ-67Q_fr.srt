1
0:00:,000 --> 0:00:07,000
Traducteur: Claire Ghyselen Relecteur: Anne-Sophie Matichard

2
0:00:13.548,000 --> 0:00:15,000
Peu importe qui vous êtes ou où vous habitez,

3
0:00:16.064,000 --> 0:00:18,000
je suis convaincue que vous avez au moins un parent

4
0:00:18.48,000 --> 0:00:2,000
qui adore transférer ces courriels.

5
0:00:21.206,000 --> 0:00:22,000
Vous savez de quoi je parle,

6
0:00:23.106,000 --> 0:00:26,000
ceux avec des affirmations douteuses ou des vidéos de conspiration.

7
0:00:26.33,000 --> 0:00:28,000
Vous les avez sans doute bloqués sur Facebook

8
0:00:29.007,000 --> 0:00:31,000
parce qu'ils partagent ce genre de posts.

9
0:00:31.355,000 --> 0:00:32,000
C'est l'illustration d'une banane

10
0:00:32.939,000 --> 0:00:34,000
avec une étrange croix rouge en son cœur.

11
0:00:35.498,000 --> 0:00:37,000
Le message avertit le lecteur

12
0:00:37.661,000 --> 0:00:39,000
d'éviter de manger un fruit pareil

13
0:00:39.843,000 --> 0:00:41,000
car on y aurait injecté du sang

14
0:00:41.895,000 --> 0:00:43,000
contaminé par le virus du VIH.

15
0:00:44.049,000 --> 0:00:46,000
L'intitulé du message est simplement :

16
0:00:46.676,000 --> 0:00:48,000
« Transférez pour sauver des vies ! »

17
0:00:49.672,000 --> 0:00:52,000
Les vérificateurs de faits tentent de le désamorcer depuis des années

18
0:00:52.966,000 --> 0:00:54,000
mais c'est le genre de rumeurs qui ne veulent pas s'éteindre.

19
0:00:55.823,000 --> 0:00:56,000
Une rumeur zombie.

20
0:00:57.513,000 --> 0:00:59,000
Naturellement, totalement fausse.

21
0:01:00.18,000 --> 0:01:02,000
Cela pourrait être risible et on pourrait se demander

22
0:01:03.163,000 --> 0:01:04,000
qui va bien pouvoir croire ça.

23
0:01:05.419,000 --> 0:01:06,000
C'est précisément une rumeur zombie

24
0:01:07.135,000 --> 0:01:1,000
car elle se nourrit des peurs les plus profondes des gens quant à leur sécurité

25
0:01:10.982,000 --> 0:01:12,000
et celles des personnes qui leur sont chères.

26
0:01:13.783,000 --> 0:01:16,000
Quand on passe autant de temps que moi à étudier la désinformation,

27
0:01:17.056,000 --> 0:01:19,000
on comprend vite qu'il s'agit d'un exemple parmi tant d'autres

28
0:01:19.996,000 --> 0:01:21,000
qui fait appel aux craintes et vulnérabilités des gens.

29
0:01:23.214,000 --> 0:01:27,000
Tous les jours, partout dans le monde, des nouveaux mèmes sur Instagram

30
0:01:27.607,000 --> 0:01:3,000
encouragent les parents à ne pas vacciner leurs enfants,

31
0:01:30.67,000 --> 0:01:34,000
des vidéos sur YouTube expliquent que le changement climatique est un hoax.

32
0:01:35.226,000 --> 0:01:39,000
Sur toutes les plateformes, un nombre infini de messages diabolisent l'autre

33
0:01:39.552,000 --> 0:01:42,000
sur base de leur race, de leur religion ou de leur orientation sexuelle.

34
0:01:44.314,000 --> 0:01:47,000
Bienvenue au cœur d'un des plus grands défis de notre temps.

35
0:01:47.647,000 --> 0:01:51,000
Comment perpétuer un Internet fondé sur la liberté d'expression

36
0:01:51.696,000 --> 0:01:54,000
tout en garantissant que le contenu qui y est propagé

37
0:01:55.023,000 --> 0:01:58,000
ne cause pas de préjudice irréversible à nos démocraties, à nos communautés

38
0:01:58.933,000 --> 0:02:,000
et à notre bien-être physique et mental ?

39
0:02:01.998,000 --> 0:02:03,000
Nous vivons en effet dans l'ère de l'information

40
0:02:04.265,000 --> 0:02:07,000
et pourtant notre devise principale, celle dont on dépend tous, l'information,

41
0:02:07.926,000 --> 0:02:09,000
n'est plus considérée totalement fiable

42
0:02:10.061,000 --> 0:02:12,000
et semble parfois même franchement dangereuse.

43
0:02:12.811,000 --> 0:02:15,000
C'est partiellement dû à la croissance monstrueuse des plateformes sociales

44
0:02:16.772,000 --> 0:02:17,000
qui nous permettent de parcourir

45
0:02:18.414,000 --> 0:02:2,000
contenus mensongers et des faits l'un à côté de l'autre

46
0:02:20.986,000 --> 0:02:22,000
mais sans les signaux traditionnels de véracité.

47
0:02:24.268,000 --> 0:02:27,000
Et bon dieu, notre langage à ce sujet est affreusement confus.

48
0:02:27.911,000 --> 0:02:3,000
On est encore obsédé par le terme « intox »,

49
0:02:31.038,000 --> 0:02:33,000
même s'il est particulièrement obscur

50
0:02:33.593,000 --> 0:02:36,000
et utilisé pour décrire des phénomènes très différents :

51
0:02:37.077,000 --> 0:02:4,000
mensonges, rumeurs, hoax, conspirations et propagande.

52
0:02:40.911,000 --> 0:02:42,000
Je souhaite tant qu'on cesse d'utiliser cette expression

53
0:02:43.847,000 --> 0:02:45,000
adoptée par les politiciens partout dans le monde,

54
0:02:46.733,000 --> 0:02:47,000
de gauche comme de droite,

55
0:02:48.228,000 --> 0:02:51,000
et brandie comme une arme contre une presse libre et indépendante.

56
0:02:52.307,000 --> 0:02:56,000
(Applaudissements)

57
0:02:57.033,000 --> 0:03:,000
Car un journalisme professionnel est plus que jamais indispensable.

58
0:03:00.882,000 --> 0:03:03,000
De plus, la plupart de ces contenus ne se déguisent même pas en info.

59
0:03:04.279,000 --> 0:03:06,000
Ce sont des mèmes, des vidéos et des posts sociaux.

60
0:03:06.945,000 --> 0:03:09,000
La plupart ne sont pas faux, ils sont trompeurs.

61
0:03:10.422,000 --> 0:03:13,000
On a tendance à se concentrer sur ce qui est vrai ou faux.

62
0:03:13.461,000 --> 0:03:17,000
Toutefois, la préoccupation principale est le détournement abusif du contexte.

63
0:03:18.855,000 --> 0:03:19,000
Car la désinformation la plus efficace

64
0:03:20.847,000 --> 0:03:23,000
contient toujours une part de vérité.

65
0:03:23.919,000 --> 0:03:25,000
Prenons l'exemple de Londres.

66
0:03:25.969,000 --> 0:03:27,000
Un tweet circule largement depuis mars 2017,

67
0:03:28.983,000 --> 0:03:31,000
juste après l'attentat terroriste sur le pont de Westminster.

68
0:03:32.594,000 --> 0:03:34,000
Cette photo est vraie, ce n'est pas un montage.

69
0:03:35.046,000 --> 0:03:38,000
On a interviewé la femme qui apparaît sur la photo par la suite

70
0:03:38.239,000 --> 0:03:4,000
et elle a expliqué qu'elle était tétanisée de peur.

71
0:03:40.672,000 --> 0:03:41,000
Elle parlait à une personne chère

72
0:03:42.434,000 --> 0:03:44,000
et détournait le regard de la victime par respect.

73
0:03:45.076,000 --> 0:03:48,000
Mais la photo a circulé largement avec un titre islamophobe

74
0:03:49.06,000 --> 0:03:52,000
et de multiples hashtags dont : #BanIslam.

75
0:03:52.425,000 --> 0:03:54,000
Si vous étiez employé par Twitter, comment réagiriez-vous ?

76
0:03:55.203,000 --> 0:03:57,000
Vous le supprimeriez ou le laisseriez ?

77
0:03:58.553,000 --> 0:04:01,000
Ma réaction viscérale et émotionnelle serait de le supprimer.

78
0:04:02.006,000 --> 0:04:04,000
Je déteste le choix de l'angle.

79
0:04:04.585,000 --> 0:04:06,000
Mais la liberté d'expression est un droit de l'homme.

80
0:04:07.103,000 --> 0:04:1,000
Et si on commence à censurer les propos qui nous mettent mal à l'aise,

81
0:04:10.402,000 --> 0:04:11,000
c'est le début des problèmes.

82
0:04:11.826,000 --> 0:04:12,000
Ce cas semble évident

83
0:04:13.818,000 --> 0:04:14,000
mais la plupart ne le sont pas.

84
0:04:15.54,000 --> 0:04:17,000
La ligne de démarcation est floue.

85
0:04:18,000 --> 0:04:2,000
Qu'est-ce qui sépare une décision bien intentionnée

86
0:04:20.421,000 --> 0:04:21,000
de la censure ?

87
0:04:22.759,000 --> 0:04:24,000
Nous savons à présent que ce compte, Texas Lone Star,

88
0:04:25.712,000 --> 0:04:28,000
faisait partie d'une campagne russe de désinformation

89
0:04:28.966,000 --> 0:04:3,000
qui a été démantelée depuis lors.

90
0:04:31.141,000 --> 0:04:32,000
Cela change-t-il votre opinion ?

91
0:04:33.322,000 --> 0:04:34,000
Ça influencerait la mienne

92
0:04:34.601,000 --> 0:04:36,000
car il s'agit d'une campagne orchestrée

93
0:04:36.83,000 --> 0:04:37,000
pour semer la zizanie.

94
0:04:38.069,000 --> 0:04:39,000
Et pour ceux et celles qui pensent

95
0:04:40.03,000 --> 0:04:42,000
que l'intelligence artificielle va résoudre tous nos problèmes,

96
0:04:42.991,000 --> 0:04:43,000
Nous sommes tous d'accord pour dire

97
0:04:44.914,000 --> 0:04:47,000
que le temps où l'IA pourra comprendre ça n'est pas encore venu.

98
0:04:48.856,000 --> 0:04:5,000
Je vais d'abord vous présenter trois enjeux entrelacés

99
0:04:51.633,000 --> 0:04:53,000
qui rendent tout ça si compliqué.

100
0:04:53.76,000 --> 0:04:56,000
Ensuite, je vous proposerai quelques manières de réfléchir à ces défis.

101
0:04:57.348,000 --> 0:05:,000
D'abord, notre relation à l'information n'est pas rationnelle,

102
0:05:01.262,000 --> 0:05:02,000
elle est émotionnelle.

103
0:05:02.754,000 --> 0:05:05,000
Il n'est pas vrai que davantage de faits vont amender les choses

104
0:05:06.572,000 --> 0:05:09,000
car les algorithmes déterminent ce que nous voyons,

105
0:05:09.696,000 --> 0:05:12,000
ils sont conçus pour récompenser nos réactions émotionnelles.

106
0:05:12.847,000 --> 0:05:13,000
Or, quand on a peur,

107
0:05:14.252,000 --> 0:05:17,000
les narrations simplifiées à outrance, les théories de conspiration

108
0:05:17.45,000 --> 0:05:2,000
et les termes qui diabolisent l'autre sont encore plus efficaces.

109
0:05:21.538,000 --> 0:05:23,000
De surcroît, le modèle de fonctionnement de ces plateformes

110
0:05:24.352,000 --> 0:05:25,000
est basé sur l'attention.

111
0:05:26.006,000 --> 0:05:29,000
Ça signifie que leurs algorithmes sont conçus pour favoriser l'émotion.

112
0:05:30.371,000 --> 0:05:34,000
Ensuite, la plupart des discours que j'évoque sont légaux.

113
0:05:35.081,000 --> 0:05:36,000
La situation serait différente

114
0:05:36.551,000 --> 0:05:38,000
si on parlait d'images de pédophilie

115
0:05:38.916,000 --> 0:05:39,000
ou de contenu incitant à la violence.

116
0:05:40.867,000 --> 0:05:43,000
Il est parfaitement légal de poster un mensonge patent.

117
0:05:45.13,000 --> 0:05:49,000
Toutefois, on continue de censurer les contenus problématiques ou nuisibles

118
0:05:49.188,000 --> 0:05:51,000
sans définition claire de ce que ça signifie.

119
0:05:51.797,000 --> 0:05:52,000
Mark Zuckerberg n'y échappe pas

120
0:05:53.361,000 --> 0:05:56,000
quand il plaide en faveur d'une réglementation mondiale des discours.

121
0:05:56.87,000 --> 0:05:58,000
Je crains que les gouvernements

122
0:05:59.109,000 --> 0:06:,000
partout dans le monde

123
0:06:00.425,000 --> 0:06:02,000
mettent en place des décisions politiques précipitées

124
0:06:03.101,000 --> 0:06:06,000
qui pourraient en réalité générer des conséquences bien plus graves

125
0:06:06.311,000 --> 0:06:07,000
en matière d'expression.

126
0:06:08.006,000 --> 0:06:11,000
Même si nous pouvions décider quels contenus conserver ou éliminer,

127
0:06:11.736,000 --> 0:06:13,000
nous n'avons jamais eu tant de contenus.

128
0:06:13.934,000 --> 0:06:15,000
Chaque seconde, des millions de contenus

129
0:06:16.065,000 --> 0:06:18,000
sont téléchargés partout dans le monde,

130
0:06:18.222,000 --> 0:06:19,000
dans des langues différentes

131
0:06:19.598,000 --> 0:06:21,000
et basés sur des contextes culturels multiples.

132
0:06:22.204,000 --> 0:06:24,000
Nous n'avons aucun mécanisme efficace

133
0:06:24.736,000 --> 0:06:26,000
pour modérer les discours à cette échelle,

134
0:06:26.754,000 --> 0:06:28,000
que ce soit avec des humains ou de la technologie.

135
0:06:30.284,000 --> 0:06:33,000
Troisièmement, ces sociétés, Google, Twitter, Facebook, WhatsApp,

136
0:06:34.252,000 --> 0:06:36,000
font part d'un écosystème d'information plus vaste.

137
0:06:37.117,000 --> 0:06:4,000
On a tendance à leur faire porter le blâme mais en fait,

138
0:06:40.493,000 --> 0:06:43,000
les médias et les représentants politiques peuvent jouer un rôle aussi important

139
0:06:44.323,000 --> 0:06:47,000
pour amplifier les rumeurs et les conspirations qui les arrangent.

140
0:06:47.8,000 --> 0:06:51,000
Nous aussi, nous sommes susceptibles de diffuser du contenu trompeur

141
0:06:52.768,000 --> 0:06:53,000
sans le vouloir.

142
0:06:54.077,000 --> 0:06:55,000
On accentue la pollution.

143
0:06:57.236,000 --> 0:06:59,000
Nous cherchons tous un remède miracle.

144
0:06:59.878,000 --> 0:07:,000
Mais ça n'existe pas.

145
0:07:01.95,000 --> 0:07:05,000
Toute solution devra être mise en place à une échelle massive, celle d'Internet,

146
0:07:06.419,000 --> 0:07:09,000
et les plateformes en fait, sont habituées à cette échelle.

147
0:07:09.704,000 --> 0:07:12,000
Mais pouvons-nous et devrions-nous les autoriser à gérer ces problèmes ?

148
0:07:13.668,000 --> 0:07:14,000
Certes, elles essaient.

149
0:07:14.924,000 --> 0:07:18,000
Nous sommes presque tous d'accord de ne pas souhaiter voir des multinationales

150
0:07:19.01,000 --> 0:07:21,000
devenir les gardiens de la vérité et de l'équité sur Internet.

151
0:07:21.932,000 --> 0:07:23,000
Les plateformes seront d'accord aussi, je crois.

152
0:07:24.317,000 --> 0:07:27,000
Actuellement, elles sont en train de s'accorder des bons points.

153
0:07:27.328,000 --> 0:07:28,000
Elles nous disent

154
0:07:28.384,000 --> 0:07:3,000
que leurs interventions sont efficaces.

155
0:07:30.603,000 --> 0:07:33,000
Mais elles rédigent elles-mêmes leurs rapports sur leur transparence.

156
0:07:34.183,000 --> 0:07:36,000
Il n'y a donc aucun moyen indépendant pour vérifier les faits.

157
0:07:38.431,000 --> 0:07:41,000
(Applaudissements)

158
0:07:41.773,000 --> 0:07:44,000
Pour être franche, la plupart des changements que nous observons

159
0:07:44.795,000 --> 0:07:46,000
ont lieu après une enquête journalistique

160
0:07:47.767,000 --> 0:07:48,000
et la mise à nu de preuves de biais

161
0:07:49.468,000 --> 0:07:52,000
ou de contenus qui enfreignent les règles de fonctionnement.

162
0:07:52.815,000 --> 0:07:56,000
Bien sûr, ces entreprises doivent jouer un rôle crucial dans le processus

163
0:07:57.41,000 --> 0:07:58,000
mais elles ne peuvent pas le contrôler.

164
0:07:59.855,000 --> 0:08:,000
Qu'en est-il alors des gouvernements ?

165
0:08:01.863,000 --> 0:08:04,000
Beaucoup fondent leur dernier espoir dans une réglementation mondiale

166
0:08:05.159,000 --> 0:08:07,000
pour rendre notre écosystème propre.

167
0:08:07.887,000 --> 0:08:1,000
Mais les politiciens peinent à rester à jour

168
0:08:11.077,000 --> 0:08:13,000
avec la vélocité de l'évolution technologique.

169
0:08:13.442,000 --> 0:08:14,000
Pire, ils travaillent dans le noir

170
0:08:15.37,000 --> 0:08:16,000
car ils n'ont pas accès aux données

171
0:08:17.215,000 --> 0:08:19,000
pour comprendre ce qu'il se passe sur ces plateformes.

172
0:08:20.26,000 --> 0:08:23,000
Et puis, en quel gouvernement pourrions-nous faire confiance ?

173
0:08:23.355,000 --> 0:08:25,000
La réaction doit être mondiale et pas au niveau national.

174
0:08:27.419,000 --> 0:08:29,000
Le maillon manquant, c'est nous.

175
0:08:29.696,000 --> 0:08:32,000
Nous, femmes et hommes qui utilisons ces technologies tous les jours.

176
0:08:33.26,000 --> 0:08:35,000
Saurons-nous concevoir une nouvelle infrastructure

177
0:08:35.801,000 --> 0:08:37,000
pour véhiculer une information de qualité ?

178
0:08:38.371,000 --> 0:08:39,000
Je pense que oui.

179
0:08:39.625,000 --> 0:08:42,000
J'ai quelques idées pour y arriver.

180
0:08:43.006,000 --> 0:08:46,000
D'abord, si nous sommes déterminés à engager le public,

181
0:08:46.109,000 --> 0:08:48,000
Wikipedia pourrait être une source d'inspiration.

182
0:08:48.52,000 --> 0:08:49,000
Ils ont montré que c'est faisable.

183
0:08:50.482,000 --> 0:08:51,000
Ce n'est pas parfait

184
0:08:51.561,000 --> 0:08:53,000
mais ils ont démontré qu'avec les structures adéquates,

185
0:08:54.219,000 --> 0:08:56,000
une vision mondiale et une transparence totale,

186
0:08:56.854,000 --> 0:08:59,000
on peut construire quelque chose qui gagnera la confiance du public.

187
0:09:00.05,000 --> 0:09:03,000
Nous devons trouver un moyen de puiser dans la sagesse collective

188
0:09:03.184,000 --> 0:09:05,000
et l'expérience de tous les utilisateurs,

189
0:09:05.517,000 --> 0:09:07,000
particulièrement les femmes, les personnes de couleur

190
0:09:08.187,000 --> 0:09:09,000
et les minorités.

191
0:09:09.557,000 --> 0:09:09,000
Pourquoi ?

192
0:09:10.483,000 --> 0:09:12,000
Parce qu'ils sont spécialisés dans la haine et la désinformation

193
0:09:13.506,000 --> 0:09:16,000
dès lors qu'ils sont la cible de ces campagnes depuis si longtemps.

194
0:09:17.046,000 --> 0:09:19,000
Cela fait des années qu'ils sonnent l'alarme

195
0:09:19.42,000 --> 0:09:2,000
mais personne ne les écoute.

196
0:09:21.109,000 --> 0:09:22,000
Cela doit changer.

197
0:09:22.807,000 --> 0:09:26,000
Est-il possible de construire un Wikipedia de la confiance ?

198
0:09:27.157,000 --> 0:09:31,000
Comment les utilisateurs pourraient-ils nous nourrir de leurs idées ?

199
0:09:31.37,000 --> 0:09:34,000
Des idées au sujet des décisions complexes de la modération de contenu.

200
0:09:35.091,000 --> 0:09:39,000
Un retour quand les plateformes décident de mettre en œuvre des changements.

201
0:09:40.241,000 --> 0:09:44,000
Ensuite, l'expérience que les gens ont de l'information est personnalisée.

202
0:09:44.427,000 --> 0:09:46,000
Mon fil de Facebook est différent du vôtre.

203
0:09:47.094,000 --> 0:09:49,000
Vos recommandations Youtube sont différentes des miennes.

204
0:09:49.863,000 --> 0:09:51,000
Ceci rend impossible l'analyse en profondeur

205
0:09:52.379,000 --> 0:09:54,000
des informations vues par les gens.

206
0:09:54.815,000 --> 0:09:55,000
On pourrait imaginer

207
0:09:56.228,000 --> 0:10:,000
de créer une sorte de répertoire ouvert pour les données anonymisées

208
0:10:01.03,000 --> 0:10:03,000
qui prenne en considération vie privée et éthique.

209
0:10:04.22,000 --> 0:10:05,000
Imaginez le potentiel de compréhension

210
0:10:06.052,000 --> 0:10:09,000
issu de la création d'un réseau mondial de citoyens engagés

211
0:10:09.307,000 --> 0:10:12,000
qui souhaitent faire don de leurs données sociales à la science.

212
0:10:13.141,000 --> 0:10:14,000
Nous ignorons en effet

213
0:10:14.863,000 --> 0:10:17,000
les conséquences à long terme de la haine et de la désinformation

214
0:10:18.157,000 --> 0:10:2,000
sur l'état d'esprit et les comportements.

215
0:10:20.236,000 --> 0:10:21,000
Et ce que nous en savons

216
0:10:21.427,000 --> 0:10:23,000
provient d'études réalisées aux États-Unis,

217
0:10:23.644,000 --> 0:10:25,000
alors que le problème est mondial.

218
0:10:26.049,000 --> 0:10:27,000
Nous devons aussi remédier à ça.

219
0:10:28.192,000 --> 0:10:29,000
Troisièmement,

220
0:10:29.366,000 --> 0:10:31,000
comment relier toutes les parties ?

221
0:10:31.7,000 --> 0:10:34,000
Aucun secteur seul, encore moins une ONG, une start-up ou un gouvernement

222
0:10:35.138,000 --> 0:10:36,000
ne pourra résoudre ça.

223
0:10:36.56,000 --> 0:10:38,000
Mais de nombreuses personnes intelligentes dans le monde

224
0:10:39.196,000 --> 0:10:41,000
s'attaquent à ces défis dans les salles de rédaction,

225
0:10:41.667,000 --> 0:10:44,000
la société civile, le monde académique, les communautés activistes.

226
0:10:44.831,000 --> 0:10:45,000
En voici quelques-uns.

227
0:10:46.099,000 --> 0:10:48,000
Certains élaborent des indicateurs de crédibilité des contenus.

228
0:10:49.05,000 --> 0:10:5,000
D'autres vérifient les faits

229
0:10:50.404,000 --> 0:10:53,000
pour dégrader le taux de référencement des allégations, vidéos et images fausses.

230
0:10:54.219,000 --> 0:10:56,000
J'ai participé à la création d'une ONG, First Draft,

231
0:10:56.656,000 --> 0:10:58,000
qui collabore avec des salles de rédaction dans le monde

232
0:10:59.308,000 --> 0:11:02,000
pour les aider à élaborer des programmes collaboratifs d'investigation.

233
0:11:03.231,000 --> 0:11:05,000
Dennis Hillis, un programmateur,

234
0:11:05.564,000 --> 0:11:07,000
conçoit un nouveau système appelé The Underlay,

235
0:11:07.835,000 --> 0:11:1,000
qui sera l'archive de toutes les déclarations publiques de faits

236
0:11:10.898,000 --> 0:11:12,000
reliées à leurs sources afin que le public et les algorithmes

237
0:11:13.859,000 --> 0:11:15,000
puissent mieux évaluer ce qui est crédible.

238
0:11:16.72,000 --> 0:11:19,000
Des éducateurs dans le monde testent diverses techniques

239
0:11:20.18,000 --> 0:11:23,000
pour aiguiser l'esprit critique des gens par rapport aux contenus.

240
0:11:24.633,000 --> 0:11:27,000
Tous ces efforts sont impressionnants mais ils sont dans des silos

241
0:11:27.798,000 --> 0:11:29,000
et la plupart sont lamentablement sous-financés.

242
0:11:30.502,000 --> 0:11:32,000
Des centaines de personnes brillantes

243
0:11:32.579,000 --> 0:11:33,000
travaillent dans ces entreprises

244
0:11:34.231,000 --> 0:11:36,000
mais à nouveau, leurs efforts ne sont pas consolidés

245
0:11:36.684,000 --> 0:11:39,000
car ils développent des solutions différentes aux mêmes problèmes.

246
0:11:41.205,000 --> 0:11:43,000
Comment associer ces personnes

247
0:11:43.498,000 --> 0:11:46,000
dans un seul lieu physique, pendant quelques jours ou semaines,

248
0:11:46.8,000 --> 0:11:48,000
pour leur permettre de réfléchir ensemble à ces problèmes

249
0:11:49.52,000 --> 0:11:5,000
mais avec leur propre perspective ?

250
0:11:51.222,000 --> 0:11:52,000
Est-ce possible ?

251
0:11:52.426,000 --> 0:11:55,000
Est-il possible de créer une réponse ambitieuse et coordonnée,

252
0:11:55.689,000 --> 0:11:58,000
à la hauteur de l'échelle et de la complexité de ce problème ?

253
0:11:59.819,000 --> 0:12:,000
J'en suis convaincue.

254
0:12:01.216,000 --> 0:12:04,000
Ensemble, reconstruisons nos fondations communes de l'information.

255
0:12:04.819,000 --> 0:12:05,000
Merci.

256
0:12:06.033,000 --> 0:12:09,000
(Applaudissements)

