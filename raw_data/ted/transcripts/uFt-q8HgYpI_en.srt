1
0:00:12.836,000 --> 0:00:14,000
[This talk contains mature content]

2
0:00:17.741,000 --> 0:00:18,000
Five years ago,

3
0:00:19.385,000 --> 0:00:22,000
I received a phone call that would change my life.

4
0:00:23.795,000 --> 0:00:25,000
I remember so vividly that day.

5
0:00:27.245,000 --> 0:00:28,000
It was about this time of year,

6
0:00:29.17,000 --> 0:00:3,000
and I was sitting in my office.

7
0:00:31.692,000 --> 0:00:34,000
I remember the sun streaming through the window.

8
0:00:35.592,000 --> 0:00:36,000
And my phone rang.

9
0:00:37.565,000 --> 0:00:38,000
And I picked it up,

10
0:00:39.594,000 --> 0:00:42,000
and it was two federal agents, asking for my help

11
0:00:43.596,000 --> 0:00:45,000
in identifying a little girl

12
0:00:46.326,000 --> 0:00:51,000
featured in hundreds of child sexual abuse images they had found online.

13
0:00:53.145,000 --> 0:00:55,000
They had just started working the case,

14
0:00:55.745,000 --> 0:00:57,000
but what they knew

15
0:00:58.639,000 --> 0:01:02,000
was that her abuse had been broadcast to the world for years

16
0:01:03.486,000 --> 0:01:08,000
on dark web sites dedicated to the sexual abuse of children.

17
0:01:09.605,000 --> 0:01:13,000
And her abuser was incredibly technologically sophisticated:

18
0:01:14.014,000 --> 0:01:18,000
new images and new videos every few weeks,

19
0:01:18.626,000 --> 0:01:22,000
but very few clues as to who she was

20
0:01:22.699,000 --> 0:01:23,000
or where she was.

21
0:01:25.324,000 --> 0:01:26,000
And so they called us,

22
0:01:26.734,000 --> 0:01:28,000
because they had heard we were a new nonprofit

23
0:01:29.448,000 --> 0:01:32,000
building technology to fight child sexual abuse.

24
0:01:33.596,000 --> 0:01:35,000
But we were only two years old,

25
0:01:35.907,000 --> 0:01:38,000
and we had only worked on child sex trafficking.

26
0:01:39.944,000 --> 0:01:41,000
And I had to tell them

27
0:01:42.091,000 --> 0:01:43,000
we had nothing.

28
0:01:44.28,000 --> 0:01:47,000
We had nothing that could help them stop this abuse.

29
0:01:49.263,000 --> 0:01:52,000
It took those agents another year

30
0:01:52.861,000 --> 0:01:55,000
to ultimately find that child.

31
0:01:56.853,000 --> 0:01:58,000
And by the time she was rescued,

32
0:01:59.152,000 --> 0:02:05,000
hundreds of images and videos documenting her rape had gone viral,

33
0:02:05.664,000 --> 0:02:06,000
from the dark web

34
0:02:07.379,000 --> 0:02:1,000
to peer-to-peer networks, private chat rooms

35
0:02:10.42,000 --> 0:02:13,000
and to the websites you and I use

36
0:02:13.664,000 --> 0:02:15,000
every single day.

37
0:02:17.216,000 --> 0:02:2,000
And today, as she struggles to recover,

38
0:02:21.055,000 --> 0:02:25,000
she lives with the fact that thousands around the world

39
0:02:25.218,000 --> 0:02:28,000
continue to watch her abuse.

40
0:02:29.994,000 --> 0:02:31,000
I have come to learn in the last five years

41
0:02:32.446,000 --> 0:02:34,000
that this case is far from unique.

42
0:02:36.119,000 --> 0:02:39,000
How did we get here as a society?

43
0:02:41.49,000 --> 0:02:44,000
In the late 1980s, child pornography --

44
0:02:45.273,000 --> 0:02:5,000
or what it actually is, child sexual abuse material --

45
0:02:50.55,000 --> 0:02:51,000
was nearly eliminated.

46
0:02:53.209,000 --> 0:02:57,000
New laws and increased prosecutions made it simply too risky

47
0:02:57.737,000 --> 0:02:58,000
to trade it through the mail.

48
0:03:00.233,000 --> 0:03:04,000
And then came the internet, and the market exploded.

49
0:03:05.31,000 --> 0:03:08,000
The amount of content in circulation today

50
0:03:08.69,000 --> 0:03:1,000
is massive and growing.

51
0:03:12.421,000 --> 0:03:15,000
This is a truly global problem,

52
0:03:15.689,000 --> 0:03:16,000
but if we just look at the US:

53
0:03:17.546,000 --> 0:03:19,000
in the US alone last year,

54
0:03:20.281,000 --> 0:03:25,000
more than 45 million images and videos of child sexual abuse material

55
0:03:25.579,000 --> 0:03:28,000
were reported to the National Center for Missing and Exploited Children,

56
0:03:29.281,000 --> 0:03:33,000
and that is nearly double the amount the year prior.

57
0:03:34.627,000 --> 0:03:39,000
And the details behind these numbers are hard to contemplate,

58
0:03:39.937,000 --> 0:03:44,000
with more than 60 percent of the images featuring children younger than 12,

59
0:03:45.681,000 --> 0:03:49,000
and most of them including extreme acts of sexual violence.

60
0:03:50.85,000 --> 0:03:55,000
Abusers are cheered on in chat rooms dedicated to the abuse of children,

61
0:03:56.174,000 --> 0:03:58,000
where they gain rank and notoriety

62
0:03:58.681,000 --> 0:04:,000
with more abuse and more victims.

63
0:04:02.243,000 --> 0:04:04,000
In this market,

64
0:04:04.862,000 --> 0:04:07,000
the currency has become the content itself.

65
0:04:10.023,000 --> 0:04:13,000
It's clear that abusers have been quick to leverage new technologies,

66
0:04:13.853,000 --> 0:04:16,000
but our response as a society has not.

67
0:04:17.671,000 --> 0:04:21,000
These abusers don't read user agreements of websites,

68
0:04:21.866,000 --> 0:04:24,000
and the content doesn't honor geographic boundaries.

69
0:04:26.656,000 --> 0:04:32,000
And they win when we look at one piece of the puzzle at a time,

70
0:04:32.793,000 --> 0:04:36,000
which is exactly how our response today is designed.

71
0:04:36.834,000 --> 0:04:39,000
Law enforcement works in one jurisdiction.

72
0:04:40.345,000 --> 0:04:43,000
Companies look at just their platform.

73
0:04:43.783,000 --> 0:04:45,000
And whatever data they learn along the way

74
0:04:46.511,000 --> 0:04:48,000
is rarely shared.

75
0:04:49.402,000 --> 0:04:54,000
It is so clear that this disconnected approach is not working.

76
0:04:55.643,000 --> 0:04:59,000
We have to redesign our response to this epidemic

77
0:04:59.777,000 --> 0:05:,000
for the digital age.

78
0:05:01.702,000 --> 0:05:03,000
And that's exactly what we're doing at Thorn.

79
0:05:05.311,000 --> 0:05:08,000
We're building the technology to connect these dots,

80
0:05:08.952,000 --> 0:05:1,000
to arm everyone on the front lines --

81
0:05:11.298,000 --> 0:05:13,000
law enforcement, NGOs and companies --

82
0:05:14.146,000 --> 0:05:17,000
with the tools they need to ultimately eliminate

83
0:05:17.679,000 --> 0:05:19,000
child sexual abuse material from the internet.

84
0:05:21.571,000 --> 0:05:22,000
Let's talk for a minute --

85
0:05:22.866,000 --> 0:05:23,000
(Applause)

86
0:05:24.409,000 --> 0:05:25,000
Thank you.

87
0:05:25.737,000 --> 0:05:27,000
(Applause)

88
0:05:29.8,000 --> 0:05:31,000
Let's talk for a minute about what those dots are.

89
0:05:33.323,000 --> 0:05:36,000
As you can imagine, this content is horrific.

90
0:05:36.558,000 --> 0:05:39,000
If you don't have to look at it, you don't want to look at it.

91
0:05:40.43,000 --> 0:05:44,000
And so, most companies or law enforcement agencies

92
0:05:45.423,000 --> 0:05:46,000
that have this content

93
0:05:47.11,000 --> 0:05:5,000
can translate every file into a unique string of numbers.

94
0:05:50.586,000 --> 0:05:51,000
This is called a "hash."

95
0:05:52.084,000 --> 0:05:54,000
It's essentially a fingerprint

96
0:05:54.251,000 --> 0:05:56,000
for each file or each video.

97
0:05:56.673,000 --> 0:06:,000
And what this allows them to do is use the information in investigations

98
0:06:01.31,000 --> 0:06:04,000
or for a company to remove the content from their platform,

99
0:06:04.361,000 --> 0:06:09,000
without having to relook at every image and every video each time.

100
0:06:10.196,000 --> 0:06:12,000
The problem today, though,

101
0:06:12.371,000 --> 0:06:15,000
is that there are hundreds of millions of these hashes

102
0:06:16.146,000 --> 0:06:19,000
sitting in siloed databases all around the world.

103
0:06:20.214,000 --> 0:06:21,000
In a silo,

104
0:06:21.389,000 --> 0:06:24,000
it might work for the one agency that has control over it,

105
0:06:24.489,000 --> 0:06:28,000
but not connecting this data means we don't know how many are unique.

106
0:06:28.643,000 --> 0:06:31,000
We don't know which ones represent children who have already been rescued

107
0:06:32.183,000 --> 0:06:34,000
or need to be identified still.

108
0:06:35.096,000 --> 0:06:39,000
So our first, most basic premise is that all of this data

109
0:06:39.29,000 --> 0:06:41,000
must be connected.

110
0:06:42.318,000 --> 0:06:48,000
There are two ways where this data, combined with software on a global scale,

111
0:06:48.511,000 --> 0:06:51,000
can have transformative impact in this space.

112
0:06:52.464,000 --> 0:06:54,000
The first is with law enforcement:

113
0:06:55.11,000 --> 0:06:58,000
helping them identify new victims faster,

114
0:06:58.765,000 --> 0:06:59,000
stopping abuse

115
0:07:00.005,000 --> 0:07:02,000
and stopping those producing this content.

116
0:07:03.441,000 --> 0:07:05,000
The second is with companies:

117
0:07:06.131,000 --> 0:07:09,000
using it as clues to identify the hundreds of millions of files

118
0:07:09.776,000 --> 0:07:1,000
in circulation today,

119
0:07:11.394,000 --> 0:07:12,000
pulling it down

120
0:07:12.605,000 --> 0:07:18,000
and then stopping the upload of new material before it ever goes viral.

121
0:07:21.694,000 --> 0:07:22,000
Four years ago,

122
0:07:23.364,000 --> 0:07:24,000
when that case ended,

123
0:07:26.3,000 --> 0:07:29,000
our team sat there, and we just felt this, um ...

124
0:07:31.635,000 --> 0:07:34,000
... deep sense of failure, is the way I can put it,

125
0:07:34.997,000 --> 0:07:37,000
because we watched that whole year

126
0:07:38.672,000 --> 0:07:39,000
while they looked for her.

127
0:07:40.016,000 --> 0:07:43,000
And we saw every place in the investigation

128
0:07:44.007,000 --> 0:07:46,000
where, if the technology would have existed,

129
0:07:46.419,000 --> 0:07:48,000
they would have found her faster.

130
0:07:49.684,000 --> 0:07:5,000
And so we walked away from that

131
0:07:51.644,000 --> 0:07:53,000
and we went and we did the only thing we knew how to do:

132
0:07:54.623,000 --> 0:07:56,000
we began to build software.

133
0:07:57.689,000 --> 0:07:59,000
So we've started with law enforcement.

134
0:07:59.965,000 --> 0:08:03,000
Our dream was an alarm bell on the desks of officers all around the world

135
0:08:04.41,000 --> 0:08:08,000
so that if anyone dare post a new victim online,

136
0:08:08.978,000 --> 0:08:11,000
someone would start looking for them immediately.

137
0:08:13.324,000 --> 0:08:15,000
I obviously can't talk about the details of that software,

138
0:08:16.305,000 --> 0:08:18,000
but today it's at work in 38 countries,

139
0:08:18.938,000 --> 0:08:2,000
having reduced the time it takes to get to a child

140
0:08:21.936,000 --> 0:08:23,000
by more than 65 percent.

141
0:08:24.29,000 --> 0:08:28,000
(Applause)

142
0:08:33.442,000 --> 0:08:36,000
And now we're embarking on that second horizon:

143
0:08:36.481,000 --> 0:08:41,000
building the software to help companies identify and remove this content.

144
0:08:43.193,000 --> 0:08:45,000
Let's talk for a minute about these companies.

145
0:08:46.27,000 --> 0:08:51,000
So, I told you -- 45 million images and videos in the US alone last year.

146
0:08:52.28,000 --> 0:08:55,000
Those come from just 12 companies.

147
0:08:57.883,000 --> 0:09:03,000
Twelve companies, 45 million files of child sexual abuse material.

148
0:09:04.335,000 --> 0:09:06,000
These come from those companies that have the money

149
0:09:07.159,000 --> 0:09:11,000
to build the infrastructure that it takes to pull this content down.

150
0:09:11.74,000 --> 0:09:13,000
But there are hundreds of other companies,

151
0:09:14.175,000 --> 0:09:16,000
small- to medium-size companies around the world,

152
0:09:16.865,000 --> 0:09:18,000
that need to do this work,

153
0:09:18.943,000 --> 0:09:23,000
but they either: 1) can't imagine that their platform would be used for abuse,

154
0:09:24.392,000 --> 0:09:29,000
or 2) don't have the money to spend on something that is not driving revenue.

155
0:09:30.932,000 --> 0:09:33,000
So we went ahead and built it for them,

156
0:09:34.245,000 --> 0:09:38,000
and this system now gets smarter with the more companies that participate.

157
0:09:39.965,000 --> 0:09:4,000
Let me give you an example.

158
0:09:42.459,000 --> 0:09:45,000
Our first partner, Imgur -- if you haven't heard of this company,

159
0:09:46.361,000 --> 0:09:49,000
it's one of the most visited websites in the US --

160
0:09:49.527,000 --> 0:09:54,000
millions of pieces of user-generated content uploaded every single day,

161
0:09:54.559,000 --> 0:09:56,000
in a mission to make the internet a more fun place.

162
0:09:58.012,000 --> 0:09:59,000
They partnered with us first.

163
0:09:59.888,000 --> 0:10:02,000
Within 20 minutes of going live on our system,

164
0:10:03.255,000 --> 0:10:06,000
someone tried to upload a known piece of abuse material.

165
0:10:06.851,000 --> 0:10:08,000
They were able to stop it, they pull it down,

166
0:10:08.983,000 --> 0:10:11,000
they report it to the National Center for Missing and Exploited Children.

167
0:10:12.473,000 --> 0:10:13,000
But they went a step further,

168
0:10:14.405,000 --> 0:10:18,000
and they went and inspected the account of the person who had uploaded it.

169
0:10:19.086,000 --> 0:10:23,000
Hundreds more pieces of child sexual abuse material

170
0:10:23.821,000 --> 0:10:24,000
that we had never seen.

171
0:10:26.152,000 --> 0:10:29,000
And this is where we start to see exponential impact.

172
0:10:29.708,000 --> 0:10:3,000
We pull that material down,

173
0:10:31.5,000 --> 0:10:34,000
it gets reported to the National Center for Missing and Exploited Children

174
0:10:35.074,000 --> 0:10:37,000
and then those hashes go back into the system

175
0:10:37.609,000 --> 0:10:39,000
and benefit every other company on it.

176
0:10:40.097,000 --> 0:10:44,000
And when the millions of hashes we have lead to millions more and, in real time,

177
0:10:44.905,000 --> 0:10:48,000
companies around the world are identifying and pulling this content down,

178
0:10:49.467,000 --> 0:10:53,000
we will have dramatically increased the speed at which we are removing

179
0:10:54.052,000 --> 0:10:58,000
child sexual abuse material from the internet around the world.

180
0:10:58.37,000 --> 0:11:03,000
(Applause)

181
0:11:06.208,000 --> 0:11:09,000
But this is why it can't just be about software and data,

182
0:11:09.452,000 --> 0:11:1,000
it has to be about scale.

183
0:11:11.248,000 --> 0:11:14,000
We have to activate thousands of officers,

184
0:11:14.785,000 --> 0:11:16,000
hundreds of companies around the world

185
0:11:17.186,000 --> 0:11:2,000
if technology will allow us to outrun the perpetrators

186
0:11:20.818,000 --> 0:11:24,000
and dismantle the communities that are normalizing child sexual abuse

187
0:11:24.967,000 --> 0:11:25,000
around the world today.

188
0:11:27.064,000 --> 0:11:29,000
And the time to do this is now.

189
0:11:30.288,000 --> 0:11:35,000
We can no longer say we don't know the impact this is having on our children.

190
0:11:36.688,000 --> 0:11:4,000
The first generation of children whose abuse has gone viral

191
0:11:41.17,000 --> 0:11:42,000
are now young adults.

192
0:11:43.451,000 --> 0:11:45,000
The Canadian Centre for Child Protection

193
0:11:46.06,000 --> 0:11:48,000
just did a recent study of these young adults

194
0:11:48.78,000 --> 0:11:52,000
to understand the unique trauma they try to recover from,

195
0:11:53.44,000 --> 0:11:55,000
knowing that their abuse lives on.

196
0:11:57.213,000 --> 0:12:01,000
Eighty percent of these young adults have thought about suicide.

197
0:12:02.566,000 --> 0:12:06,000
More than 60 percent have attempted suicide.

198
0:12:07.572,000 --> 0:12:12,000
And most of them live with the fear every single day

199
0:12:12.813,000 --> 0:12:16,000
that as they walk down the street or they interview for a job

200
0:12:17.3,000 --> 0:12:19,000
or they go to school

201
0:12:19.614,000 --> 0:12:21,000
or they meet someone online,

202
0:12:22.063,000 --> 0:12:25,000
that that person has seen their abuse.

203
0:12:26.547,000 --> 0:12:3,000
And the reality came true for more than 30 percent of them.

204
0:12:32.256,000 --> 0:12:36,000
They had been recognized from their abuse material online.

205
0:12:38.022,000 --> 0:12:41,000
This is not going to be easy,

206
0:12:41.322,000 --> 0:12:43,000
but it is not impossible.

207
0:12:44.189,000 --> 0:12:46,000
Now it's going to take the will,

208
0:12:46.889,000 --> 0:12:47,000
the will of our society

209
0:12:48.502,000 --> 0:12:51,000
to look at something that is really hard to look at,

210
0:12:52.08,000 --> 0:12:54,000
to take something out of the darkness

211
0:12:54.447,000 --> 0:12:56,000
so these kids have a voice;

212
0:12:58.11,000 --> 0:13:02,000
the will of companies to take action and make sure that their platforms

213
0:13:03.08,000 --> 0:13:06,000
are not complicit in the abuse of a child;

214
0:13:07.205,000 --> 0:13:1,000
the will of governments to invest with their law enforcement

215
0:13:11.18,000 --> 0:13:16,000
for the tools they need to investigate a digital first crime,

216
0:13:16.298,000 --> 0:13:2,000
even when the victims cannot speak for themselves.

217
0:13:21.746,000 --> 0:13:24,000
This audacious commitment is part of that will.

218
0:13:26.269,000 --> 0:13:31,000
It's a declaration of war against one of humanity's darkest evils.

219
0:13:32.263,000 --> 0:13:33,000
But what I hang on to

220
0:13:34.227,000 --> 0:13:37,000
is that it's actually an investment in a future

221
0:13:37.7,000 --> 0:13:4,000
where every child can simply be a kid.

222
0:13:41.357,000 --> 0:13:42,000
Thank you.

223
0:13:42.896,000 --> 0:13:48,000
(Applause)

