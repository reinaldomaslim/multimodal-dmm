1
0:00:,000 --> 0:00:07,000
Traducteur: Pierre Granchamp Relecteur: Nhu PHAM

2
0:00:12.641,000 --> 0:00:14,000
Je voudrais vous raconter une histoire

3
0:00:14.995,000 --> 0:00:17,000
qui fait le lien entre le célèbre incident qui s'est produit

4
0:00:18.171,000 --> 0:00:2,000
dans la vie privée d'Adam et Eve,

5
0:00:20.94,000 --> 0:00:23,000
et le remarquable déplacement de la frontière

6
0:00:24.386,000 --> 0:00:26,000
entre vie publique et vie privée qui s'est produit

7
0:00:27.072,000 --> 0:00:28,000
dans les 10 dernières années.

8
0:00:28.842,000 --> 0:00:29,000
Vous connaissez l'incident.

9
0:00:30.14,000 --> 0:00:33,000
Adam et Eve, un jour, au Jardin d'Eden,

10
0:00:33.47,000 --> 0:00:34,000
réalisent qu'ils sont nus.

11
0:00:35.313,000 --> 0:00:36,000
Ils paniquent.

12
0:00:36.813,000 --> 0:00:38,000
On connait la suite.

13
0:00:39.57,000 --> 0:00:41,000
De nos jours, Adam et Eve

14
0:00:41.758,000 --> 0:00:43,000
se comporteraient sans doute différemment.

15
0:00:44.119,000 --> 0:00:46,000
[@Adam Cette nuit était trop top ! Adoré 7 pom LOL]

16
0:00:46.387,000 --> 0:00:47,000
[@Eve Ok...Baby, T sais ce ki est arrivé à mon pantalon ?

17
0:00:48.26,000 --> 0:00:5,000
Nous révélons tellement plus d'informations

18
0:00:50.896,000 --> 0:00:53,000
sur nous-mêmes, en ligne, que jamais auparavant,

19
0:00:54.23,000 --> 0:00:55,000
et tant d'informations qui nous concernent

20
0:00:55.934,000 --> 0:00:57,000
sont collectées par des organisations.

21
0:00:58.158,000 --> 0:01:01,000
Nous avons certes beaucoup à gagner et à bénéficier

22
0:01:01.44,000 --> 0:01:03,000
de cette analyse géante d'informations personnelles,

23
0:01:03.886,000 --> 0:01:04,000
ou Big Data,

24
0:01:05.832,000 --> 0:01:07,000
mais il y a aussi des contreparties complexes

25
0:01:08.47,000 --> 0:01:11,000
à abandonner notre vie privée.

26
0:01:11.568,000 --> 0:01:15,000
Mon histoire parle de ces contreparties.

27
0:01:15.591,000 --> 0:01:17,000
Commençons par une observation qui, dans mon esprit,

28
0:01:18.175,000 --> 0:01:21,000
est devenue de plus en plus claire ces dernières années :

29
0:01:21.502,000 --> 0:01:23,000
toute information personnelle

30
0:01:23.599,000 --> 0:01:25,000
peut devenir une information sensible.

31
0:01:25.884,000 --> 0:01:29,000
En 2000, environ 100 milliards de photos

32
0:01:30.009,000 --> 0:01:31,000
ont été prises dans le monde,

33
0:01:31.921,000 --> 0:01:34,000
mais seule une infime proportion de celles-ci

34
0:01:34.986,000 --> 0:01:35,000
ont été mises en ligne.

35
0:01:36.869,000 --> 0:01:39,000
En 2010, rien que sur Facebook, en un seul mois,

36
0:01:40.23,000 --> 0:01:43,000
2,5 milliards de photos ont été mises en ligne,

37
0:01:43.5,000 --> 0:01:44,000
la plupart identifiées.

38
0:01:45.382,000 --> 0:01:46,000
Pendant ce temps,

39
0:01:47.262,000 --> 0:01:51,000
la capacité des ordinateurs à reconnaître des personnes sur photo

40
0:01:52.132,000 --> 0:01:55,000
s'est améliorée de trois ordres de grandeur.

41
0:01:55.74,000 --> 0:01:56,000
Que se passe-t-il quand on combine

42
0:01:57.622,000 --> 0:01:58,000
ces technologies ?

43
0:01:59.123,000 --> 0:02:01,000
Disponibilité croissante des données faciales ;

44
0:02:01.781,000 --> 0:02:04,000
capacité améliorée de reconnaissance faciale par les ordinateurs ;

45
0:02:05.429,000 --> 0:02:07,000
mais aussi le cloud computing,

46
0:02:07.611,000 --> 0:02:08,000
qui donne à chacun de nous dans cette salle

47
0:02:09.499,000 --> 0:02:1,000
la puissance de calcul

48
0:02:11.059,000 --> 0:02:12,000
qui, il y a quelques années à peine, était du registre

49
0:02:12.945,000 --> 0:02:13,000
des services spéciaux ;

50
0:02:14.727,000 --> 0:02:15,000
et l'informatique omniprésente,

51
0:02:16.105,000 --> 0:02:18,000
qui permet à mon téléphone, pourtant pas un super-ordinateur,

52
0:02:18.997,000 --> 0:02:19,000
de se connecter à Internet

53
0:02:20.668,000 --> 0:02:22,000
et d'y réaliser des centaines de milliers

54
0:02:23.002,000 --> 0:02:25,000
de mesures faciales en quelques secondes.

55
0:02:25.641,000 --> 0:02:27,000
Et bien, nous émettons l'hypothèse

56
0:02:28.269,000 --> 0:02:3,000
que le résultat de cette combinaison de technologies

57
0:02:30.333,000 --> 0:02:32,000
sera un changement radical de nos conceptions mêmes

58
0:02:33.221,000 --> 0:02:35,000
de la vie privée et de l'anonymat.

59
0:02:35.478,000 --> 0:02:36,000
Pour tester cela, on a réalisé une expérience

60
0:02:37.471,000 --> 0:02:39,000
sur le campus de l'Université Carnegie Mellon.

61
0:02:39.592,000 --> 0:02:41,000
On a demandé à des étudiants qui passaient

62
0:02:41.691,000 --> 0:02:42,000
de participer à une étude,

63
0:02:43.47,000 --> 0:02:45,000
on a pris une photo d'eux avec une webcam,

64
0:02:46.032,000 --> 0:02:48,000
et on leur a demandé de répondre à un sondage sur un PC portable.

65
0:02:48.814,000 --> 0:02:49,000
Pendant qu'ils répondaient au sondage,

66
0:02:50.793,000 --> 0:02:52,000
on a téléchargé leur photo sur un groupe de cloud-computing,

67
0:02:53.59,000 --> 0:02:54,000
et on a utilisé un outil de reconnaissance faciale

68
0:02:55.317,000 --> 0:02:57,000
pour faire correspondre cette photo à une base de données

69
0:02:57.722,000 --> 0:02:59,000
de centaines de milliers d'images,

70
0:03:00.115,000 --> 0:03:03,000
qu'on avait téléchargée depuis des profils Facebook.

71
0:03:03.711,000 --> 0:03:06,000
Avant que le sujet n'ait atteint la dernière page du sondage,

72
0:03:06.97,000 --> 0:03:09,000
celle-ci avait été mise à jour de façon dynamique

73
0:03:10.317,000 --> 0:03:12,000
avec les 10 meilleures photos correspondantes

74
0:03:12.63,000 --> 0:03:14,000
que l'outil de reconnaissance avait trouvées,

75
0:03:14.915,000 --> 0:03:15,000
et on a demandé aux sujets d'indiquer

76
0:03:16.653,000 --> 0:03:2,000
si ils ou elles s'étaient reconnus sur la photo.

77
0:03:20.773,000 --> 0:03:23,000
Vous voyez le sujet ?

78
0:03:24.472,000 --> 0:03:26,000
Et bien, l'ordinateur l'avait vu, et en fait, il l'a vu

79
0:03:27.317,000 --> 0:03:29,000
chez un sujet sur trois.

80
0:03:29.466,000 --> 0:03:32,000
On peut donc partir d'un visage anonyme,

81
0:03:32.65,000 --> 0:03:35,000
en ligne ou hors ligne, et on peut utiliser la reconnaissance faciale

82
0:03:36.134,000 --> 0:03:38,000
pour donner un nom à ce visage anonyme

83
0:03:38.494,000 --> 0:03:4,000
grâce aux données des réseaux sociaux.

84
0:03:40.602,000 --> 0:03:41,000
Mais quelques années auparavant, on a fait quelque chose d'autre.

85
0:03:42.474,000 --> 0:03:43,000
A partir des données des réseaux sociaux,

86
0:03:44.297,000 --> 0:03:47,000
combinées statistiquement avec les données

87
0:03:47.348,000 --> 0:03:49,000
de la Sécurité Sociale du gouvernement américain,

88
0:03:49.45,000 --> 0:03:52,000
on a réussi à déduire les numéros de sécurité sociale,

89
0:03:52.774,000 --> 0:03:53,000
ce qui est, aux Etats-Unis,

90
0:03:54.286,000 --> 0:03:56,000
une information extrêmement sensible.

91
0:03:56.326,000 --> 0:03:58,000
Vous voyez où je veux en venir ?

92
0:03:58.419,000 --> 0:04:,000
Si vous combinez les deux études,

93
0:04:01.341,000 --> 0:04:02,000
la question devient :

94
0:04:02.853,000 --> 0:04:04,000
peut-on partir d'un visage et,

95
0:04:05.573,000 --> 0:04:07,000
en utilisant la reconnaissance faciale, trouver le nom

96
0:04:07.884,000 --> 0:04:09,000
et les informations disponibles de façon publique

97
0:04:10.553,000 --> 0:04:11,000
sur ce nom et sur cette personne,

98
0:04:12.485,000 --> 0:04:14,000
puis, à partir de ces informations publiques,

99
0:04:14.733,000 --> 0:04:16,000
en déduire des informations non publiques,

100
0:04:16.775,000 --> 0:04:17,000
beaucoup plus sensibles,

101
0:04:18.381,000 --> 0:04:19,000
que l'on peut relier au visage ?

102
0:04:19.873,000 --> 0:04:2,000
Et la réponse est, oui, on peut, et on l'a fait.

103
0:04:21.789,000 --> 0:04:23,000
Bien sûr, la précision est de pire en pire.

104
0:04:24.357,000 --> 0:04:24,000
[27% des 5 premiers chiffres du numéro de SS identifiés (après 4 essais)]

105
0:04:25.301,000 --> 0:04:28,000
En fait, nous avons même décidé de développer une application iPhone

106
0:04:29.128,000 --> 0:04:31,000
qui utilise la caméra interne du téléphone

107
0:04:31.843,000 --> 0:04:32,000
pour prendre un sujet en photo

108
0:04:33.443,000 --> 0:04:34,000
et la télécharger sur le cloud

109
0:04:34.93,000 --> 0:04:36,000
et puis faire ce que je vous ai décrit en temps réel :

110
0:04:37.592,000 --> 0:04:39,000
chercher une correspondance, trouver des informations publiques,

111
0:04:39.68,000 --> 0:04:4,000
essayer d'en déduire des informations sensibles,

112
0:04:41.41,000 --> 0:04:43,000
et la renvoyer sur le téléphone

113
0:04:44.001,000 --> 0:04:47,000
pour qu'elle s'affiche sur le visage du sujet,

114
0:04:47.61,000 --> 0:04:48,000
un exemple de réalité augmentée,

115
0:04:49.511,000 --> 0:04:51,000
probablement un exemple effrayant de réalité augmentée.

116
0:04:51.962,000 --> 0:04:54,000
En fait, nous n'avons pas développé l'appli pour la rendre publique,

117
0:04:55.301,000 --> 0:04:56,000
mais seulement comme une preuve du concept.

118
0:04:57.223,000 --> 0:04:59,000
En fait, prenez ces technologies,

119
0:04:59.536,000 --> 0:05:,000
et poussez-les jusqu'à leur extrémité logique.

120
0:05:01.373,000 --> 0:05:03,000
Imaginez un futur où des inconnus autour de vous

121
0:05:04.092,000 --> 0:05:06,000
vous regarderont à travers leurs Google Glasses

122
0:05:06.403,000 --> 0:05:08,000
ou bien, un jour, leurs lentilles de contact,

123
0:05:08.71,000 --> 0:05:12,000
et utiliseront 7 ou 8 données sur vous

124
0:05:12.73,000 --> 0:05:14,000
pour en déduire n'importe quoi d'autre

125
0:05:15.312,000 --> 0:05:17,000
qui pourrait être connu à votre sujet.

126
0:05:17.915,000 --> 0:05:21,000
A quoi ressemblera ce futur sans secrets ?

127
0:05:22.709,000 --> 0:05:23,000
Et devons-nous nous en préoccuper ?

128
0:05:24.673,000 --> 0:05:25,000
On pourrait aimer croire

129
0:05:26.564,000 --> 0:05:29,000
qu'un futur avec une telle richesse de données

130
0:05:29.604,000 --> 0:05:31,000
serait un futur sans plus de parti-pris,

131
0:05:32.118,000 --> 0:05:35,000
mais en fait, avoir autant d'informations

132
0:05:35.701,000 --> 0:05:37,000
ne veut pas dire que nous prendrons des décisions

133
0:05:37.892,000 --> 0:05:38,000
plus objectives.

134
0:05:39.598,000 --> 0:05:41,000
Dans une autre expérience, nous avons présenté à nos sujets

135
0:05:42.158,000 --> 0:05:44,000
des informations à propos d'un candidat potentiel à un emploi.

136
0:05:44.404,000 --> 0:05:47,000
Nous avons inclus dans ces informations des références à des choses

137
0:05:47.582,000 --> 0:05:49,000
plutôt drôles, absolument légales,

138
0:05:50.228,000 --> 0:05:52,000
mais peut-être un peu embarrassantes

139
0:05:52.693,000 --> 0:05:54,000
que le sujet avait postées en ligne.

140
0:05:54.713,000 --> 0:05:56,000
De façon intéressante, parmi nos sujets,

141
0:05:57.079,000 --> 0:06:,000
certains avaient posté des choses de même nature,

142
0:06:00.162,000 --> 0:06:02,000
et d'autres non.

143
0:06:02.524,000 --> 0:06:03,000
Quel groupe, d'après vous,

144
0:06:04.473,000 --> 0:06:08,000
a été le plus enclin à juger durement notre sujet ?

145
0:06:09.025,000 --> 0:06:1,000
De façon paradoxale, c'est le groupe

146
0:06:10.982,000 --> 0:06:11,000
qui avait posté des choses similaires,

147
0:06:12.715,000 --> 0:06:14,000
un exemple de dissonance morale.

148
0:06:15.657,000 --> 0:06:16,000
Vous pourriez vous dire,

149
0:06:17.407,000 --> 0:06:18,000
ceci ne s'applique pas à moi,

150
0:06:19.109,000 --> 0:06:21,000
parce que je n'ai rien à cacher.

151
0:06:21.271,000 --> 0:06:23,000
Mais en réalité, la vie privée n'a rien à voir

152
0:06:23.753,000 --> 0:06:26,000
avec le fait d'avoir quelque chose de négatif à cacher.

153
0:06:27.429,000 --> 0:06:29,000
Imaginez que vous êtes le directeur des R.H.

154
0:06:29.783,000 --> 0:06:31,000
d'une certaine organisation, et que vous receviez des CV,

155
0:06:32.73,000 --> 0:06:34,000
et que vous décidiez de trouver plus d'informations au sujet de vos candidats.

156
0:06:35.203,000 --> 0:06:37,000
Pour cela, vous Googlez leur noms

157
0:06:37.663,000 --> 0:06:39,000
et dans un certain univers,

158
0:06:39.903,000 --> 0:06:41,000
vous trouvez ces informations.

159
0:06:41.911,000 --> 0:06:45,000
Ou, dans un univers parallèle, vous trouvez celles-ci.

160
0:06:46.348,000 --> 0:06:48,000
Pensez-vous que chaque candidat aurait autant de chance

161
0:06:49.065,000 --> 0:06:51,000
que vous l'appeliez pour un entretien ?

162
0:06:51.868,000 --> 0:06:53,000
SI vous pensez ça, alors vous n'êtes pas

163
0:06:54.15,000 --> 0:06:56,000
comme les employeurs américains qui, en fait, font

164
0:06:56.732,000 --> 0:06:59,000
partie de notre expérience, je veux dire que c'est exactement ce qu'on a fait.

165
0:07:00.039,000 --> 0:07:03,000
On a créé des profils Facebook, déformé certains faits,

166
0:07:03.221,000 --> 0:07:05,000
et on a commencé à envoyer nos CV à des sociétés aux Etats-Unis,

167
0:07:06.072,000 --> 0:07:07,000
puis nous avons détecté et suivi

168
0:07:07.98,000 --> 0:07:09,000
s'ils faisaient des recherches sur nos candidats,

169
0:07:10.373,000 --> 0:07:11,000
et s'ils agissaient en fonction des informations

170
0:07:12.205,000 --> 0:07:13,000
qu'ils avaient trouvées sur les réseaux sociaux. Et ils le faisaient.

171
0:07:14.143,000 --> 0:07:16,000
La discrimination se faisait à travers les réseaux sociaux

172
0:07:16.244,000 --> 0:07:19,000
pour des candidats de même niveau de qualification.

173
0:07:19.317,000 --> 0:07:23,000
Les spécialistes du marketing voudraient nous faire croire

174
0:07:23.892,000 --> 0:07:25,000
que toutes les informations nous concernant seront toujours

175
0:07:26.161,000 --> 0:07:29,000
utilisées pour notre bénéfice.

176
0:07:29.434,000 --> 0:07:32,000
Mais pensez-y. Pourquoi devrait-ce toujours être le cas ?

177
0:07:33.149,000 --> 0:07:35,000
Dans un film sorti il y a quelques années,

178
0:07:35.813,000 --> 0:07:37,000
« Minority Report », une scène célèbre

179
0:07:38.366,000 --> 0:07:4,000
montre Tom Cruise marchant dans un centre commercial

180
0:07:40.942,000 --> 0:07:43,000
alors qu'une une publicité holographique personnalisée

181
0:07:44.718,000 --> 0:07:45,000
apparaît autour de lui.

182
0:07:46.553,000 --> 0:07:49,000
Ce film était censé se passer en 2054,

183
0:07:49.78,000 --> 0:07:5,000
dans environ 40 ans,

184
0:07:51.422,000 --> 0:07:53,000
et aussi excitante que semble cette technologie,

185
0:07:54.33,000 --> 0:07:56,000
elle sous-estime déjà largement

186
0:07:56.976,000 --> 0:07:58,000
la quantité d'informations que les organisations

187
0:07:59.116,000 --> 0:08:01,000
peuvent recueillir à votre sujet, et comment elles peuvent les utiliser

188
0:08:01.599,000 --> 0:08:04,000
pour vous influencer d'une manière que vous ne détecterez même pas.

189
0:08:04.997,000 --> 0:08:06,000
A titre d'exemple, voici une autre expérience

190
0:08:07.1,000 --> 0:08:09,000
qui est encore en cours, pas encore terminée.

191
0:08:09.373,000 --> 0:08:11,000
Imaginez qu'une organisation ait accès

192
0:08:11.692,000 --> 0:08:13,000
à votre liste d'amis sur Facebook,

193
0:08:13.748,000 --> 0:08:14,000
et grâce à une sorte d'algorithme

194
0:08:15.52,000 --> 0:08:18,000
elle peut détecter les deux amis que vous aimez le plus.

195
0:08:19.254,000 --> 0:08:21,000
Puis elle crée, en temps réel,

196
0:08:21.534,000 --> 0:08:23,000
un composé du visage de ces deux amis.

197
0:08:24.376,000 --> 0:08:27,000
Des études avant les nôtres ont démontré que les gens

198
0:08:27.445,000 --> 0:08:29,000
ne se reconnaissent même plus eux-mêmes

199
0:08:30.33,000 --> 0:08:32,000
dans des visages composés, mais ils réagissent

200
0:08:32.792,000 --> 0:08:34,000
envers ces composés d'une manière favorable.

201
0:08:34.909,000 --> 0:08:37,000
Ainsi, la prochaine fois que vous chercherez un produit donné,

202
0:08:38.324,000 --> 0:08:4,000
et qu'il y aura une pub vous proposant de l'acheter,

203
0:08:40.883,000 --> 0:08:42,000
ce ne sera pas juste un acteur standard.

204
0:08:43.79,000 --> 0:08:45,000
Ce sera l'un de vos amis,

205
0:08:46.103,000 --> 0:08:49,000
et vous ne remarquerez même pas que cela se passe comme ça.

206
0:08:49.406,000 --> 0:08:51,000
Aujourd'hui, le problème est que

207
0:08:51.819,000 --> 0:08:53,000
les mécanismes de régulations actuels

208
0:08:54.338,000 --> 0:08:57,000
pour nous protéger contre les abus liés à l'utilisation des informations personnelles

209
0:08:57.776,000 --> 0:08:59,000
sont comme affronter une mitraillette armé seulement d'un couteau.

210
0:09:00.76,000 --> 0:09:02,000
L'un de ces mécanismes est la transparence,

211
0:09:03.673,000 --> 0:09:06,000
on doit informer les personnes de ce que l'on va faire avec leurs données.

212
0:09:06.873,000 --> 0:09:08,000
Et, en principe, c'est une très bonne chose.

213
0:09:08.979,000 --> 0:09:11,000
C'est nécessaire, mais pas suffisant.

214
0:09:12.646,000 --> 0:09:15,000
La transparence peut être détournée.

215
0:09:16.344,000 --> 0:09:18,000
Vous pouvez dire aux gens ce que vous allez faire,

216
0:09:18.448,000 --> 0:09:2,000
et les pousser encore à divulguer

217
0:09:20.68,000 --> 0:09:22,000
des quantités arbitraires d'informations personnelles.

218
0:09:23.303,000 --> 0:09:25,000
Ainsi, dans une autre expérience, menée avec des étudiants,

219
0:09:26.189,000 --> 0:09:29,000
nous leur avons demandé de fournir des informations

220
0:09:29.247,000 --> 0:09:3,000
sur leur comportement sur le campus,

221
0:09:31.06,000 --> 0:09:33,000
y compris des questions très sensibles, comme celle-ci :

222
0:09:34,000 --> 0:09:34,000
[Avez-vous déjà triché à un examen ? ]

223
0:09:34.621,000 --> 0:09:36,000
Nous avons dit à un groupe de sujets :

224
0:09:36.921,000 --> 0:09:38,000
«Seuls d'autres étudiants vont voir vos réponses.»

225
0:09:39.762,000 --> 0:09:4,000
Nous avons dit à un autre groupe :

226
0:09:41.341,000 --> 0:09:44,000
«Les étudiants et les professeurs vont voir vos réponses.»

227
0:09:44.902,000 --> 0:09:46,000
Transparence. Notification. Et, bien sûr, ça a marché,

228
0:09:47.493,000 --> 0:09:48,000
dans le sens où le premier groupe de sujets

229
0:09:48.9,000 --> 0:09:5,000
était beaucoup plus enclin à donner des informations que le second.

230
0:09:51.468,000 --> 0:09:52,000
C'est logique, n'est-ce pas ?

231
0:09:52.988,000 --> 0:09:53,000
Mais nous avons ensuite ajouté le détournement.

232
0:09:54.478,000 --> 0:09:56,000
Nous avons répété l'expérience avec les deux mêmes groupes,

233
0:09:57.238,000 --> 0:09:59,000
cette fois-ci, en ajoutant un délai

234
0:09:59.665,000 --> 0:10:01,000
entre le moment où on a dit aux sujets

235
0:10:02.6,000 --> 0:10:04,000
comment nous utiliserions leurs données

236
0:10:04.68,000 --> 0:10:08,000
et le moment où on a commencé à leur poser des questions.

237
0:10:09.068,000 --> 0:10:11,000
Combien de temps pensez-vous que nous ayonsdû ajouter

238
0:10:11.629,000 --> 0:10:15,000
pour neutraliser l'effet inhibiteur

239
0:10:16.242,000 --> 0:10:19,000
de savoir que les professeurs verraient vos réponses ?

240
0:10:19.653,000 --> 0:10:2,000
Dix minutes ?

241
0:10:21.433,000 --> 0:10:22,000
Cinq minutes ?

242
0:10:23.224,000 --> 0:10:24,000
Une minute ?

243
0:10:25,000 --> 0:10:27,000
Que diriez-vous de 15 secondes ?

244
0:10:27.049,000 --> 0:10:29,000
Quinze secondes ont suffit pour que les deux groupes

245
0:10:29.717,000 --> 0:10:3,000
divulguent la même quantité d'informations,

246
0:10:31.285,000 --> 0:10:33,000
comme si le deuxième groupe ne se souciait plus

247
0:10:34.031,000 --> 0:10:36,000
que les professeurs puissent lire ses réponses.

248
0:10:36.687,000 --> 0:10:39,000
Je dois reconnaître que cette conférence, jusqu'ici,

249
0:10:40.023,000 --> 0:10:42,000
peut sembler excessivement sombre,

250
0:10:42.503,000 --> 0:10:43,000
mais ce n'est pas l'essentiel de mon message.

251
0:10:44.224,000 --> 0:10:46,000
En fait, je veux vous montrer

252
0:10:46.923,000 --> 0:10:47,000
qu'il existe des alternatives.

253
0:10:48.695,000 --> 0:10:5,000
La façon dont on fait les choses aujourd'hui

254
0:10:51.194,000 --> 0:10:54,000
n'est pas la seule manière de faire,

255
0:10:54.231,000 --> 0:10:56,000
ni certainement la meilleure.

256
0:10:56.258,000 --> 0:11:,000
Si quelqu'un vous dit, « Les gens se fichent de préserver leur vie privée »,

257
0:11:00.429,000 --> 0:11:02,000
demandez-vous si le jeu n'a pas été conçu

258
0:11:03.071,000 --> 0:11:05,000
et truqué dans le but qu'ils ne puissent pas s'en soucier.

259
0:11:05.795,000 --> 0:11:08,000
Réaliser que ces manipulations se produisent,

260
0:11:09.057,000 --> 0:11:1,000
c'est déjà être à mi-chemin du processus

261
0:11:10.664,000 --> 0:11:12,000
qui nous rend capables de nous protéger nous-mêmes.

262
0:11:12.922,000 --> 0:11:15,000
Si quelqu'un vous dit que la protection de la vie privée est incompatible

263
0:11:16.632,000 --> 0:11:17,000
avec les avantages du Big Data,

264
0:11:18.481,000 --> 0:11:2,000
considérez qu'au cours des 20 dernières années,

265
0:11:20.954,000 --> 0:11:21,000
les chercheurs ont créé des technologies

266
0:11:22.871,000 --> 0:11:25,000
qui permettent à virtuellement toutes les transactions électroniques

267
0:11:26.189,000 --> 0:11:29,000
de se dérouler d'une manière plus respectueuse de la vie privée.

268
0:11:29.938,000 --> 0:11:31,000
On peut naviguer sur Internet de façon anonyme.

269
0:11:32.493,000 --> 0:11:34,000
On peut envoyer des mails qui ne pourront être lus

270
0:11:35.171,000 --> 0:11:38,000
que par le destinataire, pas même par la NSA.

271
0:11:38.88,000 --> 0:11:4,000
Il peut même y avoir une exploitation des bases de données respectueuse de la vie privée.

272
0:11:41.877,000 --> 0:11:44,000
En d'autres termes, nous pouvons avoir les avantages du Big Data

273
0:11:45.771,000 --> 0:11:47,000
tout en protégeant la vie privée.

274
0:11:47.903,000 --> 0:11:5,000
Bien sûr, ces technologies impliquent un changement

275
0:11:51.694,000 --> 0:11:52,000
de la répartition des coûts et des revenus

276
0:11:53.24,000 --> 0:11:55,000
entre les détenteurs de données et les personnes concernées,

277
0:11:55.347,000 --> 0:11:58,000
ce qui explique peut-être pourquoi vous n'en entendez pas beaucoup parler.

278
0:11:58.8,000 --> 0:12:01,000
Ce qui me ramène au Jardin d'Eden.

279
0:12:02.506,000 --> 0:12:04,000
Il y a une seconde interprétation à propos de la vie privée

280
0:12:05.286,000 --> 0:12:06,000
dans l'histoire du Jardin d'Eden

281
0:12:07.095,000 --> 0:12:09,000
qui n'a rien à voir avec le fait

282
0:12:09.191,000 --> 0:12:11,000
qu'Adam et Eve se sentent nus

283
0:12:11.416,000 --> 0:12:13,000
et honteux.

284
0:12:13.797,000 --> 0:12:15,000
Vous pouvez trouver les échos de cette interprétation

285
0:12:16.578,000 --> 0:12:18,000
dans "Le Paradis perdu" de John Milton.

286
0:12:19.36,000 --> 0:12:23,000
Dans le jardin, Adam et Eve sont contentés sur le plan matériel.

287
0:12:23.557,000 --> 0:12:25,000
Ils sont heureux. Ils sont satisfaits.

288
0:12:25.661,000 --> 0:12:27,000
Cependant, ils n'ont ni la connaissance,

289
0:12:27.954,000 --> 0:12:28,000
ni la conscience d'eux-mêmes.

290
0:12:29.594,000 --> 0:12:32,000
Au moment où ils mangent le bien nommé

291
0:12:32.913,000 --> 0:12:33,000
fruit de la connaissance,

292
0:12:34.206,000 --> 0:12:36,000
c'est là qu'ils se découvrent eux-mêmes.

293
0:12:36.811,000 --> 0:12:4,000
Ils prennent conscience. Ils parviennent à l'autonomie.

294
0:12:40.842,000 --> 0:12:43,000
Cependant, le prix à payer, c'est de quitter le jardin.

295
0:12:43.968,000 --> 0:12:46,000
La vie privée, d'une certaine manière, c'est à la fois le moyen

296
0:12:47.849,000 --> 0:12:49,000
et le prix à payer pour la liberté.

297
0:12:50.811,000 --> 0:12:52,000
Encore une fois, les spécialistes du marketing nous disent

298
0:12:53.581,000 --> 0:12:56,000
que le Big Data et les réseaux sociaux

299
0:12:56.6,000 --> 0:12:58,000
ne sont pas seulement pour eux un paradis du profit,

300
0:12:59.579,000 --> 0:13:01,000
mais aussi un Jardin d'Eden pour nous autres.

301
0:13:02.036,000 --> 0:13:03,000
Nous bénéficions de contenus gratuits.

302
0:13:03.274,000 --> 0:13:06,000
Nous avons la chance de jouer à Angry Birds. Nous avons des applications ciblées.

303
0:13:06.397,000 --> 0:13:08,000
Mais en fait, dans quelques années, ces organisations

304
0:13:09.294,000 --> 0:13:1,000
sauront tant de choses sur nous,

305
0:13:10.903,000 --> 0:13:12,000
qu'ils seront en mesure de déduire nos désirs

306
0:13:13.613,000 --> 0:13:15,000
avant que nous les ayons formés, et peut-être

307
0:13:15.817,000 --> 0:13:17,000
d'acheter des produits en notre nom

308
0:13:18.264,000 --> 0:13:2,000
avant qu'on ne réalise qu'on en a besoin.

309
0:13:20.538,000 --> 0:13:23,000
Il y a un auteur anglais

310
0:13:23.775,000 --> 0:13:26,000
qui a anticipé ce genre de futur

311
0:13:26.82,000 --> 0:13:27,000
dans lequel nous abandonnerions

312
0:13:28.225,000 --> 0:13:31,000
notre autonomie et notre liberté pour du confort.

313
0:13:31.773,000 --> 0:13:33,000
Plus encore que George Orwell,

314
0:13:33.934,000 --> 0:13:35,000
cet auteur est, bien sûr, Aldous Huxley.

315
0:13:36.695,000 --> 0:13:38,000
Dans « Le Meilleur des Mondes », il imagine une société

316
0:13:39.549,000 --> 0:13:41,000
dans laquelle les technologies que nous avons créées

317
0:13:41.72,000 --> 0:13:42,000
à l'origine pour la liberté

318
0:13:43.579,000 --> 0:13:45,000
finissent par nous contraindre.

319
0:13:46.146,000 --> 0:13:5,000
Toutefois, dans ce livre, il nous offre aussi une porte de sortie

320
0:13:50.937,000 --> 0:13:53,000
de cette société, semblable au chemin

321
0:13:54.375,000 --> 0:13:57,000
qu'Adam et Ève ont eu à suivre pour quitter le jardin.

322
0:13:58.33,000 --> 0:14:,000
Selon les termes du Sauvage,

323
0:14:00.477,000 --> 0:14:03,000
retrouver l'autonomie et la liberté est possible,

324
0:14:03.546,000 --> 0:14:05,000
bien que le prix à payer soit élevé.

325
0:14:06.225,000 --> 0:14:11,000
Je crois fermement que l'un des combats décisifs

326
0:14:11.94,000 --> 0:14:13,000
de notre époque sera le combat

327
0:14:14.503,000 --> 0:14:16,000
pour le contrôle des informations personnelles,

328
0:14:16.89,000 --> 0:14:19,000
le combat pour savoir si les Big Data peuvent devenir un vecteur

329
0:14:20.397,000 --> 0:14:21,000
de liberté,

330
0:14:21.686,000 --> 0:14:25,000
plutôt qu'un moyen de nous manipuler à notre insu.

331
0:14:26.432,000 --> 0:14:28,000
À l'heure actuelle, bon nombre d'entre nous

332
0:14:29.025,000 --> 0:14:31,000
ne savent même pas que le combat a commencé,

333
0:14:31.778,000 --> 0:14:33,000
mais c'est le cas, que ça vous plaise ou non.

334
0:14:34.45,000 --> 0:14:36,000
Et au risque de jouer les serpents,

335
0:14:37.254,000 --> 0:14:39,000
je vous dirais que les outils pour ce combat

336
0:14:40.151,000 --> 0:14:43,000
sont là, la conscience de ce qui est en train de se passer,

337
0:14:43.16,000 --> 0:14:44,000
et dans vos mains,

338
0:14:44.515,000 --> 0:14:47,000
à quelques clics seulement.

339
0:14:48.255,000 --> 0:14:49,000
Merci.

340
0:14:49.737,000 --> 0:14:53,000
(Applaudissements)

