1
0:00:,000 --> 0:00:07,000
Translator: Joseph Geni Reviewer: Joanna Pietrulewicz

2
0:00:12.58,000 --> 0:00:15,000
So you probably have the sense, as most people do,

3
0:00:15.66,000 --> 0:00:18,000
that polarization is getting worse in our country,

4
0:00:19.34,000 --> 0:00:22,000
that the divide between the left and the right

5
0:00:22.82,000 --> 0:00:25,000
is as bad as it's been in really any of our lifetimes.

6
0:00:26.38,000 --> 0:00:31,000
But you might also reasonably wonder if research backs up your intuition.

7
0:00:32.38,000 --> 0:00:36,000
And in a nutshell, the answer is sadly yes.

8
0:00:38.74,000 --> 0:00:4,000
In study after study, we find

9
0:00:40.78,000 --> 0:00:43,000
that liberals and conservatives have grown further apart.

10
0:00:45.26,000 --> 0:00:49,000
They increasingly wall themselves off in these ideological silos,

11
0:00:50.06,000 --> 0:00:54,000
consuming different news, talking only to like-minded others

12
0:00:54.22,000 --> 0:00:57,000
and more and more choosing to live in different parts of the country.

13
0:00:58.54,000 --> 0:01:01,000
And I think that most alarming of all of it

14
0:01:01.78,000 --> 0:01:04,000
is seeing this rising animosity on both sides.

15
0:01:06.26,000 --> 0:01:07,000
Liberals and conservatives,

16
0:01:07.94,000 --> 0:01:08,000
Democrats and Republicans,

17
0:01:09.86,000 --> 0:01:12,000
more and more they just don't like one another.

18
0:01:14.14,000 --> 0:01:16,000
You see it in many different ways.

19
0:01:16.18,000 --> 0:01:19,000
They don't want to befriend one another. They don't want to date one another.

20
0:01:19.86,000 --> 0:01:22,000
If they do, if they find out, they find each other less attractive,

21
0:01:23.18,000 --> 0:01:26,000
and they more and more don't want their children to marry someone

22
0:01:26.3,000 --> 0:01:27,000
who supports the other party,

23
0:01:28.02,000 --> 0:01:29,000
a particularly shocking statistic.

24
0:01:31.46,000 --> 0:01:33,000
You know, in my lab, the students that I work with,

25
0:01:34.3,000 --> 0:01:37,000
we're talking about some sort of social pattern --

26
0:01:37.78,000 --> 0:01:4,000
I'm a movie buff, and so I'm often like,

27
0:01:41.34,000 --> 0:01:43,000
what kind of movie are we in here with this pattern?

28
0:01:44.9,000 --> 0:01:47,000
So what kind of movie are we in with political polarization?

29
0:01:48.9,000 --> 0:01:5,000
Well, it could be a disaster movie.

30
0:01:52.7,000 --> 0:01:53,000
It certainly seems like a disaster.

31
0:01:54.74,000 --> 0:01:56,000
Could be a war movie.

32
0:01:57.46,000 --> 0:01:58,000
Also fits.

33
0:01:59.3,000 --> 0:02:02,000
But what I keep thinking is that we're in a zombie apocalypse movie.

34
0:02:03.14,000 --> 0:02:04,000
(Laughter)

35
0:02:04.62,000 --> 0:02:06,000
Right? You know the kind.

36
0:02:06.94,000 --> 0:02:08,000
There's people wandering around in packs,

37
0:02:09.38,000 --> 0:02:1,000
not thinking for themselves,

38
0:02:11.18,000 --> 0:02:12,000
seized by this mob mentality

39
0:02:12.82,000 --> 0:02:15,000
trying to spread their disease and destroy society.

40
0:02:17.3,000 --> 0:02:19,000
And you probably think, as I do,

41
0:02:19.66,000 --> 0:02:22,000
that you're the good guy in the zombie apocalypse movie,

42
0:02:23.14,000 --> 0:02:26,000
and all this hate and polarization, it's being propagated by the other people,

43
0:02:26.86,000 --> 0:02:27,000
because we're Brad Pitt, right?

44
0:02:29.58,000 --> 0:02:31,000
Free-thinking, righteous,

45
0:02:32.5,000 --> 0:02:34,000
just trying to hold on to what we hold dear,

46
0:02:34.82,000 --> 0:02:37,000
you know, not foot soldiers in the army of the undead.

47
0:02:38.42,000 --> 0:02:39,000
Not that.

48
0:02:39.9,000 --> 0:02:4,000
Never that.

49
0:02:41.9,000 --> 0:02:42,000
But here's the thing:

50
0:02:43.42,000 --> 0:02:45,000
what movie do you suppose they think they're in?

51
0:02:47.3,000 --> 0:02:48,000
Right?

52
0:02:48.54,000 --> 0:02:5,000
Well, they absolutely think that they're the good guys

53
0:02:51.1,000 --> 0:02:52,000
in the zombie apocalypse movie. Right?

54
0:02:52.98,000 --> 0:02:54,000
And you'd better believe that they think that they're Brad Pitt

55
0:02:55.98,000 --> 0:02:57,000
and that we, we are the zombies.

56
0:03:00.94,000 --> 0:03:02,000
And who's to say that they're wrong?

57
0:03:04.26,000 --> 0:03:07,000
I think that the truth is that we're all a part of this.

58
0:03:08.06,000 --> 0:03:11,000
And the good side of that is that we can be a part of the solution.

59
0:03:12.1,000 --> 0:03:14,000
So what are we going to do?

60
0:03:15.14,000 --> 0:03:19,000
What can we do to chip away at polarization in everyday life?

61
0:03:19.42,000 --> 0:03:22,000
What could we do to connect with and communicate with

62
0:03:23.26,000 --> 0:03:24,000
our political counterparts?

63
0:03:25.54,000 --> 0:03:29,000
Well, these were exactly the questions that I and my colleague, Matt Feinberg,

64
0:03:29.7,000 --> 0:03:3,000
became fascinated with a few years ago,

65
0:03:31.582,000 --> 0:03:33,000
and we started doing research on this topic.

66
0:03:34.74,000 --> 0:03:36,000
And one of the first things that we discovered

67
0:03:37.74,000 --> 0:03:4,000
that I think is really helpful for understanding polarization

68
0:03:41.22,000 --> 0:03:42,000
is to understand

69
0:03:42.46,000 --> 0:03:46,000
that the political divide in our country is undergirded by a deeper moral divide.

70
0:03:46.9,000 --> 0:03:5,000
So one of the most robust findings in the history of political psychology

71
0:03:51.7,000 --> 0:03:54,000
is this pattern identified by Jon Haidt and Jesse Graham,

72
0:03:55.42,000 --> 0:03:56,000
psychologists,

73
0:03:56.66,000 --> 0:04:,000
that liberals and conservatives tend to endorse different values

74
0:04:00.7,000 --> 0:04:01,000
to different degrees.

75
0:04:02.42,000 --> 0:04:07,000
So for example, we find that liberals tend to endorse values like equality

76
0:04:07.94,000 --> 0:04:1,000
and fairness and care and protection from harm

77
0:04:11.62,000 --> 0:04:13,000
more than conservatives do.

78
0:04:13.78,000 --> 0:04:18,000
And conservatives tend to endorse values like loyalty, patriotism,

79
0:04:19.06,000 --> 0:04:22,000
respect for authority and moral purity

80
0:04:22.54,000 --> 0:04:24,000
more than liberals do.

81
0:04:25.74,000 --> 0:04:29,000
And Matt and I were thinking that maybe this moral divide

82
0:04:29.82,000 --> 0:04:32,000
might be helpful for understanding how it is

83
0:04:32.94,000 --> 0:04:34,000
that liberals and conservatives talk to one another

84
0:04:35.38,000 --> 0:04:37,000
and why they so often seem to talk past one another

85
0:04:37.82,000 --> 0:04:38,000
when they do.

86
0:04:39.06,000 --> 0:04:4,000
So we conducted a study

87
0:04:41.06,000 --> 0:04:44,000
where we recruited liberals to a study

88
0:04:44.18,000 --> 0:04:46,000
where they were supposed to write a persuasive essay

89
0:04:46.66,000 --> 0:04:5,000
that would be compelling to a conservative in support of same-sex marriage.

90
0:04:51.62,000 --> 0:04:54,000
And what we found was that liberals tended to make arguments

91
0:04:54.9,000 --> 0:04:58,000
in terms of the liberal moral values of equality and fairness.

92
0:04:59.1,000 --> 0:05:,000
So they said things like,

93
0:05:00.86,000 --> 0:05:03,000
"Everyone should have the right to love whoever they choose,"

94
0:05:04.26,000 --> 0:05:06,000
and, "They" -- they being gay Americans --

95
0:05:06.86,000 --> 0:05:08,000
"deserve the same equal rights as other Americans."

96
0:05:10.18,000 --> 0:05:13,000
Overall, we found that 69 percent of liberals

97
0:05:13.42,000 --> 0:05:18,000
invoked one of the more liberal moral values in constructing their essay,

98
0:05:18.86,000 --> 0:05:21,000
and only nine percent invoked one of the more conservative moral values,

99
0:05:22.58,000 --> 0:05:25,000
even though they were supposed to be trying to persuade conservatives.

100
0:05:26.02,000 --> 0:05:3,000
And when we studied conservatives and had them make persuasive arguments

101
0:05:30.34,000 --> 0:05:32,000
in support of making English the official language of the US,

102
0:05:33.26,000 --> 0:05:35,000
a classically conservative political position,

103
0:05:35.82,000 --> 0:05:37,000
we found that they weren't much better at this.

104
0:05:38.06,000 --> 0:05:39,000
59 percent of them made arguments

105
0:05:39.7,000 --> 0:05:41,000
in terms of one of the more conservative moral values,

106
0:05:42.42,000 --> 0:05:44,000
and just eight percent invoked a liberal moral value,

107
0:05:44.94,000 --> 0:05:47,000
even though they were supposed to be targeting liberals for persuasion.

108
0:05:49.3,000 --> 0:05:53,000
Now, you can see right away why we're in trouble here. Right?

109
0:05:54.1,000 --> 0:05:57,000
People's moral values, they're their most deeply held beliefs.

110
0:05:57.62,000 --> 0:06:,000
People are willing to fight and die for their values.

111
0:06:01.54,000 --> 0:06:03,000
Why are they going to give that up just to agree with you

112
0:06:04.26,000 --> 0:06:07,000
on something that they don't particularly want to agree with you on anyway?

113
0:06:07.82,000 --> 0:06:1,000
If that persuasive appeal that you're making to your Republican uncle

114
0:06:11.1,000 --> 0:06:13,000
means that he doesn't just have to change his view,

115
0:06:13.54,000 --> 0:06:15,000
he's got to change his underlying values, too,

116
0:06:15.73,000 --> 0:06:16,000
that's not going to go very far.

117
0:06:17.9,000 --> 0:06:18,000
So what would work better?

118
0:06:20.02,000 --> 0:06:24,000
Well, we believe it's a technique that we call moral reframing,

119
0:06:24.34,000 --> 0:06:26,000
and we've studied it in a series of experiments.

120
0:06:26.98,000 --> 0:06:27,000
In one of these experiments,

121
0:06:28.5,000 --> 0:06:31,000
we recruited liberals and conservatives to a study

122
0:06:31.66,000 --> 0:06:33,000
where they read one of three essays

123
0:06:33.98,000 --> 0:06:36,000
before having their environmental attitudes surveyed.

124
0:06:37.46,000 --> 0:06:38,000
And the first of these essays

125
0:06:38.98,000 --> 0:06:41,000
was a relatively conventional pro-environmental essay

126
0:06:42.38,000 --> 0:06:46,000
that invoked the liberal values of care and protection from harm.

127
0:06:46.42,000 --> 0:06:48,000
It said things like, "In many important ways

128
0:06:48.98,000 --> 0:06:5,000
we are causing real harm to the places we live in,"

129
0:06:51.82,000 --> 0:06:53,000
and, "It is essential that we take steps now

130
0:06:54.66,000 --> 0:06:56,000
to prevent further destruction from being done to our Earth."

131
0:06:58.94,000 --> 0:06:59,000
Another group of participants

132
0:07:00.38,000 --> 0:07:02,000
were assigned to read a really different essay

133
0:07:02.62,000 --> 0:07:06,000
that was designed to tap into the conservative value of moral purity.

134
0:07:08.01,000 --> 0:07:09,000
It was a pro-environmental essay as well,

135
0:07:10.02,000 --> 0:07:11,000
and it said things like,

136
0:07:11.54,000 --> 0:07:15,000
"Keeping our forests, drinking water, and skies pure is of vital importance."

137
0:07:16.82,000 --> 0:07:17,000
"We should regard the pollution

138
0:07:18.34,000 --> 0:07:2,000
of the places we live in to be disgusting."

139
0:07:20.98,000 --> 0:07:22,000
And, "Reducing pollution can help us preserve

140
0:07:23.1,000 --> 0:07:26,000
what is pure and beautiful about the places we live."

141
0:07:27.7,000 --> 0:07:28,000
And then we had a third group

142
0:07:29.14,000 --> 0:07:31,000
that were assigned to read just a nonpolitical essay.

143
0:07:31.66,000 --> 0:07:33,000
It was just a comparison group so we could get a baseline.

144
0:07:34.42,000 --> 0:07:35,000
And what we found when we surveyed people

145
0:07:36.397,000 --> 0:07:38,000
about their environmental attitudes afterwards,

146
0:07:38.62,000 --> 0:07:4,000
we found that liberals, it didn't matter what essay they read.

147
0:07:41.58,000 --> 0:07:44,000
They tended to have highly pro-environmental attitudes regardless.

148
0:07:44.7,000 --> 0:07:46,000
Liberals are on board for environmental protection.

149
0:07:47.14,000 --> 0:07:48,000
Conservatives, however,

150
0:07:48.38,000 --> 0:07:52,000
were significantly more supportive of progressive environmental policies

151
0:07:52.82,000 --> 0:07:53,000
and environmental protection

152
0:07:54.38,000 --> 0:07:56,000
if they had read the moral purity essay

153
0:07:56.46,000 --> 0:07:58,000
than if they read one of the other two essays.

154
0:07:59.98,000 --> 0:08:02,000
We even found that conservatives who read the moral purity essay

155
0:08:03.1,000 --> 0:08:06,000
were significantly more likely to say that they believed in global warming

156
0:08:06.62,000 --> 0:08:07,000
and were concerned about global warming,

157
0:08:08.549,000 --> 0:08:1,000
even though this essay didn't even mention global warming.

158
0:08:11.3,000 --> 0:08:13,000
That's just a related environmental issue.

159
0:08:13.78,000 --> 0:08:16,000
But that's how robust this moral reframing effect was.

160
0:08:17.78,000 --> 0:08:2,000
And we've studied this on a whole slew of different political issues.

161
0:08:21.54,000 --> 0:08:24,000
So if you want to move conservatives

162
0:08:25.3,000 --> 0:08:28,000
on issues like same-sex marriage or national health insurance,

163
0:08:28.42,000 --> 0:08:31,000
it helps to tie these liberal political issues to conservative values

164
0:08:31.9,000 --> 0:08:33,000
like patriotism and moral purity.

165
0:08:35.62,000 --> 0:08:37,000
And we studied it the other way, too.

166
0:08:37.74,000 --> 0:08:4,000
If you want to move liberals to the right on conservative policy issues

167
0:08:41.58,000 --> 0:08:45,000
like military spending and making English the official language of the US,

168
0:08:46.22,000 --> 0:08:47,000
you're going to be more persuasive

169
0:08:47.9,000 --> 0:08:5,000
if you tie those conservative policy issues to liberal moral values

170
0:08:51.26,000 --> 0:08:52,000
like equality and fairness.

171
0:08:54.46,000 --> 0:08:56,000
All these studies have the same clear message:

172
0:08:57.34,000 --> 0:08:59,000
if you want to persuade someone on some policy,

173
0:09:00.3,000 --> 0:09:03,000
it's helpful to connect that policy to their underlying moral values.

174
0:09:05.34,000 --> 0:09:07,000
And when you say it like that

175
0:09:07.54,000 --> 0:09:08,000
it seems really obvious. Right?

176
0:09:09.06,000 --> 0:09:1,000
Like, why did we come here tonight?

177
0:09:10.86,000 --> 0:09:11,000
Why --

178
0:09:12.1,000 --> 0:09:13,000
(Laughter)

179
0:09:13.66,000 --> 0:09:15,000
It's incredibly intuitive.

180
0:09:17.22,000 --> 0:09:2,000
And even though it is, it's something we really struggle to do.

181
0:09:20.54,000 --> 0:09:23,000
You know, it turns out that when we go to persuade somebody on a political issue,

182
0:09:24.42,000 --> 0:09:26,000
we talk like we're speaking into a mirror.

183
0:09:27.18,000 --> 0:09:31,000
We don't persuade so much as we rehearse our own reasons

184
0:09:31.58,000 --> 0:09:33,000
for why we believe some sort of political position.

185
0:09:35.22,000 --> 0:09:39,000
We kept saying when we were designing these reframed moral arguments,

186
0:09:39.66,000 --> 0:09:41,000
"Empathy and respect, empathy and respect."

187
0:09:42.86,000 --> 0:09:43,000
If you can tap into that,

188
0:09:44.34,000 --> 0:09:45,000
you can connect

189
0:09:46.02,000 --> 0:09:48,000
and you might be able to persuade somebody in this country.

190
0:09:49.38,000 --> 0:09:51,000
So thinking again

191
0:09:51.82,000 --> 0:09:53,000
about what movie we're in,

192
0:09:55.02,000 --> 0:09:56,000
maybe I got carried away before.

193
0:09:56.62,000 --> 0:09:57,000
Maybe it's not a zombie apocalypse movie.

194
0:09:59.34,000 --> 0:10:,000
Maybe instead it's a buddy cop movie.

195
0:10:01.86,000 --> 0:10:03,000
(Laughter)

196
0:10:03.9,000 --> 0:10:05,000
Just roll with it, just go with it please.

197
0:10:05.94,000 --> 0:10:06,000
(Laughter)

198
0:10:08.3,000 --> 0:10:1,000
You know the kind: there's a white cop and a black cop,

199
0:10:11.02,000 --> 0:10:13,000
or maybe a messy cop and an organized cop.

200
0:10:13.18,000 --> 0:10:15,000
Whatever it is, they don't get along

201
0:10:15.26,000 --> 0:10:16,000
because of this difference.

202
0:10:17.34,000 --> 0:10:2,000
But in the end, when they have to come together and they cooperate,

203
0:10:20.58,000 --> 0:10:21,000
the solidarity that they feel,

204
0:10:22.54,000 --> 0:10:25,000
it's greater because of that gulf that they had to cross. Right?

205
0:10:27.1,000 --> 0:10:28,000
And remember that in these movies,

206
0:10:29.1,000 --> 0:10:31,000
it's usually worst in the second act

207
0:10:32.02,000 --> 0:10:34,000
when our leads are further apart than ever before.

208
0:10:35.26,000 --> 0:10:37,000
And so maybe that's where we are in this country,

209
0:10:37.62,000 --> 0:10:39,000
late in the second act of a buddy cop movie --

210
0:10:39.82,000 --> 0:10:41,000
(Laughter)

211
0:10:42.42,000 --> 0:10:45,000
torn apart but about to come back together.

212
0:10:47.22,000 --> 0:10:48,000
It sounds good,

213
0:10:48.9,000 --> 0:10:49,000
but if we want it to happen,

214
0:10:50.78,000 --> 0:10:52,000
I think the responsibility is going to start with us.

215
0:10:54.34,000 --> 0:10:56,000
So this is my call to you:

216
0:10:57.3,000 --> 0:10:59,000
let's put this country back together.

217
0:11:00.9,000 --> 0:11:03,000
Let's do it despite the politicians

218
0:11:03.98,000 --> 0:11:05,000
and the media and Facebook and Twitter

219
0:11:06.86,000 --> 0:11:07,000
and Congressional redistricting

220
0:11:08.42,000 --> 0:11:1,000
and all of it, all the things that divide us.

221
0:11:12.18,000 --> 0:11:14,000
Let's do it because it's right.

222
0:11:15.74,000 --> 0:11:19,000
And let's do it because this hate and contempt

223
0:11:20.18,000 --> 0:11:22,000
that flows through all of us every day

224
0:11:23.22,000 --> 0:11:26,000
makes us ugly and it corrupts us,

225
0:11:26.42,000 --> 0:11:29,000
and it threatens the very fabric of our society.

226
0:11:31.78,000 --> 0:11:33,000
We owe it to one another and our country

227
0:11:34.46,000 --> 0:11:36,000
to reach out and try to connect.

228
0:11:37.82,000 --> 0:11:4,000
We can't afford to hate them any longer,

229
0:11:42.02,000 --> 0:11:44,000
and we can't afford to let them hate us either.

230
0:11:45.7,000 --> 0:11:46,000
Empathy and respect.

231
0:11:47.7,000 --> 0:11:48,000
Empathy and respect.

232
0:11:49.74,000 --> 0:11:52,000
If you think about it, it's the very least that we owe our fellow citizens.

233
0:11:54.22,000 --> 0:11:55,000
Thank you.

234
0:11:55.46,000 --> 0:11:59,000
(Applause)

