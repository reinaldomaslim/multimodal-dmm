1
0:00:,000 --> 0:00:07,000
Traductor: Larisa Esteche Revisor: Sonia Escudero Sánchez

2
0:00:13.373,000 --> 0:00:14,000
¿Soy solo yo,

3
0:00:15.141,000 --> 0:00:17,000
o hay otras personas aquí

4
0:00:17.497,000 --> 0:00:19,000
que están un poco decepcionadas con la democracia?

5
0:00:20.986,000 --> 0:00:22,000
(Aplausos)

6
0:00:24.141,000 --> 0:00:26,000
Analicemos algunas cifras.

7
0:00:26.934,000 --> 0:00:28,000
En todo el mundo,

8
0:00:29.131,000 --> 0:00:32,000
la participación media en las elecciones presidenciales

9
0:00:33.047,000 --> 0:00:34,000
en los últimos 30 años

10
0:00:34.722,000 --> 0:00:36,000
ha sido solo del 67 %.

11
0:00:38.329,000 --> 0:00:39,000
Ahora bien, en Europa

12
0:00:40.326,000 --> 0:00:46,000
la participación media de la gente en las elecciones parlamentarias de la UE,

13
0:00:46.873,000 --> 0:00:47,000
es solo del 42 %.

14
0:00:50.125,000 --> 0:00:51,000
Ahora vayamos a Nueva York,

15
0:00:51.818,000 --> 0:00:55,000
y veamos cuánta gente votó en la última elección para alcalde.

16
0:00:56.523,000 --> 0:00:59,000
Veremos que solo un 24 % se presentó a votar.

17
0:01:01.063,000 --> 0:01:04,000
Eso significa que, si todavía dieran "Friends",

18
0:01:04.182,000 --> 0:01:07,000
Joey y tal vez Phoebe se habrían presentado a votar.

19
0:01:07.554,000 --> 0:01:08,000
(Risas)

20
0:01:09.434,000 --> 0:01:13,000
Y no se los puede culpar, porque la gente está cansada de los políticos.

21
0:01:13.784,000 --> 0:01:16,000
Y está cansada de que otras personas utilicen esa información

22
0:01:17.135,000 --> 0:01:19,000
que generaron al comunicarse con sus amigos y su familia

23
0:01:19.997,000 --> 0:01:21,000
para mandarles propaganda política.

24
0:01:22.519,000 --> 0:01:24,000
Pero esto no es algo nuevo.

25
0:01:25.271,000 --> 0:01:28,000
Hoy en día, la gente usa los "me gusta" para enviarte propaganda

26
0:01:28.52,000 --> 0:01:31,000
antes de usar tu código postal, tu género o tu edad,

27
0:01:31.917,000 --> 0:01:35,000
porque la idea de enviar propaganda política es tan vieja como la política.

28
0:01:37.53,000 --> 0:01:38,000
Y esto es posible

29
0:01:39.832,000 --> 0:01:42,000
porque la democracia tiene una vulnerabilidad básica.

30
0:01:43.71,000 --> 0:01:44,000
Esta es la idea de un representante.

31
0:01:45.979,000 --> 0:01:47,000
En principio, la democracia es la capacidad de las personas

32
0:01:48.857,000 --> 0:01:49,000
de ejercer el poder.

33
0:01:50.017,000 --> 0:01:53,000
Pero en la práctica, tenemos que delegar ese poder a un representante

34
0:01:53.859,000 --> 0:01:55,000
que pueda ejercer ese poder en nuestro nombre.

35
0:01:56.311,000 --> 0:01:59,000
Ese representante es un cuello de botella, o un punto débil.

36
0:01:59.731,000 --> 0:02:02,000
Es el lugar al que se debe apuntar si se quiere atacar a la democracia,

37
0:02:03.644,000 --> 0:02:06,000
porque se puede capturar a la democracia ya sea capturando al representante

38
0:02:07.217,000 --> 0:02:09,000
o capturando la manera en que la gente lo elige.

39
0:02:10.065,000 --> 0:02:11,000
La gran pregunta es:

40
0:02:11.505,000 --> 0:02:12,000
¿es el fin de la historia?

41
0:02:13.989,000 --> 0:02:16,000
¿Es lo mejor que podemos hacer

42
0:02:17.878,000 --> 0:02:2,000
o, en realidad, hay alternativas?

43
0:02:22.13,000 --> 0:02:24,000
Algunas personas han pensado en alternativas

44
0:02:24.508,000 --> 0:02:27,000
y una de las ideas que da vueltas es la idea de la democracia directa.

45
0:02:29,000 --> 0:02:31,000
Es la idea de evitar la intervención de los políticos

46
0:02:31.523,000 --> 0:02:33,000
y hacer que la gente vote directamente sobre los asuntos,

47
0:02:34.316,000 --> 0:02:35,000
que vote las leyes directamente.

48
0:02:36.415,000 --> 0:02:37,000
Pero esta idea es ingenua

49
0:02:37.775,000 --> 0:02:4,000
porque hay demasiadas cosas que deberíamos elegir.

50
0:02:40.97,000 --> 0:02:42,000
Si miran el 114° Congreso estadounidense

51
0:02:43.776,000 --> 0:02:45,000
habrán visto que la Cámara de Representantes

52
0:02:46.287,000 --> 0:02:48,000
examinó más de 6000 proyectos de ley,

53
0:02:49.2,000 --> 0:02:51,000
el Senado examinó más de 3000,

54
0:02:51.88,000 --> 0:02:53,000
y aprobaron más de 300 leyes.

55
0:02:54.712,000 --> 0:02:57,000
Esas serían las muchas decisiones que cada persona debería tomar por semana

56
0:02:58.526,000 --> 0:03:,000
sobre temas que no son de su conocimiento.

57
0:03:01.109,000 --> 0:03:03,000
Es decir, hay un gran problema de distancia cognitiva

58
0:03:03.68,000 --> 0:03:06,000
si pensamos en la democracia directa como alternativa viable.

59
0:03:08.205,000 --> 0:03:12,000
Algunas personas piensan en la idea de la democracia líquida o delegativa,

60
0:03:12.664,000 --> 0:03:15,000
en la que se entrega el poder político a alguien

61
0:03:16.464,000 --> 0:03:17,000
que pueda dárselo a alguien más,

62
0:03:18.188,000 --> 0:03:2,000
y, con el tiempo, se crea una gran red de seguidores

63
0:03:20.753,000 --> 0:03:23,000
en la que, al final, unas pocas personas toman las decisiones

64
0:03:24.071,000 --> 0:03:27,000
en nombre de todos sus seguidores y los seguidores de estos.

65
0:03:28.326,000 --> 0:03:31,000
Pero esta idea tampoco resuelve el problema cognitivo

66
0:03:32.479,000 --> 0:03:36,000
y, para ser sincero, es bastante similar a la idea de tener un representante.

67
0:03:36.795,000 --> 0:03:38,000
Lo que haré hoy es ser un poco provocativo,

68
0:03:40.067,000 --> 0:03:41,000
y voy a preguntarles:

69
0:03:42.601,000 --> 0:03:47,000
¿y si en vez de evitar a los políticos,

70
0:03:49.187,000 --> 0:03:51,000
tratamos de automatizarlos?

71
0:03:57.871,000 --> 0:03:59,000
La idea de automatización no es nueva.

72
0:04:00.821,000 --> 0:04:02,000
Comenzó hace más de 300 años,

73
0:04:02.925,000 --> 0:04:05,000
cuando tejedores franceses decidieron automatizar el telar.

74
0:04:06.82,000 --> 0:04:09,000
El ganador de esa guerra industrial fue Joseph-Marie Jacquard.

75
0:04:11.204,000 --> 0:04:12,000
Fue un tejedor y vendedor francés

76
0:04:12.979,000 --> 0:04:16,000
que combinó el telar con la máquina de vapor para crear telares autónomos.

77
0:04:17.657,000 --> 0:04:19,000
Y con esos telares autónomos logró tener el control.

78
0:04:20.434,000 --> 0:04:23,000
Ahora podía hacer telas que eran más complejas y sofisticadas

79
0:04:24.343,000 --> 0:04:26,000
que las que podían hacer a mano.

80
0:04:27.193,000 --> 0:04:29,000
Pero además, al ganar esa guerra industrial,

81
0:04:29.849,000 --> 0:04:32,000
sentó la base de lo que se convirtió en el modelo de la automatización.

82
0:04:33.955,000 --> 0:04:36,000
La manera en que automatizamos las cosas en los últimos 300 años

83
0:04:37.029,000 --> 0:04:38,000
siempre ha sido la misma:

84
0:04:39.006,000 --> 0:04:41,000
primero, identificamos una necesidad,

85
0:04:41.539,000 --> 0:04:43,000
después creamos una herramienta para satisfacer esa necesidad,

86
0:04:44.747,000 --> 0:04:45,000
como el telar, en este caso,

87
0:04:46.541,000 --> 0:04:48,000
y luego estudiamos cómo la gente utiliza esa herramienta

88
0:04:49.226,000 --> 0:04:5,000
para automatizar a ese usuario.

89
0:04:51.242,000 --> 0:04:55,000
Así pasamos del telar mecánico al telar autónomo,

90
0:04:56.247,000 --> 0:04:57,000
y eso nos llevó mil años.

91
0:04:58.391,000 --> 0:05:,000
Ahora, solo nos llevó cien años

92
0:05:00.486,000 --> 0:05:02,000
usar el mismo guion para automatizar el coche.

93
0:05:05.286,000 --> 0:05:07,000
La cuestión es que, en esta ocasión,

94
0:05:07.762,000 --> 0:05:08,000
la automatización es real.

95
0:05:09.915,000 --> 0:05:12,000
Este es un video que un colega de Toshiba compartió conmigo

96
0:05:13.26,000 --> 0:05:16,000
de la fábrica que elabora unidades de estado sólido.

97
0:05:16.543,000 --> 0:05:18,000
La fábrica entera es un robot.

98
0:05:18.585,000 --> 0:05:19,000
No hay humanos en esa fábrica.

99
0:05:21.033,000 --> 0:05:23,000
Y pronto, los robots dejarán las fábricas

100
0:05:23.278,000 --> 0:05:26,000
y serán parte de nuestro mundo, parte de nuestra fuerza laboral.

101
0:05:27.183,000 --> 0:05:28,000
Lo que hago diariamente en mi trabajo

102
0:05:28.98,000 --> 0:05:31,000
es crear herramientas que integran información para países enteros

103
0:05:32.996,000 --> 0:05:35,000
para que finalmente podamos tener los cimientos que necesitamos

104
0:05:36.486,000 --> 0:05:39,000
para un futuro en el que necesitemos manejar esas máquinas.

105
0:05:41.195,000 --> 0:05:43,000
Pero hoy no estoy aquí para hablarles de estas herramientas

106
0:05:44.125,000 --> 0:05:45,000
que integran información para los países,

107
0:05:46.463,000 --> 0:05:48,000
sino para hablarles de otra idea

108
0:05:49.109,000 --> 0:05:53,000
que puede ayudarnos a usar inteligencia artificial en la democracia.

109
0:05:53.998,000 --> 0:05:57,000
Porque las herramientas que hago están diseñadas para decisiones ejecutivas

110
0:05:58.755,000 --> 0:06:01,000
y estas decisiones pueden encasillarse en algún tipo de término de objetividad:

111
0:06:02.621,000 --> 0:06:03,000
decisiones de inversión pública.

112
0:06:04.885,000 --> 0:06:06,000
Pero hay decisiones legislativas,

113
0:06:07.54,000 --> 0:06:1,000
y estas decisiones legislativas requieren comunicación entre las personas

114
0:06:11.241,000 --> 0:06:12,000
que tienen distintos puntos de vista,

115
0:06:13.075,000 --> 0:06:16,000
requieren participación, debate, deliberación.

116
0:06:18.241,000 --> 0:06:2,000
Y durante mucho tiempo pensamos

117
0:06:20.619,000 --> 0:06:23,000
que lo necesario para mejorar la democracia es más comunicación.

118
0:06:24.553,000 --> 0:06:27,000
Todas las tecnologías que desarrollamos en el contexto de la democracia,

119
0:06:28.066,000 --> 0:06:3,000
ya sea en los periódicos o en las redes sociales,

120
0:06:30.568,000 --> 0:06:32,000
trataron de brindarnos más comunicación.

121
0:06:33.813,000 --> 0:06:37,000
Pero ya hemos caído en esa trampa, y sabemos que no resuelve el problema.

122
0:06:38.071,000 --> 0:06:41,000
Porque no es un problema de comunicación, sino un problema de distancia cognitiva.

123
0:06:42.513,000 --> 0:06:44,000
Si el problema es cognitivo,

124
0:06:44.903,000 --> 0:06:48,000
agregar más comunicación no será la solución.

125
0:06:50.282,000 --> 0:06:53,000
En cambio, lo que necesitaremos es tener otras tecnologías

126
0:06:53.419,000 --> 0:06:57,000
que nos ayuden a manejar la comunicación que tenemos en exceso.

127
0:06:58.755,000 --> 0:06:59,000
Piensen en un pequeño avatar,

128
0:07:00.478,000 --> 0:07:01,000
un agente de software,

129
0:07:01.841,000 --> 0:07:02,000
un Pepito Grillo digital,

130
0:07:03.743,000 --> 0:07:04,000
(Risas)

131
0:07:05.005,000 --> 0:07:09,000
que básicamente puede responder cosas por ti.

132
0:07:09.759,000 --> 0:07:1,000
Y si tuviéramos esa tecnología,

133
0:07:11.57,000 --> 0:07:13,000
podríamos descargar parte de la comunicación

134
0:07:14.072,000 --> 0:07:18,000
y ayudar, tal vez, a tomar mejores decisiones, o a mayor escala.

135
0:07:18.86,000 --> 0:07:21,000
La cuestión es que la idea de agentes de software tampoco es nueva.

136
0:07:22.603,000 --> 0:07:24,000
Ya la usamos todo el tiempo.

137
0:07:25.216,000 --> 0:07:26,000
Usamos agentes de software

138
0:07:26.761,000 --> 0:07:29,000
para saber cómo llegar a cierto lugar,

139
0:07:31.07,000 --> 0:07:33,000
qué música vamos a escuchar

140
0:07:33.468,000 --> 0:07:36,000
o para recibir sugerencias de los próximos libros que deberíamos leer.

141
0:07:37.994,000 --> 0:07:39,000
Hay una idea obvia en el siglo XXI

142
0:07:40.592,000 --> 0:07:42,000
que fue tan obvia como la idea

143
0:07:43.259,000 --> 0:07:48,000
de combinar una máquina de vapor con un telar en la época de Jacquard.

144
0:07:49.538,000 --> 0:07:53,000
Y esa idea es combinar la democracia directa con agentes de software.

145
0:07:54.849,000 --> 0:07:57,000
Imaginen, por un segundo, un mundo donde, en vez de tener un representante

146
0:07:58.86,000 --> 0:08:,000
que los represente a ustedes y a millones de otras personas,

147
0:08:01.838,000 --> 0:08:04,000
pueden tener un representante que solo los represente a Uds.

148
0:08:05.504,000 --> 0:08:07,000
con sus opiniones políticas diferentes,

149
0:08:07.782,000 --> 0:08:1,000
esa rara combinación de libertario y liberal

150
0:08:11.15,000 --> 0:08:13,000
y tal vez un poco conservador en algunos asuntos,

151
0:08:13.566,000 --> 0:08:14,000
y muy progresista en otros.

152
0:08:15.628,000 --> 0:08:18,000
Hoy en día, los políticos son paquetes, y están llenos de compromisos.

153
0:08:18.989,000 --> 0:08:21,000
Pero pueden tener a alguien que los represente solo a Uds.

154
0:08:22.647,000 --> 0:08:26,000
si están dispuestos a abandonar la idea de que ese representante sea un humano.

155
0:08:27.119,000 --> 0:08:29,000
Si ese representante es un agente de software,

156
0:08:29.335,000 --> 0:08:32,000
podríamos tener un senado que tenga tantos senadores como ciudadanos.

157
0:08:33.529,000 --> 0:08:35,000
Y esos senadores podrán leer cada proyecto de ley,

158
0:08:36.411,000 --> 0:08:37,000
y podrán votar por cada uno de ellos.

159
0:08:39.822,000 --> 0:08:41,000
Hay una idea obvia, entonces, que tal vez debamos considerar.

160
0:08:42.802,000 --> 0:08:46,000
Pero entiendo que en esta época, esta idea puede ser alarmante.

161
0:08:48.391,000 --> 0:08:51,000
De hecho, pensar en un robot que venga del futuro

162
0:08:51.855,000 --> 0:08:53,000
para ayudarnos a gobernar suena aterrador.

163
0:08:56.223,000 --> 0:08:57,000
Pero ya lo hemos hecho.

164
0:08:57.898,000 --> 0:08:58,000
(Risas)

165
0:08:59.195,000 --> 0:09:01,000
Y de hecho era un buen tipo.

166
0:09:05.057,000 --> 0:09:09,000
¿Cómo se vería la versión del telar de Jacquard en esta idea?

167
0:09:10.135,000 --> 0:09:11,000
Sería un sistema muy simple.

168
0:09:12.06,000 --> 0:09:15,000
Imaginen un sistema en el que inician sesión y crean un avatar,

169
0:09:15.542,000 --> 0:09:16,000
y luego comienzan a entrenar su avatar.

170
0:09:17.472,000 --> 0:09:2,000
Podemos dotar nuestro avatar con nuestros hábitos de lectura,

171
0:09:20.728,000 --> 0:09:21,000
o conectarlo a nuestras redes sociales,

172
0:09:22.613,000 --> 0:09:24,000
o conectarlo a otra información,

173
0:09:25.045,000 --> 0:09:27,000
por ejemplo mediante pruebas psicológicas.

174
0:09:27.341,000 --> 0:09:29,000
Y lo bueno de esto es que no hay engaño.

175
0:09:30.333,000 --> 0:09:33,000
No proporcionan información para comunicarse con sus amigos y familia

176
0:09:33.696,000 --> 0:09:35,000
que luego se utiliza en un sistema político.

177
0:09:36.871,000 --> 0:09:39,000
Proporciona información a un sistema que está diseñado para ser utilizado

178
0:09:40.599,000 --> 0:09:42,000
para tomar decisiones políticas en nuestro nombre.

179
0:09:43.264,000 --> 0:09:46,000
Luego, toma esa información y elige un algoritmo de entrenamiento,

180
0:09:47.268,000 --> 0:09:48,000
porque es un mercado abierto

181
0:09:48.665,000 --> 0:09:51,000
en el que diferentes personas pueden presentar distintos algoritmos

182
0:09:51.821,000 --> 0:09:55,000
para predecir cómo votarán, basándose en la información que proporcionaron.

183
0:09:56.083,000 --> 0:09:59,000
Y el sistema es abierto. Nadie controla los algoritmos.

184
0:09:59.562,000 --> 0:10:01,000
Hay algoritmos que se hacen más populares y otros no tanto.

185
0:10:02.515,000 --> 0:10:04,000
Con el tiempo, se puede auditar el sistema.

186
0:10:04.626,000 --> 0:10:07,000
Pueden ver cómo funciona su avatar, y dejarlo en piloto automático si quieren.

187
0:10:08.647,000 --> 0:10:09,000
Si quieren controlar un poco más,

188
0:10:10.593,000 --> 0:10:13,000
pueden elegir que les consulte cuando toma una decisión,

189
0:10:14.057,000 --> 0:10:15,000
o puede estar entre esas opciones.

190
0:10:16.466,000 --> 0:10:19,000
Una de las razones por las que usamos tan poco la democracia

191
0:10:20.011,000 --> 0:10:23,000
puede ser porque la democracia tiene una interfaz de usuario muy mala.

192
0:10:23.303,000 --> 0:10:25,000
Si mejoramos la interfaz de usuario de la democracia,

193
0:10:25.87,000 --> 0:10:26,000
podríamos utilizarla más.

194
0:10:28.452,000 --> 0:10:3,000
Claro, hay muchas preguntas que pueden surgir.

195
0:10:32.473,000 --> 0:10:34,000
Por ejemplo, ¿cómo entrenar a esos avatares?

196
0:10:34.658,000 --> 0:10:35,000
¿Cómo mantener segura la información?

197
0:10:36.576,000 --> 0:10:39,000
¿Cómo mantener los sistemas distribuidos y auditables?

198
0:10:39.848,000 --> 0:10:42,000
¿Cómo hace mi abuela, que tiene 80 años y no sabe usar internet?

199
0:10:44.262,000 --> 0:10:45,000
Créanme, las escuché a todas.

200
0:10:46.507,000 --> 0:10:5,000
Cuando piensan en una idea como esta, tengan cuidado con los pesimistas

201
0:10:51.091,000 --> 0:10:55,000
porque suelen tener un problema para cada solución.

202
0:10:55.434,000 --> 0:10:56,000
(Risas)

203
0:10:57.283,000 --> 0:10:59,000
Quiero invitarlos a pensar en las ideas más grandes.

204
0:11:00.347,000 --> 0:11:03,000
Las preguntas que acabo de mostrarles son pequeñas ideas

205
0:11:03.997,000 --> 0:11:05,000
porque son preguntas sobre cómo esto no funcionaría.

206
0:11:07.502,000 --> 0:11:1,000
Las grandes ideas son ideas sobre qué más podemos hacer con esto

207
0:11:11.338,000 --> 0:11:12,000
si funcionara.

208
0:11:13.774,000 --> 0:11:16,000
Y una de esas ideas es: ¿quién redacta las leyes?

209
0:11:17.854,000 --> 0:11:21,000
Al principio, podíamos hacer que los avatares que ya tenemos

210
0:11:22.101,000 --> 0:11:25,000
voten leyes que están redactadas por los senadores o los políticos

211
0:11:25.622,000 --> 0:11:26,000
que ya tenemos.

212
0:11:27.491,000 --> 0:11:28,000
Pero si esto funcionara,

213
0:11:29.902,000 --> 0:11:33,000
podrían crear un algoritmo que intente redactar una ley

214
0:11:34.45,000 --> 0:11:36,000
que obtendría un cierto porcentaje de aprobación,

215
0:11:36.895,000 --> 0:11:37,000
y pueden revertir el proceso.

216
0:11:38.701,000 --> 0:11:41,000
Ahora bien, pueden pensar que es una idea absurda y no deberíamos hacerlo,

217
0:11:42.237,000 --> 0:11:44,000
pero no pueden negar que esta idea solo es posible

218
0:11:44.907,000 --> 0:11:47,000
en un mundo donde la democracia directa y los agentes de software

219
0:11:48.091,000 --> 0:11:5,000
son una forma viable de participación.

220
0:11:52.596,000 --> 0:11:54,000
Pues bien, ¿cómo empezamos la revolución?

221
0:11:56.238,000 --> 0:11:59,000
No se empieza esta revolución con vallas de contención o protestas,

222
0:11:59.572,000 --> 0:12:02,000
ni exigiendo que nuestros actuales políticos se cambien por robots.

223
0:12:03.786,000 --> 0:12:04,000
No funcionará.

224
0:12:05.359,000 --> 0:12:08,000
Esto es mucho más sencillo, más lento y mucho más humilde.

225
0:12:09.616,000 --> 0:12:12,000
Esta revolución empieza creando sistemas simples

226
0:12:12.989,000 --> 0:12:15,000
en escuelas de posgrado, en bibliotecas y en ONG.

227
0:12:16.107,000 --> 0:12:18,000
Y tratamos de descifrar todas esas pequeñas preguntas

228
0:12:18.785,000 --> 0:12:19,000
y esos pequeños problemas

229
0:12:20.03,000 --> 0:12:23,000
que tendremos que resolver para hacer esta idea viable,

230
0:12:23.785,000 --> 0:12:25,000
para hacer de esta idea algo confiable.

231
0:12:26.33,000 --> 0:12:29,000
Y al crear esos sistemas que permiten a cientos o miles de personas,

232
0:12:29.949,000 --> 0:12:32,000
a cien mil personas votar de manera políticamente no obligatoria,

233
0:12:33.783,000 --> 0:12:36,000
esta idea irá sumando confianza, el mundo cambiará,

234
0:12:37.368,000 --> 0:12:4,000
y quienes ahora son tan pequeños como mi hija, van a crecer.

235
0:12:42.58,000 --> 0:12:44,000
Y para cuando mi hija tenga mi edad,

236
0:12:44.973,000 --> 0:12:48,000
tal vez esta idea que hoy parece tan absurda

237
0:12:49.433,000 --> 0:12:52,000
no sea absurda para ella y sus amigos.

238
0:12:53.956,000 --> 0:12:54,000
Y en ese momento,

239
0:12:55.817,000 --> 0:13:,000
estaremos al final de nuestra historia, pero ellos estarán al comienzo de la suya.

240
0:13:01.646,000 --> 0:13:02,000
Gracias.

241
0:13:02.853,000 --> 0:13:05,000
(Aplausos)

