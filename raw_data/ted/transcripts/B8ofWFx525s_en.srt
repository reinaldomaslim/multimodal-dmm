1
0:00:15.26,000 --> 0:00:17,000
Mark Zuckerberg,

2
0:00:17.26,000 --> 0:00:2,000
a journalist was asking him a question about the news feed.

3
0:00:20.26,000 --> 0:00:22,000
And the journalist was asking him,

4
0:00:22.26,000 --> 0:00:24,000
"Why is this so important?"

5
0:00:24.26,000 --> 0:00:26,000
And Zuckerberg said,

6
0:00:26.26,000 --> 0:00:28,000
"A squirrel dying in your front yard

7
0:00:28.26,000 --> 0:00:31,000
may be more relevant to your interests right now

8
0:00:31.26,000 --> 0:00:34,000
than people dying in Africa."

9
0:00:34.26,000 --> 0:00:36,000
And I want to talk about

10
0:00:36.26,000 --> 0:00:39,000
what a Web based on that idea of relevance might look like.

11
0:00:40.26,000 --> 0:00:42,000
So when I was growing up

12
0:00:42.26,000 --> 0:00:44,000
in a really rural area in Maine,

13
0:00:44.26,000 --> 0:00:47,000
the Internet meant something very different to me.

14
0:00:47.26,000 --> 0:00:49,000
It meant a connection to the world.

15
0:00:49.26,000 --> 0:00:52,000
It meant something that would connect us all together.

16
0:00:52.26,000 --> 0:00:55,000
And I was sure that it was going to be great for democracy

17
0:00:55.26,000 --> 0:00:58,000
and for our society.

18
0:00:58.26,000 --> 0:01:,000
But there's this shift

19
0:01:00.26,000 --> 0:01:02,000
in how information is flowing online,

20
0:01:02.26,000 --> 0:01:05,000
and it's invisible.

21
0:01:05.26,000 --> 0:01:07,000
And if we don't pay attention to it,

22
0:01:07.26,000 --> 0:01:1,000
it could be a real problem.

23
0:01:10.26,000 --> 0:01:13,000
So I first noticed this in a place I spend a lot of time --

24
0:01:13.26,000 --> 0:01:15,000
my Facebook page.

25
0:01:15.26,000 --> 0:01:18,000
I'm progressive, politically -- big surprise --

26
0:01:18.26,000 --> 0:01:2,000
but I've always gone out of my way to meet conservatives.

27
0:01:20.26,000 --> 0:01:22,000
I like hearing what they're thinking about;

28
0:01:22.26,000 --> 0:01:24,000
I like seeing what they link to;

29
0:01:24.26,000 --> 0:01:26,000
I like learning a thing or two.

30
0:01:26.26,000 --> 0:01:29,000
And so I was surprised when I noticed one day

31
0:01:29.26,000 --> 0:01:32,000
that the conservatives had disappeared from my Facebook feed.

32
0:01:33.26,000 --> 0:01:35,000
And what it turned out was going on

33
0:01:35.26,000 --> 0:01:39,000
was that Facebook was looking at which links I clicked on,

34
0:01:39.26,000 --> 0:01:41,000
and it was noticing that, actually,

35
0:01:41.26,000 --> 0:01:43,000
I was clicking more on my liberal friends' links

36
0:01:43.26,000 --> 0:01:46,000
than on my conservative friends' links.

37
0:01:46.26,000 --> 0:01:48,000
And without consulting me about it,

38
0:01:48.26,000 --> 0:01:5,000
it had edited them out.

39
0:01:50.26,000 --> 0:01:53,000
They disappeared.

40
0:01:54.26,000 --> 0:01:56,000
So Facebook isn't the only place

41
0:01:56.26,000 --> 0:01:58,000
that's doing this kind of invisible, algorithmic

42
0:01:58.26,000 --> 0:02:01,000
editing of the Web.

43
0:02:01.26,000 --> 0:02:03,000
Google's doing it too.

44
0:02:03.26,000 --> 0:02:06,000
If I search for something, and you search for something,

45
0:02:06.26,000 --> 0:02:08,000
even right now at the very same time,

46
0:02:08.26,000 --> 0:02:11,000
we may get very different search results.

47
0:02:11.26,000 --> 0:02:14,000
Even if you're logged out, one engineer told me,

48
0:02:14.26,000 --> 0:02:16,000
there are 57 signals

49
0:02:16.26,000 --> 0:02:19,000
that Google looks at --

50
0:02:19.26,000 --> 0:02:22,000
everything from what kind of computer you're on

51
0:02:22.26,000 --> 0:02:24,000
to what kind of browser you're using

52
0:02:24.26,000 --> 0:02:26,000
to where you're located --

53
0:02:26.26,000 --> 0:02:29,000
that it uses to personally tailor your query results.

54
0:02:29.26,000 --> 0:02:31,000
Think about it for a second:

55
0:02:31.26,000 --> 0:02:35,000
there is no standard Google anymore.

56
0:02:35.26,000 --> 0:02:38,000
And you know, the funny thing about this is that it's hard to see.

57
0:02:38.26,000 --> 0:02:4,000
You can't see how different your search results are

58
0:02:40.26,000 --> 0:02:42,000
from anyone else's.

59
0:02:42.26,000 --> 0:02:44,000
But a couple of weeks ago,

60
0:02:44.26,000 --> 0:02:47,000
I asked a bunch of friends to Google "Egypt"

61
0:02:47.26,000 --> 0:02:5,000
and to send me screen shots of what they got.

62
0:02:50.26,000 --> 0:02:53,000
So here's my friend Scott's screen shot.

63
0:02:54.26,000 --> 0:02:57,000
And here's my friend Daniel's screen shot.

64
0:02:57.26,000 --> 0:02:59,000
When you put them side-by-side,

65
0:02:59.26,000 --> 0:03:01,000
you don't even have to read the links

66
0:03:01.26,000 --> 0:03:03,000
to see how different these two pages are.

67
0:03:03.26,000 --> 0:03:05,000
But when you do read the links,

68
0:03:05.26,000 --> 0:03:08,000
it's really quite remarkable.

69
0:03:09.26,000 --> 0:03:12,000
Daniel didn't get anything about the protests in Egypt at all

70
0:03:12.26,000 --> 0:03:14,000
in his first page of Google results.

71
0:03:14.26,000 --> 0:03:16,000
Scott's results were full of them.

72
0:03:16.26,000 --> 0:03:18,000
And this was the big story of the day at that time.

73
0:03:18.26,000 --> 0:03:21,000
That's how different these results are becoming.

74
0:03:21.26,000 --> 0:03:24,000
So it's not just Google and Facebook either.

75
0:03:24.26,000 --> 0:03:26,000
This is something that's sweeping the Web.

76
0:03:26.26,000 --> 0:03:29,000
There are a whole host of companies that are doing this kind of personalization.

77
0:03:29.26,000 --> 0:03:32,000
Yahoo News, the biggest news site on the Internet,

78
0:03:32.26,000 --> 0:03:35,000
is now personalized -- different people get different things.

79
0:03:36.26,000 --> 0:03:39,000
Huffington Post, the Washington Post, the New York Times --

80
0:03:39.26,000 --> 0:03:42,000
all flirting with personalization in various ways.

81
0:03:42.26,000 --> 0:03:45,000
And this moves us very quickly

82
0:03:45.26,000 --> 0:03:47,000
toward a world in which

83
0:03:47.26,000 --> 0:03:51,000
the Internet is showing us what it thinks we want to see,

84
0:03:51.26,000 --> 0:03:54,000
but not necessarily what we need to see.

85
0:03:54.26,000 --> 0:03:57,000
As Eric Schmidt said,

86
0:03:57.26,000 --> 0:04:,000
"It will be very hard for people to watch or consume something

87
0:04:00.26,000 --> 0:04:02,000
that has not in some sense

88
0:04:02.26,000 --> 0:04:05,000
been tailored for them."

89
0:04:05.26,000 --> 0:04:07,000
So I do think this is a problem.

90
0:04:07.26,000 --> 0:04:1,000
And I think, if you take all of these filters together,

91
0:04:10.26,000 --> 0:04:12,000
you take all these algorithms,

92
0:04:12.26,000 --> 0:04:15,000
you get what I call a filter bubble.

93
0:04:16.26,000 --> 0:04:19,000
And your filter bubble is your own personal,

94
0:04:19.26,000 --> 0:04:21,000
unique universe of information

95
0:04:21.26,000 --> 0:04:23,000
that you live in online.

96
0:04:23.26,000 --> 0:04:26,000
And what's in your filter bubble

97
0:04:26.26,000 --> 0:04:29,000
depends on who you are, and it depends on what you do.

98
0:04:29.26,000 --> 0:04:33,000
But the thing is that you don't decide what gets in.

99
0:04:33.26,000 --> 0:04:35,000
And more importantly,

100
0:04:35.26,000 --> 0:04:38,000
you don't actually see what gets edited out.

101
0:04:38.26,000 --> 0:04:4,000
So one of the problems with the filter bubble

102
0:04:40.26,000 --> 0:04:43,000
was discovered by some researchers at Netflix.

103
0:04:43.26,000 --> 0:04:46,000
And they were looking at the Netflix queues, and they noticed something kind of funny

104
0:04:46.26,000 --> 0:04:48,000
that a lot of us probably have noticed,

105
0:04:48.26,000 --> 0:04:5,000
which is there are some movies

106
0:04:50.26,000 --> 0:04:53,000
that just sort of zip right up and out to our houses.

107
0:04:53.26,000 --> 0:04:56,000
They enter the queue, they just zip right out.

108
0:04:56.26,000 --> 0:04:58,000
So "Iron Man" zips right out,

109
0:04:58.26,000 --> 0:05:,000
and "Waiting for Superman"

110
0:05:00.26,000 --> 0:05:02,000
can wait for a really long time.

111
0:05:02.26,000 --> 0:05:04,000
What they discovered

112
0:05:04.26,000 --> 0:05:06,000
was that in our Netflix queues

113
0:05:06.26,000 --> 0:05:09,000
there's this epic struggle going on

114
0:05:09.26,000 --> 0:05:12,000
between our future aspirational selves

115
0:05:12.26,000 --> 0:05:15,000
and our more impulsive present selves.

116
0:05:15.26,000 --> 0:05:17,000
You know we all want to be someone

117
0:05:17.26,000 --> 0:05:19,000
who has watched "Rashomon,"

118
0:05:19.26,000 --> 0:05:21,000
but right now

119
0:05:21.26,000 --> 0:05:24,000
we want to watch "Ace Ventura" for the fourth time.

120
0:05:24.26,000 --> 0:05:27,000
(Laughter)

121
0:05:27.26,000 --> 0:05:29,000
So the best editing gives us a bit of both.

122
0:05:29.26,000 --> 0:05:31,000
It gives us a little bit of Justin Bieber

123
0:05:31.26,000 --> 0:05:33,000
and a little bit of Afghanistan.

124
0:05:33.26,000 --> 0:05:35,000
It gives us some information vegetables;

125
0:05:35.26,000 --> 0:05:38,000
it gives us some information dessert.

126
0:05:38.26,000 --> 0:05:4,000
And the challenge with these kinds of algorithmic filters,

127
0:05:40.26,000 --> 0:05:42,000
these personalized filters,

128
0:05:42.26,000 --> 0:05:44,000
is that, because they're mainly looking

129
0:05:44.26,000 --> 0:05:48,000
at what you click on first,

130
0:05:48.26,000 --> 0:05:52,000
it can throw off that balance.

131
0:05:52.26,000 --> 0:05:55,000
And instead of a balanced information diet,

132
0:05:55.26,000 --> 0:05:57,000
you can end up surrounded

133
0:05:57.26,000 --> 0:05:59,000
by information junk food.

134
0:05:59.26,000 --> 0:06:01,000
What this suggests

135
0:06:01.26,000 --> 0:06:04,000
is actually that we may have the story about the Internet wrong.

136
0:06:04.26,000 --> 0:06:06,000
In a broadcast society --

137
0:06:06.26,000 --> 0:06:08,000
this is how the founding mythology goes --

138
0:06:08.26,000 --> 0:06:1,000
in a broadcast society,

139
0:06:10.26,000 --> 0:06:12,000
there were these gatekeepers, the editors,

140
0:06:12.26,000 --> 0:06:15,000
and they controlled the flows of information.

141
0:06:15.26,000 --> 0:06:18,000
And along came the Internet and it swept them out of the way,

142
0:06:18.26,000 --> 0:06:2,000
and it allowed all of us to connect together,

143
0:06:20.26,000 --> 0:06:22,000
and it was awesome.

144
0:06:22.26,000 --> 0:06:25,000
But that's not actually what's happening right now.

145
0:06:26.26,000 --> 0:06:29,000
What we're seeing is more of a passing of the torch

146
0:06:29.26,000 --> 0:06:31,000
from human gatekeepers

147
0:06:31.26,000 --> 0:06:34,000
to algorithmic ones.

148
0:06:34.26,000 --> 0:06:37,000
And the thing is that the algorithms

149
0:06:37.26,000 --> 0:06:4,000
don't yet have the kind of embedded ethics

150
0:06:40.26,000 --> 0:06:43,000
that the editors did.

151
0:06:43.26,000 --> 0:06:46,000
So if algorithms are going to curate the world for us,

152
0:06:46.26,000 --> 0:06:49,000
if they're going to decide what we get to see and what we don't get to see,

153
0:06:49.26,000 --> 0:06:51,000
then we need to make sure

154
0:06:51.26,000 --> 0:06:54,000
that they're not just keyed to relevance.

155
0:06:54.26,000 --> 0:06:56,000
We need to make sure that they also show us things

156
0:06:56.26,000 --> 0:06:59,000
that are uncomfortable or challenging or important --

157
0:06:59.26,000 --> 0:07:01,000
this is what TED does --

158
0:07:01.26,000 --> 0:07:03,000
other points of view.

159
0:07:03.26,000 --> 0:07:05,000
And the thing is, we've actually been here before

160
0:07:05.26,000 --> 0:07:07,000
as a society.

161
0:07:08.26,000 --> 0:07:11,000
In 1915, it's not like newspapers were sweating a lot

162
0:07:11.26,000 --> 0:07:14,000
about their civic responsibilities.

163
0:07:14.26,000 --> 0:07:16,000
Then people noticed

164
0:07:16.26,000 --> 0:07:19,000
that they were doing something really important.

165
0:07:19.26,000 --> 0:07:21,000
That, in fact, you couldn't have

166
0:07:21.26,000 --> 0:07:23,000
a functioning democracy

167
0:07:23.26,000 --> 0:07:27,000
if citizens didn't get a good flow of information,

168
0:07:28.26,000 --> 0:07:31,000
that the newspapers were critical because they were acting as the filter,

169
0:07:31.26,000 --> 0:07:33,000
and then journalistic ethics developed.

170
0:07:33.26,000 --> 0:07:35,000
It wasn't perfect,

171
0:07:35.26,000 --> 0:07:38,000
but it got us through the last century.

172
0:07:38.26,000 --> 0:07:4,000
And so now,

173
0:07:40.26,000 --> 0:07:43,000
we're kind of back in 1915 on the Web.

174
0:07:44.26,000 --> 0:07:47,000
And we need the new gatekeepers

175
0:07:47.26,000 --> 0:07:49,000
to encode that kind of responsibility

176
0:07:49.26,000 --> 0:07:51,000
into the code that they're writing.

177
0:07:51.26,000 --> 0:07:54,000
I know that there are a lot of people here from Facebook and from Google --

178
0:07:54.26,000 --> 0:07:56,000
Larry and Sergey --

179
0:07:56.26,000 --> 0:07:58,000
people who have helped build the Web as it is,

180
0:07:58.26,000 --> 0:08:,000
and I'm grateful for that.

181
0:08:00.26,000 --> 0:08:03,000
But we really need you to make sure

182
0:08:03.26,000 --> 0:08:06,000
that these algorithms have encoded in them

183
0:08:06.26,000 --> 0:08:09,000
a sense of the public life, a sense of civic responsibility.

184
0:08:09.26,000 --> 0:08:12,000
We need you to make sure that they're transparent enough

185
0:08:12.26,000 --> 0:08:14,000
that we can see what the rules are

186
0:08:14.26,000 --> 0:08:17,000
that determine what gets through our filters.

187
0:08:17.26,000 --> 0:08:19,000
And we need you to give us some control

188
0:08:19.26,000 --> 0:08:21,000
so that we can decide

189
0:08:21.26,000 --> 0:08:24,000
what gets through and what doesn't.

190
0:08:24.26,000 --> 0:08:26,000
Because I think

191
0:08:26.26,000 --> 0:08:28,000
we really need the Internet to be that thing

192
0:08:28.26,000 --> 0:08:3,000
that we all dreamed of it being.

193
0:08:30.26,000 --> 0:08:33,000
We need it to connect us all together.

194
0:08:33.26,000 --> 0:08:36,000
We need it to introduce us to new ideas

195
0:08:36.26,000 --> 0:08:39,000
and new people and different perspectives.

196
0:08:40.26,000 --> 0:08:42,000
And it's not going to do that

197
0:08:42.26,000 --> 0:08:45,000
if it leaves us all isolated in a Web of one.

198
0:08:45.26,000 --> 0:08:47,000
Thank you.

199
0:08:47.26,000 --> 0:08:58,000
(Applause)

