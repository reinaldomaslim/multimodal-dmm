1
0:00:12.151,000 --> 0:00:17,000
Bryn Freedman: You're a guy whose company funds these AI programs and invests.

2
0:00:18.111,000 --> 0:00:23,000
So why should we trust you to not have a bias

3
0:00:23.18,000 --> 0:00:26,000
and tell us something really useful for the rest of us

4
0:00:26.38,000 --> 0:00:28,000
about the future of work?

5
0:00:29.046,000 --> 0:00:3,000
Roy Bahat: Yes, I am.

6
0:00:30.561,000 --> 0:00:32,000
And when you wake up in the morning and you read the newspaper

7
0:00:33.573,000 --> 0:00:36,000
and it says, "The robots are coming, they may take all our jobs,"

8
0:00:37.046,000 --> 0:00:39,000
as a start-up investor focused on the future of work,

9
0:00:39.601,000 --> 0:00:41,000
our fund was the first one to say

10
0:00:41.942,000 --> 0:00:43,000
artificial intelligence should be a focus for us.

11
0:00:44.276,000 --> 0:00:46,000
So I woke up one morning and read that and said,

12
0:00:46.553,000 --> 0:00:49,000
"Oh, my gosh, they're talking about me. That's me who's doing that."

13
0:00:50.839,000 --> 0:00:52,000
And then I thought: wait a minute.

14
0:00:52.887,000 --> 0:00:54,000
If things continue,

15
0:00:55.323,000 --> 0:01:,000
then maybe not only will the start-ups in which we invest struggle

16
0:01:00.601,000 --> 0:01:02,000
because there won't be people to have jobs

17
0:01:03.186,000 --> 0:01:05,000
to pay for the things that they make and buy them,

18
0:01:06.072,000 --> 0:01:08,000
but our economy and society might struggle, too.

19
0:01:09.013,000 --> 0:01:12,000
And look, I should be the guy who sits here and tells you,

20
0:01:12.258,000 --> 0:01:15,000
"Everything is going to be fine. It's all going to work out great.

21
0:01:15.413,000 --> 0:01:17,000
Hey, when they introduced the ATM machine,

22
0:01:17.469,000 --> 0:01:19,000
years later, there's more tellers in banks."

23
0:01:19.629,000 --> 0:01:2,000
It's true.

24
0:01:20.811,000 --> 0:01:23,000
And yet, when I looked at it, I thought, "This is going to accelerate.

25
0:01:24.151,000 --> 0:01:27,000
And if it does accelerate, there's a chance the center doesn't hold."

26
0:01:27.435,000 --> 0:01:29,000
But I figured somebody must know the answer to this;

27
0:01:29.942,000 --> 0:01:3,000
there are so many ideas out there.

28
0:01:31.641,000 --> 0:01:34,000
And I read all the books, and I went to the conferences,

29
0:01:34.719,000 --> 0:01:39,000
and at one point, we counted more than 100 efforts to study the future of work.

30
0:01:40.387,000 --> 0:01:42,000
And it was a frustrating experience,

31
0:01:43.268,000 --> 0:01:47,000
because I'd hear the same back-and-forth over and over again:

32
0:01:47.307,000 --> 0:01:48,000
"The robots are coming!"

33
0:01:49.109,000 --> 0:01:5,000
And then somebody else would say,

34
0:01:50.736,000 --> 0:01:53,000
"Oh, don't worry about that, they've always said that and it turns out OK."

35
0:01:54.324,000 --> 0:01:55,000
Then somebody else would say,

36
0:01:55.737,000 --> 0:01:57,000
"Well, it's really about the meaning of your job, anyway."

37
0:01:58.493,000 --> 0:02:,000
And then everybody would shrug and go off and have a drink.

38
0:02:01.3,000 --> 0:02:04,000
And it felt like there was this Kabuki theater of this discussion,

39
0:02:04.426,000 --> 0:02:05,000
where nobody was talking to each other.

40
0:02:06.325,000 --> 0:02:09,000
And many of the people that I knew and worked with in the technology world

41
0:02:09.835,000 --> 0:02:1,000
were not speaking to policy makers;

42
0:02:11.534,000 --> 0:02:13,000
the policy makers were not speaking to them.

43
0:02:13.627,000 --> 0:02:17,000
And so we partnered with a nonpartisan think tank NGO called New America

44
0:02:17.833,000 --> 0:02:18,000
to study this issue.

45
0:02:19.19,000 --> 0:02:21,000
And we brought together a group of people,

46
0:02:21.562,000 --> 0:02:24,000
including an AI czar at a technology company

47
0:02:25.016,000 --> 0:02:26,000
and a video game designer

48
0:02:26.912,000 --> 0:02:27,000
and a heartland conservative

49
0:02:28.419,000 --> 0:02:29,000
and a Wall Street investor

50
0:02:29.687,000 --> 0:02:3,000
and a socialist magazine editor --

51
0:02:31.671,000 --> 0:02:33,000
literally, all in the same room; it was occasionally awkward --

52
0:02:34.663,000 --> 0:02:36,000
to try to figure out what is it that will happen here.

53
0:02:37.268,000 --> 0:02:39,000
The question we asked was simple.

54
0:02:40.704,000 --> 0:02:43,000
It was: What is the effect of technology on work going to be?

55
0:02:44.141,000 --> 0:02:45,000
And we looked out 10 to 20 years,

56
0:02:45.792,000 --> 0:02:48,000
because we wanted to look out far enough that there could be real change,

57
0:02:49.3,000 --> 0:02:52,000
but soon enough that we weren't talking about teleportation or anything like that.

58
0:02:53.293,000 --> 0:02:54,000
And we recognized --

59
0:02:54.695,000 --> 0:02:57,000
and I think every year we're reminded of this in the world --

60
0:02:57.768,000 --> 0:02:59,000
that predicting what's going to happen is hard.

61
0:02:59.999,000 --> 0:03:01,000
So instead of predicting, there are other things you can do.

62
0:03:02.879,000 --> 0:03:04,000
You can try to imagine alternate possible futures,

63
0:03:05.699,000 --> 0:03:06,000
which is what we did.

64
0:03:06.875,000 --> 0:03:07,000
We did a scenario-planning exercise,

65
0:03:08.657,000 --> 0:03:11,000
and we imagined cases where no job is safe.

66
0:03:11.747,000 --> 0:03:14,000
We imagined cases where every job is safe.

67
0:03:14.874,000 --> 0:03:18,000
And we imagined every distinct possibility we could.

68
0:03:18.937,000 --> 0:03:21,000
And the result, which really surprised us,

69
0:03:22.244,000 --> 0:03:25,000
was when you think through those futures and you think what should we do,

70
0:03:25.966,000 --> 0:03:28,000
the answers about what we should do actually turn out to be the same,

71
0:03:29.905,000 --> 0:03:3,000
no matter what happens.

72
0:03:31.425,000 --> 0:03:34,000
And the irony of looking out 10 to 20 years into the future is,

73
0:03:35.191,000 --> 0:03:37,000
you realize that the things we want to act on

74
0:03:37.624,000 --> 0:03:38,000
are actually already happening right now.

75
0:03:39.609,000 --> 0:03:41,000
The automation is right now, the future is right now.

76
0:03:42.419,000 --> 0:03:44,000
BF: So what does that mean, and what does that tell us?

77
0:03:45.038,000 --> 0:03:47,000
If the future is now, what is it that we should be doing,

78
0:03:47.76,000 --> 0:03:48,000
and what should we be thinking about?

79
0:03:49.654,000 --> 0:03:51,000
RB: We have to understand the problem first.

80
0:03:51.758,000 --> 0:03:55,000
And so the data are that as the economy becomes more productive

81
0:03:55.821,000 --> 0:03:57,000
and individual workers become more productive,

82
0:03:57.99,000 --> 0:03:58,000
their wages haven't risen.

83
0:03:59.276,000 --> 0:04:02,000
If you look at the proportion of prime working-age men,

84
0:04:02.49,000 --> 0:04:03,000
in the United States at least,

85
0:04:04.006,000 --> 0:04:07,000
who work now versus in 1960,

86
0:04:07.779,000 --> 0:04:09,000
we have three times as many men not working.

87
0:04:10.271,000 --> 0:04:11,000
And then you hear the stories.

88
0:04:11.739,000 --> 0:04:13,000
I sat down with a group of Walmart workers and said,

89
0:04:14.202,000 --> 0:04:17,000
"What do you think about this cashier, this futuristic self-checkout thing?"

90
0:04:17.869,000 --> 0:04:2,000
They said, "That's nice, but have you heard about the cash recycler?

91
0:04:21.098,000 --> 0:04:23,000
That's a machine that's being installed right now,

92
0:04:23.495,000 --> 0:04:25,000
and is eliminating two jobs at every Walmart right now."

93
0:04:26.156,000 --> 0:04:29,000
And so we just thought, "Geez. We don't understand the problem."

94
0:04:29.223,000 --> 0:04:32,000
And so we looked at the voices that were the ones that were excluded,

95
0:04:32.525,000 --> 0:04:34,000
which is all of the people affected by this change.

96
0:04:35.149,000 --> 0:04:36,000
And we decided to listen to them,

97
0:04:36.76,000 --> 0:04:37,000
sort of "automation and its discontents."

98
0:04:38.768,000 --> 0:04:4,000
And I've spent the last couple of years doing that.

99
0:04:41.195,000 --> 0:04:43,000
I've been to Flint, Michigan, and Youngstown, Ohio,

100
0:04:43.615,000 --> 0:04:45,000
talking about entrepreneurs, trying to make it work

101
0:04:46.07,000 --> 0:04:48,000
in a very different environment from New York or San Francisco

102
0:04:49.042,000 --> 0:04:5,000
or London or Tokyo.

103
0:04:50.551,000 --> 0:04:51,000
I've been to prisons twice

104
0:04:52.172,000 --> 0:04:54,000
to talk to inmates about their jobs after they leave.

105
0:04:55.077,000 --> 0:04:58,000
I've sat down with truck drivers to ask them about the self-driving truck,

106
0:04:58.848,000 --> 0:05:,000
with people who, in addition to their full-time job,

107
0:05:01.326,000 --> 0:05:02,000
care for an aging relative.

108
0:05:03.134,000 --> 0:05:04,000
And when you talk to people,

109
0:05:04.722,000 --> 0:05:06,000
there were two themes that came out loud and clear.

110
0:05:08.285,000 --> 0:05:12,000
The first one was that people are less looking for more money

111
0:05:13.153,000 --> 0:05:16,000
or get out of the fear of the robot taking their job,

112
0:05:16.455,000 --> 0:05:17,000
and they just want something stable.

113
0:05:18.375,000 --> 0:05:19,000
They want something predictable.

114
0:05:19.939,000 --> 0:05:22,000
So if you survey people and ask them what they want out of work,

115
0:05:23.638,000 --> 0:05:26,000
for everybody who makes less than 150,000 dollars a year,

116
0:05:27.142,000 --> 0:05:3,000
they'll take a more stable and secure income, on average,

117
0:05:30.522,000 --> 0:05:31,000
over earning more money.

118
0:05:32.411,000 --> 0:05:34,000
And if you think about the fact that

119
0:05:34.649,000 --> 0:05:37,000
not only for all of the people across the earth who don't earn a living,

120
0:05:38.061,000 --> 0:05:39,000
but for those who do,

121
0:05:39.276,000 --> 0:05:41,000
the vast majority earn a different amount from month to month

122
0:05:42.26,000 --> 0:05:43,000
and have an instability,

123
0:05:43.498,000 --> 0:05:44,000
all of a sudden you realize,

124
0:05:44.919,000 --> 0:05:46,000
"Wait a minute. We have a real problem on our hands."

125
0:05:47.441,000 --> 0:05:5,000
And the second thing they say, which took us a longer time to understand,

126
0:05:51.188,000 --> 0:05:53,000
is they say they want dignity.

127
0:05:53.894,000 --> 0:05:58,000
And that concept of self-worth through work

128
0:05:59.03,000 --> 0:06:01,000
emerged again and again and again in our conversations.

129
0:06:01.665,000 --> 0:06:03,000
BF: So, I certainly appreciate this answer.

130
0:06:04.673,000 --> 0:06:05,000
But you can't eat dignity,

131
0:06:06.168,000 --> 0:06:08,000
you can't clothe your children with self-esteem.

132
0:06:09.077,000 --> 0:06:12,000
So, what is that, how do you reconcile --

133
0:06:12.589,000 --> 0:06:13,000
what does dignity mean,

134
0:06:14.454,000 --> 0:06:17,000
and what is the relationship between dignity and stability?

135
0:06:18.141,000 --> 0:06:2,000
RB: You can't eat dignity. You need stability first.

136
0:06:20.649,000 --> 0:06:21,000
And the good news is,

137
0:06:21.911,000 --> 0:06:23,000
many of the conversations that are happening right now

138
0:06:24.69,000 --> 0:06:25,000
are about how we solve that.

139
0:06:26.276,000 --> 0:06:29,000
You know, I'm a proponent of studying guaranteed income,

140
0:06:30.165,000 --> 0:06:31,000
as one example,

141
0:06:31.466,000 --> 0:06:33,000
conversations about how health care gets provided

142
0:06:33.778,000 --> 0:06:34,000
and other benefits.

143
0:06:35.041,000 --> 0:06:36,000
Those conversations are happening,

144
0:06:36.842,000 --> 0:06:38,000
and we're at a time where we must figure that out.

145
0:06:39.262,000 --> 0:06:4,000
It is the crisis of our era.

146
0:06:40.927,000 --> 0:06:42,000
And my point of view after talking to people

147
0:06:43.863,000 --> 0:06:45,000
is that we may do that,

148
0:06:45.927,000 --> 0:06:46,000
and it still might not be enough.

149
0:06:47.531,000 --> 0:06:49,000
Because what we need to do from the beginning is understand

150
0:06:50.362,000 --> 0:06:52,000
what is it about work that gives people dignity,

151
0:06:52.626,000 --> 0:06:55,000
so they can live the lives that they want to live.

152
0:06:55.99,000 --> 0:06:59,000
And so that concept of dignity is ...

153
0:07:00.029,000 --> 0:07:01,000
it's difficult to get your hands around,

154
0:07:01.985,000 --> 0:07:04,000
because when many people hear it -- especially, to be honest, rich people --

155
0:07:05.608,000 --> 0:07:06,000
they hear "meaning."

156
0:07:06.788,000 --> 0:07:07,000
They hear "My work is important to me."

157
0:07:08.718,000 --> 0:07:11,000
And again, if you survey people and you ask them,

158
0:07:12.379,000 --> 0:07:15,000
"How important is it to you that your work be important to you?"

159
0:07:15.903,000 --> 0:07:18,000
only people who make 150,000 dollars a year or more

160
0:07:19.228,000 --> 0:07:23,000
say that it is important to them that their work be important.

161
0:07:24.05,000 --> 0:07:25,000
BF: Meaning, meaningful?

162
0:07:25.744,000 --> 0:07:27,000
RB: Just defined as, "Is your work important to you?"

163
0:07:29.95,000 --> 0:07:3,000
Whatever somebody took that to mean.

164
0:07:31.69,000 --> 0:07:32,000
And yet, of course dignity is essential.

165
0:07:33.628,000 --> 0:07:34,000
We talked to truck drivers who said,

166
0:07:35.372,000 --> 0:07:38,000
"I saw my cousin drive, and I got on the open road and it was amazing.

167
0:07:39.36,000 --> 0:07:42,000
And I started making more money than people who went to college."

168
0:07:42.454,000 --> 0:07:45,000
Then they'd get to the end of their thought and say something like,

169
0:07:45.652,000 --> 0:07:47,000
"People need their fruits and vegetables in the morning,

170
0:07:48.321,000 --> 0:07:49,000
and I'm the guy who gets it to them."

171
0:07:50.144,000 --> 0:07:53,000
We talked to somebody who, in addition to his job, was caring for his aunt.

172
0:07:53.708,000 --> 0:07:54,000
He was making plenty of money.

173
0:07:55.231,000 --> 0:07:56,000
At one point we just asked,

174
0:07:56.572,000 --> 0:08:,000
"What is it about caring for your aunt? Can't you just pay somebody to do it?"

175
0:08:00.858,000 --> 0:08:02,000
He said, "My aunt doesn't want somebody we pay for.

176
0:08:03.375,000 --> 0:08:04,000
My aunt wants me."

177
0:08:04.601,000 --> 0:08:07,000
So there was this concept there of being needed.

178
0:08:08.292,000 --> 0:08:1,000
If you study the word "dignity," it's fascinating.

179
0:08:10.773,000 --> 0:08:13,000
It's one of the oldest words in the English language, from antiquity.

180
0:08:14.072,000 --> 0:08:15,000
And it has two meanings:

181
0:08:15.248,000 --> 0:08:16,000
one is self-worth,

182
0:08:16.426,000 --> 0:08:2,000
and the other is that something is suitable, it's fitting,

183
0:08:20.677,000 --> 0:08:22,000
meaning that you're part of something greater than yourself,

184
0:08:23.559,000 --> 0:08:24,000
and it connects to some broader whole.

185
0:08:25.406,000 --> 0:08:26,000
In other words, that you're needed.

186
0:08:27.147,000 --> 0:08:28,000
BF: So how do you answer this question,

187
0:08:29.045,000 --> 0:08:31,000
this concept that we don't pay teachers,

188
0:08:31.119,000 --> 0:08:33,000
and we don't pay eldercare workers,

189
0:08:33.411,000 --> 0:08:36,000
and we don't pay people who really care for people

190
0:08:36.561,000 --> 0:08:38,000
and are needed, enough?

191
0:08:38.871,000 --> 0:08:41,000
RB: Well, the good news is, people are finally asking the question.

192
0:08:42.077,000 --> 0:08:44,000
So as AI investors, we often get phone calls

193
0:08:44.404,000 --> 0:08:46,000
from foundations or CEOs and boardrooms saying,

194
0:08:47.158,000 --> 0:08:48,000
"What do we do about this?"

195
0:08:48.487,000 --> 0:08:49,000
And they used to be asking,

196
0:08:49.82,000 --> 0:08:51,000
"What do we do about introducing automation?"

197
0:08:51.956,000 --> 0:08:53,000
And now they're asking, "What do we do about self-worth?"

198
0:08:54.678,000 --> 0:08:56,000
And they know that the employees who work for them

199
0:08:57.125,000 --> 0:08:59,000
who have a spouse who cares for somebody,

200
0:08:59.395,000 --> 0:09:02,000
that dignity is essential to their ability to just do their job.

201
0:09:02.942,000 --> 0:09:03,000
I think there's two kinds of answers:

202
0:09:04.742,000 --> 0:09:06,000
there's the money side of just making your life work.

203
0:09:07.292,000 --> 0:09:09,000
That's stability. You need to eat.

204
0:09:09.641,000 --> 0:09:11,000
And then you think about our culture more broadly,

205
0:09:12.002,000 --> 0:09:14,000
and you ask: Who do we make into heroes?

206
0:09:14.704,000 --> 0:09:18,000
And, you know, what I want is to see the magazine cover

207
0:09:19.188,000 --> 0:09:21,000
that is the person who is the heroic caregiver.

208
0:09:22.292,000 --> 0:09:24,000
Or the Netflix series that dramatizes the person

209
0:09:25.046,000 --> 0:09:28,000
who makes all of our other lives work so we can do the things we do.

210
0:09:28.307,000 --> 0:09:29,000
Let's make heroes out of those people.

211
0:09:30.154,000 --> 0:09:32,000
That's the Netflix show that I would binge.

212
0:09:32.211,000 --> 0:09:34,000
And we've had chroniclers of this before --

213
0:09:34.514,000 --> 0:09:35,000
Studs Terkel,

214
0:09:35.752,000 --> 0:09:38,000
the oral history of the working experience in the United States.

215
0:09:39.482,000 --> 0:09:42,000
And what we need is the experience of needing one another

216
0:09:42.658,000 --> 0:09:43,000
and being connected to each other.

217
0:09:44.307,000 --> 0:09:47,000
Maybe that's the answer for how we all fit as a society.

218
0:09:47.371,000 --> 0:09:48,000
And the thought exercise, to me, is:

219
0:09:49.117,000 --> 0:09:51,000
if you were to go back 100 years and have people --

220
0:09:51.719,000 --> 0:09:54,000
my grandparents, great-grandparents, a tailor, worked in a mine --

221
0:09:55.514,000 --> 0:09:58,000
they look at what all of us do for a living and say, "That's not work."

222
0:09:59.363,000 --> 0:10:02,000
We sit there and type and talk, and there's no danger of getting hurt.

223
0:10:03.526,000 --> 0:10:06,000
And my guess is that if you were to imagine 100 years from now,

224
0:10:07.041,000 --> 0:10:09,000
we'll still be doing things for each other.

225
0:10:09.077,000 --> 0:10:1,000
We'll still need one another.

226
0:10:10.507,000 --> 0:10:12,000
And we just will think of it as work.

227
0:10:12.538,000 --> 0:10:13,000
The entire thing I'm trying to say

228
0:10:14.236,000 --> 0:10:16,000
is that dignity should not just be about having a job.

229
0:10:17.133,000 --> 0:10:19,000
Because if you say you need a job to have dignity,

230
0:10:20.014,000 --> 0:10:21,000
which many people say,

231
0:10:21.482,000 --> 0:10:23,000
the second you say that, you say to all the parents

232
0:10:24.323,000 --> 0:10:26,000
and all the teachers and all the caregivers

233
0:10:26.792,000 --> 0:10:27,000
that all of a sudden,

234
0:10:27.99,000 --> 0:10:29,000
because they're not being paid for what they're doing,

235
0:10:30.561,000 --> 0:10:32,000
it somehow lacks this essential human quality.

236
0:10:32.878,000 --> 0:10:34,000
To me, that's the great puzzle of our time:

237
0:10:35.006,000 --> 0:10:38,000
Can we figure out how to provide that stability throughout life,

238
0:10:38.141,000 --> 0:10:4,000
and then can we figure out how to create an inclusive,

239
0:10:40.698,000 --> 0:10:44,000
not just racially, gender, but multigenerationally inclusive --

240
0:10:44.989,000 --> 0:10:48,000
I mean, every different human experience included --

241
0:10:49.892,000 --> 0:10:52,000
in this way of understanding how we can be needed by one another.

242
0:10:53.119,000 --> 0:10:54,000
BF: Thank you. RB: Thank you.

243
0:10:54.509,000 --> 0:10:56,000
BF: Thank you very much for your participation.

244
0:10:56.73,000 --> 0:10:57,000
(Applause)

