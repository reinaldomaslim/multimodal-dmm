1
0:00:,000 --> 0:00:07,000
Traductor: Denise RQ Revisor: Marta Palacio

2
0:00:12.532,000 --> 0:00:13,000
Este es Lee Sedol.

3
0:00:14.244,000 --> 0:00:16,000
Lee Sedol es uno de los mejores jugadores de Go del mundo.

4
0:00:17.174,000 --> 0:00:2,000
Y está teniendo lo que mis amigos de Silicon Valley llaman

5
0:00:20.629,000 --> 0:00:21,000
un momento "¡Bendito Dios!".

6
0:00:22.549,000 --> 0:00:23,000
(Risas)

7
0:00:23.65,000 --> 0:00:25,000
Un momento en el que nos damos cuenta

8
0:00:25.823,000 --> 0:00:28,000
de que la IA está avanzando mucho más rápido de lo que esperábamos.

9
0:00:29.618,000 --> 0:00:31,000
Los humanos han perdido en el tablero de Go.

10
0:00:31.848,000 --> 0:00:32,000
¿Y en el mundo real?

11
0:00:33.114,000 --> 0:00:35,000
Bueno, el mundo real es mucho más grande y complicado

12
0:00:36.035,000 --> 0:00:37,000
que el tablero de Go.

13
0:00:37.335,000 --> 0:00:38,000
Es mucho menos visible.

14
0:00:38.755,000 --> 0:00:4,000
Pero sigue siendo un problema de decisión.

15
0:00:42.585,000 --> 0:00:46,000
Y si pensamos en algunas de las tecnologías que están por venir

16
0:00:47.368,000 --> 0:00:51,000
Noriko [Arai] mencionó que las máquinas aún no saben leer,

17
0:00:51.703,000 --> 0:00:52,000
al menos no comprendiendo,

18
0:00:53.503,000 --> 0:00:55,000
pero lo harán, y cuando eso suceda,

19
0:00:56.917,000 --> 0:01:01,000
poco después las máquinas habrán leído todo lo que la raza humana ha escrito.

20
0:01:03.67,000 --> 0:01:04,000
Eso permitirá a las máquinas,

21
0:01:05.498,000 --> 0:01:08,000
junto a su habilidad mirar más allá de lo que pueden los humanos,

22
0:01:08.608,000 --> 0:01:09,000
como ya hemos visto en el Go,

23
0:01:09.988,000 --> 0:01:11,000
si también tienen acceso a más información,

24
0:01:12.372,000 --> 0:01:15,000
serán capaces de tomar mejores decisiones en el mundo real

25
0:01:15.612,000 --> 0:01:16,000
que nosotros.

26
0:01:18.612,000 --> 0:01:19,000
¿Es eso bueno?

27
0:01:21.718,000 --> 0:01:23,000
Bueno, espero que sí.

28
0:01:26.514,000 --> 0:01:29,000
Toda nuestra civilización, todo lo que valoramos,

29
0:01:29.793,000 --> 0:01:31,000
se basa en nuestra inteligencia.

30
0:01:31.885,000 --> 0:01:34,000
Y si tuviéramos acceso a mucha más inteligencia,

31
0:01:35.603,000 --> 0:01:38,000
entonces no existirían límites para lo que la raza humana pueda hacer.

32
0:01:40.485,000 --> 0:01:42,000
Y creo que este podría ser, como han dicho algunos,

33
0:01:43.224,000 --> 0:01:45,000
el mayor acontecimiento de la historia de la humanidad.

34
0:01:48.485,000 --> 0:01:5,000
Entonces, ¿por qué la gente afirma cosas como esta?

35
0:01:51.338,000 --> 0:01:54,000
Que la inteligencia artificial podría significar el fin de la raza humana.

36
0:01:55.258,000 --> 0:01:56,000
¿Es esto algo nuevo?

37
0:01:56.941,000 --> 0:02:,000
¿Se trata solo de Elon Musk y Bill Gates y Stephen Hawking?

38
0:02:01.773,000 --> 0:02:04,000
En realidad, no. Esta idea no es nueva.

39
0:02:05.059,000 --> 0:02:06,000
He aquí una cita:

40
0:02:07.045,000 --> 0:02:11,000
"Incluso si pudiéramos mantener las máquinas en una posición servil,

41
0:02:11.419,000 --> 0:02:13,000
por ejemplo, desconectándolas en momentos estratégicos"

42
0:02:14.427,000 --> 0:02:17,000
--volveré a esa idea de "quitar la corriente" más adelante--

43
0:02:17.688,000 --> 0:02:19,000
"deberíamos, como especie, sentirnos humillados".

44
0:02:21.996,000 --> 0:02:24,000
¿Quién dijo esto? Este es Alan Turing, en 1951.

45
0:02:26.12,000 --> 0:02:28,000
Alan Turing, como Uds. saben, es el padre de la informática

46
0:02:28.907,000 --> 0:02:31,000
y en muchos sentidos también el padre de la IA.

47
0:02:33.059,000 --> 0:02:34,000
Así que si pensamos en este problema,

48
0:02:34.965,000 --> 0:02:37,000
el problema de crear algo más inteligente que tu propia especie,

49
0:02:38.776,000 --> 0:02:4,000
podríamos llamar a esto "el problema del gorila".

50
0:02:42.165,000 --> 0:02:45,000
Porque los antepasados de los gorilas hicieron esto hace unos millones de años,

51
0:02:45.939,000 --> 0:02:47,000
y ahora podríamos preguntar a los gorilas:

52
0:02:48.572,000 --> 0:02:49,000
¿Fue una buena idea?

53
0:02:49.756,000 --> 0:02:52,000
Aquí están, reunidos para discutir si fue una buena idea,

54
0:02:53.746,000 --> 0:02:55,000
y pasado un tiempo concluyen que no.

55
0:02:56.736,000 --> 0:02:57,000
Fue una idea terrible.

56
0:02:58.095,000 --> 0:02:59,000
Nuestra especie está en apuros.

57
0:03:00.358,000 --> 0:03:04,000
De hecho, pueden ver la tristeza existencial en sus ojos.

58
0:03:04.645,000 --> 0:03:05,000
(Risas)

59
0:03:06.309,000 --> 0:03:09,000
Así que esta sensación mareante de que crear algo más inteligente

60
0:03:10.103,000 --> 0:03:13,000
que tu propia especie tal vez no sea buena idea...

61
0:03:14.308,000 --> 0:03:15,000
¿Qué podemos hacer al respecto?

62
0:03:15.823,000 --> 0:03:19,000
Bueno, nada en realidad, excepto dejar de hacer IA.

63
0:03:20.614,000 --> 0:03:22,000
Y por todos los beneficios que he mencionado

64
0:03:23.164,000 --> 0:03:24,000
y porque soy un investigador de IA,

65
0:03:24.888,000 --> 0:03:25,000
no voy a tomar eso.

66
0:03:27.103,000 --> 0:03:29,000
Sin duda quiero seguir creando IA.

67
0:03:30.435,000 --> 0:03:32,000
Así que necesitamos precisar el problema un poco más.

68
0:03:33.137,000 --> 0:03:34,000
¿Cuál es el problema?

69
0:03:34.532,000 --> 0:03:37,000
¿Por qué tener mejor IA puede ser una catástrofe?

70
0:03:39.218,000 --> 0:03:4,000
Aquí hay otra cita:

71
0:03:41.755,000 --> 0:03:43,000
"Más nos vale estar seguros de que el propósito

72
0:03:43.955,000 --> 0:03:45,000
que introducimos en la máquina es el que de verdad deseamos".

73
0:03:47.954,000 --> 0:03:5,000
Esto fue dicho por Norbert Wiener en 1960,

74
0:03:51.132,000 --> 0:03:55,000
poco después de ver a uno de los primeros sistemas de aprendizaje

75
0:03:55.31,000 --> 0:03:57,000
aprender a jugar a las damas mejor que su creador.

76
0:04:00.422,000 --> 0:04:03,000
Pero esto podría haberlo dicho de igual modo el Rey Midas.

77
0:04:03.985,000 --> 0:04:06,000
El Rey Midas dijo, "Deseo que todo lo que toque se convierta en oro".

78
0:04:07.966,000 --> 0:04:09,000
Y obtuvo justo lo que pidió.

79
0:04:10.487,000 --> 0:04:13,000
Fue el propósito que introdujo en la máquina, por así decirlo.

80
0:04:14.807,000 --> 0:04:17,000
Y luego su comida, su bebida y sus familiares se convirtieron en oro

81
0:04:18.175,000 --> 0:04:2,000
y murió miserable y muerto de hambre.

82
0:04:22.264,000 --> 0:04:24,000
Así que llamaremos a esto "el problema del rey Midas",

83
0:04:24.875,000 --> 0:04:27,000
el de indicar un objetivo que no está realmente

84
0:04:28.033,000 --> 0:04:3,000
alineado con lo que de verdad queremos.

85
0:04:30.395,000 --> 0:04:33,000
En términos modernos, lo llamamos "el problema de alineación de valor".

86
0:04:36.867,000 --> 0:04:39,000
Introducir un objetivo equivocado no es la única parte del problema.

87
0:04:40.376,000 --> 0:04:41,000
Hay otra parte.

88
0:04:41.98,000 --> 0:04:42,000
Al introducir un objetivo en una máquina

89
0:04:43.947,000 --> 0:04:45,000
incluso algo tan simple como "Trae el café",

90
0:04:47.728,000 --> 0:04:48,000
la máquina se dice a sí misma,

91
0:04:50.553,000 --> 0:04:52,000
"¿Cómo podría fallar yendo a buscar el café?

92
0:04:52.953,000 --> 0:04:53,000
Alguien podría desconectarme.

93
0:04:54.773,000 --> 0:04:56,000
Vale, debo tomar medidas para evitarlo.

94
0:04:57.839,000 --> 0:04:59,000
Desactivaré mi interruptor de 'apagado'.

95
0:04:59.97,000 --> 0:05:02,000
Haré cualquier cosa para protegerme de interferencias

96
0:05:03.082,000 --> 0:05:05,000
con este objetivo que me han dado.

97
0:05:05.988,000 --> 0:05:07,000
Así que esta persecución obsesiva

98
0:05:08.884,000 --> 0:05:11,000
de un modo muy defensivo para lograr un objetivo

99
0:05:11.984,000 --> 0:05:13,000
que no está alineado con los verdaderos objetivos

100
0:05:14.314,000 --> 0:05:15,000
de la raza humana...

101
0:05:15.467,000 --> 0:05:17,000
ese es el problema al que nos enfrentamos.

102
0:05:18.526,000 --> 0:05:22,000
Y de hecho esa es la lección más valiosa de esta charla.

103
0:05:23.437,000 --> 0:05:25,000
Si quieren recordar una cosa

104
0:05:25.697,000 --> 0:05:27,000
es que no se puede ir a buscar el café si se está muerto.

105
0:05:28.396,000 --> 0:05:29,000
(Risas)

106
0:05:29.481,000 --> 0:05:32,000
Es muy simple. Solo recuerden eso. Repítanlo tres veces al día.

107
0:05:33.334,000 --> 0:05:34,000
(Risas)

108
0:05:35.179,000 --> 0:05:37,000
Y de hecho, este es el mismo argumento

109
0:05:37.957,000 --> 0:05:39,000
de "2001: [Una odisea del espacio]".

110
0:05:41.046,000 --> 0:05:43,000
HAL tiene un objetivo, una misión,

111
0:05:43.16,000 --> 0:05:46,000
que no está alineada con los objetivos de los humanos,

112
0:05:46.916,000 --> 0:05:47,000
y eso conduce a este conflicto.

113
0:05:49.314,000 --> 0:05:51,000
Por suerte HAL no es superinteligente.

114
0:05:52.307,000 --> 0:05:54,000
Es bastante inteligente, pero llegado el momento,

115
0:05:54.848,000 --> 0:05:56,000
Dave lo supera y logra apagarlo.

116
0:06:01.648,000 --> 0:06:02,000
Pero tal vez no tengamos tanta suerte.

117
0:06:08.013,000 --> 0:06:09,000
Entonces, ¿qué vamos a hacer?

118
0:06:12.191,000 --> 0:06:14,000
Estoy tratando de redefinir la IA

119
0:06:14.816,000 --> 0:06:16,000
para alejarnos de esta noción clásica

120
0:06:16.901,000 --> 0:06:2,000
de máquinas que persiguen objetivos de manera inteligente.

121
0:06:22.532,000 --> 0:06:23,000
Hay tres principios implicados.

122
0:06:24.354,000 --> 0:06:27,000
El primero es un principio de altruismo, por así decirlo,

123
0:06:27.667,000 --> 0:06:3,000
el único objetivo del robot

124
0:06:30.953,000 --> 0:06:34,000
es maximizar la realización de los objetivos humanos,

125
0:06:35.223,000 --> 0:06:36,000
de los valores humanos.

126
0:06:36.637,000 --> 0:06:39,000
Y por valores aquí no me refiero a valores sentimentales o de bondad.

127
0:06:39.991,000 --> 0:06:42,000
Solo quiero decir aquello más similar a la vida

128
0:06:43.802,000 --> 0:06:44,000
que un humano preferiría.

129
0:06:47.184,000 --> 0:06:49,000
Y esto viola la ley de Asimov

130
0:06:49.307,000 --> 0:06:51,000
de que el robot debe proteger su propia existencia.

131
0:06:51.786,000 --> 0:06:54,000
No tiene ningún interés en preservar su existencia en absoluto.

132
0:06:57.24,000 --> 0:07:,000
La segunda ley es una ley de humildad, digamos.

133
0:07:01.794,000 --> 0:07:04,000
Y resulta muy importante para que los robots sean seguros.

134
0:07:05.561,000 --> 0:07:08,000
Dice que el robot no sabe

135
0:07:08.727,000 --> 0:07:1,000
cuáles son esos valores humanos,

136
0:07:10.779,000 --> 0:07:13,000
así que debe maximizarlos, pero no sabe lo que son.

137
0:07:15.074,000 --> 0:07:17,000
Lo cual evita el problema de la búsqueda obsesiva

138
0:07:17.724,000 --> 0:07:18,000
de un objetivo.

139
0:07:18.96,000 --> 0:07:2,000
Esta incertidumbre resulta crucial.

140
0:07:21.546,000 --> 0:07:22,000
Claro que para sernos útiles,

141
0:07:23.255,000 --> 0:07:25,000
deben tener alguna idea de lo que queremos.

142
0:07:27.043,000 --> 0:07:32,000
Obtiene esa información sobre todo observando elecciones humanas,

143
0:07:32.494,000 --> 0:07:34,000
para que nuestras propias decisiones revelen información

144
0:07:35.319,000 --> 0:07:38,000
sobre lo que nosotros preferimos para nuestras vidas.

145
0:07:40.452,000 --> 0:07:41,000
Estos son los tres principios.

146
0:07:42.159,000 --> 0:07:44,000
Veamos cómo se aplica a esta cuestión

147
0:07:44.501,000 --> 0:07:46,000
de "apagar la máquina", como sugirió Turing.

148
0:07:48.893,000 --> 0:07:49,000
He aquí un robot PR2.

149
0:07:50.657,000 --> 0:07:52,000
Es uno que tenemos en nuestro laboratorio,

150
0:07:52.882,000 --> 0:07:55,000
y tiene un gran botón rojo de 'apagado' en la parte posterior.

151
0:07:56.361,000 --> 0:07:58,000
La pregunta es: ¿Va a dejar que lo apaguen?

152
0:07:58.76,000 --> 0:07:59,000
Si lo hacemos a la manera clásica,

153
0:08:00.489,000 --> 0:08:03,000
le damos el objetivo de traer el café. "Debo traer el café.

154
0:08:03.995,000 --> 0:08:05,000
No puedo traer el café si estoy muerto".

155
0:08:06.599,000 --> 0:08:09,000
Obviamente el PR2 ha escuchado mi charla,

156
0:08:09.964,000 --> 0:08:1,000
y por tanto, decide

157
0:08:11.866,000 --> 0:08:13,000
"Debo inhabilitar mi botón de 'apagado'".

158
0:08:14.796,000 --> 0:08:17,000
"Y probablemente electrocutar al resto de personas en el Starbucks

159
0:08:17.98,000 --> 0:08:18,000
que podrían interferir".

160
0:08:19.254,000 --> 0:08:2,000
(Risas)

161
0:08:21.184,000 --> 0:08:23,000
Así que esto parece ser inevitable, ¿verdad?

162
0:08:23.361,000 --> 0:08:25,000
Este tipo de error parece ser inevitable,

163
0:08:25.783,000 --> 0:08:28,000
y sucede por tener un objetivo concreto, definido.

164
0:08:30.632,000 --> 0:08:33,000
Entonces, ¿qué pasa si la máquina no tiene claro el objetivo?

165
0:08:33.8,000 --> 0:08:35,000
Bueno, razona de una manera diferente.

166
0:08:35.951,000 --> 0:08:39,000
Dice, "El humano podría desconectarme, pero solo si hago algo malo.

167
0:08:41.577,000 --> 0:08:45,000
No tengo claro lo que es malo pero sé que no quiero hacerlo".

168
0:08:45.606,000 --> 0:08:47,000
Ahí están el primer y el segundo principio.

169
0:08:49.179,000 --> 0:08:52,000
"Así que debería dejar que el humano me desconecte".

170
0:08:53.541,000 --> 0:08:56,000
De hecho se puede calcular el incentivo que tiene el robot

171
0:08:57.161,000 --> 0:08:59,000
para permitir que el humano lo apague.

172
0:09:00.038,000 --> 0:09:02,000
Y está directamente ligado al grado de incertidumbre

173
0:09:02.802,000 --> 0:09:04,000
sobre el objetivo subyacente.

174
0:09:05.246,000 --> 0:09:08,000
Y entonces cuando la máquina está apagada,

175
0:09:08.842,000 --> 0:09:09,000
el tercer principio entra en juego.

176
0:09:10.599,000 --> 0:09:13,000
Aprende algo sobre los objetivos que debe perseguir,

177
0:09:13.685,000 --> 0:09:15,000
porque aprende que lo que hizo no estaba bien.

178
0:09:16.242,000 --> 0:09:19,000
De hecho, podemos, con el uso adecuado de los símbolos griegos,

179
0:09:19.836,000 --> 0:09:21,000
como suelen hacer los matemáticos,

180
0:09:21.991,000 --> 0:09:22,000
podemos probar un teorema

181
0:09:23.999,000 --> 0:09:26,000
que dice que tal robot es probablemente beneficioso para el humano.

182
0:09:27.576,000 --> 0:09:3,000
Se está demostrablemente mejor con una máquina que se diseña de esta manera

183
0:09:31.403,000 --> 0:09:32,000
que sin ella.

184
0:09:33.057,000 --> 0:09:35,000
Este es un ejemplo muy simple, pero este es el primer paso

185
0:09:35.987,000 --> 0:09:38,000
en lo que estamos tratando de hacer con IA compatible con humanos.

186
0:09:42.477,000 --> 0:09:45,000
Ahora, este tercer principio,

187
0:09:45.758,000 --> 0:09:48,000
es probablemente el que está haciendo que se rasquen la cabeza.

188
0:09:48.894,000 --> 0:09:51,000
Probablemente piensen: "Yo me comporto mal.

189
0:09:51.904,000 --> 0:09:54,000
No quiero que mi robot se comporte como yo.

190
0:09:54.994,000 --> 0:09:57,000
Me escabullo en mitad de la noche y tomo cosas de la nevera,

191
0:09:58.084,000 --> 0:09:59,000
hago esto y hago aquello".

192
0:09:59.365,000 --> 0:10:01,000
Hay todo tipo de cosas que no quieres que haga el robot.

193
0:10:02.416,000 --> 0:10:04,000
Pero lo cierto es que no funciona así.

194
0:10:04.644,000 --> 0:10:06,000
Solo porque uno se comporte mal

195
0:10:06.806,000 --> 0:10:08,000
no significa que el robot vaya a copiar su comportamiento.

196
0:10:09.591,000 --> 0:10:12,000
Va a entender sus motivaciones y tal vez a ayudarle a resistirlas,

197
0:10:13.436,000 --> 0:10:14,000
si es apropiado.

198
0:10:16.026,000 --> 0:10:17,000
Pero sigue siendo difícil.

199
0:10:18.122,000 --> 0:10:2,000
Lo que estamos tratando de hacer, de hecho,

200
0:10:20.691,000 --> 0:10:24,000
es permitir que las máquinas predigan para cualquier persona

201
0:10:24.711,000 --> 0:10:26,000
y para cualquier vida posible que podrían vivir,

202
0:10:27.696,000 --> 0:10:28,000
y las vidas de todos los demás

203
0:10:29.317,000 --> 0:10:31,000
lo que preferirían.

204
0:10:33.881,000 --> 0:10:35,000
Y hay muchas, muchas dificultades ligadas a hacer esto.

205
0:10:36.859,000 --> 0:10:38,000
No espero que vaya a resolverse pronto.

206
0:10:39.815,000 --> 0:10:41,000
Las verdaderas dificultades, de hecho, somos nosotros.

207
0:10:43.969,000 --> 0:10:46,000
Como ya he mencionado, nos comportamos mal.

208
0:10:47.11,000 --> 0:10:49,000
De hecho, algunos de nosotros somos francamente desagradables.

209
0:10:50.251,000 --> 0:10:53,000
Como he dicho, el robot no tiene que copiar el comportamiento.

210
0:10:53.327,000 --> 0:10:55,000
El robot no tiene ningún objetivo propio.

211
0:10:56.142,000 --> 0:10:57,000
Es puramente altruista.

212
0:10:59.113,000 --> 0:11:04,000
Y no está diseñado solo para satisfacer los deseos de una persona, el usuario,

213
0:11:04.358,000 --> 0:11:07,000
sino que tiene que respetar las preferencias de todos.

214
0:11:09.083,000 --> 0:11:11,000
Así que puede lidiar con cierta cantidad de maldad,

215
0:11:11.677,000 --> 0:11:14,000
e incluso puede entender que su maldad, por ejemplo...

216
0:11:15.402,000 --> 0:11:17,000
Ud. puede aceptar sobornos como controlador de pasaportes

217
0:11:18.213,000 --> 0:11:21,000
porque necesita alimentar a su familia y que sus hijos vayan a la escuela.

218
0:11:21.933,000 --> 0:11:23,000
Puede entender eso; no significa que vaya a robar.

219
0:11:24.863,000 --> 0:11:26,000
De hecho, solo le ayudará a que sus hijos vayan al colegio.

220
0:11:28.796,000 --> 0:11:31,000
También estamos limitados computacionalmente.

221
0:11:31.832,000 --> 0:11:33,000
Lee Sedol es un jugador brillante de Go,

222
0:11:34.361,000 --> 0:11:35,000
pero aun así perdió.

223
0:11:35.71,000 --> 0:11:39,000
Si nos fijamos en sus acciones, tomó una decisión que le hizo perder.

224
0:11:39.973,000 --> 0:11:41,000
Eso no significa que él quisiera perder.

225
0:11:43.16,000 --> 0:11:45,000
Así que para entender su comportamiento,

226
0:11:45.224,000 --> 0:11:48,000
en realidad tenemos que invertir, a través de un modelo cognitivo humano

227
0:11:48.892,000 --> 0:11:5,000
que incluye nuestras limitaciones computacionales,

228
0:11:51.633,000 --> 0:11:53,000
y se trata de un modelo muy complicado.

229
0:11:53.893,000 --> 0:11:55,000
Pero es algo en lo que podemos trabajar para comprender.

230
0:11:57.406,000 --> 0:11:59,000
Puede que la parte más difícil, desde mi punto de vista

231
0:12:00.1,000 --> 0:12:01,000
como investigador de IA,

232
0:12:01.32,000 --> 0:12:04,000
es el hecho de que hay muchos de nosotros,

233
0:12:06.114,000 --> 0:12:08,000
con lo cual la máquina tiene que sopesar

234
0:12:08.969,000 --> 0:12:1,000
las preferencias de mucha gente diferente.

235
0:12:11.968,000 --> 0:12:12,000
Hay diferentes maneras de hacer eso.

236
0:12:13.898,000 --> 0:12:16,000
Economistas, sociólogos, filósofos morales han comprendido esto

237
0:12:17.611,000 --> 0:12:19,000
y estamos buscando colaboración de manera activa.

238
0:12:20.09,000 --> 0:12:23,000
Vamos a ver lo que sucede cuando esto se hace mal.

239
0:12:23.365,000 --> 0:12:25,000
Ud. puede estar hablando, por ejemplo,

240
0:12:25.522,000 --> 0:12:26,000
con su asistente personal inteligente

241
0:12:27.49,000 --> 0:12:29,000
que podría estar disponible dentro de unos años.

242
0:12:29.799,000 --> 0:12:31,000
Piensen en Siri con esteroides.

243
0:12:33.447,000 --> 0:12:37,000
Siri dice "Su esposa llamó para recordarle la cena de esta noche".

244
0:12:38.436,000 --> 0:12:41,000
Por supuesto, lo había olvidado. ¿Qué cena? ¿De qué está hablando?

245
0:12:42.614,000 --> 0:12:44,000
"Su 20 aniversario, a las 7pm".

246
0:12:48.566,000 --> 0:12:51,000
"No puedo, me reúno con el secretario general a las 7:30.

247
0:12:51.975,000 --> 0:12:53,000
¿Cómo ha podido suceder esto?".

248
0:12:54.315,000 --> 0:12:58,000
"Bueno, le advertí, pero ignoró mi recomendación".

249
0:12:58.994,000 --> 0:13:01,000
"¿Qué voy a hacer? No puedo decirles que estoy demasiado ocupado".

250
0:13:03.948,000 --> 0:13:07,000
"No se preocupe, he hecho que su avión se retrase".

251
0:13:07.976,000 --> 0:13:09,000
(Risas)

252
0:13:10.174,000 --> 0:13:12,000
"Algún tipo de error en el ordenador".

253
0:13:12.625,000 --> 0:13:12,000
(Risas)

254
0:13:13.43,000 --> 0:13:14,000
"¿En serio? ¿Puede hacer eso?".

255
0:13:16.22,000 --> 0:13:18,000
"Le envía sinceras disculpas

256
0:13:18.423,000 --> 0:13:2,000
y espera poder conocerle mañana para el almuerzo".

257
0:13:21.002,000 --> 0:13:22,000
(Risas)

258
0:13:22.325,000 --> 0:13:26,000
Así que los valores aquí... aquí hay un pequeño fallo.

259
0:13:26.752,000 --> 0:13:29,000
Claramente está siguiendo los valores de mi esposa

260
0:13:29.785,000 --> 0:13:31,000
que son "esposa feliz, vida feliz".

261
0:13:31.878,000 --> 0:13:32,000
(Risas)

262
0:13:33.485,000 --> 0:13:34,000
Podría suceder al revés.

263
0:13:35.461,000 --> 0:13:37,000
Podría llegar a casa tras un duro día de trabajo,

264
0:13:37.866,000 --> 0:13:39,000
y el ordenador dice "¿Un día duro?".

265
0:13:40.111,000 --> 0:13:41,000
"Sí, ni tuve tiempo de almorzar".

266
0:13:41.947,000 --> 0:13:42,000
"Debe tener mucha hambre".

267
0:13:43.759,000 --> 0:13:45,000
"Me muero de hambre, sí, ¿podría preparar algo de cena?".

268
0:13:47.89,000 --> 0:13:49,000
"Hay algo que necesito decirle".

269
0:13:50.004,000 --> 0:13:51,000
(Risas)

270
0:13:52.013,000 --> 0:13:56,000
"Hay humanos en Sudán del Sur más necesitados que Ud.".

271
0:13:56.942,000 --> 0:13:57,000
(Risas)

272
0:13:58.07,000 --> 0:14:,000
"Así que me voy, hágase su propia cena".

273
0:14:00.169,000 --> 0:14:02,000
(Risas)

274
0:14:02.403,000 --> 0:14:04,000
Así que tenemos que resolver estos problemas,

275
0:14:04.522,000 --> 0:14:05,000
y tengo ganas de trabajar en ellos.

276
0:14:06.945,000 --> 0:14:07,000
Hay razones para ser optimistas.

277
0:14:08.812,000 --> 0:14:11,000
Una razón es que hay gran cantidad de datos

278
0:14:11.891,000 --> 0:14:14,000
Recuerden, leerán todo lo que la raza humana ha escrito.

279
0:14:15.321,000 --> 0:14:18,000
La mayoría de lo que escribimos trata sobre humanos haciendo cosas

280
0:14:18.943,000 --> 0:14:19,000
y cómo estas molestan a otras personas.

281
0:14:20.961,000 --> 0:14:22,000
Así que hay muchos datos de los que aprender.

282
0:14:23.383,000 --> 0:14:25,000
También hay un fuerte incentivo económico

283
0:14:26.791,000 --> 0:14:27,000
para que esto funcione bien.

284
0:14:28.151,000 --> 0:14:3,000
Imagine que su robot doméstico está en casa

285
0:14:30.252,000 --> 0:14:31,000
Ud. llega tarde del trabajo,

286
0:14:31.772,000 --> 0:14:33,000
el robot tiene que dar de comer a los niños,

287
0:14:33.907,000 --> 0:14:35,000
los niños tienen hambre y no hay nada en la nevera.

288
0:14:36.33,000 --> 0:14:38,000
Y el robot ve al gato.

289
0:14:38.953,000 --> 0:14:39,000
(Risas)

290
0:14:40.669,000 --> 0:14:44,000
Y el robot no ha aprendido del todo bien la función del valor humano

291
0:14:44.773,000 --> 0:14:45,000
por lo que no entiende

292
0:14:46.284,000 --> 0:14:5,000
que el valor sentimental del gato supera el valor nutricional del gato.

293
0:14:50.586,000 --> 0:14:51,000
(Risas)

294
0:14:51.681,000 --> 0:14:52,000
Entonces, ¿qué pasa?

295
0:14:53.177,000 --> 0:14:57,000
Bueno, sucede lo siguiente:

296
0:14:57.238,000 --> 0:14:59,000
"Robot desquiciado cocina a un gatito para la cena familiar".

297
0:15:00.226,000 --> 0:15:04,000
Ese único incidente acabaría con la industria de robots domésticos.

298
0:15:04.773,000 --> 0:15:07,000
Así que hay un gran incentivo para hacer esto bien.

299
0:15:08.169,000 --> 0:15:1,000
mucho antes de llegar a las máquinas superinteligentes.

300
0:15:11.948,000 --> 0:15:12,000
Así que para resumir:

301
0:15:13.507,000 --> 0:15:15,000
Estoy intentando cambiar la definición de IA

302
0:15:16.412,000 --> 0:15:18,000
para que tengamos máquinas demostrablemente beneficiosas.

303
0:15:19.429,000 --> 0:15:2,000
Y los principios son:

304
0:15:20.675,000 --> 0:15:21,000
Máquinas que son altruistas,

305
0:15:22.193,000 --> 0:15:24,000
que desean lograr solo nuestros objetivos,

306
0:15:24.845,000 --> 0:15:27,000
pero que no están seguras de cuáles son esos objetivos

307
0:15:27.885,000 --> 0:15:29,000
y nos observarán a todos

308
0:15:30.123,000 --> 0:15:33,000
para aprender qué es lo que realmente queremos.

309
0:15:34.193,000 --> 0:15:37,000
Y con suerte, en el proceso, aprenderemos a ser mejores personas.

310
0:15:37.776,000 --> 0:15:38,000
Muchas gracias.

311
0:15:38.991,000 --> 0:15:41,000
(Aplausos)

312
0:15:42.484,000 --> 0:15:44,000
Chris Anderson: Muy interesante, Stuart.

313
0:15:44.642,000 --> 0:15:46,000
Vamos a estar aquí un poco porque creo que están preparando

314
0:15:47.4,000 --> 0:15:48,000
a nuestro próximo orador.

315
0:15:48.985,000 --> 0:15:49,000
Un par de preguntas.

316
0:15:50.547,000 --> 0:15:55,000
La idea de programar ignorancia parece intuitivamente muy poderosa.

317
0:15:55.974,000 --> 0:15:56,000
Al llegar a la superinteligencia,

318
0:15:57.738,000 --> 0:15:59,000
¿qué puede impedir que un robot

319
0:16:,000 --> 0:16:02,000
lea literatura y descubra esta idea de que el conocimiento

320
0:16:02.77,000 --> 0:16:03,000
es mejor que la ignorancia,

321
0:16:04.452,000 --> 0:16:08,000
cambiando sus propios objetivos y reescribiendo su programación?

322
0:16:09.512,000 --> 0:16:15,000
Stuart Russell: Queremos que aprenda más, como he dicho,

323
0:16:15.802,000 --> 0:16:16,000
sobre nuestros objetivos.

324
0:16:17.279,000 --> 0:16:22,000
Solo ganará seguridad cuanto más acierte.

325
0:16:22.748,000 --> 0:16:23,000
La evidencia estará ahí,

326
0:16:24.717,000 --> 0:16:26,000
y estará diseñado para interpretarla adecuadamente.

327
0:16:27.465,000 --> 0:16:3,000
Comprenderá, por ejemplo, que los libros son muy sesgados

328
0:16:31.325,000 --> 0:16:32,000
en la evidencia que contienen.

329
0:16:33.008,000 --> 0:16:35,000
Solo hablan de reyes y príncipes

330
0:16:35.459,000 --> 0:16:37,000
y hombres blancos poderosos haciendo cosas.

331
0:16:38.197,000 --> 0:16:39,000
Es un problema complicado,

332
0:16:40.177,000 --> 0:16:43,000
pero conforme aprenda más sobre nuestros objetivos

333
0:16:44.113,000 --> 0:16:46,000
será cada vez más útil para nosotros.

334
0:16:46.3,000 --> 0:16:48,000
CA: Y no podría reducirse a una ley,

335
0:16:48.906,000 --> 0:16:49,000
ya sabe, grabada a fuego,

336
0:16:50.62,000 --> 0:16:53,000
"Si un humano alguna vez intenta apagarme

337
0:16:53.771,000 --> 0:16:55,000
yo obedezco, obedezco".

338
0:16:55.8,000 --> 0:16:56,000
SR: Absolutamente no.

339
0:16:57.122,000 --> 0:16:58,000
Sería una idea terrible.

340
0:16:58.565,000 --> 0:17:,000
Imagine, tiene un auto que se conduce solo

341
0:17:01.328,000 --> 0:17:04,000
y quiere llevar a su hijo de cinco años al jardín de infancia.

342
0:17:04.715,000 --> 0:17:07,000
¿Quiere que su hijo de cinco años pueda apagar el coche mientras conduce?

343
0:17:08.198,000 --> 0:17:09,000
Probablemente no.

344
0:17:09.456,000 --> 0:17:14,000
Por tanto necesita entender cuán racional y sensata es la persona.

345
0:17:15.169,000 --> 0:17:16,000
Cuanto más racional sea la persona,

346
0:17:16.885,000 --> 0:17:18,000
más dispuesto estará a dejar que lo apaguen.

347
0:17:18.992,000 --> 0:17:2,000
Si la persona es impredecible o incluso malintencionada

348
0:17:21.649,000 --> 0:17:23,000
estará menos dispuesto a permitir que lo apaguen.

349
0:17:24.039,000 --> 0:17:25,000
CA: Stuart, permítame decir

350
0:17:25.353,000 --> 0:17:27,000
que de veras espero que resuelva esto por todos nosotros.

351
0:17:28.058,000 --> 0:17:3,000
Muchas gracias por su charla. Ha sido increíble, gracias.

352
0:17:30.893,000 --> 0:17:31,000
(Aplausos)

