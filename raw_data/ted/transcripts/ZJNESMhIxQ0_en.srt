1
0:00:12.843,000 --> 0:00:14,000
In 2007, I became the attorney general

2
0:00:15.434,000 --> 0:00:16,000
of the state of New Jersey.

3
0:00:17.159,000 --> 0:00:19,000
Before that, I'd been a criminal prosecutor,

4
0:00:19.439,000 --> 0:00:21,000
first in the Manhattan district attorney's office,

5
0:00:22.12,000 --> 0:00:24,000
and then at the United States Department of Justice.

6
0:00:24.77,000 --> 0:00:26,000
But when I became the attorney general,

7
0:00:26.971,000 --> 0:00:29,000
two things happened that changed the way I see criminal justice.

8
0:00:30.866,000 --> 0:00:32,000
The first is that I asked what I thought

9
0:00:32.896,000 --> 0:00:34,000
were really basic questions.

10
0:00:35.082,000 --> 0:00:37,000
I wanted to understand who we were arresting,

11
0:00:37.938,000 --> 0:00:38,000
who we were charging,

12
0:00:39.602,000 --> 0:00:41,000
and who we were putting in our nation's jails

13
0:00:41.73,000 --> 0:00:42,000
and prisons.

14
0:00:43.146,000 --> 0:00:44,000
I also wanted to understand

15
0:00:44.794,000 --> 0:00:45,000
if we were making decisions

16
0:00:46.123,000 --> 0:00:48,000
in a way that made us safer.

17
0:00:48.641,000 --> 0:00:51,000
And I couldn't get this information out.

18
0:00:51.893,000 --> 0:00:54,000
It turned out that most big criminal justice agencies

19
0:00:55.25,000 --> 0:00:56,000
like my own

20
0:00:56.552,000 --> 0:00:58,000
didn't track the things that matter.

21
0:00:58.934,000 --> 0:01:01,000
So after about a month of being incredibly frustrated,

22
0:01:02.252,000 --> 0:01:03,000
I walked down into a conference room

23
0:01:04.223,000 --> 0:01:05,000
that was filled with detectives

24
0:01:06.113,000 --> 0:01:08,000
and stacks and stacks of case files,

25
0:01:08.895,000 --> 0:01:09,000
and the detectives were sitting there

26
0:01:10.071,000 --> 0:01:12,000
with yellow legal pads taking notes.

27
0:01:12.305,000 --> 0:01:13,000
They were trying to get the information

28
0:01:13.891,000 --> 0:01:14,000
I was looking for

29
0:01:15.109,000 --> 0:01:17,000
by going through case by case

30
0:01:17.154,000 --> 0:01:18,000
for the past five years.

31
0:01:19.052,000 --> 0:01:2,000
And as you can imagine,

32
0:01:20.705,000 --> 0:01:22,000
when we finally got the results, they weren't good.

33
0:01:23.348,000 --> 0:01:24,000
It turned out that we were doing

34
0:01:25.003,000 --> 0:01:27,000
a lot of low-level drug cases

35
0:01:27.023,000 --> 0:01:28,000
on the streets just around the corner

36
0:01:28.498,000 --> 0:01:3,000
from our office in Trenton.

37
0:01:30.766,000 --> 0:01:31,000
The second thing that happened

38
0:01:32.233,000 --> 0:01:35,000
is that I spent the day in the Camden, New Jersey police department.

39
0:01:35.907,000 --> 0:01:36,000
Now, at that time, Camden, New Jersey,

40
0:01:37.794,000 --> 0:01:39,000
was the most dangerous city in America.

41
0:01:40.446,000 --> 0:01:43,000
I ran the Camden Police Department because of that.

42
0:01:44.273,000 --> 0:01:46,000
I spent the day in the police department,

43
0:01:46.385,000 --> 0:01:48,000
and I was taken into a room with senior police officials,

44
0:01:49.111,000 --> 0:01:5,000
all of whom were working hard

45
0:01:50.786,000 --> 0:01:53,000
and trying very hard to reduce crime in Camden.

46
0:01:54.043,000 --> 0:01:55,000
And what I saw in that room,

47
0:01:55.869,000 --> 0:01:57,000
as we talked about how to reduce crime,

48
0:01:58.114,000 --> 0:02:01,000
were a series of officers with a lot of little yellow sticky notes.

49
0:02:01.973,000 --> 0:02:03,000
And they would take a yellow sticky and they would write something on it

50
0:02:04.823,000 --> 0:02:05,000
and they would put it up on a board.

51
0:02:06.622,000 --> 0:02:08,000
And one of them said, "We had a robbery two weeks ago.

52
0:02:08.793,000 --> 0:02:09,000
We have no suspects."

53
0:02:10.504,000 --> 0:02:15,000
And another said, "We had a shooting in this neighborhood last week. We have no suspects."

54
0:02:15.531,000 --> 0:02:17,000
We weren't using data-driven policing.

55
0:02:18.114,000 --> 0:02:2,000
We were essentially trying to fight crime

56
0:02:20.156,000 --> 0:02:22,000
with yellow Post-it notes.

57
0:02:22.683,000 --> 0:02:24,000
Now, both of these things made me realize

58
0:02:24.818,000 --> 0:02:27,000
fundamentally that we were failing.

59
0:02:28.069,000 --> 0:02:31,000
We didn't even know who was in our criminal justice system,

60
0:02:31.192,000 --> 0:02:34,000
we didn't have any data about the things that mattered,

61
0:02:34.427,000 --> 0:02:36,000
and we didn't share data or use analytics

62
0:02:36.995,000 --> 0:02:38,000
or tools to help us make better decisions

63
0:02:39.146,000 --> 0:02:41,000
and to reduce crime.

64
0:02:41.149,000 --> 0:02:43,000
And for the first time, I started to think

65
0:02:43.373,000 --> 0:02:44,000
about how we made decisions.

66
0:02:45.283,000 --> 0:02:46,000
When I was an assistant D.A.,

67
0:02:46.68,000 --> 0:02:47,000
and when I was a federal prosecutor,

68
0:02:48.55,000 --> 0:02:49,000
I looked at the cases in front of me,

69
0:02:50.296,000 --> 0:02:52,000
and I generally made decisions based on my instinct

70
0:02:52.922,000 --> 0:02:53,000
and my experience.

71
0:02:54.614,000 --> 0:02:55,000
When I became attorney general,

72
0:02:56.273,000 --> 0:02:57,000
I could look at the system as a whole,

73
0:02:57.912,000 --> 0:02:58,000
and what surprised me is that I found

74
0:02:59.73,000 --> 0:03:,000
that that was exactly how we were doing it

75
0:03:01.635,000 --> 0:03:03,000
across the entire system --

76
0:03:03.938,000 --> 0:03:05,000
in police departments, in prosecutors's offices,

77
0:03:06.339,000 --> 0:03:08,000
in courts and in jails.

78
0:03:09.139,000 --> 0:03:11,000
And what I learned very quickly

79
0:03:11.336,000 --> 0:03:14,000
is that we weren't doing a good job.

80
0:03:14.969,000 --> 0:03:16,000
So I wanted to do things differently.

81
0:03:16.985,000 --> 0:03:18,000
I wanted to introduce data and analytics

82
0:03:19.182,000 --> 0:03:21,000
and rigorous statistical analysis

83
0:03:21.231,000 --> 0:03:22,000
into our work.

84
0:03:22.631,000 --> 0:03:24,000
In short, I wanted to moneyball criminal justice.

85
0:03:25.601,000 --> 0:03:27,000
Now, moneyball, as many of you know,

86
0:03:27.628,000 --> 0:03:28,000
is what the Oakland A's did,

87
0:03:29.197,000 --> 0:03:3,000
where they used smart data and statistics

88
0:03:31.17,000 --> 0:03:32,000
to figure out how to pick players

89
0:03:32.792,000 --> 0:03:33,000
that would help them win games,

90
0:03:34.313,000 --> 0:03:36,000
and they went from a system that was based on baseball scouts

91
0:03:37.293,000 --> 0:03:38,000
who used to go out and watch players

92
0:03:39.153,000 --> 0:03:4,000
and use their instinct and experience,

93
0:03:40.79,000 --> 0:03:41,000
the scouts' instincts and experience,

94
0:03:42.533,000 --> 0:03:43,000
to pick players, from one to use

95
0:03:44.246,000 --> 0:03:46,000
smart data and rigorous statistical analysis

96
0:03:47.068,000 --> 0:03:5,000
to figure out how to pick players that would help them win games.

97
0:03:50.439,000 --> 0:03:51,000
It worked for the Oakland A's,

98
0:03:52.237,000 --> 0:03:54,000
and it worked in the state of New Jersey.

99
0:03:54.456,000 --> 0:03:56,000
We took Camden off the top of the list

100
0:03:56.529,000 --> 0:03:58,000
as the most dangerous city in America.

101
0:03:58.7,000 --> 0:04:01,000
We reduced murders there by 41 percent,

102
0:04:01.855,000 --> 0:04:03,000
which actually means 37 lives were saved.

103
0:04:04.837,000 --> 0:04:07,000
And we reduced all crime in the city by 26 percent.

104
0:04:08.577,000 --> 0:04:11,000
We also changed the way we did criminal prosecutions.

105
0:04:11.816,000 --> 0:04:13,000
So we went from doing low-level drug crimes

106
0:04:13.821,000 --> 0:04:14,000
that were outside our building

107
0:04:15.463,000 --> 0:04:17,000
to doing cases of statewide importance,

108
0:04:17.805,000 --> 0:04:2,000
on things like reducing violence with the most violent offenders,

109
0:04:20.963,000 --> 0:04:21,000
prosecuting street gangs,

110
0:04:22.821,000 --> 0:04:25,000
gun and drug trafficking, and political corruption.

111
0:04:26.229,000 --> 0:04:28,000
And all of this matters greatly,

112
0:04:28.731,000 --> 0:04:29,000
because public safety to me

113
0:04:30.676,000 --> 0:04:32,000
is the most important function of government.

114
0:04:33.212,000 --> 0:04:35,000
If we're not safe, we can't be educated,

115
0:04:35.51,000 --> 0:04:36,000
we can't be healthy,

116
0:04:36.858,000 --> 0:04:38,000
we can't do any of the other things we want to do in our lives.

117
0:04:39.803,000 --> 0:04:4,000
And we live in a country today

118
0:04:41.504,000 --> 0:04:44,000
where we face serious criminal justice problems.

119
0:04:44.638,000 --> 0:04:47,000
We have 12 million arrests every single year.

120
0:04:48.299,000 --> 0:04:5,000
The vast majority of those arrests

121
0:04:50.342,000 --> 0:04:53,000
are for low-level crimes, like misdemeanors,

122
0:04:53.354,000 --> 0:04:54,000
70 to 80 percent.

123
0:04:55.088,000 --> 0:04:56,000
Less than five percent of all arrests

124
0:04:57.079,000 --> 0:04:58,000
are for violent crime.

125
0:04:58.974,000 --> 0:05:,000
Yet we spend 75 billion,

126
0:05:01.029,000 --> 0:05:02,000
that's b for billion,

127
0:05:02.447,000 --> 0:05:06,000
dollars a year on state and local corrections costs.

128
0:05:06.574,000 --> 0:05:08,000
Right now, today, we have 2.3 million people

129
0:05:09.415,000 --> 0:05:1,000
in our jails and prisons.

130
0:05:11.315,000 --> 0:05:13,000
And we face unbelievable public safety challenges

131
0:05:14.111,000 --> 0:05:15,000
because we have a situation

132
0:05:16.05,000 --> 0:05:18,000
in which two thirds of the people in our jails

133
0:05:18.948,000 --> 0:05:19,000
are there waiting for trial.

134
0:05:20.702,000 --> 0:05:22,000
They haven't yet been convicted of a crime.

135
0:05:22.837,000 --> 0:05:24,000
They're just waiting for their day in court.

136
0:05:24.956,000 --> 0:05:27,000
And 67 percent of people come back.

137
0:05:28.504,000 --> 0:05:31,000
Our recidivism rate is amongst the highest in the world.

138
0:05:31.532,000 --> 0:05:33,000
Almost seven in 10 people who are released

139
0:05:33.635,000 --> 0:05:34,000
from prison will be rearrested

140
0:05:35.286,000 --> 0:05:38,000
in a constant cycle of crime and incarceration.

141
0:05:39.241,000 --> 0:05:41,000
So when I started my job at the Arnold Foundation,

142
0:05:41.823,000 --> 0:05:43,000
I came back to looking at a lot of these questions,

143
0:05:44.559,000 --> 0:05:45,000
and I came back to thinking about how

144
0:05:46.213,000 --> 0:05:48,000
we had used data and analytics to transform

145
0:05:48.596,000 --> 0:05:5,000
the way we did criminal justice in New Jersey.

146
0:05:51.18,000 --> 0:05:53,000
And when I look at the criminal justice system

147
0:05:53.324,000 --> 0:05:54,000
in the United States today,

148
0:05:54.98,000 --> 0:05:55,000
I feel the exact same way that I did

149
0:05:56.619,000 --> 0:05:58,000
about the state of New Jersey when I started there,

150
0:05:59.085,000 --> 0:06:02,000
which is that we absolutely have to do better,

151
0:06:02.313,000 --> 0:06:03,000
and I know that we can do better.

152
0:06:04.236,000 --> 0:06:05,000
So I decided to focus

153
0:06:05.941,000 --> 0:06:07,000
on using data and analytics

154
0:06:08.158,000 --> 0:06:1,000
to help make the most critical decision

155
0:06:10.519,000 --> 0:06:11,000
in public safety,

156
0:06:12.125,000 --> 0:06:14,000
and that decision is the determination

157
0:06:14.146,000 --> 0:06:16,000
of whether, when someone has been arrested,

158
0:06:16.681,000 --> 0:06:17,000
whether they pose a risk to public safety

159
0:06:18.596,000 --> 0:06:19,000
and should be detained,

160
0:06:20.122,000 --> 0:06:22,000
or whether they don't pose a risk to public safety

161
0:06:22.478,000 --> 0:06:23,000
and should be released.

162
0:06:24.115,000 --> 0:06:25,000
Everything that happens in criminal cases

163
0:06:26.034,000 --> 0:06:27,000
comes out of this one decision.

164
0:06:27.806,000 --> 0:06:28,000
It impacts everything.

165
0:06:29.302,000 --> 0:06:3,000
It impacts sentencing.

166
0:06:30.652,000 --> 0:06:31,000
It impacts whether someone gets drug treatment.

167
0:06:32.553,000 --> 0:06:34,000
It impacts crime and violence.

168
0:06:34.876,000 --> 0:06:35,000
And when I talk to judges around the United States,

169
0:06:36.813,000 --> 0:06:37,000
which I do all the time now,

170
0:06:38.741,000 --> 0:06:39,000
they all say the same thing,

171
0:06:40.578,000 --> 0:06:43,000
which is that we put dangerous people in jail,

172
0:06:43.685,000 --> 0:06:46,000
and we let non-dangerous, nonviolent people out.

173
0:06:47.21,000 --> 0:06:49,000
They mean it and they believe it.

174
0:06:49.443,000 --> 0:06:5,000
But when you start to look at the data,

175
0:06:51.176,000 --> 0:06:53,000
which, by the way, the judges don't have,

176
0:06:53.64,000 --> 0:06:54,000
when we start to look at the data,

177
0:06:55.252,000 --> 0:06:57,000
what we find time and time again,

178
0:06:57.67,000 --> 0:06:58,000
is that this isn't the case.

179
0:06:59.652,000 --> 0:07:,000
We find low-risk offenders,

180
0:07:01.333,000 --> 0:07:04,000
which makes up 50 percent of our entire criminal justice population,

181
0:07:05.047,000 --> 0:07:07,000
we find that they're in jail.

182
0:07:07.446,000 --> 0:07:09,000
Take Leslie Chew, who was a Texas man

183
0:07:09.932,000 --> 0:07:11,000
who stole four blankets on a cold winter night.

184
0:07:12.816,000 --> 0:07:14,000
He was arrested, and he was kept in jail

185
0:07:15.411,000 --> 0:07:17,000
on 3,500 dollars bail,

186
0:07:17.464,000 --> 0:07:19,000
an amount that he could not afford to pay.

187
0:07:20.24,000 --> 0:07:22,000
And he stayed in jail for eight months

188
0:07:22.828,000 --> 0:07:24,000
until his case came up for trial,

189
0:07:24.893,000 --> 0:07:27,000
at a cost to taxpayers of more than 9,000 dollars.

190
0:07:28.798,000 --> 0:07:29,000
And at the other end of the spectrum,

191
0:07:30.795,000 --> 0:07:32,000
we're doing an equally terrible job.

192
0:07:33.077,000 --> 0:07:34,000
The people who we find

193
0:07:34.649,000 --> 0:07:36,000
are the highest-risk offenders,

194
0:07:36.668,000 --> 0:07:38,000
the people who we think have the highest likelihood

195
0:07:39.165,000 --> 0:07:4,000
of committing a new crime if they're released,

196
0:07:41.117,000 --> 0:07:43,000
we see nationally that 50 percent of those people

197
0:07:44.067,000 --> 0:07:45,000
are being released.

198
0:07:46.041,000 --> 0:07:49,000
The reason for this is the way we make decisions.

199
0:07:49.215,000 --> 0:07:5,000
Judges have the best intentions

200
0:07:50.924,000 --> 0:07:51,000
when they make these decisions about risk,

201
0:07:52.876,000 --> 0:07:54,000
but they're making them subjectively.

202
0:07:55.36,000 --> 0:07:57,000
They're like the baseball scouts 20 years ago

203
0:07:57.506,000 --> 0:07:59,000
who were using their instinct and their experience

204
0:07:59.637,000 --> 0:08:01,000
to try to decide what risk someone poses.

205
0:08:02.316,000 --> 0:08:03,000
They're being subjective,

206
0:08:03.846,000 --> 0:08:06,000
and we know what happens with subjective decision making,

207
0:08:06.906,000 --> 0:08:08,000
which is that we are often wrong.

208
0:08:09.649,000 --> 0:08:1,000
What we need in this space

209
0:08:11.032,000 --> 0:08:13,000
are strong data and analytics.

210
0:08:13.584,000 --> 0:08:14,000
What I decided to look for

211
0:08:15.331,000 --> 0:08:17,000
was a strong data and analytic risk assessment tool,

212
0:08:18.167,000 --> 0:08:2,000
something that would let judges actually understand

213
0:08:20.931,000 --> 0:08:22,000
with a scientific and objective way

214
0:08:23.19,000 --> 0:08:24,000
what the risk was that was posed

215
0:08:24.837,000 --> 0:08:25,000
by someone in front of them.

216
0:08:26.447,000 --> 0:08:27,000
I looked all over the country,

217
0:08:28.096,000 --> 0:08:29,000
and I found that between five and 10 percent

218
0:08:30.038,000 --> 0:08:31,000
of all U.S. jurisdictions

219
0:08:31.367,000 --> 0:08:33,000
actually use any type of risk assessment tool,

220
0:08:34.345,000 --> 0:08:35,000
and when I looked at these tools,

221
0:08:35.97,000 --> 0:08:36,000
I quickly realized why.

222
0:08:37.83,000 --> 0:08:39,000
They were unbelievably expensive to administer,

223
0:08:40.52,000 --> 0:08:41,000
they were time-consuming,

224
0:08:42.048,000 --> 0:08:44,000
they were limited to the local jurisdiction

225
0:08:44.155,000 --> 0:08:45,000
in which they'd been created.

226
0:08:45.585,000 --> 0:08:46,000
So basically, they couldn't be scaled

227
0:08:47.378,000 --> 0:08:49,000
or transferred to other places.

228
0:08:49.587,000 --> 0:08:51,000
So I went out and built a phenomenal team

229
0:08:51.824,000 --> 0:08:53,000
of data scientists and researchers

230
0:08:53.868,000 --> 0:08:54,000
and statisticians

231
0:08:55.494,000 --> 0:08:57,000
to build a universal risk assessment tool,

232
0:08:58.339,000 --> 0:09:,000
so that every single judge in the United States of America

233
0:09:00.732,000 --> 0:09:04,000
can have an objective, scientific measure of risk.

234
0:09:05.056,000 --> 0:09:06,000
In the tool that we've built,

235
0:09:06.714,000 --> 0:09:08,000
what we did was we collected 1.5 million cases

236
0:09:09.582,000 --> 0:09:1,000
from all around the United States,

237
0:09:11.28,000 --> 0:09:12,000
from cities, from counties,

238
0:09:12.924,000 --> 0:09:13,000
from every single state in the country,

239
0:09:14.435,000 --> 0:09:15,000
the federal districts.

240
0:09:16.181,000 --> 0:09:18,000
And with those 1.5 million cases,

241
0:09:18.37,000 --> 0:09:19,000
which is the largest data set on pretrial

242
0:09:20.31,000 --> 0:09:21,000
in the United States today,

243
0:09:22.115,000 --> 0:09:23,000
we were able to basically find that there were

244
0:09:23.98,000 --> 0:09:26,000
900-plus risk factors that we could look at

245
0:09:27.302,000 --> 0:09:29,000
to try to figure out what mattered most.

246
0:09:30.168,000 --> 0:09:32,000
And we found that there were nine specific things

247
0:09:32.249,000 --> 0:09:34,000
that mattered all across the country

248
0:09:34.484,000 --> 0:09:36,000
and that were the most highly predictive of risk.

249
0:09:37.461,000 --> 0:09:4,000
And so we built a universal risk assessment tool.

250
0:09:41.166,000 --> 0:09:42,000
And it looks like this.

251
0:09:42.611,000 --> 0:09:44,000
As you'll see, we put some information in,

252
0:09:45.223,000 --> 0:09:47,000
but most of it is incredibly simple,

253
0:09:47.236,000 --> 0:09:48,000
it's easy to use,

254
0:09:48.668,000 --> 0:09:5,000
it focuses on things like the defendant's prior convictions,

255
0:09:51.637,000 --> 0:09:52,000
whether they've been sentenced to incarceration,

256
0:09:53.616,000 --> 0:09:55,000
whether they've engaged in violence before,

257
0:09:55.88,000 --> 0:09:57,000
whether they've even failed to come back to court.

258
0:09:58.273,000 --> 0:10:,000
And with this tool, we can predict three things.

259
0:10:00.773,000 --> 0:10:01,000
First, whether or not someone will commit

260
0:10:02.626,000 --> 0:10:03,000
a new crime if they're released.

261
0:10:04.191,000 --> 0:10:05,000
Second, for the first time,

262
0:10:05.855,000 --> 0:10:06,000
and I think this is incredibly important,

263
0:10:07.716,000 --> 0:10:08,000
we can predict whether someone will commit

264
0:10:09.639,000 --> 0:10:1,000
an act of violence if they're released.

265
0:10:11.473,000 --> 0:10:12,000
And that's the single most important thing

266
0:10:13.36,000 --> 0:10:14,000
that judges say when you talk to them.

267
0:10:15.167,000 --> 0:10:16,000
And third, we can predict whether someone

268
0:10:16.995,000 --> 0:10:17,000
will come back to court.

269
0:10:18.985,000 --> 0:10:21,000
And every single judge in the United States of America can use it,

270
0:10:22.018,000 --> 0:10:25,000
because it's been created on a universal data set.

271
0:10:25.83,000 --> 0:10:27,000
What judges see if they run the risk assessment tool

272
0:10:28.439,000 --> 0:10:3,000
is this -- it's a dashboard.

273
0:10:30.559,000 --> 0:10:32,000
At the top, you see the New Criminal Activity Score,

274
0:10:33.407,000 --> 0:10:34,000
six of course being the highest,

275
0:10:35.336,000 --> 0:10:37,000
and then in the middle you see, "Elevated risk of violence."

276
0:10:37.739,000 --> 0:10:38,000
What that says is that this person

277
0:10:39.485,000 --> 0:10:41,000
is someone who has an elevated risk of violence

278
0:10:41.545,000 --> 0:10:42,000
that the judge should look twice at.

279
0:10:43.43,000 --> 0:10:44,000
And then, towards the bottom,

280
0:10:44.766,000 --> 0:10:45,000
you see the Failure to Appear Score,

281
0:10:46.734,000 --> 0:10:47,000
which again is the likelihood

282
0:10:48.126,000 --> 0:10:51,000
that someone will come back to court.

283
0:10:51.139,000 --> 0:10:53,000
Now I want to say something really important.

284
0:10:53.352,000 --> 0:10:55,000
It's not that I think we should be eliminating

285
0:10:56.079,000 --> 0:10:58,000
the judge's instinct and experience

286
0:10:58.323,000 --> 0:10:59,000
from this process.

287
0:10:59.927,000 --> 0:11:,000
I don't.

288
0:11:00.985,000 --> 0:11:02,000
I actually believe the problem that we see

289
0:11:02.992,000 --> 0:11:04,000
and the reason that we have these incredible system errors,

290
0:11:05.846,000 --> 0:11:08,000
where we're incarcerating low-level, nonviolent people

291
0:11:08.933,000 --> 0:11:11,000
and we're releasing high-risk, dangerous people,

292
0:11:12.105,000 --> 0:11:14,000
is that we don't have an objective measure of risk.

293
0:11:14.828,000 --> 0:11:15,000
But what I believe should happen

294
0:11:16.128,000 --> 0:11:18,000
is that we should take that data-driven risk assessment

295
0:11:18.928,000 --> 0:11:21,000
and combine that with the judge's instinct and experience

296
0:11:21.969,000 --> 0:11:23,000
to lead us to better decision making.

297
0:11:24.927,000 --> 0:11:27,000
The tool went statewide in Kentucky on July 1,

298
0:11:28.23,000 --> 0:11:31,000
and we're about to go up in a number of other U.S. jurisdictions.

299
0:11:31.581,000 --> 0:11:33,000
Our goal, quite simply, is that every single judge

300
0:11:34.172,000 --> 0:11:36,000
in the United States will use a data-driven risk tool

301
0:11:36.364,000 --> 0:11:38,000
within the next five years.

302
0:11:38.455,000 --> 0:11:39,000
We're now working on risk tools

303
0:11:39.807,000 --> 0:11:42,000
for prosecutors and for police officers as well,

304
0:11:43.091,000 --> 0:11:45,000
to try to take a system that runs today

305
0:11:45.791,000 --> 0:11:47,000
in America the same way it did 50 years ago,

306
0:11:48.587,000 --> 0:11:5,000
based on instinct and experience,

307
0:11:50.684,000 --> 0:11:51,000
and make it into one that runs

308
0:11:52.539,000 --> 0:11:54,000
on data and analytics.

309
0:11:55.008,000 --> 0:11:56,000
Now, the great news about all this,

310
0:11:56.929,000 --> 0:11:57,000
and we have a ton of work left to do,

311
0:11:58.546,000 --> 0:11:59,000
and we have a lot of culture to change,

312
0:12:00.403,000 --> 0:12:01,000
but the great news about all of it

313
0:12:02.149,000 --> 0:12:03,000
is that we know it works.

314
0:12:04.017,000 --> 0:12:06,000
It's why Google is Google,

315
0:12:06.17,000 --> 0:12:08,000
and it's why all these baseball teams use moneyball

316
0:12:08.632,000 --> 0:12:09,000
to win games.

317
0:12:10.413,000 --> 0:12:11,000
The great news for us as well

318
0:12:12.15,000 --> 0:12:13,000
is that it's the way that we can transform

319
0:12:14.046,000 --> 0:12:16,000
the American criminal justice system.

320
0:12:16.367,000 --> 0:12:18,000
It's how we can make our streets safer,

321
0:12:18.724,000 --> 0:12:2,000
we can reduce our prison costs,

322
0:12:21.023,000 --> 0:12:23,000
and we can make our system much fairer

323
0:12:23.09,000 --> 0:12:24,000
and more just.

324
0:12:24.815,000 --> 0:12:26,000
Some people call it data science.

325
0:12:26.977,000 --> 0:12:28,000
I call it moneyballing criminal justice.

326
0:12:29.278,000 --> 0:12:3,000
Thank you.

327
0:12:31.082,000 --> 0:12:35,000
(Applause)

