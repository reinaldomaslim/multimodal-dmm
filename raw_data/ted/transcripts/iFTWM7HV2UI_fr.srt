1
0:00:,000 --> 0:00:07,000
Traducteur: Morgane Quilfen Relecteur: Claire Ghyselen

2
0:00:12.76,000 --> 0:00:15,000
Quand les gens expriment de la peur envers l'intelligence artificielle,

3
0:00:16.32,000 --> 0:00:19,000
très souvent, ils évoquent des images de robots humanoïdes qui se déchaînent.

4
0:00:20.32,000 --> 0:00:21,000
Vous voyez ? Terminator ?

5
0:00:22.4,000 --> 0:00:24,000
Ce pourrait être à considérer

6
0:00:24.76,000 --> 0:00:25,000
mais c'est une menace très lointaine.

7
0:00:26.64,000 --> 0:00:29,000
Ou nous nous tracassons au sujet de la surveillance numérique

8
0:00:30.12,000 --> 0:00:31,000
avec des métaphores du passé.

9
0:00:31.92,000 --> 0:00:33,000
« 1984 » de George Orwell

10
0:00:34.6,000 --> 0:00:36,000
est à nouveau parmi les livres les plus vendus.

11
0:00:37.96,000 --> 0:00:38,000
C'est un super livre,

12
0:00:39.4,000 --> 0:00:42,000
mais ce n'est pas la bonne dystopie pour le 21e siècle.

13
0:00:44.08,000 --> 0:00:45,000
Ce que nous devons craindre

14
0:00:45.52,000 --> 0:00:49,000
n'est pas ce que l'intelligence artificielle fera seule,

15
0:00:50.32,000 --> 0:00:54,000
mais comment les gens au pouvoir utiliseront l'intelligence artificielle

16
0:00:55.08,000 --> 0:00:57,000
pour nous contrôler et nous manipuler

17
0:00:57.92,000 --> 0:01:,000
de façons nouvelles, parfois cachées,

18
0:01:01.08,000 --> 0:01:04,000
subtiles et inattendues.

19
0:01:04.12,000 --> 0:01:05,000
La plupart des technologies

20
0:01:06,000 --> 0:01:1,000
qui menacent notre liberté et notre dignité dans un avenir proche

21
0:01:10.36,000 --> 0:01:11,000
sont développées par des entreprises

22
0:01:12.24,000 --> 0:01:15,000
dans le domaine de l'enregistrement et de la vente

23
0:01:15.47,000 --> 0:01:16,000
de nos données et notre attention

24
0:01:17.2,000 --> 0:01:19,000
à des publicitaires et autres :

25
0:01:19.48,000 --> 0:01:22,000
Facebook, Google, Amazon,

26
0:01:22.92,000 --> 0:01:23,000
Alibaba, Tencent.

27
0:01:26.04,000 --> 0:01:31,000
L'intelligence artificielle a également commencé à stimuler leur marché.

28
0:01:31.56,000 --> 0:01:33,000
On pourrait croire que l'intelligence artificielle

29
0:01:33.936,000 --> 0:01:35,000
sera la suite des publicités en ligne.

30
0:01:36.56,000 --> 0:01:37,000
Ce n'est pas le cas.

31
0:01:37.8,000 --> 0:01:39,000
C'est un saut dans la catégorie.

32
0:01:40.28,000 --> 0:01:42,000
C'est un monde complètement différent

33
0:01:42.88,000 --> 0:01:44,000
qui a un énorme potentiel.

34
0:01:45.52,000 --> 0:01:49,000
Ça pourrait accélérer notre compréhension

35
0:01:50.12,000 --> 0:01:53,000
de nombreux domaines d'étude et de recherche.

36
0:01:53.12,000 --> 0:01:56,000
Pour paraphraser un célèbre philosophe hollywoodien :

37
0:01:56.64,000 --> 0:01:59,000
« Un grand pouvoir implique de grandes responsabilités. »

38
0:02:01.12,000 --> 0:02:04,000
Considérons un fait fondamental de nos vies numériques : les publicités.

39
0:02:05.08,000 --> 0:02:07,000
Nous les ignorons.

40
0:02:08,000 --> 0:02:09,000
Elles semblent grossières, inefficaces.

41
0:02:1,000 --> 0:02:14,000
Nous avons tous été suivis sur internet

42
0:02:14.24,000 --> 0:02:17,000
par une pub basée sur une chose que nous avions cherchée ou lue.

43
0:02:17.246,000 --> 0:02:18,000
Vous cherchez un paire de bottes

44
0:02:18.96,000 --> 0:02:21,000
et pendant une semaine, ces bottes vous suivent partout où vous allez.

45
0:02:22.36,000 --> 0:02:24,000
Même après avoir succombé et les avoir achetées,

46
0:02:24.6,000 --> 0:02:25,000
elles vous suivent encore.

47
0:02:26.04,000 --> 0:02:29,000
Nous sommes un peu protégés contre cette manipulation bon marché.

48
0:02:29.096,000 --> 0:02:32,000
Nous levons les yeux au ciel et pensons : « Ça ne marche pas. »

49
0:02:33.72,000 --> 0:02:35,000
Mis à part qu'en ligne,

50
0:02:35.84,000 --> 0:02:38,000
les technologies numériques ne sont pas que des publicités.

51
0:02:40.24,000 --> 0:02:43,000
Pour le comprendre, réfléchissons à un exemple du monde physique.

52
0:02:43.84,000 --> 0:02:47,000
Vous savez comment à la caisse au supermarché, près du caissier,

53
0:02:48.52,000 --> 0:02:51,000
il y a des bonbons et chewing-gums à hauteur des yeux des enfants ?

54
0:02:52.8,000 --> 0:02:55,000
C'est conçu pour les faire pleurnicher auprès de leurs parents

55
0:02:56.32,000 --> 0:02:59,000
alors que les parents sont sur le point de payer.

56
0:03:00.04,000 --> 0:03:02,000
C'est une architecture de persuasion.

57
0:03:03.16,000 --> 0:03:06,000
Ce n'est pas sympa, mais ça marche.

58
0:03:06.28,000 --> 0:03:08,000
C'est pourquoi vous le voyez partout.

59
0:03:08.72,000 --> 0:03:09,000
Dans le monde physique,

60
0:03:10.44,000 --> 0:03:12,000
de telles architectures de persuasion sont limitées

61
0:03:12.96,000 --> 0:03:16,000
car on ne peut mettre qu'un certain nombre de choses près de la caisse.

62
0:03:17.8,000 --> 0:03:21,000
Les bonbons et chewing-gums sont les mêmes pour tout le monde,

63
0:03:22.12,000 --> 0:03:23,000
même si ça fonctionne

64
0:03:23.6,000 --> 0:03:27,000
surtout pour les gens ayant des petits êtres pleurnichant à leurs côtés.

65
0:03:29.16,000 --> 0:03:32,000
Dans le monde physique, nous vivons avec ces limites.

66
0:03:34.28,000 --> 0:03:35,000
Dans le monde numérique cependant,

67
0:03:36.24,000 --> 0:03:38,000
les architectures de persuasion peuvent être adaptées

68
0:03:38.84,000 --> 0:03:41,000
pour des milliards de personnes,

69
0:03:41.84,000 --> 0:03:44,000
elles peuvent cibler, déduire, comprendre

70
0:03:45.72,000 --> 0:03:47,000
et être déployées pour des individus,

71
0:03:48.64,000 --> 0:03:49,000
un par un,

72
0:03:49.88,000 --> 0:03:51,000
en déterminant nos faiblesses

73
0:03:52.04,000 --> 0:03:57,000
et elles peuvent être envoyées sur l'écran privé des téléphones

74
0:03:57.68,000 --> 0:03:59,000
afin que ce ne soit pas visible à nos yeux.

75
0:03:59.96,000 --> 0:04:,000
C'est différent.

76
0:04:01.24,000 --> 0:04:04,000
Ce n'est qu'une chose basique dont est capable l'intelligence artificielle.

77
0:04:04.84,000 --> 0:04:05,000
Prenons un exemple.

78
0:04:06.2,000 --> 0:04:08,000
Vous voulez vendre des billets d'avion pour Las Vegas.

79
0:04:08.92,000 --> 0:04:11,000
Dans l'ancien monde, vous pensiez à des segments démographiques à cibler

80
0:04:12.44,000 --> 0:04:14,000
d'après votre expérience et ce que vous pouvez imaginer.

81
0:04:15.56,000 --> 0:04:17,000
Vous pourriez faire de la publicité

82
0:04:18.4,000 --> 0:04:2,000
aux hommes ayant entre 25 et 35 ans

83
0:04:20.92,000 --> 0:04:23,000
ou aux gens qui ont une limite élevée sur leur carte de crédit

84
0:04:24.88,000 --> 0:04:25,000
ou aux couples à la retraite.

85
0:04:26.296,000 --> 0:04:27,000
C'est ce que vous auriez fait.

86
0:04:28.12,000 --> 0:04:3,000
Avec le big data et l'apprentissage des machines,

87
0:04:31.04,000 --> 0:04:32,000
ça ne marche plus comme ça.

88
0:04:33.32,000 --> 0:04:35,000
Pour l'imaginer,

89
0:04:35.52,000 --> 0:04:38,000
pensez à toutes les données que Facebook a sur vous :

90
0:04:39.4,000 --> 0:04:41,000
tous les status tapés,

91
0:04:41.96,000 --> 0:04:43,000
toutes les conversations Messenger,

92
0:04:44,000 --> 0:04:45,000
tous vos lieux de connexion,

93
0:04:48.4,000 --> 0:04:51,000
toutes les photos que vous avez téléchargées.

94
0:04:51.6,000 --> 0:04:54,000
Si vous tapez quelque chose, changez d'avis et le supprimez,

95
0:04:55.4,000 --> 0:04:58,000
Facebook le garde et l'analyse également.

96
0:04:59.16,000 --> 0:05:02,000
De plus en plus, il essaye de vous relier à vos données hors ligne.

97
0:05:03.12,000 --> 0:05:06,000
Il achète également des données auprès de courtiers de données.

98
0:05:06.32,000 --> 0:05:09,000
Ce pourrait être n'importe quoi, de vos documents financiers

99
0:05:09.76,000 --> 0:05:11,000
à votre historique de recherche.

100
0:05:12.36,000 --> 0:05:17,000
Aux Etats-Unis, de telles données sont systématiquement collectées,

101
0:05:17.8,000 --> 0:05:18,000
comparées et vendues.

102
0:05:20.32,000 --> 0:05:22,000
En Europe, les règles sont plus strictes.

103
0:05:23.68,000 --> 0:05:25,000
Ce qu'il se passe alors c'est que,

104
0:05:26.92,000 --> 0:05:27,000
en fouillant toutes ces données,

105
0:05:28.76,000 --> 0:05:3,000
ces algorithmes d'apprentissage des machines --

106
0:05:30.96,000 --> 0:05:33,000
ils sont appelés algorithmes d'apprentissage pour cette raison --

107
0:05:34.026,000 --> 0:05:37,000
ils apprennent à comprendre les caractéristiques des gens

108
0:05:38,000 --> 0:05:4,000
ayant déjà acheté des billets pour Las Vegas.

109
0:05:41.76,000 --> 0:05:44,000
Quand ils l'apprennent de données existantes,

110
0:05:45.32,000 --> 0:05:48,000
ils apprennent aussi comment l'appliquer à de nouvelles personnes.

111
0:05:49.16,000 --> 0:05:52,000
Face à une nouvelle personne,

112
0:05:52.24,000 --> 0:05:53,000
ils peuvent classifier

113
0:05:53.55,000 --> 0:05:57,000
si cette personne a des chances d'acheter un billet pour Las Vegas ou pas.

114
0:05:57.76,000 --> 0:06:02,000
Bien. Vous vous dites qu'une offre pour acheter des billets pour Las Vegas,

115
0:06:03.2,000 --> 0:06:04,000
vous pouvez l'ignorer.

116
0:06:04.68,000 --> 0:06:06,000
Mais le problème n'est pas là.

117
0:06:06.92,000 --> 0:06:07,000
Le problème,

118
0:06:08.24,000 --> 0:06:1,000
c'est que nous ne comprenons plus vraiment

119
0:06:10.49,000 --> 0:06:12,000
comment fonctionnent ces algorithmes complexes.

120
0:06:12.68,000 --> 0:06:15,000
Nous ne comprenons pas comment ils font cette catégorisation.

121
0:06:16.16,000 --> 0:06:2,000
Ce sont d'énormes matrices, des milliers de lignes et colonnes,

122
0:06:20.6,000 --> 0:06:21,000
peut-être même des millions,

123
0:06:23.32,000 --> 0:06:25,000
et ni les programmeurs,

124
0:06:26.76,000 --> 0:06:27,000
ni quiconque les regardant,

125
0:06:29.44,000 --> 0:06:3,000
même avec toutes les données,

126
0:06:30.96,000 --> 0:06:34,000
ne comprend plus comment ça opère exactement,

127
0:06:35.6,000 --> 0:06:38,000
pas plus que vous ne sauriez ce que je pense en ce moment

128
0:06:39.4,000 --> 0:06:42,000
si l'on vous montrait une coupe transversale de mon cerveau.

129
0:06:44.36,000 --> 0:06:46,000
C'est comme si nous ne programmions plus,

130
0:06:46.96,000 --> 0:06:5,000
nous élevons une intelligence que nous ne comprenons pas vraiment.

131
0:06:52.52,000 --> 0:06:55,000
Ces choses fonctionnent seulement s'il y a un énorme volume de données,

132
0:06:56.52,000 --> 0:07:01,000
elles pourraient donc aussi encourager une surveillance intensifiée à notre égard

133
0:07:01.64,000 --> 0:07:03,000
afin que les algorithmes d'apprentissage marchent.

134
0:07:03.97,000 --> 0:07:06,000
C'est pour ça que Facebook veut collecter toutes les données sur vous.

135
0:07:07.296,000 --> 0:07:08,000
Les algorithmes marchent mieux.

136
0:07:08.8,000 --> 0:07:1,000
Allons un peu plus loin avec cet exemple de Las Vegas.

137
0:07:11.52,000 --> 0:07:14,000
Et si le système que nous ne comprenons pas

138
0:07:16.2,000 --> 0:07:21,000
déterminait qu'il est plus simple de vendre des billets pour Las Vegas

139
0:07:21.36,000 --> 0:07:24,000
aux gens bipolaires qui sont sur le point d'entrer dans la phase maniaque.

140
0:07:25.64,000 --> 0:07:29,000
De telles personnes ont tendance à dépenser et parier de façon compulsive.

141
0:07:31.28,000 --> 0:07:35,000
Ils pourraient le faire, vous ignoreriez que c'était là leur conclusion.

142
0:07:35.76,000 --> 0:07:38,000
Une fois, j'ai donné cet exemple à quelques informaticiens

143
0:07:39.4,000 --> 0:07:41,000
et après, l'un d'eux est venu me voir.

144
0:07:41.48,000 --> 0:07:44,000
Il était préoccupé : « C'est pour ça que je n'ai pas pu le publier ».

145
0:07:45.6,000 --> 0:07:46,000
J'ai dit : « Publier quoi ? »

146
0:07:47.8,000 --> 0:07:52,000
Il avait essayé de voir s'il l'on pouvait détecter le début d'une manie

147
0:07:53.68,000 --> 0:07:56,000
d'après les posts sur les réseaux sociaux avant les symptômes cliniques

148
0:07:57.016,000 --> 0:07:58,000
et ça avait fonctionné,

149
0:07:58.72,000 --> 0:08:,000
ça avait très bien fonctionné,

150
0:08:00.8,000 --> 0:08:04,000
et il n'avait aucune idée de comment ça marchait ou ce que ça détectait.

151
0:08:06.84,000 --> 0:08:1,000
Le problème n'est pas résolu s'il ne le publie pas

152
0:08:11.28,000 --> 0:08:12,000
car il y a déjà des entreprises

153
0:08:13.2,000 --> 0:08:15,000
qui développent ce genre de technologie

154
0:08:15.76,000 --> 0:08:17,000
et beaucoup de choses existent déjà.

155
0:08:19.24,000 --> 0:08:21,000
Ce n'est plus très compliqué.

156
0:08:21.84,000 --> 0:08:24,000
Vous arrive-t-il d'aller sur YouTube pour regarder une vidéo

157
0:08:25.32,000 --> 0:08:27,000
et, une heure plus tard, d'en avoir regardées 27 ?

158
0:08:28.76,000 --> 0:08:3,000
Vous voyez cette colonne que YouTube a sur la droite,

159
0:08:31.28,000 --> 0:08:33,000
qui dit « A suivre »

160
0:08:33.52,000 --> 0:08:34,000
et qui se lance automatiquement ?

161
0:08:35.36,000 --> 0:08:36,000
C'est un algorithme

162
0:08:36.6,000 --> 0:08:39,000
qui choisit ce qu'il pense qui pourrait vous intéresser

163
0:08:40.24,000 --> 0:08:41,000
et que vous ne trouveriez pas seul.

164
0:08:41.906,000 --> 0:08:42,000
Ce n'est pas un humain,

165
0:08:43.08,000 --> 0:08:44,000
ce sont des algorithmes.

166
0:08:44.52,000 --> 0:08:48,000
Il considère ce que vous avez regardé et ce que d'autres comme vous ont regardé

167
0:08:49.28,000 --> 0:08:53,000
et il déduit que ce doit être ce qui vous intéresse,

168
0:08:53.52,000 --> 0:08:54,000
ce dont vous voulez voir plus

169
0:08:54.935,000 --> 0:08:55,000
et vous en montre plus.

170
0:08:56.159,000 --> 0:08:58,000
Ça semble être une fonction bénigne et utile,

171
0:08:59.28,000 --> 0:09:,000
mais ça ne l'est pas.

172
0:09:01.64,000 --> 0:09:07,000
En 2016, j'ai été à des rassemblements du candidat Donald Trump

173
0:09:09.84,000 --> 0:09:12,000
pour étudier en tant que chercheuse le mouvement le soutenant.

174
0:09:13.2,000 --> 0:09:16,000
J'étudie les mouvements sociaux, alors j'étudiais ça aussi.

175
0:09:16.68,000 --> 0:09:19,000
Puis j'ai voulu écrire quelque chose au sujet d'un de ses rassemblements

176
0:09:20.056,000 --> 0:09:21,000
alors je l'ai regardé sur YouTube.

177
0:09:23.24,000 --> 0:09:26,000
YouTube a commencé à me recommander

178
0:09:26.36,000 --> 0:09:3,000
et à lancer automatiquement des vidéos de suprématistes blancs

179
0:09:30.64,000 --> 0:09:32,000
étant de plus en plus extrémistes.

180
0:09:33.32,000 --> 0:09:34,000
Si j'en regardais une,

181
0:09:35.16,000 --> 0:09:37,000
il m'en recommandait une encore plus extrême

182
0:09:38.16,000 --> 0:09:39,000
et la lançais automatiquement.

183
0:09:40.32,000 --> 0:09:44,000
Si vous regardez du contenu d'Hillary Clinton ou Bernie Sanders,

184
0:09:44.88,000 --> 0:09:48,000
YouTube recommande et lance des vidéos gauchistes de conspiration

185
0:09:49.6,000 --> 0:09:5,000
et plus ça va, plus ça empire.

186
0:09:52.48,000 --> 0:09:55,000
Vous pensez peut-être que c'est de la politique, mais non.

187
0:09:55.56,000 --> 0:09:56,000
Il ne s'agit pas de politique.

188
0:09:57.006,000 --> 0:09:59,000
Ce n'est qu'un algorithme déterminant le comportement humain.

189
0:09:59.96,000 --> 0:10:03,000
Une fois, j'ai regardé une vidéo sur le végétarisme sur YouTube

190
0:10:04.76,000 --> 0:10:08,000
et YouTube a recommandé et lancé une vidéo sur le fait d'être végétalien.

191
0:10:09.72,000 --> 0:10:12,000
Vous n'êtes jamais assez extrême pour YouTube.

192
0:10:12.76,000 --> 0:10:13,000
(Rires)

193
0:10:14.36,000 --> 0:10:15,000
Que se passe-t-il ?

194
0:10:16.52,000 --> 0:10:19,000
L'algorithme de YouTube est propriétaire,

195
0:10:20.08,000 --> 0:10:22,000
mais voici ce qui, à mon avis, se passe.

196
0:10:23.36,000 --> 0:10:25,000
L'algorithme a déterminé

197
0:10:25.48,000 --> 0:10:28,000
que si vous pouvez pousser les gens

198
0:10:29.2,000 --> 0:10:32,000
à penser que vous pouvez leur montrer quelque chose de plus extrême,

199
0:10:32.96,000 --> 0:10:34,000
ils ont plus de chances de rester sur le site

200
0:10:35.4,000 --> 0:10:39,000
à regarder vidéo sur vidéo, descendant dans le terrier du lapin

201
0:10:39.84,000 --> 0:10:4,000
pendant que Google leur sert des pubs.

202
0:10:43.76,000 --> 0:10:46,000
Puisque personne ne fait attention à l'éthique du magasin,

203
0:10:47.72,000 --> 0:10:51,000
ces sites peuvent profiler des gens

204
0:10:53.68,000 --> 0:10:54,000
comme haïssant les juifs,

205
0:10:56.36,000 --> 0:10:58,000
pensant que les juifs sont des parasites,

206
0:11:00.32,000 --> 0:11:04,000
qui ont du contenu anti-sémite très explicite,

207
0:11:06.08,000 --> 0:11:08,000
et vous laisser les cibler avec des pubs.

208
0:11:09.2,000 --> 0:11:12,000
Ils peuvent mobiliser les algorithmes

209
0:11:12.76,000 --> 0:11:15,000
pour trouver des publics similaires,

210
0:11:15.92,000 --> 0:11:2,000
des gens n'ayant pas de contenu anti-sémite si explicite sur leur profil

211
0:11:21.52,000 --> 0:11:27,000
mais que les algorithmes détectent comme étant sensibles à de tels messages,

212
0:11:27.72,000 --> 0:11:29,000
et vous laisser les cibler avec des pubs.

213
0:11:30.68,000 --> 0:11:32,000
Ça peut sembler être un exemple peu plausible,

214
0:11:33.44,000 --> 0:11:34,000
mais c'est vrai.

215
0:11:35.48,000 --> 0:11:37,000
ProPublica a enquêté sur ça

216
0:11:37.64,000 --> 0:11:4,000
et a découvert que vous pouviez vraiment le faire sur Facebook

217
0:11:41.28,000 --> 0:11:43,000
et Facebook, serviable, offrait des suggestions

218
0:11:43.72,000 --> 0:11:44,000
sur comment étendre ce public.

219
0:11:46.72,000 --> 0:11:49,000
BuzzFeed l'a essayé pour Google et a rapidement découvert

220
0:11:49.76,000 --> 0:11:5,000
que vous pouvez le faire sur Google.

221
0:11:51.52,000 --> 0:11:52,000
Ce n'était même pas cher.

222
0:11:53.24,000 --> 0:11:57,000
Le reporter de ProPublica a dépensé environ 30 dollars

223
0:11:57.68,000 --> 0:11:59,000
pour cibler cette catégorie.

224
0:12:02.6,000 --> 0:12:06,000
L'année dernière, le responsable des réseaux sociaux de Donald Trump

225
0:12:06.96,000 --> 0:12:12,000
a révélé utiliser les dark posts de Facebook pour démobiliser les gens,

226
0:12:13.28,000 --> 0:12:14,000
pas pour les persuader,

227
0:12:14.68,000 --> 0:12:16,000
mais pour les convaincre de ne pas voter du tout.

228
0:12:18.52,000 --> 0:12:21,000
Pour ce faire, ils ont spécifiquement ciblé,

229
0:12:22.12,000 --> 0:12:25,000
par exemple, les hommes afro-américains dans des villes clés comme Philadelphie

230
0:12:26.04,000 --> 0:12:28,000
et je vais lire exactement ce qu'il a dit.

231
0:12:28.52,000 --> 0:12:29,000
Je cite.

232
0:12:29.76,000 --> 0:12:32,000
Ils utilisaient « des posts non publics

233
0:12:32.8,000 --> 0:12:34,000
dont la campagne contrôle l'audience

234
0:12:35,000 --> 0:12:38,000
afin que seuls les voient les gens dont nous voulons qu'ils les voient.

235
0:12:38.8,000 --> 0:12:39,000
Nous l'avons façonné.

236
0:12:40.04,000 --> 0:12:44,000
Ça affectera considérablement sa capacité à retourner ces gens. »

237
0:12:45.72,000 --> 0:12:47,000
Qu'y a-t-il dans ces dark posts ?

238
0:12:48.48,000 --> 0:12:49,000
Nous l'ignorons.

239
0:12:50.16,000 --> 0:12:51,000
Facebook refuse de le dire.

240
0:12:52.48,000 --> 0:12:56,000
Facebook arrange également algorithmiquement les posts

241
0:12:56.88,000 --> 0:12:59,000
que vos amis mettent sur Facebook ou des pages que vous suivez.

242
0:13:00.64,000 --> 0:13:02,000
Il ne vous montre pas tout chronologiquement.

243
0:13:02.88,000 --> 0:13:06,000
Il le met dans l'ordre qui, selon l'algorithme, devrait vous pousser

244
0:13:07.72,000 --> 0:13:08,000
à rester plus longtemps sur le site.

245
0:13:11.04,000 --> 0:13:14,000
Ça a de nombreuses conséquences.

246
0:13:14.44,000 --> 0:13:17,000
Vous pensez peut-être que quelqu'un vous ignore sur Facebook.

247
0:13:18.8,000 --> 0:13:21,000
L'algorithme ne lui montre peut-être jamais vos posts.

248
0:13:22.08,000 --> 0:13:27,000
L'algorithme en priorise certains et en enterre d'autres.

249
0:13:29.32,000 --> 0:13:3,000
Des expériences montrent

250
0:13:30.64,000 --> 0:13:32,000
que ce que l'algorithme choisit de vous montrer

251
0:13:33.6,000 --> 0:13:36,000
peut influencer vos émotions.

252
0:13:36.6,000 --> 0:13:37,000
Mais ce n'est pas tout.

253
0:13:38.28,000 --> 0:13:4,000
Ça influence aussi votre comportement politique.

254
0:13:41.36,000 --> 0:13:45,000
En 2010, aux élections de mi-mandat,

255
0:13:46.04,000 --> 0:13:51,000
Facebook a conduit une expérience sur 61 millions de personnes américaines

256
0:13:51.96,000 --> 0:13:52,000
qui a été révélée après les faits.

257
0:13:53.88,000 --> 0:13:56,000
Certaines personnes ont vu « Aujourd'hui, c'est jour d'élection »,

258
0:13:57.32,000 --> 0:13:58,000
le plus simple,

259
0:13:58.72,000 --> 0:14:01,000
et certaines personnes ont vu celui avec ce petit ajustement

260
0:14:02.64,000 --> 0:14:04,000
avec les petites photos

261
0:14:04.76,000 --> 0:14:06,000
de vos amis ayant cliqué sur « j'ai voté ».

262
0:14:09,000 --> 0:14:1,000
Ce simple ajustement.

263
0:14:11.52,000 --> 0:14:15,000
D'accord ? Les photos étaient la seule modification

264
0:14:15.84,000 --> 0:14:18,000
et ce post montré qu'une seule fois

265
0:14:19.12,000 --> 0:14:25,000
a attiré 340 000 électeurs supplémentaires

266
0:14:25.2,000 --> 0:14:26,000
dans cette élection,

267
0:14:26.92,000 --> 0:14:27,000
d'après cette recherche

268
0:14:28.64,000 --> 0:14:3,000
et la liste des électeurs l'a confirmé.

269
0:14:32.92,000 --> 0:14:33,000
Un hasard ? Non.

270
0:14:34.6,000 --> 0:14:39,000
Parce qu'en 2012, ils ont reconduit cette même expérience.

271
0:14:40.84,000 --> 0:14:41,000
A cette époque-là,

272
0:14:42.6,000 --> 0:14:45,000
ce message civique montré une seule fois

273
0:14:45.92,000 --> 0:14:49,000
a attiré 270 000 électeurs supplémentaires.

274
0:14:51.16,000 --> 0:14:56,000
En guise de référence, l'élection présidentielle de 2016

275
0:14:56.4,000 --> 0:14:59,000
s'est jouée à environ 100 000 votes.

276
0:15:01.36,000 --> 0:15:05,000
Facebook peut aussi très facilement déduire vos opinions politiques,

277
0:15:06.12,000 --> 0:15:08,000
même si vous ne les avez pas révélées sur le site.

278
0:15:08.456,000 --> 0:15:1,000
Ces algorithmes peuvent le faire assez facilement.

279
0:15:11.96,000 --> 0:15:14,000
Et si une plateforme avec un tel pouvoir

280
0:15:15.88,000 --> 0:15:2,000
décidait de retourner les partisans d'un candidat pour l'autre ?

281
0:15:21.68,000 --> 0:15:23,000
Comment le saurions-nous ?

282
0:15:25.56,000 --> 0:15:29,000
Nous sommes partis de quelque chose semble-t-il inoffensif --

283
0:15:29.72,000 --> 0:15:31,000
les pubs nous suivant partout en ligne --

284
0:15:31.96,000 --> 0:15:32,000
et sommes arrivés ailleurs.

285
0:15:35.48,000 --> 0:15:37,000
En tant que public et citoyens,

286
0:15:37.96,000 --> 0:15:4,000
nous ne savons plus si nous voyons les mêmes informations

287
0:15:41.4,000 --> 0:15:42,000
ou ce que voient les autres

288
0:15:43.68,000 --> 0:15:45,000
et sans base d'informations commune,

289
0:15:46.28,000 --> 0:15:47,000
peu à peu,

290
0:15:47.92,000 --> 0:15:5,000
le débat public devient impossible

291
0:15:51.16,000 --> 0:15:53,000
et nous n'en sommes qu'aux premières phases.

292
0:15:54.16,000 --> 0:15:57,000
Ces algorithmes peuvent facilement déduire

293
0:15:57.64,000 --> 0:16:01,000
des choses comme votre ethnie, vos opinions religieuses, politiques,

294
0:16:01.936,000 --> 0:16:02,000
vos traits de personnalité,

295
0:16:03.28,000 --> 0:16:06,000
votre intelligence, votre bonheur, votre usage de substances addictives,

296
0:16:06.68,000 --> 0:16:09,000
la situation maritale de vos parents, votre âge et votre sexe,

297
0:16:09.84,000 --> 0:16:11,000
uniquement grâce aux « J'aime » de Facebook.

298
0:16:13.44,000 --> 0:16:17,000
Ces algorithmes peuvent identifier des manifestants

299
0:16:17.52,000 --> 0:16:19,000
même si leur visage est partiellement caché.

300
0:16:21.72,000 --> 0:16:27,000
Ces algorithmes peuvent peut-être détecter l'orientation sexuelle des gens

301
0:16:28.36,000 --> 0:16:31,000
simplement grâce à leur photo de profil sur un site de rencontres.

302
0:16:33.56,000 --> 0:16:35,000
Ce sont des conjectures statistiques,

303
0:16:36.2,000 --> 0:16:38,000
ils n'auront pas raison à 100%,

304
0:16:39.12,000 --> 0:16:43,000
mais je ne vois pas les puissants résister à la tentation d'utiliser ces technologies

305
0:16:44.04,000 --> 0:16:46,000
simplement parce qu'il y a des faux positifs,

306
0:16:46.24,000 --> 0:16:49,000
ce qui, bien sûr, créera un couche supplémentaire de problèmes.

307
0:16:49.52,000 --> 0:16:51,000
Imaginez ce qu'un Etat peut faire

308
0:16:52.48,000 --> 0:16:55,000
avec l'énorme volume de données qu'il a sur ces citoyens.

309
0:16:56.68,000 --> 0:17:,000
La Chine utilise déjà une technologie de détection du visage

310
0:17:01.48,000 --> 0:17:03,000
pour identifier et arrêter des gens.

311
0:17:05.28,000 --> 0:17:07,000
Voici la tragédie :

312
0:17:07.44,000 --> 0:17:12,000
nous instaurons cette infrastructure de surveillance dautoritarisme

313
0:17:13,000 --> 0:17:15,000
simplement pour que les gens cliquent sur des pubs.

314
0:17:17.24,000 --> 0:17:19,000
Ce ne sera pas lautoritarisme d'Orwell.

315
0:17:19.839,000 --> 0:17:2,000
Ce n'est pas « 1984 ».

316
0:17:21.76,000 --> 0:17:25,000
Si l'autoritarisme une peur déclarée pour nous terroriser,

317
0:17:26.359,000 --> 0:17:28,000
nous aurons tous peur mais nous le saurons,

318
0:17:29.28,000 --> 0:17:31,000
nous détesterons ça et résisterons.

319
0:17:32.88,000 --> 0:17:36,000
Mais si les gens au pouvoir utilisent ces algorithmes

320
0:17:37.319,000 --> 0:17:4,000
pour nous surveiller discrètement,

321
0:17:40.72,000 --> 0:17:42,000
pour nous juger et nous inciter,

322
0:17:43.72,000 --> 0:17:47,000
pour prédire et identifier les fauteurs de trouble et les rebelles,

323
0:17:47.92,000 --> 0:17:5,000
pour déployer une architecture de persuasion à grande échelle

324
0:17:51.84,000 --> 0:17:55,000
et pour manipuler les individus un par un

325
0:17:56,000 --> 0:18:01,000
grâce à leurs faiblesses et vulnérabilités personnelles et individuelles

326
0:18:02.72,000 --> 0:18:04,000
et s'ils le font à grande échelle

327
0:18:06.08,000 --> 0:18:07,000
via nos écrans privés

328
0:18:07.84,000 --> 0:18:08,000
afin que nous ne sachions même pas

329
0:18:09.52,000 --> 0:18:11,000
ce que les autres citoyens et nos voisins voient,

330
0:18:13.56,000 --> 0:18:17,000
l'autoritarisme nous enveloppera tel une toile d'araignée

331
0:18:18.4,000 --> 0:18:2,000
et nous ignorons même que nous sommes dans la toile.

332
0:18:22.44,000 --> 0:18:24,000
La capitalisation du marché de Facebook

333
0:18:25.4,000 --> 0:18:28,000
approche le demi milliard de dollars.

334
0:18:28.72,000 --> 0:18:31,000
C'est parce que c'est une très bonne architecture de persuasion.

335
0:18:33.76,000 --> 0:18:35,000
Mais la structure de cette architecture

336
0:18:36.6,000 --> 0:18:39,000
est la même que vous vendiez des chaussures

337
0:18:39.84,000 --> 0:18:41,000
ou de la politique.

338
0:18:42.36,000 --> 0:18:45,000
Les algorithmes ne connaissent pas la différence.

339
0:18:46.24,000 --> 0:18:49,000
Les mêmes algorithmes utilisés sur nous

340
0:18:49.56,000 --> 0:18:52,000
pour nous rendre plus malléables face aux publicités

341
0:18:52.76,000 --> 0:18:58,000
organisent aussi nos flux d'informations politiques, personnelles et sociales

342
0:18:59.52,000 --> 0:19:,000
et ça doit changer.

343
0:19:02.24,000 --> 0:19:04,000
Ne vous méprenez pas,

344
0:19:04.56,000 --> 0:19:07,000
nous utilisons les plateformes numériques car elles ont beaucoup de valeur.

345
0:19:09.12,000 --> 0:19:1,000
J'utilise Facebook pour garder contact

346
0:19:11,000 --> 0:19:14,000
avec des amis et de la famille à travers le monde.

347
0:19:14,000 --> 0:19:19,000
J'ai écrit sur l'importance des réseaux sociaux pour les mouvements sociaux.

348
0:19:19.8,000 --> 0:19:22,000
J'ai étudié comment ces technologies peuvent être utilisées

349
0:19:22.84,000 --> 0:19:24,000
pour contourner la censure à travers le monde.

350
0:19:27.28,000 --> 0:19:33,000
Ce n'est pas que les dirigeants de Facebook ou Google

351
0:19:33.72,000 --> 0:19:35,000
essayent perfidement et délibérément

352
0:19:36.44,000 --> 0:19:4,000
de polariser plus le pays ou le monde

353
0:19:40.92,000 --> 0:19:41,000
et d'encourager l'extrémisme.

354
0:19:43.44,000 --> 0:19:46,000
J'ai lu les nombreuses déclarations bien intentionnées

355
0:19:47.44,000 --> 0:19:5,000
que ces gens ont publiées.

356
0:19:51.6,000 --> 0:19:54,000
Mais ce n'est pas l'intention ou les déclarations

357
0:19:54.68,000 --> 0:19:57,000
des gens dans les technologies qui comptent,

358
0:19:57.68,000 --> 0:20:,000
ce sont les structures et les modèles commerciaux qu'ils créent.

359
0:20:02.36,000 --> 0:20:04,000
C'est le cur du problème.

360
0:20:04.48,000 --> 0:20:08,000
Soit Facebook est un escroc d'un demi milliard de dollars

361
0:20:10.2,000 --> 0:20:11,000
et les pubs ne marchent pas sur le site,

362
0:20:12.12,000 --> 0:20:14,000
ça ne marche pas comme une architecture de persuasion

363
0:20:14.84,000 --> 0:20:18,000
ou son pouvoir d'influence est très préoccupant.

364
0:20:20.56,000 --> 0:20:21,000
C'est l'un ou l'autre.

365
0:20:22.36,000 --> 0:20:23,000
Il en va de même pour Google.

366
0:20:24.88,000 --> 0:20:26,000
Que pouvons-nous faire ?

367
0:20:27.36,000 --> 0:20:28,000
Ça doit changer.

368
0:20:29.32,000 --> 0:20:31,000
Je ne peux pas offrir de recette simple

369
0:20:31.92,000 --> 0:20:33,000
car nous devons restructurer

370
0:20:34.2,000 --> 0:20:37,000
tout le fonctionnement de notre technologie numérique.

371
0:20:37.24,000 --> 0:20:41,000
Tout, de la façon dont la technologie est développée

372
0:20:41.36,000 --> 0:20:44,000
à la façon dont les incitations, économiques et autres,

373
0:20:45.24,000 --> 0:20:47,000
sont intégrées au système.

374
0:20:48.48,000 --> 0:20:51,000
Nous devons faire face et gérer

375
0:20:51.96,000 --> 0:20:55,000
le manque de transparence créé par les algorithmes propriétaires,

376
0:20:56.64,000 --> 0:20:59,000
le défi structurel de l'opacité de l'apprentissage des machines,

377
0:21:00.48,000 --> 0:21:03,000
toutes ces données aveuglément collectées sur nous.

378
0:21:05,000 --> 0:21:07,000
Nous avons devant nous une tâche importante.

379
0:21:08.16,000 --> 0:21:1,000
Nous devons mobiliser notre technologie,

380
0:21:11.76,000 --> 0:21:12,000
notre créativité

381
0:21:13.36,000 --> 0:21:14,000
et oui, notre science politique

382
0:21:16.24,000 --> 0:21:18,000
pour créer une intelligence artificielle

383
0:21:18.92,000 --> 0:21:21,000
qui soutient nos objectifs humains

384
0:21:22.8,000 --> 0:21:25,000
mais qui est aussi contrainte par nos valeurs humaines.

385
0:21:27.6,000 --> 0:21:29,000
Je comprends que ce ne sera pas simple.

386
0:21:30.36,000 --> 0:21:33,000
Nous ne serons pas facilement d'accord sur le sens de ces termes.

387
0:21:34.92,000 --> 0:21:36,000
Mais si nous prenons au sérieux

388
0:21:38.24,000 --> 0:21:43,000
le fonctionnement de ces systèmes dont nous sommes si dépendants,

389
0:21:44.24,000 --> 0:21:48,000
je ne vois pas comment nous pouvons encore retarder cette conversation.

390
0:21:49.2,000 --> 0:21:54,000
Ces structures organisent notre fonctionnement

391
0:21:55.88,000 --> 0:21:59,000
et elles contrôlent ce que nous pouvons faire ou non.

392
0:22:00.84,000 --> 0:22:02,000
Beaucoup de ces plateformes financées par les pubs

393
0:22:03.32,000 --> 0:22:04,000
se proclament gratuites.

394
0:22:04.92,000 --> 0:22:08,000
Dans ce contexte, ça signifie que nous sommes le produit qui est vendu.

395
0:22:10.84,000 --> 0:22:12,000
Nous avons besoin d'une économie numérique

396
0:22:13.6,000 --> 0:22:16,000
où nos données et notre attention

397
0:22:17.12,000 --> 0:22:22,000
ne sont pas à vendre à l'autoritariste ou au démagogue le plus offrant.

398
0:22:23.16,000 --> 0:22:26,000
(Applaudissements)

399
0:22:30.48,000 --> 0:22:33,000
Pour revenir à cette paraphrase hollywoodienne,

400
0:22:33.76,000 --> 0:22:36,000
nous voulons que le grand pouvoir

401
0:22:37.52,000 --> 0:22:4,000
de l'intelligence artificielle et de la technologie numérique fleurisse

402
0:22:41.4,000 --> 0:22:45,000
mais pour ça , nous devons faire face à une grande menace,

403
0:22:46.36,000 --> 0:22:47,000
les yeux ouverts et maintenant.

404
0:22:48.32,000 --> 0:22:49,000
Merci.

405
0:22:49.56,000 --> 0:22:53,000
(Applaudissements)

