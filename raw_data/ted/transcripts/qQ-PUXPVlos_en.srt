1
0:00:12.944,000 --> 0:00:16,000
Back in the 1980s, actually, I gave my first talk at TED,

2
0:00:16.977,000 --> 0:00:2,000
and I brought some of the very, very first public demonstrations

3
0:00:21.263,000 --> 0:00:25,000
of virtual reality ever to the TED stage.

4
0:00:26.375,000 --> 0:00:32,000
And at that time, we knew that we were facing a knife-edge future

5
0:00:33.266,000 --> 0:00:38,000
where the technology we needed,

6
0:00:38.491,000 --> 0:00:39,000
the technology we loved,

7
0:00:40.366,000 --> 0:00:42,000
could also be our undoing.

8
0:00:43.266,000 --> 0:00:47,000
We knew that if we thought of our technology

9
0:00:47.381,000 --> 0:00:5,000
as a means to ever more power,

10
0:00:50.659,000 --> 0:00:53,000
if it was just a power trip, we'd eventually destroy ourselves.

11
0:00:54.39,000 --> 0:00:55,000
That's what happens

12
0:00:55.595,000 --> 0:00:57,000
when you're on a power trip and nothing else.

13
0:00:59.509,000 --> 0:01:02,000
So the idealism

14
0:01:02.922,000 --> 0:01:06,000
of digital culture back then

15
0:01:07.755,000 --> 0:01:11,000
was all about starting with that recognition of the possible darkness

16
0:01:12.518,000 --> 0:01:15,000
and trying to imagine a way to transcend it

17
0:01:15.892,000 --> 0:01:17,000
with beauty and creativity.

18
0:01:19.033,000 --> 0:01:25,000
I always used to end my early TED Talks with a rather horrifying line, which is,

19
0:01:26.478,000 --> 0:01:29,000
"We have a challenge.

20
0:01:30.368,000 --> 0:01:34,000
We have to create a culture around technology

21
0:01:34.416,000 --> 0:01:37,000
that is so beautiful, so meaningful,

22
0:01:38.408,000 --> 0:01:4,000
so deep, so endlessly creative,

23
0:01:40.973,000 --> 0:01:43,000
so filled with infinite potential

24
0:01:44.013,000 --> 0:01:47,000
that it draws us away from committing mass suicide."

25
0:01:48.519,000 --> 0:01:53,000
So we talked about extinction as being one and the same

26
0:01:54.131,000 --> 0:01:58,000
as the need to create an alluring, infinitely creative future.

27
0:01:59.639,000 --> 0:02:04,000
And I still believe that that alternative of creativity

28
0:02:05.045,000 --> 0:02:06,000
as an alternative to death

29
0:02:07.043,000 --> 0:02:08,000
is very real and true,

30
0:02:09.036,000 --> 0:02:1,000
maybe the most true thing there is.

31
0:02:11.87,000 --> 0:02:13,000
In the case of virtual reality --

32
0:02:13.989,000 --> 0:02:15,000
well, the way I used to talk about it

33
0:02:16.295,000 --> 0:02:18,000
is that it would be something like

34
0:02:18.954,000 --> 0:02:2,000
what happened when people discovered language.

35
0:02:21.828,000 --> 0:02:25,000
With language came new adventures, new depth, new meaning,

36
0:02:26.527,000 --> 0:02:28,000
new ways to connect, new ways to coordinate,

37
0:02:28.631,000 --> 0:02:32,000
new ways to imagine, new ways to raise children,

38
0:02:32.689,000 --> 0:02:36,000
and I imagined, with virtual reality, we'd have this new thing

39
0:02:36.975,000 --> 0:02:37,000
that would be like a conversation

40
0:02:38.592,000 --> 0:02:41,000
but also like waking-state intentional dreaming.

41
0:02:41.96,000 --> 0:02:43,000
We called it post-symbolic communication,

42
0:02:44.637,000 --> 0:02:48,000
because it would be like just directly making the thing you experienced

43
0:02:49.019,000 --> 0:02:52,000
instead of indirectly making symbols to refer to things.

44
0:02:53.466,000 --> 0:02:57,000
It was a beautiful vision, and it's one I still believe in,

45
0:02:57.828,000 --> 0:03:,000
and yet, haunting that beautiful vision

46
0:03:01.067,000 --> 0:03:04,000
was the dark side of how it could also turn out.

47
0:03:04.241,000 --> 0:03:09,000
And I suppose I could mention

48
0:03:09.313,000 --> 0:03:12,000
from one of the very earliest computer scientists,

49
0:03:12.401,000 --> 0:03:14,000
whose name was Norbert Wiener,

50
0:03:14.56,000 --> 0:03:17,000
and he wrote a book back in the '50s, from before I was even born,

51
0:03:18.338,000 --> 0:03:2,000
called "The Human Use of Human Beings."

52
0:03:21.779,000 --> 0:03:25,000
And in the book, he described the potential

53
0:03:25.975,000 --> 0:03:31,000
to create a computer system that would be gathering data from people

54
0:03:32.18,000 --> 0:03:35,000
and providing feedback to those people in real time

55
0:03:35.776,000 --> 0:03:4,000
in order to put them kind of partially, statistically, in a Skinner box,

56
0:03:40.935,000 --> 0:03:42,000
in a behaviorist system,

57
0:03:43.403,000 --> 0:03:45,000
and he has this amazing line where he says,

58
0:03:45.928,000 --> 0:03:47,000
one could imagine, as a thought experiment --

59
0:03:48.69,000 --> 0:03:5,000
and I'm paraphrasing, this isn't a quote --

60
0:03:51.175,000 --> 0:03:54,000
one could imagine a global computer system

61
0:03:54.279,000 --> 0:03:56,000
where everybody has devices on them all the time,

62
0:03:57.145,000 --> 0:04:,000
and the devices are giving them feedback based on what they did,

63
0:04:00.441,000 --> 0:04:01,000
and the whole population

64
0:04:02.34,000 --> 0:04:05,000
is subject to a degree of behavior modification.

65
0:04:05.94,000 --> 0:04:08,000
And such a society would be insane,

66
0:04:09.51,000 --> 0:04:12,000
could not survive, could not face its problems.

67
0:04:12.631,000 --> 0:04:14,000
And then he says, but this is only a thought experiment,

68
0:04:15.276,000 --> 0:04:18,000
and such a future is technologically infeasible.

69
0:04:18.72,000 --> 0:04:19,000
(Laughter)

70
0:04:19.836,000 --> 0:04:22,000
And yet, of course, it's what we have created,

71
0:04:22.862,000 --> 0:04:25,000
and it's what we must undo if we are to survive.

72
0:04:27.457,000 --> 0:04:28,000
So --

73
0:04:28.632,000 --> 0:04:31,000
(Applause)

74
0:04:32.631,000 --> 0:04:37,000
I believe that we made a very particular mistake,

75
0:04:38.632,000 --> 0:04:4,000
and it happened early on,

76
0:04:40.89,000 --> 0:04:42,000
and by understanding the mistake we made,

77
0:04:42.988,000 --> 0:04:43,000
we can undo it.

78
0:04:44.871,000 --> 0:04:46,000
It happened in the '90s,

79
0:04:47.454,000 --> 0:04:49,000
and going into the turn of the century,

80
0:04:50.22,000 --> 0:04:51,000
and here's what happened.

81
0:04:53.2,000 --> 0:04:54,000
Early digital culture,

82
0:04:54.598,000 --> 0:04:58,000
and indeed, digital culture to this day,

83
0:04:59.594,000 --> 0:05:05,000
had a sense of, I would say, lefty, socialist mission about it,

84
0:05:05.927,000 --> 0:05:07,000
that unlike other things that have been done,

85
0:05:08.111,000 --> 0:05:09,000
like the invention of books,

86
0:05:09.569,000 --> 0:05:12,000
everything on the internet must be purely public,

87
0:05:13.006,000 --> 0:05:15,000
must be available for free,

88
0:05:15.355,000 --> 0:05:18,000
because if even one person cannot afford it,

89
0:05:18.767,000 --> 0:05:2,000
then that would create this terrible inequity.

90
0:05:21.912,000 --> 0:05:23,000
Now of course, there's other ways to deal with that.

91
0:05:24.46,000 --> 0:05:27,000
If books cost money, you can have public libraries.

92
0:05:27.5,000 --> 0:05:28,000
And so forth.

93
0:05:28.698,000 --> 0:05:3,000
But we were thinking, no, no, no, this is an exception.

94
0:05:31.34,000 --> 0:05:35,000
This must be pure public commons, that's what we want.

95
0:05:35.969,000 --> 0:05:37,000
And so that spirit lives on.

96
0:05:38.627,000 --> 0:05:41,000
You can experience it in designs like the Wikipedia, for instance,

97
0:05:42.366,000 --> 0:05:43,000
many others.

98
0:05:43.731,000 --> 0:05:44,000
But at the same time,

99
0:05:45.629,000 --> 0:05:47,000
we also believed, with equal fervor,

100
0:05:48.241,000 --> 0:05:51,000
in this other thing that was completely incompatible,

101
0:05:52.202,000 --> 0:05:55,000
which is we loved our tech entrepreneurs.

102
0:05:55.853,000 --> 0:05:58,000
We loved Steve Jobs; we loved this Nietzschean myth

103
0:05:59.616,000 --> 0:06:02,000
of the techie who could dent the universe.

104
0:06:03.108,000 --> 0:06:04,000
Right?

105
0:06:04.45,000 --> 0:06:09,000
And that mythical power still has a hold on us, as well.

106
0:06:10.322,000 --> 0:06:14,000
So you have these two different passions,

107
0:06:14.805,000 --> 0:06:15,000
for making everything free

108
0:06:16.766,000 --> 0:06:21,000
and for the almost supernatural power of the tech entrepreneur.

109
0:06:21.956,000 --> 0:06:25,000
How do you celebrate entrepreneurship when everything's free?

110
0:06:26.332,000 --> 0:06:29,000
Well, there was only one solution back then,

111
0:06:29.481,000 --> 0:06:31,000
which was the advertising model.

112
0:06:31.592,000 --> 0:06:35,000
And so therefore, Google was born free, with ads,

113
0:06:35.619,000 --> 0:06:38,000
Facebook was born free, with ads.

114
0:06:39.325,000 --> 0:06:42,000
Now in the beginning, it was cute,

115
0:06:43.214,000 --> 0:06:44,000
like with the very earliest Google.

116
0:06:45.198,000 --> 0:06:46,000
(Laughter)

117
0:06:46.508,000 --> 0:06:48,000
The ads really were kind of ads.

118
0:06:49.429,000 --> 0:06:51,000
They would be, like, your local dentist or something.

119
0:06:51.938,000 --> 0:06:52,000
But there's thing called Moore's law

120
0:06:53.882,000 --> 0:06:56,000
that makes the computers more and more efficient and cheaper.

121
0:06:57.048,000 --> 0:06:58,000
Their algorithms get better.

122
0:06:58.93,000 --> 0:07:,000
We actually have universities where people study them,

123
0:07:01.55,000 --> 0:07:02,000
and they get better and better.

124
0:07:03.202,000 --> 0:07:07,000
And the customers and other entities who use these systems

125
0:07:07.678,000 --> 0:07:11,000
just got more and more experienced and got cleverer and cleverer.

126
0:07:11.829,000 --> 0:07:13,000
And what started out as advertising

127
0:07:14.25,000 --> 0:07:16,000
really can't be called advertising anymore.

128
0:07:16.751,000 --> 0:07:18,000
It turned into behavior modification,

129
0:07:19.687,000 --> 0:07:23,000
just as Norbert Wiener had worried it might.

130
0:07:24.204,000 --> 0:07:28,000
And so I can't call these things social networks anymore.

131
0:07:28.848,000 --> 0:07:31,000
I call them behavior modification empires.

132
0:07:32.686,000 --> 0:07:34,000
(Applause)

133
0:07:34.945,000 --> 0:07:38,000
And I refuse to vilify the individuals.

134
0:07:39.183,000 --> 0:07:41,000
I have dear friends at these companies,

135
0:07:41.478,000 --> 0:07:45,000
sold a company to Google, even though I think it's one of these empires.

136
0:07:46.262,000 --> 0:07:51,000
I don't think this is a matter of bad people who've done a bad thing.

137
0:07:51.346,000 --> 0:07:55,000
I think this is a matter of a globally tragic,

138
0:07:55.946,000 --> 0:07:59,000
astoundingly ridiculous mistake,

139
0:08:00.542,000 --> 0:08:04,000
rather than a wave of evil.

140
0:08:04.695,000 --> 0:08:06,000
Let me give you just another layer of detail

141
0:08:07.401,000 --> 0:08:1,000
into how this particular mistake functions.

142
0:08:11.337,000 --> 0:08:13,000
So with behaviorism,

143
0:08:14.068,000 --> 0:08:19,000
you give the creature, whether it's a rat or a dog or a person,

144
0:08:19.156,000 --> 0:08:21,000
little treats and sometimes little punishments

145
0:08:22.02,000 --> 0:08:23,000
as feedback to what they do.

146
0:08:24.71,000 --> 0:08:29,000
So if you have an animal in a cage, it might be candy and electric shocks.

147
0:08:30.646,000 --> 0:08:32,000
But if you have a smartphone,

148
0:08:33.194,000 --> 0:08:39,000
it's not those things, it's symbolic punishment and reward.

149
0:08:40.144,000 --> 0:08:42,000
Pavlov, one of the early behaviorists,

150
0:08:42.611,000 --> 0:08:44,000
demonstrated the famous principle.

151
0:08:45.587,000 --> 0:08:48,000
You could train a dog to salivate just with the bell, just with the symbol.

152
0:08:49.572,000 --> 0:08:5,000
So on social networks,

153
0:08:51.182,000 --> 0:08:56,000
social punishment and social reward function as the punishment and reward.

154
0:08:56.286,000 --> 0:08:58,000
And we all know the feeling of these things.

155
0:08:58.387,000 --> 0:08:59,000
You get this little thrill --

156
0:08:59.862,000 --> 0:09:01,000
"Somebody liked my stuff and it's being repeated."

157
0:09:02.236,000 --> 0:09:04,000
Or the punishment: "Oh my God, they don't like me,

158
0:09:04.594,000 --> 0:09:06,000
maybe somebody else is more popular, oh my God."

159
0:09:06.857,000 --> 0:09:08,000
So you have those two very common feelings,

160
0:09:09.107,000 --> 0:09:12,000
and they're doled out in such a way that you get caught in this loop.

161
0:09:12.695,000 --> 0:09:16,000
As has been publicly acknowledged by many of the founders of the system,

162
0:09:16.814,000 --> 0:09:18,000
everybody knew this is what was going on.

163
0:09:19.871,000 --> 0:09:2,000
But here's the thing:

164
0:09:21.514,000 --> 0:09:26,000
traditionally, in the academic study of the methods of behaviorism,

165
0:09:26.832,000 --> 0:09:31,000
there have been comparisons of positive and negative stimuli.

166
0:09:32.292,000 --> 0:09:34,000
In this setting, a commercial setting,

167
0:09:34.68,000 --> 0:09:35,000
there's a new kind of difference

168
0:09:36.3,000 --> 0:09:38,000
that has kind of evaded the academic world for a while,

169
0:09:39.093,000 --> 0:09:43,000
and that difference is that whether positive stimuli

170
0:09:43.165,000 --> 0:09:46,000
are more effective than negative ones in different circumstances,

171
0:09:46.498,000 --> 0:09:48,000
the negative ones are cheaper.

172
0:09:48.626,000 --> 0:09:5,000
They're the bargain stimuli.

173
0:09:50.706,000 --> 0:09:55,000
So what I mean by that is it's much easier

174
0:09:56.433,000 --> 0:09:59,000
to lose trust than to build trust.

175
0:09:59.573,000 --> 0:10:02,000
It takes a long time to build love.

176
0:10:02.769,000 --> 0:10:04,000
It takes a short time to ruin love.

177
0:10:05.399,000 --> 0:10:09,000
Now the customers of these behavior modification empires

178
0:10:10.011,000 --> 0:10:11,000
are on a very fast loop.

179
0:10:11.458,000 --> 0:10:13,000
They're almost like high-frequency traders.

180
0:10:13.527,000 --> 0:10:15,000
They're getting feedbacks from their spends

181
0:10:15.575,000 --> 0:10:17,000
or whatever their activities are if they're not spending,

182
0:10:18.401,000 --> 0:10:21,000
and they see what's working, and then they do more of that.

183
0:10:21.695,000 --> 0:10:23,000
And so they're getting the quick feedback,

184
0:10:23.759,000 --> 0:10:26,000
which means they're responding more to the negative emotions,

185
0:10:26.823,000 --> 0:10:29,000
because those are the ones that rise faster, right?

186
0:10:30.784,000 --> 0:10:33,000
And so therefore, even well-intentioned players

187
0:10:34.356,000 --> 0:10:36,000
who think all they're doing is advertising toothpaste

188
0:10:37.245,000 --> 0:10:4,000
end up advancing the cause of the negative people,

189
0:10:40.3,000 --> 0:10:42,000
the negative emotions, the cranks,

190
0:10:42.658,000 --> 0:10:43,000
the paranoids,

191
0:10:44.126,000 --> 0:10:47,000
the cynics, the nihilists.

192
0:10:47.23,000 --> 0:10:5,000
Those are the ones who get amplified by the system.

193
0:10:50.747,000 --> 0:10:55,000
And you can't pay one of these companies to make the world suddenly nice

194
0:10:56.422,000 --> 0:10:57,000
and improve democracy

195
0:10:57.597,000 --> 0:11:,000
nearly as easily as you can pay to ruin those things.

196
0:11:01.462,000 --> 0:11:04,000
And so this is the dilemma we've gotten ourselves into.

197
0:11:05.856,000 --> 0:11:1,000
The alternative is to turn back the clock, with great difficulty,

198
0:11:11.112,000 --> 0:11:13,000
and remake that decision.

199
0:11:13.977,000 --> 0:11:17,000
Remaking it would mean two things.

200
0:11:18.039,000 --> 0:11:21,000
It would mean first that many people, those who could afford to,

201
0:11:21.991,000 --> 0:11:23,000
would actually pay for these things.

202
0:11:24.222,000 --> 0:11:28,000
You'd pay for search, you'd pay for social networking.

203
0:11:28.653,000 --> 0:11:31,000
How would you pay? Maybe with a subscription fee,

204
0:11:32.138,000 --> 0:11:34,000
maybe with micro-payments as you use them.

205
0:11:34.9,000 --> 0:11:35,000
There's a lot of options.

206
0:11:36.726,000 --> 0:11:38,000
If some of you are recoiling, and you're thinking,

207
0:11:39.147,000 --> 0:11:41,000
"Oh my God, I would never pay for these things.

208
0:11:41.537,000 --> 0:11:43,000
How could you ever get anyone to pay?"

209
0:11:43.656,000 --> 0:11:46,000
I want to remind you of something that just happened.

210
0:11:46.919,000 --> 0:11:48,000
Around this same time

211
0:11:48.997,000 --> 0:11:53,000
that companies like Google and Facebook were formulating their free idea,

212
0:11:54.728,000 --> 0:11:58,000
a lot of cyber culture also believed that in the future,

213
0:11:59.256,000 --> 0:12:02,000
televisions and movies would be created in the same way,

214
0:12:02.302,000 --> 0:12:03,000
kind of like the Wikipedia.

215
0:12:04.456,000 --> 0:12:09,000
But then, companies like Netflix, Amazon, HBO,

216
0:12:09.544,000 --> 0:12:12,000
said, "Actually, you know, subscribe. We'll give you give you great TV."

217
0:12:13.307,000 --> 0:12:14,000
And it worked!

218
0:12:14.704,000 --> 0:12:17,000
We now are in this period called "peak TV," right?

219
0:12:18.602,000 --> 0:12:22,000
So sometimes when you pay for stuff, things get better.

220
0:12:22.824,000 --> 0:12:24,000
We can imagine a hypothetical --

221
0:12:25.134,000 --> 0:12:29,000
(Applause)

222
0:12:29.829,000 --> 0:12:32,000
We can imagine a hypothetical world of "peak social media."

223
0:12:33.512,000 --> 0:12:34,000
What would that be like?

224
0:12:34.885,000 --> 0:12:36,000
It would mean when you get on, you can get really useful,

225
0:12:37.679,000 --> 0:12:4,000
authoritative medical advice instead of cranks.

226
0:12:41.143,000 --> 0:12:44,000
It could mean when you want to get factual information,

227
0:12:44.477,000 --> 0:12:47,000
there's not a bunch of weird, paranoid conspiracy theories.

228
0:12:47.755,000 --> 0:12:51,000
We can imagine this wonderful other possibility.

229
0:12:52.014,000 --> 0:12:53,000
Ah.

230
0:12:53.299,000 --> 0:12:55,000
I dream of it. I believe it's possible.

231
0:12:55.453,000 --> 0:12:58,000
I'm certain it's possible.

232
0:12:58.779,000 --> 0:13:02,000
And I'm certain that the companies, the Googles and the Facebooks,

233
0:13:03.55,000 --> 0:13:05,000
would actually do better in this world.

234
0:13:05.886,000 --> 0:13:08,000
I don't believe we need to punish Silicon Valley.

235
0:13:09.076,000 --> 0:13:11,000
We just need to remake the decision.

236
0:13:12.702,000 --> 0:13:13,000
Of the big tech companies,

237
0:13:14.608,000 --> 0:13:19,000
it's really only two that depend on behavior modification and spying

238
0:13:20.195,000 --> 0:13:21,000
as their business plan.

239
0:13:21.476,000 --> 0:13:22,000
It's Google and Facebook.

240
0:13:23.259,000 --> 0:13:24,000
(Laughter)

241
0:13:24.593,000 --> 0:13:25,000
And I love you guys.

242
0:13:26.308,000 --> 0:13:28,000
Really, I do. Like, the people are fantastic.

243
0:13:30.371,000 --> 0:13:33,000
I want to point out, if I may,

244
0:13:33.577,000 --> 0:13:34,000
if you look at Google,

245
0:13:34.752,000 --> 0:13:39,000
they can propagate cost centers endlessly with all of these companies,

246
0:13:39.863,000 --> 0:13:41,000
but they cannot propagate profit centers.

247
0:13:41.935,000 --> 0:13:44,000
They cannot diversify, because they're hooked.

248
0:13:45.14,000 --> 0:13:47,000
They're hooked on this model, just like their own users.

249
0:13:47.791,000 --> 0:13:49,000
They're in the same trap as their users,

250
0:13:50.113,000 --> 0:13:52,000
and you can't run a big corporation that way.

251
0:13:52.641,000 --> 0:13:55,000
So this is ultimately totally in the benefit of the shareholders

252
0:13:56.268,000 --> 0:13:58,000
and other stakeholders of these companies.

253
0:13:58.737,000 --> 0:14:,000
It's a win-win solution.

254
0:14:01.111,000 --> 0:14:03,000
It'll just take some time to figure it out.

255
0:14:03.65,000 --> 0:14:05,000
A lot of details to work out,

256
0:14:05.936,000 --> 0:14:06,000
totally doable.

257
0:14:07.79,000 --> 0:14:09,000
(Laughter)

258
0:14:10.229,000 --> 0:14:13,000
I don't believe our species can survive unless we fix this.

259
0:14:14.087,000 --> 0:14:16,000
We cannot have a society

260
0:14:16.401,000 --> 0:14:18,000
in which, if two people wish to communicate,

261
0:14:19.386,000 --> 0:14:22,000
the only way that can happen is if it's financed by a third person

262
0:14:22.85,000 --> 0:14:24,000
who wishes to manipulate them.

263
0:14:25.22,000 --> 0:14:31,000
(Applause)

264
0:14:35.077,000 --> 0:14:36,000
(Applause ends)

265
0:14:36.942,000 --> 0:14:38,000
In the meantime, if the companies won't change,

266
0:14:39.911,000 --> 0:14:4,000
delete your accounts, OK?

267
0:14:41.601,000 --> 0:14:42,000
(Laughter)

268
0:14:42.894,000 --> 0:14:43,000
(Applause)

269
0:14:43.964,000 --> 0:14:44,000
That's enough for now.

270
0:14:45.497,000 --> 0:14:46,000
Thank you so much.

271
0:14:46.672,000 --> 0:14:52,000
(Applause)

