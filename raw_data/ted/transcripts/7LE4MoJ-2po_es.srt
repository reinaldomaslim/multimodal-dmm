1
0:00:,000 --> 0:00:07,000
Traductor: Lidia Cámara de la Fuente Revisor: Ciro Gomez

2
0:00:12.03,000 --> 0:00:15,000
Solo se tiene una oportunidad de causar una buena primera impresión,

3
0:00:15.44,000 --> 0:00:18,000
y eso es así, ya bien se sea un robot o una persona.

4
0:00:18.64,000 --> 0:00:21,000
La primera vez que conocí a uno de estos robots

5
0:00:21.68,000 --> 0:00:23,000
estaba en un lugar llamado Willow Garage en 2008.

6
0:00:24.6,000 --> 0:00:26,000
Cuando fui allí, mi anfitrión me acompañó al edificio

7
0:00:27.44,000 --> 0:00:28,000
y conocimos a este pequeño hombre.

8
0:00:29.24,000 --> 0:00:3,000
Estaba rodando por el pasillo,

9
0:00:30.92,000 --> 0:00:31,000
se acercó a mí, se sentó allí,

10
0:00:32.76,000 --> 0:00:34,000
me miró sin comprender,

11
0:00:35.04,000 --> 0:00:36,000
no hizo nada un rato,

12
0:00:36.72,000 --> 0:00:38,000
para rápidamente girar su cabeza 180 grados

13
0:00:38.966,000 --> 0:00:39,000
y huir.

14
0:00:40.24,000 --> 0:00:42,000
Y esa no fue una buena primera impresión.

15
0:00:42.44,000 --> 0:00:44,000
Lo que aprendí sobre los robots ese día

16
0:00:44.64,000 --> 0:00:46,000
es que hacen su trabajo

17
0:00:46.84,000 --> 0:00:48,000
sin ser plenamente conscientes de nosotros.

18
0:00:49,000 --> 0:00:52,000
Y creo que cuando experimentamos con estos posibles futuros de robots,

19
0:00:52.549,000 --> 0:00:54,000
en realidad aprendemos mucho más sobre nosotros mismos

20
0:00:55.166,000 --> 0:00:56,000
que sobre estas máquinas.

21
0:00:56.64,000 --> 0:00:57,000
Y lo que aprendí ese día

22
0:00:58,000 --> 0:01:01,000
fue que yo tenía muchas expectativas en este pequeño amigo.

23
0:01:01.44,000 --> 0:01:04,000
Pensaba que él no solo podía navegar el mundo físico,

24
0:01:04.64,000 --> 0:01:06,000
sino también mi mundo social,

25
0:01:07.32,000 --> 0:01:09,000
él está en mi espacio; es un robot personal.

26
0:01:09.52,000 --> 0:01:11,000
¿Por qué no me entendió?

27
0:01:11.56,000 --> 0:01:12,000
Mi anfitrión me explicó,

28
0:01:12.84,000 --> 0:01:15,000
"El robot intenta ir del punto A al punto B,

29
0:01:16.04,000 --> 0:01:17,000
y eras un obstáculo en su camino,

30
0:01:17.84,000 --> 0:01:19,000
así que tuvo que replanificar su camino,

31
0:01:19.88,000 --> 0:01:2,000
averiguar a dónde ir,

32
0:01:21.09,000 --> 0:01:22,000
para luego llegar de otra manera".

33
0:01:22.74,000 --> 0:01:24,000
Lo que en realidad no resulto ser algo muy eficiente.

34
0:01:25.28,000 --> 0:01:28,000
Si ese robot hubiera descubierto que yo era una persona, no una silla,

35
0:01:28.64,000 --> 0:01:3,000
y que estaba dispuesta a apartarme de su camino

36
0:01:30.966,000 --> 0:01:31,000
si él quería llegar a alguna parte,

37
0:01:32.696,000 --> 0:01:34,000
entonces realmente habría sido más eficiente

38
0:01:34.846,000 --> 0:01:35,000
al hacer su trabajo

39
0:01:35.96,000 --> 0:01:37,000
si se hubiera molestado en entender que yo era un humano

40
0:01:38.716,000 --> 0:01:42,000
y que tengo posibilidades distintas de las que tienen objetos como sillas y paredes.

41
0:01:42.866,000 --> 0:01:44,000
Tendemos a pensar que estos robots son del espacio exterior

42
0:01:45.736,000 --> 0:01:46,000
y del futuro y de la ciencia ficción.

43
0:01:47.576,000 --> 0:01:48,000
Si bien eso podría ser cierto,

44
0:01:49.106,000 --> 0:01:52,000
me gustaría exponer que los robots están aquí hoy,

45
0:01:52.216,000 --> 0:01:54,000
y viven y trabajan entre nosotros ahora.

46
0:01:54.486,000 --> 0:01:56,000
Estos son dos robots que viven en mi casa.

47
0:01:57,000 --> 0:01:59,000
Uno aspira los suelos y el otro corta la hierba

48
0:01:59.52,000 --> 0:02:,000
todos los días.

49
0:02:00.76,000 --> 0:02:03,000
Lo que es más de lo que yo haría si tuviera tiempo para hacer estas tareas,

50
0:02:04.466,000 --> 0:02:06,000
y probablemente lo hacen mejor de lo yo lo haría.

51
0:02:06.976,000 --> 0:02:08,000
Este realmente se ocupa de mi gatito.

52
0:02:09.04,000 --> 0:02:11,000
Cada vez que usa la caja, la limpia,

53
0:02:11.64,000 --> 0:02:12,000
que no es algo que tenga ganas de hacer,

54
0:02:13.64,000 --> 0:02:15,000
y hace mejor la vida de mi gato y la mía.

55
0:02:16.24,000 --> 0:02:18,000
Y mientras llamamos a estos robots:

56
0:02:18.68,000 --> 0:02:2,000
es un "robot aspirador, es un robot cortacésped,

57
0:02:21.4,000 --> 0:02:22,000
es una caja de robot más pequeña",

58
0:02:23.135,000 --> 0:02:26,000
creo que hay muchos otros robots escondidos a plena vista

59
0:02:27.08,000 --> 0:02:29,000
que se han vuelto tan increíblemente útiles

60
0:02:29.126,000 --> 0:02:3,000
y mundanos

61
0:02:30.31,000 --> 0:02:32,000
que los llamamos, por ejemplo, "lavavajillas", ¿verdad?

62
0:02:33.006,000 --> 0:02:34,000
Reciben nuevos nombres.

63
0:02:34.2,000 --> 0:02:35,000
Ya no reciben el nombre de robot

64
0:02:35.81,000 --> 0:02:37,000
porque sirven para un objetivo en nuestras vidas.

65
0:02:38.18,000 --> 0:02:39,000
Igualmente, un termostato, ¿verdad?

66
0:02:39.856,000 --> 0:02:4,000
Sé que mis amigos robotistas

67
0:02:41.27,000 --> 0:02:44,000
quizá se encojan de hombros al verme llamar a esto un robot,

68
0:02:44.356,000 --> 0:02:45,000
pero sirve a un objetivo.

69
0:02:45.696,000 --> 0:02:47,000
Su objetivo es tener mi casa a la temperatura de 19 ºC

70
0:02:48.366,000 --> 0:02:49,000
y percibir el entorno.

71
0:02:49.52,000 --> 0:02:5,000
Sabe que está un poco frío,

72
0:02:51.12,000 --> 0:02:53,000
hace un plan y luego actúa en el mundo físico.

73
0:02:53.76,000 --> 0:02:54,000
Es robótica.

74
0:02:55.04,000 --> 0:02:57,000
Incluso si no se parece a Rosie the Robot,

75
0:02:57.64,000 --> 0:02:59,000
está haciendo algo que es realmente útil en mi vida

76
0:03:00.6,000 --> 0:03:01,000
para no tener que preocuparme

77
0:03:02.186,000 --> 0:03:04,000
de subir y bajar la temperatura yo misma.

78
0:03:04.6,000 --> 0:03:07,000
Y creo que estos sistemas viven y trabajan entre nosotros ahora,

79
0:03:08.44,000 --> 0:03:1,000
y no solo estos sistemas viven entre nosotros,

80
0:03:10.68,000 --> 0:03:12,000
sino que probablemente también sean usuarios de robots.

81
0:03:13.48,000 --> 0:03:14,000
Cuando conducen el auto,

82
0:03:14.76,000 --> 0:03:16,000
es como estar manejando maquinaria.

83
0:03:17,000 --> 0:03:19,000
También uno va del punto A al punto B,

84
0:03:19.84,000 --> 0:03:21,000
pero su auto probablemente disponga de dirección asistida,

85
0:03:22.736,000 --> 0:03:24,000
probablemente tenga un sistema de frenado automático,

86
0:03:25.346,000 --> 0:03:28,000
o un cambio de marchas automático y quizá control de velocidad adaptativo.

87
0:03:28.966,000 --> 0:03:3,000
Y aunque no sea un automóvil completamente autónomo,

88
0:03:31.52,000 --> 0:03:32,000
tiene pedazos de autonomía,

89
0:03:32.84,000 --> 0:03:33,000
y son muy útiles

90
0:03:34.2,000 --> 0:03:35,000
y nos hacen manejar más seguros,

91
0:03:36.04,000 --> 0:03:39,000
y parece como si fueran invisibles al usarlos, ¿verdad?

92
0:03:39.72,000 --> 0:03:4,000
Cuando manejamos el auto,

93
0:03:41.32,000 --> 0:03:44,000
simplemente sentimos que vamos de un lugar a otro.

94
0:03:44.44,000 --> 0:03:47,000
No parece algo especial lidiar y operar

95
0:03:47.92,000 --> 0:03:48,000
y utilizar controles

96
0:03:49.02,000 --> 0:03:51,000
porque pasamos tanto tiempo aprendiendo cómo conducir

97
0:03:51.576,000 --> 0:03:54,000
que los actos se han convertido en extensiones de nosotros mismos.

98
0:03:54.686,000 --> 0:03:56,000
Cuando estacionamos el auto en ese espacio estrecho de garaje,

99
0:03:57.656,000 --> 0:03:58,000
sabemos dónde están los rincones.

100
0:03:59.256,000 --> 0:04:02,000
Y cuando conducimos un auto alquilado que no hemos conducido antes,

101
0:04:02.466,000 --> 0:04:04,000
toma algo de tiempo acostumbrarse a su nuevo cuerpo de robot.

102
0:04:05.426,000 --> 0:04:08,000
Y esto también aplica a las personas que manejan otros tipos de robots.

103
0:04:09.08,000 --> 0:04:11,000
Me gustaría compartir con Uds. algunas historias sobre eso.

104
0:04:12.05,000 --> 0:04:14,000
Lidiando con el problema de la colaboración remota.

105
0:04:14.51,000 --> 0:04:16,000
En Willow Garage tenía un compañero de trabajo llamado Dallas,

106
0:04:17.476,000 --> 0:04:18,000
y Dallas era así.

107
0:04:18.8,000 --> 0:04:21,000
Trabajaba desde su casa en Indiana en nuestra compañía en California.

108
0:04:22.11,000 --> 0:04:25,000
Él era una voz en una caja sobre la mesa en la mayoría de nuestras reuniones,

109
0:04:25.84,000 --> 0:04:27,000
lo que estaba bien, excepto

110
0:04:28.079,000 --> 0:04:31,000
si teníamos un debate acalorado y no nos gustaba lo que decía,

111
0:04:31.23,000 --> 0:04:32,000
podíamos simplemente colgar el teléfono.

112
0:04:33.226,000 --> 0:04:33,000
(Risas)

113
0:04:33.959,000 --> 0:04:35,000
Y podíamos tener una reunión tras esa reunión

114
0:04:36.2,000 --> 0:04:38,000
y tomar las decisiones en el pasillo

115
0:04:38.92,000 --> 0:04:39,000
cuando él ya no estaba allí.

116
0:04:40.36,000 --> 0:04:41,000
Y eso no era tan bueno para él.

117
0:04:41.96,000 --> 0:04:42,000
Y como compañía robótica en Willow,

118
0:04:43.72,000 --> 0:04:45,000
teníamos algunas partes extra de cuerpo del robot por ahí,

119
0:04:46.586,000 --> 0:04:47,000
y Dallas y su amigo Curt armaron esto,

120
0:04:48.6,000 --> 0:04:5,000
que se parece a Skype en un palo sobre ruedas,

121
0:04:51.56,000 --> 0:04:52,000
que parece un juguete teki, tonto,

122
0:04:53.32,000 --> 0:04:56,000
pero probablemente sea una de las herramientas más poderosas

123
0:04:56.486,000 --> 0:04:58,000
que he visto para colaboración remota.

124
0:04:59.16,000 --> 0:05:02,000
Si no respondía la pregunta de Dallas por correo electrónico,

125
0:05:02.68,000 --> 0:05:04,000
él podía, literalmente, rodar por mi oficina,

126
0:05:04.92,000 --> 0:05:06,000
bloquear mi puerta y hacerme la pregunta nuevamente

127
0:05:07.52,000 --> 0:05:08,000
(Risas)

128
0:05:08.56,000 --> 0:05:09,000
hasta que yo respondía.

129
0:05:09.8,000 --> 0:05:11,000
Y no voy a rechazarlo, ¿verdad? Eso sería grosero.

130
0:05:12.8,000 --> 0:05:14,000
No solo era bueno para estas comunicaciones uno-a-uno,

131
0:05:15.52,000 --> 0:05:17,000
sino también en la reunión general de la compañía.

132
0:05:18.48,000 --> 0:05:19,000
Sentarse en esa silla

133
0:05:19.8,000 --> 0:05:22,000
y mostrar a las personas que estás presente y comprometido con su proyecto

134
0:05:23.44,000 --> 0:05:24,000
es algo bueno

135
0:05:24.72,000 --> 0:05:26,000
y puede ayudar mucho a la colaboración remota.

136
0:05:27.036,000 --> 0:05:29,000
Vimos esto durante el período de meses y luego años,

137
0:05:29.8,000 --> 0:05:31,000
no solo en nuestra empresa, sino también en otras.

138
0:05:32.72,000 --> 0:05:34,000
Lo mejor que puede pasar con estos sistemas

139
0:05:35.08,000 --> 0:05:37,000
es que se perciben como si estuviera allí.

140
0:05:37.44,000 --> 0:05:38,000
Solo eres tú, es solo tu cuerpo,

141
0:05:39.01,000 --> 0:05:42,000
y así la gente en realidad comienza a darle a estas cosas un espacio personal.

142
0:05:42.836,000 --> 0:05:43,000
Y, cuando tienes una reunión de pie,

143
0:05:44.676,000 --> 0:05:46,000
la gente se sitúa alrededor del espacio

144
0:05:46.776,000 --> 0:05:48,000
como lo harían si uno estuviera allí en persona.

145
0:05:49.316,000 --> 0:05:5,000
Eso es genial hasta que hay averías.

146
0:05:51.236,000 --> 0:05:53,000
La gente, cuando ve estos robots por primera vez,

147
0:05:53.726,000 --> 0:05:56,000
dice, "Guau, ¿dónde están las piezas? Debe haber una cámara allí".

148
0:05:56.946,000 --> 0:05:57,000
Y comienzan a hurgar la cara.

149
0:05:58.406,000 --> 0:06:,000
"Estás hablando demasiado bajo, voy a subir tu volumen"

150
0:06:01.056,000 --> 0:06:03,000
que es como tener un compañero caminando hacia ti y decirle,

151
0:06:03.926,000 --> 0:06:05,000
"Estás hablando en voz muy baja, voy a alzarte la cara".

152
0:06:06.896,000 --> 0:06:07,000
Eso es incómodo y no está bien,

153
0:06:08.496,000 --> 0:06:1,000
y así construimos estas nuevas normas sociales

154
0:06:10.986,000 --> 0:06:11,000
al usar estos sistemas.

155
0:06:12.68,000 --> 0:06:15,000
De forma similar a como uno siente que es su cuerpo,

156
0:06:16.12,000 --> 0:06:19,000
uno comienzas a notar cosas como, "Oh, mi robot es bajito".

157
0:06:19.84,000 --> 0:06:21,000
Dallas me decía que medía 1.80 m,

158
0:06:22.52,000 --> 0:06:25,000
y lo llevaban a cócteles y cosas así,

159
0:06:26,000 --> 0:06:27,000
como lo hacen Uds.,

160
0:06:27.24,000 --> 0:06:3,000
y el robot medía aproximadamente 1.5 m, que es más o menos mi altura.

161
0:06:30.6,000 --> 0:06:31,000
Y él me dijo,

162
0:06:31.84,000 --> 0:06:33,000
"Sabes, la gente realmente no me está mirando.

163
0:06:34.4,000 --> 0:06:36,000
Siento que solo estoy viendo este mar de hombros

164
0:06:37.12,000 --> 0:06:38,000
y necesitamos un robot más alto".

165
0:06:39.12,000 --> 0:06:4,000
Y le dije,

166
0:06:40.4,000 --> 0:06:41,000
"Mmm no.

167
0:06:41.5,000 --> 0:06:43,000
Tienes que caminar en mis zapatos solo hoy.

168
0:06:43.68,000 --> 0:06:46,000
Podrás ver lo que es estar en el extremo más corto del espectro".

169
0:06:47.24,000 --> 0:06:5,000
Y en realidad él generó mucha empatía con esa experiencia,

170
0:06:50.64,000 --> 0:06:51,000
lo cual fue genial.

171
0:06:51.92,000 --> 0:06:52,000
Y cuando vino de visita en persona,

172
0:06:53.806,000 --> 0:06:55,000
ya no se izaba ante mí mientras me hablaba,

173
0:06:56.04,000 --> 0:06:58,000
se sentaba y me hablaba cara a cara,

174
0:06:58.16,000 --> 0:06:59,000
lo que fue algo hermoso.

175
0:06:59.92,000 --> 0:07:01,000
Y decidimos ver esto en el laboratorio

176
0:07:02.35,000 --> 0:07:05,000
y ver qué otras diferencias podrían hacer cosas como la altura del robot.

177
0:07:05.91,000 --> 0:07:08,000
Y la mitad de las personas en nuestro estudio usaron un robot más bajo,

178
0:07:09.366,000 --> 0:07:11,000
la otra mitad en nuestro estudio usaron un robot más alto.

179
0:07:12.156,000 --> 0:07:14,000
Y comprobamos que exactamente la misma persona

180
0:07:14.356,000 --> 0:07:16,000
con el mismo cuerpo que dice las mismas cosas que alguien,

181
0:07:17.24,000 --> 0:07:19,000
es más persuasivo y se percibe como más creíble

182
0:07:19.88,000 --> 0:07:2,000
si lo emite un robot de talla más alta.

183
0:07:21.746,000 --> 0:07:22,000
No tiene sentido racional,

184
0:07:23.4,000 --> 0:07:24,000
por eso estudiamos psicología.

185
0:07:25.12,000 --> 0:07:27,000
Y realmente, la forma en que Cliff Nass pondría esto

186
0:07:28,000 --> 0:07:31,000
es que tenemos que lidiar con estas nuevas tecnologías

187
0:07:31.04,000 --> 0:07:33,000
a pesar del hecho de tener cerebros muy viejos.

188
0:07:33.4,000 --> 0:07:36,000
La psicología humana no cambia a la misma velocidad que la tecnología

189
0:07:36.686,000 --> 0:07:37,000
y siempre jugamos a alcanzarla,

190
0:07:38.316,000 --> 0:07:39,000
tratando de dar sentido a este mundo

191
0:07:40.146,000 --> 0:07:42,000
en el que estas cosas autónomas funcionan alrededor.

192
0:07:42.58,000 --> 0:07:45,000
Por lo general, las cosas que hablan son personas, no máquinas.

193
0:07:45.586,000 --> 0:07:49,000
Y otorgamos mucho significado a cosas como la altura de una máquina,

194
0:07:50.04,000 --> 0:07:51,000
no una persona,

195
0:07:51.32,000 --> 0:07:53,000
y a atribuir eso a la persona que usa el sistema.

196
0:07:55.12,000 --> 0:07:57,000
Creo que esto es realmente importante

197
0:07:57.36,000 --> 0:07:58,000
cuando se piensa en robótica.

198
0:07:59.12,000 --> 0:08:01,000
No se trata tanto de reinventar a los humanos,

199
0:08:01.386,000 --> 0:08:03,000
se trata más de averiguar cómo nos extendemos, ¿verdad?

200
0:08:04.4,000 --> 0:08:06,000
Y terminamos usando cosas de una manera sorprendente.

201
0:08:07.4,000 --> 0:08:11,000
Y estos tipos no pueden jugar al billar porque los robots no tienen brazos,

202
0:08:11.68,000 --> 0:08:13,000
pero pueden coordinar a los tipos que están jugando al billar

203
0:08:14.656,000 --> 0:08:16,000
y eso puede ser algo importante para la unión de equipo,

204
0:08:17.436,000 --> 0:08:18,000
que sea algo ordenado.

205
0:08:18.766,000 --> 0:08:2,000
Los que se vuelvan muy buenos operando estos sistemas

206
0:08:21.276,000 --> 0:08:23,000
incluso harán cosas como inventar juegos nuevos,

207
0:08:23.566,000 --> 0:08:25,000
como el fútbol robot en el medio de la noche,

208
0:08:25.716,000 --> 0:08:26,000
empujando los botes de basura.

209
0:08:27.156,000 --> 0:08:28,000
Pero no todos lo hacen bien.

210
0:08:28.536,000 --> 0:08:3,000
Mucha gente tiene problemas para manejar estos sistemas.

211
0:08:31.246,000 --> 0:08:33,000
Este es un tipo que inició sesión vía robot

212
0:08:33.33,000 --> 0:08:35,000
y su globo ocular giró 90 grados hacia la izquierda.

213
0:08:35.92,000 --> 0:08:35,000
Él no sabía eso,

214
0:08:36.79,000 --> 0:08:38,000
así que terminó dando golpes por la oficina,

215
0:08:38.92,000 --> 0:08:41,000
corriendo hacia las mesas de las personas, sintiéndose muy avergonzado,

216
0:08:42.406,000 --> 0:08:44,000
riendo nervioso con un volumen demasiado alto.

217
0:08:44.616,000 --> 0:08:46,000
Y este tipo aquí en la imagen me está diciendo,

218
0:08:46.916,000 --> 0:08:48,000
"Necesitamos un botón para silenciar al robot".

219
0:08:49.286,000 --> 0:08:51,000
Y con eso quiso decir que no quería que fuera tan molesto.

220
0:08:52.266,000 --> 0:08:53,000
Y, como compañía de robótica,

221
0:08:53.786,000 --> 0:08:55,000
añadimos funciones para evitar obstáculos al sistema.

222
0:08:56.446,000 --> 0:08:59,000
Tenía un pequeño buscador láser con el que veía los obstáculos,

223
0:08:59.506,000 --> 0:09:01,000
y si yo, como operadora del robot decía, ve hacia la silla,

224
0:09:02.376,000 --> 0:09:04,000
no lo haría, solo planearía un camino alternativo,

225
0:09:04.886,000 --> 0:09:05,000
lo que parece ser una buena idea.

226
0:09:06.64,000 --> 0:09:09,000
La gente golpeó menos obstáculos usando ese sistema, obviamente,

227
0:09:09.84,000 --> 0:09:11,000
pero en realidad, a algunas personas,

228
0:09:11.92,000 --> 0:09:14,000
les tomó mucho más tiempo superar nuestra carrera de obstáculos,

229
0:09:15.216,000 --> 0:09:16,000
y queríamos saber por qué.

230
0:09:17.08,000 --> 0:09:2,000
Existe esta importante dimensión humana:

231
0:09:20.16,000 --> 0:09:22,000
una dimensión de la personalidad llamada locus de control.

232
0:09:22.976,000 --> 0:09:24,000
Y las personas con un fuerte locus interno de control,

233
0:09:25.64,000 --> 0:09:28,000
necesitan ser dueños de su propio destino:

234
0:09:28.72,000 --> 0:09:31,000
realmente no les gusta ceder el control a un sistema autónomo,

235
0:09:31.84,000 --> 0:09:33,000
tanto que lucharán contra la autonomía;

236
0:09:34,000 --> 0:09:37,000
"Si quiero golpear esa silla, voy a golpear esa silla".

237
0:09:37.12,000 --> 0:09:4,000
Y ellos realmente sufrirían de tener esa asistencia autónoma,

238
0:09:40.76,000 --> 0:09:42,000
lo que es importante saber para nosotros,

239
0:09:43.36,000 --> 0:09:46,000
ya que estamos construyendo autos cada vez más autónomos, por ejemplo.

240
0:09:46.886,000 --> 0:09:49,000
¿Cómo van a lidiar las personas con esa pérdida de control?

241
0:09:50.88,000 --> 0:09:52,000
Va a ser diferente según las dimensiones humanas.

242
0:09:53.6,000 --> 0:09:56,000
No podemos tratar a los humanos como un ente monolítico.

243
0:09:57.12,000 --> 0:09:59,000
Variamos por personalidad, por cultura,

244
0:09:59.56,000 --> 0:10:01,000
incluso variamos por estado emocional momento a momento.

245
0:10:02.336,000 --> 0:10:03,000
Y al diseñar estos sistemas,

246
0:10:04.04,000 --> 0:10:06,000
estos sistemas de interacción humano-robot,

247
0:10:06.36,000 --> 0:10:08,000
debemos tener en cuenta las dimensiones humanas,

248
0:10:09.12,000 --> 0:10:1,000
no solo las tecnológicas.

249
0:10:11.64,000 --> 0:10:15,000
Con una sensación de control, también surge un sentido de responsabilidad.

250
0:10:15.96,000 --> 0:10:17,000
Y si fueran operadores de robots usando uno de estos sistemas,

251
0:10:18.946,000 --> 0:10:19,000
así se vería la interfaz.

252
0:10:20.92,000 --> 0:10:21,000
Se parece un poco a un videojuego,

253
0:10:22.88,000 --> 0:10:25,000
lo que puede ser bueno porque resulta muy familiar a las personas,

254
0:10:26.066,000 --> 0:10:27,000
pero también puede ser malo

255
0:10:27.486,000 --> 0:10:29,000
porque hace que las personas sientan que es un videojuego.

256
0:10:30.416,000 --> 0:10:32,000
Tuvimos un montón de niños en Stanford jugando con el sistema

257
0:10:33.316,000 --> 0:10:35,000
conduciendo el robot por nuestra oficina en Menlo Park,

258
0:10:35.956,000 --> 0:10:36,000
y los niños decían cosas como,

259
0:10:37.456,000 --> 0:10:4,000
"10 puntos si das a ese tipo de ahí. 20 puntos por eso".

260
0:10:40.536,000 --> 0:10:41,000
Y los perseguían por el pasillo.

261
0:10:42.16,000 --> 0:10:43,000
(Risas)

262
0:10:43.2,000 --> 0:10:44,000
Les dije, "Esas personas son reales.

263
0:10:45.16,000 --> 0:10:48,000
Realmente sangrarán y sentirán dolor si los golpean".

264
0:10:48.48,000 --> 0:10:49,000
Y contestaban: "Bien, lo entendí".

265
0:10:50.12,000 --> 0:10:52,000
Pero cinco minutos después decían,

266
0:10:52.2,000 --> 0:10:55,000
"20 puntos para ese tipo de allí, parece que necesita algún golpe".

267
0:10:55.84,000 --> 0:10:57,000
Es un poco como "El juego de Ender".

268
0:10:58,000 --> 0:10:59,000
Hay un mundo real en ese otro lado

269
0:10:59.77,000 --> 0:11:02,000
y creo que es nuestra responsabilidad, como personas que diseñan esas interfaces,

270
0:11:03.656,000 --> 0:11:04,000
ayudar a la gente a recordar

271
0:11:05.126,000 --> 0:11:07,000
que hay consecuencias reales de sus acciones

272
0:11:07.456,000 --> 0:11:09,000
y obtener un sentido de responsabilidad

273
0:11:09.636,000 --> 0:11:11,000
cuando se están operando estas cosas cada vez más autónomas.

274
0:11:13.84,000 --> 0:11:15,000
Esto es un gran ejemplo

275
0:11:16.16,000 --> 0:11:19,000
de experimentar con un posible futuro robótico,

276
0:11:19.44,000 --> 0:11:22,000
y creo que es genial que podamos extendernos

277
0:11:23.32,000 --> 0:11:25,000
y aprender de las formas cómo nos extendemos

278
0:11:25.68,000 --> 0:11:26,000
en estas máquinas

279
0:11:26.92,000 --> 0:11:28,000
y al mismo tiempo expresar nuestra humanidad

280
0:11:29.64,000 --> 0:11:3,000
y nuestra personalidad.

281
0:11:30.88,000 --> 0:11:31,000
También construimos empatía para otros

282
0:11:32.836,000 --> 0:11:34,000
en términos de ser más bajo, más alto, más rápido, más lento,

283
0:11:35.786,000 --> 0:11:36,000
y tal vez incluso sin brazos,

284
0:11:37.216,000 --> 0:11:38,000
que es algo estupendo.

285
0:11:38.52,000 --> 0:11:4,000
También generamos empatía para los mismos robots.

286
0:11:40.99,000 --> 0:11:41,000
Este es uno de mis robots favoritos

287
0:11:42.76,000 --> 0:11:43,000
Se llama Tweenbot.

288
0:11:44.24,000 --> 0:11:45,000
Y este lleva una banderita que dice:

289
0:11:46.24,000 --> 0:11:48,000
"Estoy tratando de llegar a este cruce en Manhattan"

290
0:11:48.84,000 --> 0:11:5,000
y es lindo y rueda, eso es todo.

291
0:11:51.64,000 --> 0:11:54,000
No sabe cómo construir un mapa, no sabe cómo ver el mundo,

292
0:11:55.12,000 --> 0:11:56,000
solo pide ayuda.

293
0:11:56.4,000 --> 0:11:57,000
Lo bueno de las personas

294
0:11:57.76,000 --> 0:12:,000
es que realmente puede depender de la bondad de los extraños.

295
0:12:00.88,000 --> 0:12:03,000
Lo hicieron cruzar el parque al otro lado de Manhattan,

296
0:12:04.8,000 --> 0:12:05,000
lo que es bastante bueno,

297
0:12:06.08,000 --> 0:12:09,000
solo porque las personas lo recogen y lo orientan en la dirección correcta.

298
0:12:09.736,000 --> 0:12:09,000
(Risas)

299
0:12:10.52,000 --> 0:12:11,000
Y eso es genial, ¿verdad?

300
0:12:11.8,000 --> 0:12:13,000
Estamos tratando de construir este mundo humano-robot

301
0:12:14.52,000 --> 0:12:16,000
en el que podemos convivir y colaborar el uno con el otro,

302
0:12:17.44,000 --> 0:12:2,000
y no hay que ser completamente autónomos y hacer cosas solo por nuestra cuenta.

303
0:12:21.36,000 --> 0:12:22,000
De hecho, hacemos cosas juntos.

304
0:12:22.88,000 --> 0:12:23,000
Y para que eso suceda,

305
0:12:24.16,000 --> 0:12:27,000
en realidad, necesitamos ayuda de personas como los artistas y los diseñadores,

306
0:12:27.996,000 --> 0:12:28,000
los políticos, los juristas,

307
0:12:29.586,000 --> 0:12:3,000
psicólogos, sociólogos, antropólogos,

308
0:12:31.56,000 --> 0:12:33,000
necesitamos más perspectivas en la sala

309
0:12:33.666,000 --> 0:12:35,000
si vamos a hacer lo que Stu Card dice que debemos hacer,

310
0:12:36.4,000 --> 0:12:39,000
que es inventar el futuro en el que realmente queremos vivir.

311
0:12:40.36,000 --> 0:12:42,000
Y creo que podemos continuar experimentando

312
0:12:43.04,000 --> 0:12:45,000
con estos diferentes futuros robóticos juntos,

313
0:12:45.466,000 --> 0:12:49,000
y al hacerlo, terminaremos aprendiendo mucho más sobre nosotros mismos.

314
0:12:50.72,000 --> 0:12:51,000
Gracias.

315
0:12:51.96,000 --> 0:12:53,000
(Aplausos)

