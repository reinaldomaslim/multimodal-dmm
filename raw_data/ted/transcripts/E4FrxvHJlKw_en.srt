1
0:00:12.36,000 --> 0:00:15,000
[This talk contains mature content]

2
0:00:16.913,000 --> 0:00:2,000
Moritz Riesewieck: On March 23, 2013,

3
0:00:21.389,000 --> 0:00:25,000
users worldwide discovered in their news feed

4
0:00:25.47,000 --> 0:00:3,000
a video of a young girl being raped by an older man.

5
0:00:31.478,000 --> 0:00:34,000
Before this video was removed from Facebook,

6
0:00:35.358,000 --> 0:00:39,000
it was already shared 16,000 times,

7
0:00:39.998,000 --> 0:00:42,000
and it was even liked 4,000 times.

8
0:00:45.268,000 --> 0:00:48,000
This video went viral and infected the net.

9
0:00:49.873,000 --> 0:00:52,000
Hans Block: And that was the moment we asked ourselves

10
0:00:52.992,000 --> 0:00:54,000
how could something like this get on Facebook?

11
0:00:55.794,000 --> 0:00:59,000
And at the same time, why don't we see such content more often?

12
0:01:00.22,000 --> 0:01:03,000
After all, there's a lot of revolting material online,

13
0:01:03.927,000 --> 0:01:07,000
but why do we so rarely see such crap on Facebook, Twitter or Google?

14
0:01:08.958,000 --> 0:01:1,000
MR: While image-recognition software

15
0:01:11.223,000 --> 0:01:15,000
can identify the outlines of sexual organs,

16
0:01:15.53,000 --> 0:01:19,000
blood or naked skin in images and videos,

17
0:01:20.482,000 --> 0:01:25,000
it has immense difficulties to distinguish pornographic content

18
0:01:26.046,000 --> 0:01:3,000
from holiday pictures, Adonis statues

19
0:01:30.418,000 --> 0:01:32,000
or breast-cancer screening campaigns.

20
0:01:33.14,000 --> 0:01:37,000
It can't distinguish Romeo and Juliet dying onstage

21
0:01:37.419,000 --> 0:01:39,000
from a real knife attack.

22
0:01:39.998,000 --> 0:01:44,000
It can't distinguish satire from propaganda

23
0:01:45.244,000 --> 0:01:48,000
or irony from hatred, and so on and so forth.

24
0:01:50.077,000 --> 0:01:54,000
Therefore, humans are needed to decide

25
0:01:54.179,000 --> 0:01:58,000
which of the suspicious content should be deleted,

26
0:01:58.204,000 --> 0:01:59,000
and which should remain.

27
0:02:00.909,000 --> 0:02:02,000
Humans whom we know almost nothing about,

28
0:02:03.744,000 --> 0:02:04,000
because they work in secret.

29
0:02:06.053,000 --> 0:02:07,000
They sign nondisclosure agreements,

30
0:02:07.982,000 --> 0:02:09,000
which prohibit them from talking and sharing

31
0:02:10.959,000 --> 0:02:13,000
what they see on their screens and what this work does to them.

32
0:02:14.887,000 --> 0:02:18,000
They are forced to use code words in order to hide who they work for.

33
0:02:19.585,000 --> 0:02:21,000
They are monitored by private security firms

34
0:02:22.514,000 --> 0:02:25,000
in order to ensure that they don't talk to journalists.

35
0:02:26.048,000 --> 0:02:29,000
And they are threatened by fines in case they speak.

36
0:02:30.421,000 --> 0:02:33,000
All of this sounds like a weird crime story,

37
0:02:34.207,000 --> 0:02:35,000
but it's true.

38
0:02:35.561,000 --> 0:02:36,000
These people exist,

39
0:02:37.633,000 --> 0:02:41,000
and they are called content moderators.

40
0:02:42.942,000 --> 0:02:45,000
HB: We are the directors of the feature documentary film "The Cleaners,"

41
0:02:46.411,000 --> 0:02:47,000
and we would like to take you

42
0:02:48.395,000 --> 0:02:5,000
to a world that many of you may not know yet.

43
0:02:51.006,000 --> 0:02:53,000
Here's a short clip of our film.

44
0:02:58.639,000 --> 0:03:01,000
(Music)

45
0:03:04.4,000 --> 0:03:07,000
(Video) Moderator: I need to be anonymous, because we have a contract signed.

46
0:03:09.784,000 --> 0:03:12,000
We are not allowed to declare whom we are working with.

47
0:03:14.807,000 --> 0:03:15,000
The reason why I speak to you

48
0:03:16.594,000 --> 0:03:2,000
is because the world should know that we are here.

49
0:03:22.544,000 --> 0:03:24,000
There is somebody who is checking the social media.

50
0:03:26.317,000 --> 0:03:29,000
We are doing our best to make this platform

51
0:03:29.945,000 --> 0:03:3,000
safe for all of them.

52
0:03:42.438,000 --> 0:03:43,000
Delete.

53
0:03:44.278,000 --> 0:03:45,000
Ignore.

54
0:03:45.596,000 --> 0:03:46,000
Delete.

55
0:03:47.279,000 --> 0:03:48,000
Ignore.

56
0:03:48.6,000 --> 0:03:49,000
Delete.

57
0:03:50.68,000 --> 0:03:51,000
Ignore.

58
0:03:51.855,000 --> 0:03:52,000
Ignore.

59
0:03:53.625,000 --> 0:03:54,000
Delete.

60
0:03:58.03,000 --> 0:03:59,000
HB: The so-called content moderators

61
0:04:00.031,000 --> 0:04:04,000
don't get their paychecks from Facebook, Twitter or Google themselves,

62
0:04:04.055,000 --> 0:04:06,000
but from outsourcing firms around the world

63
0:04:06.396,000 --> 0:04:08,000
in order to keep the wages low.

64
0:04:08.833,000 --> 0:04:09,000
Tens of thousands of young people

65
0:04:10.824,000 --> 0:04:13,000
looking at everything we are not supposed to see.

66
0:04:14.061,000 --> 0:04:17,000
And we are talking about decapitations, mutilations,

67
0:04:17.633,000 --> 0:04:2,000
executions, necrophilia, torture, child abuse.

68
0:04:21.743,000 --> 0:04:23,000
Thousands of images in one shift --

69
0:04:24.041,000 --> 0:04:26,000
ignore, delete, day and night.

70
0:04:27.393,000 --> 0:04:3,000
And much of this work is done in Manila,

71
0:04:30.815,000 --> 0:04:33,000
where the analog toxic waste from the Western world

72
0:04:34.141,000 --> 0:04:36,000
was transported for years by container ships,

73
0:04:36.773,000 --> 0:04:4,000
now the digital waste is dumped there via fiber-optic cable.

74
0:04:40.8,000 --> 0:04:43,000
And just as the so-called scavengers

75
0:04:43.871,000 --> 0:04:46,000
rummage through gigantic tips on the edge of the city,

76
0:04:47.371,000 --> 0:04:51,000
the content moderators click their way through an endless toxic ocean

77
0:04:52.228,000 --> 0:04:56,000
of images and videos and all manner of intellectual garbage,

78
0:04:56.339,000 --> 0:04:58,000
so that we don't have to look at it.

79
0:04:58.665,000 --> 0:05:01,000
MR: But unlike the wounds of the scavengers,

80
0:05:02.229,000 --> 0:05:05,000
those of the content moderators remain invisible.

81
0:05:06.117,000 --> 0:05:09,000
Full of shocking and disturbing content,

82
0:05:09.221,000 --> 0:05:12,000
these pictures and videos burrow into their memories

83
0:05:12.508,000 --> 0:05:15,000
where, at any time, they can have unpredictable effects:

84
0:05:15.977,000 --> 0:05:18,000
eating disorders, loss of libido,

85
0:05:19.358,000 --> 0:05:22,000
anxiety disorders, alcoholism,

86
0:05:22.641,000 --> 0:05:24,000
depression, which can even lead to suicide.

87
0:05:26.1,000 --> 0:05:28,000
The pictures and videos infect them,

88
0:05:28.569,000 --> 0:05:3,000
and often never let them go again.

89
0:05:30.982,000 --> 0:05:34,000
If they are unlucky, they develop post-traumatic stress disorders,

90
0:05:35.847,000 --> 0:05:37,000
like soldiers after war missions.

91
0:05:39.445,000 --> 0:05:42,000
In our film, we tell the story of a young man

92
0:05:43.112,000 --> 0:05:48,000
who had to monitor livestreams of self-mutilations and suicide attempts,

93
0:05:48.334,000 --> 0:05:49,000
again and again,

94
0:05:50.033,000 --> 0:05:53,000
and who eventually committed suicide himself.

95
0:05:53.787,000 --> 0:05:55,000
It's not an isolated case, as we've been told.

96
0:05:57.184,000 --> 0:06:,000
This is the price all of us pay

97
0:06:01.188,000 --> 0:06:06,000
for our so-called clean and safe and "healthy"

98
0:06:06.539,000 --> 0:06:08,000
environments on social media.

99
0:06:10.482,000 --> 0:06:12,000
Never before in the history of mankind

100
0:06:13.101,000 --> 0:06:16,000
has it been easier to reach millions of people around the globe

101
0:06:16.457,000 --> 0:06:17,000
in a few seconds.

102
0:06:18.148,000 --> 0:06:21,000
What is posted on social media spreads so quickly,

103
0:06:22.117,000 --> 0:06:25,000
becomes viral and excites the minds of people all around the globe.

104
0:06:26.45,000 --> 0:06:28,000
Before it is deleted,

105
0:06:28.538,000 --> 0:06:29,000
it is often already too late.

106
0:06:30.966,000 --> 0:06:32,000
Millions of people have already been infected

107
0:06:33.22,000 --> 0:06:34,000
with hatred and anger,

108
0:06:35.101,000 --> 0:06:37,000
and they either become active online,

109
0:06:37.855,000 --> 0:06:4,000
by spreading or amplifying hatred,

110
0:06:41.022,000 --> 0:06:44,000
or they take to the streets and take up arms.

111
0:06:45.236,000 --> 0:06:47,000
HB: Therefore, an army of content moderators

112
0:06:47.8,000 --> 0:06:5,000
sit in front of a screen to avoid new collateral damage.

113
0:06:52.434,000 --> 0:06:54,000
And they are deciding, as soon as possible,

114
0:06:54.577,000 --> 0:06:58,000
whether the content stays on the platform -- ignore;

115
0:06:58.696,000 --> 0:07:,000
or disappears -- delete.

116
0:07:01.823,000 --> 0:07:03,000
But not every decision is as clear

117
0:07:04.474,000 --> 0:07:06,000
as the decision about a child-abuse video.

118
0:07:07.395,000 --> 0:07:09,000
What about controversial content, ambivalent content,

119
0:07:10.196,000 --> 0:07:13,000
uploaded by civil rights activists or citizen journalists?

120
0:07:14.048,000 --> 0:07:17,000
The content moderators often decide on such cases

121
0:07:17.294,000 --> 0:07:19,000
at the same speed as the [clear] cases.

122
0:07:21.515,000 --> 0:07:23,000
MR: We will show you a video now,

123
0:07:24.198,000 --> 0:07:27,000
and we would like to ask you to decide:

124
0:07:27.531,000 --> 0:07:28,000
Would you delete it,

125
0:07:29.245,000 --> 0:07:3,000
or would you not delete it?

126
0:07:31.07,000 --> 0:07:32,000
(Video) (Air strike sounds)

127
0:07:33.1,000 --> 0:07:35,000
(Explosion)

128
0:07:40.076,000 --> 0:07:45,000
(People speaking in Arabic)

129
0:07:46.053,000 --> 0:07:48,000
MR: Yeah, we did some blurring for you.

130
0:07:49.196,000 --> 0:07:52,000
A child would potentially be dangerously disturbed

131
0:07:52.975,000 --> 0:07:54,000
and extremely frightened by such content.

132
0:07:55.808,000 --> 0:07:57,000
So, you rather delete it?

133
0:07:59.61,000 --> 0:08:03,000
But what if this video could help investigate the war crimes in Syria?

134
0:08:04.717,000 --> 0:08:07,000
What if nobody would have heard about this air strike,

135
0:08:07.908,000 --> 0:08:1,000
because Facebook, YouTube, Twitter would have decided to take it down?

136
0:08:12.895,000 --> 0:08:16,000
Airwars, a nongovernmental organization based in London,

137
0:08:17.244,000 --> 0:08:19,000
tries to find those videos as quickly as possible

138
0:08:20.165,000 --> 0:08:22,000
whenever they are uploaded to social media,

139
0:08:22.749,000 --> 0:08:23,000
in order to archive them.

140
0:08:24.693,000 --> 0:08:26,000
Because they know, sooner or later,

141
0:08:27.55,000 --> 0:08:3,000
Facebook, YouTube, Twitter would take such content down.

142
0:08:31.345,000 --> 0:08:33,000
People armed with their mobile phones

143
0:08:33.577,000 --> 0:08:37,000
can make visible what journalists often do not have access to.

144
0:08:37.8,000 --> 0:08:4,000
Civil rights groups often do not have any better option

145
0:08:40.887,000 --> 0:08:43,000
to quickly make their recordings accessible to a large audience

146
0:08:44.712,000 --> 0:08:46,000
than by uploading them to social media.

147
0:08:47.95,000 --> 0:08:51,000
Wasn't this the empowering potential the World Wide Web should have?

148
0:08:52.966,000 --> 0:08:53,000
Weren't these the dreams

149
0:08:54.95,000 --> 0:08:58,000
people in its early stages had about the World Wide Web?

150
0:08:59.608,000 --> 0:09:01,000
Can't pictures and videos like these

151
0:09:02.427,000 --> 0:09:07,000
persuade people who have become insensitive to facts

152
0:09:07.585,000 --> 0:09:08,000
to rethink?

153
0:09:09.917,000 --> 0:09:12,000
HB: But instead, everything that might be disturbing is deleted.

154
0:09:13.543,000 --> 0:09:15,000
And there's a general shift in society.

155
0:09:15.625,000 --> 0:09:18,000
Media, for example, more and more often use trigger warnings

156
0:09:19.546,000 --> 0:09:2,000
at the top of articles

157
0:09:21.363,000 --> 0:09:24,000
which some people may perceive as offensive or troubling.

158
0:09:24.696,000 --> 0:09:27,000
Or more and more students at universities in the United States

159
0:09:28.634,000 --> 0:09:3,000
demand the banishment of antique classics

160
0:09:31.475,000 --> 0:09:34,000
which depict sexual violence or assault from the curriculum.

161
0:09:34.991,000 --> 0:09:36,000
But how far should we go with that?

162
0:09:37.875,000 --> 0:09:4,000
Physical integrity is guaranteed as a human right

163
0:09:41.279,000 --> 0:09:42,000
in constitutions worldwide.

164
0:09:43.422,000 --> 0:09:46,000
In the Charter of Fundamental Rights of the European Union,

165
0:09:47.2,000 --> 0:09:5,000
this right expressly applies to mental integrity.

166
0:09:51.347,000 --> 0:09:53,000
But even if the potentially traumatic effect

167
0:09:54.029,000 --> 0:09:56,000
of images and videos is hard to predict,

168
0:09:56.879,000 --> 0:09:57,000
do we want to become so cautious

169
0:09:58.86,000 --> 0:10:01,000
that we risk losing social awareness of injustice?

170
0:10:03.203,000 --> 0:10:04,000
So what to do?

171
0:10:04.942,000 --> 0:10:06,000
Mark Zuckerberg recently stated that in the future,

172
0:10:07.958,000 --> 0:10:1,000
the users, we, or almost everybody,

173
0:10:11.784,000 --> 0:10:13,000
will decide individually

174
0:10:14.069,000 --> 0:10:16,000
what they would like to see on the platform,

175
0:10:16.141,000 --> 0:10:18,000
by personal filter settings.

176
0:10:18.196,000 --> 0:10:21,000
So everyone could easily claim to remain undisturbed

177
0:10:21.292,000 --> 0:10:24,000
by images of war or other violent conflicts, like ...

178
0:10:25.849,000 --> 0:10:29,000
MR: I'm the type of guy who doesn't mind seeing breasts

179
0:10:30.319,000 --> 0:10:33,000
and I'm very interested in global warming,

180
0:10:34.109,000 --> 0:10:36,000
but I don't like war so much.

181
0:10:37.109,000 --> 0:10:38,000
HB: Yeah, I'm more the opposite,

182
0:10:38.855,000 --> 0:10:42,000
I have zero interest in naked breasts or naked bodies at all.

183
0:10:43.209,000 --> 0:10:45,000
But why not guns? I like guns, yes.

184
0:10:46.901,000 --> 0:10:49,000
MR: Come on, if we don't share a similar social consciousness,

185
0:10:50.67,000 --> 0:10:52,000
how shall we discuss social problems?

186
0:10:53.363,000 --> 0:10:55,000
How shall we call people to action?

187
0:10:55.784,000 --> 0:10:58,000
Even more isolated bubbles would emerge.

188
0:10:59.665,000 --> 0:11:02,000
One of the central questions is: "How, in the future,

189
0:11:02.92,000 --> 0:11:06,000
freedom of expression will be weighed against the people's need for protection."

190
0:11:08.441,000 --> 0:11:09,000
It's a matter of principle.

191
0:11:10.602,000 --> 0:11:14,000
Do we want to design an either open or closed society

192
0:11:14.874,000 --> 0:11:15,000
for the digital space?

193
0:11:17.054,000 --> 0:11:22,000
At the heart of the matter is "freedom versus security."

194
0:11:24.388,000 --> 0:11:28,000
Facebook has always wanted to be a "healthy" platform.

195
0:11:28.896,000 --> 0:11:31,000
Above all, users should feel safe and secure.

196
0:11:32.618,000 --> 0:11:34,000
It's the same choice of words

197
0:11:34.762,000 --> 0:11:36,000
the content moderators in the Philippines used

198
0:11:37.744,000 --> 0:11:38,000
in a lot of our interviews.

199
0:11:40.188,000 --> 0:11:42,000
(Video) The world that we are living in right now,

200
0:11:42.593,000 --> 0:11:44,000
I believe, is not really healthy.

201
0:11:44.783,000 --> 0:11:45,000
(Music)

202
0:11:46.355,000 --> 0:11:49,000
In this world, there is really an evil who exists.

203
0:11:49.537,000 --> 0:11:52,000
(Music)

204
0:11:52.799,000 --> 0:11:54,000
We need to watch for it.

205
0:11:54.886,000 --> 0:11:55,000
(Music)

206
0:11:56.792,000 --> 0:11:59,000
We need to control it -- good or bad.

207
0:12:00.646,000 --> 0:12:07,000
(Music)

208
0:12:10.193,000 --> 0:12:14,000
[Look up, Young man! --God]

209
0:12:14.952,000 --> 0:12:18,000
MR: For the young content moderators in the strictly Catholic Philippines,

210
0:12:19.254,000 --> 0:12:21,000
this is linked to a Christian mission.

211
0:12:22.833,000 --> 0:12:24,000
To counter the sins of the world

212
0:12:25.823,000 --> 0:12:27,000
which spread across the web.

213
0:12:28.641,000 --> 0:12:31,000
"Cleanliness is next to godliness,"

214
0:12:32.077,000 --> 0:12:35,000
is a saying everybody in the Philippines knows.

215
0:12:36.035,000 --> 0:12:37,000
HB: And others motivate themselves

216
0:12:37.718,000 --> 0:12:4,000
by comparing themselves with their president, Rodrigo Duterte.

217
0:12:41.837,000 --> 0:12:44,000
He has been ruling the Philippines since 2016,

218
0:12:45.352,000 --> 0:12:48,000
and he won the election with the promise: "I will clean up."

219
0:12:49.892,000 --> 0:12:52,000
And what that means is eliminating all kinds of problems

220
0:12:53.233,000 --> 0:12:55,000
by literally killing people on the streets

221
0:12:55.712,000 --> 0:12:57,000
who are supposed to be criminals, whatever that means.

222
0:12:58.601,000 --> 0:12:59,000
And since he was elected,

223
0:12:59.895,000 --> 0:13:02,000
an estimated 20,000 people have been killed.

224
0:13:03.655,000 --> 0:13:05,000
And one moderator in our film says,

225
0:13:06.18,000 --> 0:13:08,000
"What Duterte does on the streets,

226
0:13:08.259,000 --> 0:13:09,000
I do for the internet."

227
0:13:10.934,000 --> 0:13:13,000
And here they are, our self-proclaimed superheroes,

228
0:13:14.522,000 --> 0:13:16,000
who enforce law and order in our digital world.

229
0:13:17.522,000 --> 0:13:19,000
They clean up, they polish everything clean,

230
0:13:19.927,000 --> 0:13:21,000
they free us from everything evil.

231
0:13:22.284,000 --> 0:13:25,000
Tasks formerly reserved to state authorities

232
0:13:26.037,000 --> 0:13:29,000
have been taken over by college graduates in their early 20s,

233
0:13:29.736,000 --> 0:13:31,000
equipped with three- to five-day training --

234
0:13:32.653,000 --> 0:13:33,000
this is the qualification --

235
0:13:34.613,000 --> 0:13:37,000
who work on nothing less than the world's rescue.

236
0:13:38.756,000 --> 0:13:42,000
MR: National sovereignties have been outsourced to private companies,

237
0:13:42.999,000 --> 0:13:46,000
and they pass on their responsibilities to third parties.

238
0:13:47.031,000 --> 0:13:5,000
It's an outsourcing of the outsourcing of the outsourcing,

239
0:13:50.118,000 --> 0:13:51,000
which takes place.

240
0:13:51.618,000 --> 0:13:52,000
With social networks,

241
0:13:53.038,000 --> 0:13:56,000
we are dealing with a completely new infrastructure,

242
0:13:56.077,000 --> 0:13:57,000
with its own mechanisms,

243
0:13:57.617,000 --> 0:13:58,000
its own logic of action

244
0:13:59.22,000 --> 0:14:04,000
and therefore, also, its own new dangers,

245
0:14:04.489,000 --> 0:14:08,000
which had not yet existed in the predigitalized public sphere.

246
0:14:08.538,000 --> 0:14:1,000
HB: When Mark Zuckerberg was at the US Congress

247
0:14:10.771,000 --> 0:14:11,000
or at the European Parliament,

248
0:14:12.565,000 --> 0:14:14,000
he was confronted with all kinds of critics.

249
0:14:15.224,000 --> 0:14:17,000
And his reaction was always the same:

250
0:14:18.501,000 --> 0:14:19,000
"We will fix that,

251
0:14:19.993,000 --> 0:14:21,000
and I will follow up on that with my team."

252
0:14:23.167,000 --> 0:14:26,000
But such a debate shouldn't be held in back rooms of Facebook,

253
0:14:26.969,000 --> 0:14:27,000
Twitter or Google --

254
0:14:28.278,000 --> 0:14:32,000
such a debate should be openly discussed in new, cosmopolitan parliaments,

255
0:14:33.118,000 --> 0:14:37,000
in new institutions that reflect the diversity of people

256
0:14:38.002,000 --> 0:14:42,000
contributing to a utopian project of a global network.

257
0:14:42.568,000 --> 0:14:45,000
And while it may seem impossible to consider the values

258
0:14:45.969,000 --> 0:14:47,000
of users worldwide,

259
0:14:48.235,000 --> 0:14:49,000
it's worth believing

260
0:14:49.941,000 --> 0:14:52,000
that there's more that connects us than separates us.

261
0:14:53.624,000 --> 0:14:56,000
MR: Yeah, at a time when populism is gaining strength,

262
0:14:57.355,000 --> 0:15:,000
it becomes popular to justify the symptoms,

263
0:15:00.577,000 --> 0:15:01,000
to eradicate them,

264
0:15:01.879,000 --> 0:15:02,000
to make them invisible.

265
0:15:04.919,000 --> 0:15:07,000
This ideology is spreading worldwide,

266
0:15:08.292,000 --> 0:15:1,000
analog as well as digital,

267
0:15:11.903,000 --> 0:15:14,000
and it's our duty to stop it

268
0:15:15.419,000 --> 0:15:16,000
before it's too late.

269
0:15:17.665,000 --> 0:15:2,000
The question of freedom and democracy

270
0:15:21.673,000 --> 0:15:23,000
must not only have these two options.

271
0:15:25.053,000 --> 0:15:26,000
HB: Delete.

272
0:15:26.243,000 --> 0:15:28,000
MR: Or ignore.

273
0:15:29.3,000 --> 0:15:3,000
HB: Thank you very much.

274
0:15:30.921,000 --> 0:15:35,000
(Applause)

