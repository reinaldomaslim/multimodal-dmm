1
0:00:12.38,000 --> 0:00:13,000
Whether you like it or not,

2
0:00:13.74,000 --> 0:00:18,000
radical transparency and algorithmic decision-making is coming at you fast,

3
0:00:19.14,000 --> 0:00:2,000
and it's going to change your life.

4
0:00:21.14,000 --> 0:00:23,000
That's because it's now easy to take algorithms

5
0:00:23.98,000 --> 0:00:24,000
and embed them into computers

6
0:00:25.9,000 --> 0:00:27,000
and gather all that data that you're leaving on yourself

7
0:00:28.86,000 --> 0:00:29,000
all over the place,

8
0:00:30.26,000 --> 0:00:31,000
and know what you're like,

9
0:00:31.98,000 --> 0:00:33,000
and then direct the computers to interact with you

10
0:00:34.94,000 --> 0:00:36,000
in ways that are better than most people can.

11
0:00:37.98,000 --> 0:00:38,000
Well, that might sound scary.

12
0:00:39.62,000 --> 0:00:42,000
I've been doing this for a long time and I have found it to be wonderful.

13
0:00:43.979,000 --> 0:00:45,000
My objective has been to have meaningful work

14
0:00:46.66,000 --> 0:00:48,000
and meaningful relationships with the people I work with,

15
0:00:49.54,000 --> 0:00:51,000
and I've learned that I couldn't have that

16
0:00:51.62,000 --> 0:00:55,000
unless I had that radical transparency and that algorithmic decision-making.

17
0:00:56.5,000 --> 0:00:58,000
I want to show you why that is,

18
0:00:58.54,000 --> 0:00:59,000
I want to show you how it works.

19
0:01:00.26,000 --> 0:01:03,000
And I warn you that some of the things that I'm going to show you

20
0:01:03.38,000 --> 0:01:04,000
probably are a little bit shocking.

21
0:01:05.58,000 --> 0:01:08,000
Since I was a kid, I've had a terrible rote memory.

22
0:01:09.94,000 --> 0:01:11,000
And I didn't like following instructions,

23
0:01:12.14,000 --> 0:01:14,000
I was no good at following instructions.

24
0:01:14.58,000 --> 0:01:17,000
But I loved to figure out how things worked for myself.

25
0:01:18.5,000 --> 0:01:19,000
When I was 12,

26
0:01:19.9,000 --> 0:01:22,000
I hated school but I fell in love with trading the markets.

27
0:01:23.74,000 --> 0:01:24,000
I caddied at the time,

28
0:01:25.42,000 --> 0:01:26,000
earned about five dollars a bag.

29
0:01:27.02,000 --> 0:01:3,000
And I took my caddying money, and I put it in the stock market.

30
0:01:31.06,000 --> 0:01:34,000
And that was just because the stock market was hot at the time.

31
0:01:34.46,000 --> 0:01:35,000
And the first company I bought

32
0:01:35.94,000 --> 0:01:37,000
was a company by the name of Northeast Airlines.

33
0:01:39.18,000 --> 0:01:41,000
Northeast Airlines was the only company I heard of

34
0:01:41.94,000 --> 0:01:43,000
that was selling for less than five dollars a share.

35
0:01:44.66,000 --> 0:01:45,000
(Laughter)

36
0:01:46.66,000 --> 0:01:47,000
And I figured I could buy more shares,

37
0:01:48.54,000 --> 0:01:5,000
and if it went up, I'd make more money.

38
0:01:50.66,000 --> 0:01:52,000
So, it was a dumb strategy, right?

39
0:01:54.18,000 --> 0:01:55,000
But I tripled my money,

40
0:01:55.66,000 --> 0:01:57,000
and I tripled my money because I got lucky.

41
0:01:58.34,000 --> 0:01:59,000
The company was about to go bankrupt,

42
0:02:00.18,000 --> 0:02:02,000
but some other company acquired it,

43
0:02:02.3,000 --> 0:02:03,000
and I tripled my money.

44
0:02:03.78,000 --> 0:02:04,000
And I was hooked.

45
0:02:05.54,000 --> 0:02:07,000
And I thought, "This game is easy."

46
0:02:09.02,000 --> 0:02:1,000
With time,

47
0:02:10.26,000 --> 0:02:11,000
I learned this game is anything but easy.

48
0:02:12.7,000 --> 0:02:14,000
In order to be an effective investor,

49
0:02:14.86,000 --> 0:02:16,000
one has to bet against the consensus

50
0:02:17.78,000 --> 0:02:18,000
and be right.

51
0:02:19.06,000 --> 0:02:21,000
And it's not easy to bet against the consensus and be right.

52
0:02:21.94,000 --> 0:02:23,000
One has to bet against the consensus and be right

53
0:02:24.3,000 --> 0:02:26,000
because the consensus is built into the price.

54
0:02:27.94,000 --> 0:02:29,000
And in order to be an entrepreneur,

55
0:02:30.42,000 --> 0:02:31,000
a successful entrepreneur,

56
0:02:32.06,000 --> 0:02:35,000
one has to bet against the consensus and be right.

57
0:02:37.22,000 --> 0:02:39,000
I had to be an entrepreneur and an investor --

58
0:02:40.18,000 --> 0:02:44,000
and what goes along with that is making a lot of painful mistakes.

59
0:02:45.26,000 --> 0:02:47,000
So I made a lot of painful mistakes,

60
0:02:48.1,000 --> 0:02:49,000
and with time,

61
0:02:49.38,000 --> 0:02:51,000
my attitude about those mistakes began to change.

62
0:02:52.98,000 --> 0:02:54,000
I began to think of them as puzzles.

63
0:02:55.1,000 --> 0:02:56,000
That if I could solve the puzzles,

64
0:02:57.06,000 --> 0:02:58,000
they would give me gems.

65
0:02:58.98,000 --> 0:02:59,000
And the puzzles were:

66
0:03:00.66,000 --> 0:03:03,000
What would I do differently in the future so I wouldn't make that painful mistake?

67
0:03:05.1,000 --> 0:03:07,000
And the gems were principles

68
0:03:07.7,000 --> 0:03:1,000
that I would then write down so I would remember them

69
0:03:10.86,000 --> 0:03:11,000
that would help me in the future.

70
0:03:12.82,000 --> 0:03:14,000
And because I wrote them down so clearly,

71
0:03:15.54,000 --> 0:03:16,000
I could then --

72
0:03:16.9,000 --> 0:03:17,000
eventually discovered --

73
0:03:18.5,000 --> 0:03:21,000
I could then embed them into algorithms.

74
0:03:23.22,000 --> 0:03:26,000
And those algorithms would be embedded in computers,

75
0:03:26.7,000 --> 0:03:29,000
and the computers would make decisions along with me;

76
0:03:30.06,000 --> 0:03:33,000
and so in parallel, we would make these decisions.

77
0:03:33.22,000 --> 0:03:36,000
And I could see how those decisions then compared with my own decisions,

78
0:03:37.22,000 --> 0:03:4,000
and I could see that those decisions were a lot better.

79
0:03:40.34,000 --> 0:03:44,000
And that was because the computer could make decisions much faster,

80
0:03:45.1,000 --> 0:03:47,000
it could process a lot more information

81
0:03:47.38,000 --> 0:03:5,000
and it can process decisions much more --

82
0:03:51.7,000 --> 0:03:52,000
less emotionally.

83
0:03:54.58,000 --> 0:03:57,000
So it radically improved my decision-making.

84
0:04:00.26,000 --> 0:04:04,000
Eight years after I started Bridgewater,

85
0:04:05.18,000 --> 0:04:06,000
I had my greatest failure,

86
0:04:06.74,000 --> 0:04:07,000
my greatest mistake.

87
0:04:09.5,000 --> 0:04:11,000
It was late 1970s,

88
0:04:11.66,000 --> 0:04:12,000
I was 34 years old,

89
0:04:13.66,000 --> 0:04:16,000
and I had calculated that American banks

90
0:04:17.34,000 --> 0:04:19,000
had lent much more money to emerging countries

91
0:04:20.22,000 --> 0:04:22,000
than those countries were going to be able to pay back

92
0:04:23.06,000 --> 0:04:25,000
and that we would have the greatest debt crisis

93
0:04:25.78,000 --> 0:04:26,000
since the Great Depression.

94
0:04:28.02,000 --> 0:04:3,000
And with it, an economic crisis

95
0:04:30.26,000 --> 0:04:32,000
and a big bear market in stocks.

96
0:04:33.5,000 --> 0:04:35,000
It was a controversial view at the time.

97
0:04:35.98,000 --> 0:04:37,000
People thought it was kind of a crazy point of view.

98
0:04:39.3,000 --> 0:04:41,000
But in August 1982,

99
0:04:41.54,000 --> 0:04:42,000
Mexico defaulted on its debt,

100
0:04:44.34,000 --> 0:04:46,000
and a number of other countries followed.

101
0:04:46.62,000 --> 0:04:49,000
And we had the greatest debt crisis since the Great Depression.

102
0:04:50.9,000 --> 0:04:52,000
And because I had anticipated that,

103
0:04:53.7,000 --> 0:04:57,000
I was asked to testify to Congress and appear on "Wall Street Week,"

104
0:04:58.06,000 --> 0:04:59,000
which was the show of the time.

105
0:05:00.06,000 --> 0:05:02,000
Just to give you a flavor of that, I've got a clip here,

106
0:05:03.02,000 --> 0:05:04,000
and you'll see me in there.

107
0:05:06.3,000 --> 0:05:07,000
(Video) Mr. Chairman, Mr. Mitchell,

108
0:05:08.02,000 --> 0:05:11,000
it's a great pleasure and a great honor to be able to appear before you

109
0:05:11.42,000 --> 0:05:14,000
in examination with what is going wrong with our economy.

110
0:05:15.46,000 --> 0:05:16,000
The economy is now flat --

111
0:05:17.42,000 --> 0:05:19,000
teetering on the brink of failure.

112
0:05:19.58,000 --> 0:05:21,000
Martin Zweig: You were recently quoted in an article.

113
0:05:22.1,000 --> 0:05:24,000
You said, "I can say this with absolute certainty

114
0:05:24.46,000 --> 0:05:25,000
because I know how markets work."

115
0:05:26.1,000 --> 0:05:28,000
Ray Dalio: I can say with absolute certainty

116
0:05:28.22,000 --> 0:05:29,000
that if you look at the liquidity base

117
0:05:30.1,000 --> 0:05:33,000
in the corporations and the world as a whole,

118
0:05:33.5,000 --> 0:05:35,000
that there's such reduced level of liquidity

119
0:05:35.62,000 --> 0:05:38,000
that you can't return to an era of stagflation."

120
0:05:38.86,000 --> 0:05:41,000
I look at that now, I think, "What an arrogant jerk!"

121
0:05:41.98,000 --> 0:05:43,000
(Laughter)

122
0:05:45.58,000 --> 0:05:47,000
I was so arrogant, and I was so wrong.

123
0:05:48.06,000 --> 0:05:5,000
I mean, while the debt crisis happened,

124
0:05:50.66,000 --> 0:05:53,000
the stock market and the economy went up rather than going down,

125
0:05:54.66,000 --> 0:05:59,000
and I lost so much money for myself and for my clients

126
0:05:59.7,000 --> 0:06:02,000
that I had to shut down my operation pretty much,

127
0:06:03.14,000 --> 0:06:04,000
I had to let almost everybody go.

128
0:06:05.46,000 --> 0:06:06,000
And these were like extended family,

129
0:06:07.22,000 --> 0:06:08,000
I was heartbroken.

130
0:06:08.86,000 --> 0:06:09,000
And I had lost so much money

131
0:06:10.7,000 --> 0:06:13,000
that I had to borrow 4,000 dollars from my dad

132
0:06:14.06,000 --> 0:06:15,000
to help to pay my family bills.

133
0:06:16.66,000 --> 0:06:19,000
It was one of the most painful experiences of my life ...

134
0:06:21.06,000 --> 0:06:24,000
but it turned out to be one of the greatest experiences of my life

135
0:06:24.86,000 --> 0:06:26,000
because it changed my attitude about decision-making.

136
0:06:28.18,000 --> 0:06:31,000
Rather than thinking, "I'm right,"

137
0:06:31.26,000 --> 0:06:32,000
I started to ask myself,

138
0:06:32.86,000 --> 0:06:33,000
"How do I know I'm right?"

139
0:06:36.3,000 --> 0:06:37,000
I gained a humility that I needed

140
0:06:38.26,000 --> 0:06:4,000
in order to balance my audacity.

141
0:06:41.7,000 --> 0:06:45,000
I wanted to find the smartest people who would disagree with me

142
0:06:45.94,000 --> 0:06:46,000
to try to understand their perspective

143
0:06:47.86,000 --> 0:06:49,000
or to have them stress test my perspective.

144
0:06:51.22,000 --> 0:06:53,000
I wanted to make an idea meritocracy.

145
0:06:54.02,000 --> 0:06:55,000
In other words,

146
0:06:55.26,000 --> 0:06:58,000
not an autocracy in which I would lead and others would follow

147
0:06:59.1,000 --> 0:07:02,000
and not a democracy in which everybody's points of view were equally valued,

148
0:07:02.74,000 --> 0:07:07,000
but I wanted to have an idea meritocracy in which the best ideas would win out.

149
0:07:07.86,000 --> 0:07:08,000
And in order to do that,

150
0:07:09.14,000 --> 0:07:12,000
I realized that we would need radical truthfulness

151
0:07:12.74,000 --> 0:07:13,000
and radical transparency.

152
0:07:14.38,000 --> 0:07:17,000
What I mean by radical truthfulness and radical transparency

153
0:07:18.26,000 --> 0:07:2,000
is people needed to say what they really believed

154
0:07:20.94,000 --> 0:07:22,000
and to see everything.

155
0:07:23.3,000 --> 0:07:26,000
And we literally tape almost all conversations

156
0:07:27.26,000 --> 0:07:28,000
and let everybody see everything,

157
0:07:28.9,000 --> 0:07:29,000
because if we didn't do that,

158
0:07:30.34,000 --> 0:07:33,000
we couldn't really have an idea meritocracy.

159
0:07:34.58,000 --> 0:07:37,000
In order to have an idea meritocracy,

160
0:07:38.3,000 --> 0:07:4,000
we have let people speak and say what they want.

161
0:07:40.7,000 --> 0:07:41,000
Just to give you an example,

162
0:07:42.1,000 --> 0:07:44,000
this is an email from Jim Haskel --

163
0:07:44.82,000 --> 0:07:45,000
somebody who works for me --

164
0:07:46.22,000 --> 0:07:49,000
and this was available to everybody in the company.

165
0:07:49.62,000 --> 0:07:51,000
"Ray, you deserve a 'D-'

166
0:07:52.18,000 --> 0:07:54,000
for your performance today in the meeting ...

167
0:07:54.46,000 --> 0:07:55,000
you did not prepare at all well

168
0:07:56.18,000 --> 0:07:59,000
because there is no way you could have been that disorganized."

169
0:08:01.34,000 --> 0:08:02,000
Isn't that great?

170
0:08:02.58,000 --> 0:08:03,000
(Laughter)

171
0:08:03.82,000 --> 0:08:04,000
That's great.

172
0:08:05.06,000 --> 0:08:07,000
It's great because, first of all, I needed feedback like that.

173
0:08:08.02,000 --> 0:08:09,000
I need feedback like that.

174
0:08:09.66,000 --> 0:08:12,000
And it's great because if I don't let Jim, and people like Jim,

175
0:08:13.14,000 --> 0:08:14,000
to express their points of view,

176
0:08:14.74,000 --> 0:08:16,000
our relationship wouldn't be the same.

177
0:08:16.82,000 --> 0:08:19,000
And if I didn't make that public for everybody to see,

178
0:08:19.9,000 --> 0:08:2,000
we wouldn't have an idea meritocracy.

179
0:08:23.58,000 --> 0:08:26,000
So for that last 25 years that's how we've been operating.

180
0:08:27.46,000 --> 0:08:3,000
We've been operating with this radical transparency

181
0:08:30.54,000 --> 0:08:32,000
and then collecting these principles,

182
0:08:32.86,000 --> 0:08:34,000
largely from making mistakes,

183
0:08:34.94,000 --> 0:08:38,000
and then embedding those principles into algorithms.

184
0:08:39.38,000 --> 0:08:41,000
And then those algorithms provide --

185
0:08:42.1,000 --> 0:08:44,000
we're following the algorithms

186
0:08:44.14,000 --> 0:08:45,000
in parallel with our thinking.

187
0:08:47.1,000 --> 0:08:5,000
That has been how we've run the investment business,

188
0:08:50.3,000 --> 0:08:52,000
and it's how we also deal with the people management.

189
0:08:53.06,000 --> 0:08:56,000
In order to give you a glimmer into what this looks like,

190
0:08:56.82,000 --> 0:08:58,000
I'd like to take you into a meeting

191
0:08:59.18,000 --> 0:09:02,000
and introduce you to a tool of ours called the "Dot Collector"

192
0:09:02.34,000 --> 0:09:03,000
that helps us do this.

193
0:09:07.46,000 --> 0:09:09,000
A week after the US election,

194
0:09:09.66,000 --> 0:09:11,000
our research team held a meeting

195
0:09:11.78,000 --> 0:09:14,000
to discuss what a Trump presidency would mean for the US economy.

196
0:09:15.82,000 --> 0:09:17,000
Naturally, people had different opinions on the matter

197
0:09:18.7,000 --> 0:09:2,000
and how we were approaching the discussion.

198
0:09:21.66,000 --> 0:09:23,000
The "Dot Collector" collects these views.

199
0:09:24.46,000 --> 0:09:26,000
It has a list of a few dozen attributes,

200
0:09:26.78,000 --> 0:09:3,000
so whenever somebody thinks something about another person's thinking,

201
0:09:30.82,000 --> 0:09:32,000
it's easy for them to convey their assessment;

202
0:09:33.78,000 --> 0:09:37,000
they simply note the attribute and provide a rating from one to 10.

203
0:09:39.34,000 --> 0:09:41,000
For example, as the meeting began,

204
0:09:41.62,000 --> 0:09:44,000
a researcher named Jen rated me a three --

205
0:09:45.46,000 --> 0:09:47,000
in other words, badly --

206
0:09:47.5,000 --> 0:09:48,000
(Laughter)

207
0:09:48.9,000 --> 0:09:52,000
for not showing a good balance of open-mindedness and assertiveness.

208
0:09:53.9,000 --> 0:09:54,000
As the meeting transpired,

209
0:09:55.38,000 --> 0:09:58,000
Jen's assessments of people added up like this.

210
0:09:59.74,000 --> 0:10:01,000
Others in the room have different opinions.

211
0:10:01.94,000 --> 0:10:02,000
That's normal.

212
0:10:03.18,000 --> 0:10:05,000
Different people are always going to have different opinions.

213
0:10:06.62,000 --> 0:10:07,000
And who knows who's right?

214
0:10:09.06,000 --> 0:10:12,000
Let's look at just what people thought about how I was doing.

215
0:10:13.42,000 --> 0:10:15,000
Some people thought I did well,

216
0:10:15.66,000 --> 0:10:16,000
others, poorly.

217
0:10:17.9,000 --> 0:10:18,000
With each of these views,

218
0:10:19.26,000 --> 0:10:21,000
we can explore the thinking behind the numbers.

219
0:10:22.34,000 --> 0:10:24,000
Here's what Jen and Larry said.

220
0:10:25.58,000 --> 0:10:27,000
Note that everyone gets to express their thinking,

221
0:10:28.22,000 --> 0:10:29,000
including their critical thinking,

222
0:10:29.9,000 --> 0:10:31,000
regardless of their position in the company.

223
0:10:32.94,000 --> 0:10:35,000
Jen, who's 24 years old and right out of college,

224
0:10:36.06,000 --> 0:10:38,000
can tell me, the CEO, that I'm approaching things terribly.

225
0:10:40.3,000 --> 0:10:43,000
This tool helps people both express their opinions

226
0:10:44.1,000 --> 0:10:47,000
and then separate themselves from their opinions

227
0:10:47.22,000 --> 0:10:49,000
to see things from a higher level.

228
0:10:50.46,000 --> 0:10:54,000
When Jen and others shift their attentions from inputting their own opinions

229
0:10:55.38,000 --> 0:10:57,000
to looking down on the whole screen,

230
0:10:57.98,000 --> 0:10:58,000
their perspective changes.

231
0:11:00.5,000 --> 0:11:03,000
They see their own opinions as just one of many

232
0:11:03.66,000 --> 0:11:05,000
and naturally start asking themselves,

233
0:11:06.22,000 --> 0:11:08,000
"How do I know my opinion is right?"

234
0:11:09.3,000 --> 0:11:13,000
That shift in perspective is like going from seeing in one dimension

235
0:11:13.38,000 --> 0:11:15,000
to seeing in multiple dimensions.

236
0:11:15.66,000 --> 0:11:19,000
And it shifts the conversation from arguing over our opinions

237
0:11:19.78,000 --> 0:11:23,000
to figuring out objective criteria for determining which opinions are best.

238
0:11:24.74,000 --> 0:11:27,000
Behind the "Dot Collector" is a computer that is watching.

239
0:11:28.94,000 --> 0:11:3,000
It watches what all these people are thinking

240
0:11:31.14,000 --> 0:11:33,000
and it correlates that with how they think.

241
0:11:33.74,000 --> 0:11:36,000
And it communicates advice back to each of them based on that.

242
0:11:38.34,000 --> 0:11:41,000
Then it draws the data from all the meetings

243
0:11:41.78,000 --> 0:11:44,000
to create a pointilist painting of what people are like

244
0:11:45.02,000 --> 0:11:46,000
and how they think.

245
0:11:46.98,000 --> 0:11:48,000
And it does that guided by algorithms.

246
0:11:50.62,000 --> 0:11:53,000
Knowing what people are like helps to match them better with their jobs.

247
0:11:54.94,000 --> 0:11:55,000
For example,

248
0:11:56.18,000 --> 0:11:57,000
a creative thinker who is unreliable

249
0:11:57.94,000 --> 0:12:,000
might be matched up with someone who's reliable but not creative.

250
0:12:02.1,000 --> 0:12:05,000
Knowing what people are like also allows us to decide

251
0:12:05.46,000 --> 0:12:07,000
what responsibilities to give them

252
0:12:07.74,000 --> 0:12:1,000
and to weigh our decisions based on people's merits.

253
0:12:11.86,000 --> 0:12:12,000
We call it their believability.

254
0:12:14.38,000 --> 0:12:15,000
Here's an example of a vote that we took

255
0:12:16.38,000 --> 0:12:18,000
where the majority of people felt one way ...

256
0:12:20.74,000 --> 0:12:22,000
but when we weighed the views based on people's merits,

257
0:12:23.7,000 --> 0:12:24,000
the answer was completely different.

258
0:12:26.74,000 --> 0:12:3,000
This process allows us to make decisions not based on democracy,

259
0:12:31.34,000 --> 0:12:33,000
not based on autocracy,

260
0:12:33.5,000 --> 0:12:38,000
but based on algorithms that take people's believability into consideration.

261
0:12:41.34,000 --> 0:12:42,000
Yup, we really do this.

262
0:12:43.06,000 --> 0:12:46,000
(Laughter)

263
0:12:46.38,000 --> 0:12:48,000
We do it because it eliminates

264
0:12:49.26,000 --> 0:12:53,000
what I believe to be one of the greatest tragedies of mankind,

265
0:12:53.74,000 --> 0:12:55,000
and that is people arrogantly,

266
0:12:56.58,000 --> 0:13:,000
naïvely holding opinions in their minds that are wrong,

267
0:13:01.06,000 --> 0:13:02,000
and acting on them,

268
0:13:02.34,000 --> 0:13:04,000
and not putting them out there to stress test them.

269
0:13:05.82,000 --> 0:13:06,000
And that's a tragedy.

270
0:13:07.18,000 --> 0:13:12,000
And we do it because it elevates ourselves above our own opinions

271
0:13:12.62,000 --> 0:13:14,000
so that we start to see things through everybody's eyes,

272
0:13:15.54,000 --> 0:13:16,000
and we see things collectively.

273
0:13:18.18,000 --> 0:13:22,000
Collective decision-making is so much better than individual decision-making

274
0:13:22.54,000 --> 0:13:23,000
if it's done well.

275
0:13:24.18,000 --> 0:13:26,000
It's been the secret sauce behind our success.

276
0:13:26.82,000 --> 0:13:28,000
It's why we've made more money for our clients

277
0:13:29.02,000 --> 0:13:3,000
than any other hedge fund in existence

278
0:13:30.98,000 --> 0:13:32,000
and made money 23 out of the last 26 years.

279
0:13:35.7,000 --> 0:13:39,000
So what's the problem with being radically truthful

280
0:13:40.26,000 --> 0:13:42,000
and radically transparent with each other?

281
0:13:45.22,000 --> 0:13:47,000
People say it's emotionally difficult.

282
0:13:48.06,000 --> 0:13:52,000
Critics say it's a formula for a brutal work environment.

283
0:13:53.22,000 --> 0:13:57,000
Neuroscientists tell me it has to do with how are brains are prewired.

284
0:13:58.1,000 --> 0:14:01,000
There's a part of our brain that would like to know our mistakes

285
0:14:01.34,000 --> 0:14:04,000
and like to look at our weaknesses so we could do better.

286
0:14:05.94,000 --> 0:14:07,000
I'm told that that's the prefrontal cortex.

287
0:14:08.86,000 --> 0:14:12,000
And then there's a part of our brain which views all of this as attacks.

288
0:14:13.74,000 --> 0:14:14,000
I'm told that that's the amygdala.

289
0:14:16.26,000 --> 0:14:19,000
In other words, there are two you's inside you:

290
0:14:19.34,000 --> 0:14:2,000
there's an emotional you

291
0:14:20.78,000 --> 0:14:21,000
and there's an intellectual you,

292
0:14:22.58,000 --> 0:14:23,000
and often they're at odds,

293
0:14:24.38,000 --> 0:14:25,000
and often they work against you.

294
0:14:26.98,000 --> 0:14:29,000
It's been our experience that we can win this battle.

295
0:14:30.74,000 --> 0:14:31,000
We win it as a group.

296
0:14:32.82,000 --> 0:14:34,000
It takes about 18 months typically

297
0:14:35.18,000 --> 0:14:38,000
to find that most people prefer operating this way,

298
0:14:38.26,000 --> 0:14:4,000
with this radical transparency

299
0:14:40.3,000 --> 0:14:43,000
than to be operating in a more opaque environment.

300
0:14:43.66,000 --> 0:14:47,000
There's not politics, there's not the brutality of --

301
0:14:47.98,000 --> 0:14:49,000
you know, all of that hidden, behind-the-scenes --

302
0:14:50.38,000 --> 0:14:52,000
there's an idea meritocracy where people can speak up.

303
0:14:53.34,000 --> 0:14:54,000
And that's been great.

304
0:14:54.62,000 --> 0:14:55,000
It's given us more effective work,

305
0:14:56.3,000 --> 0:14:58,000
and it's given us more effective relationships.

306
0:14:59.22,000 --> 0:15:,000
But it's not for everybody.

307
0:15:01.5,000 --> 0:15:03,000
We found something like 25 or 30 percent of the population

308
0:15:04.46,000 --> 0:15:05,000
it's just not for.

309
0:15:06.22,000 --> 0:15:07,000
And by the way,

310
0:15:07.46,000 --> 0:15:08,000
when I say radical transparency,

311
0:15:09.3,000 --> 0:15:11,000
I'm not saying transparency about everything.

312
0:15:11.66,000 --> 0:15:14,000
I mean, you don't have to tell somebody that their bald spot is growing

313
0:15:15.5,000 --> 0:15:16,000
or their baby's ugly.

314
0:15:17.14,000 --> 0:15:19,000
So, I'm just talking about --

315
0:15:19.26,000 --> 0:15:2,000
(Laughter)

316
0:15:20.5,000 --> 0:15:22,000
talking about the important things.

317
0:15:22.7,000 --> 0:15:23,000
So --

318
0:15:23.94,000 --> 0:15:26,000
(Laughter)

319
0:15:28.42,000 --> 0:15:29,000
So when you leave this room,

320
0:15:29.86,000 --> 0:15:33,000
I'd like you to observe yourself in conversations with others.

321
0:15:35.18,000 --> 0:15:38,000
Imagine if you knew what they were really thinking,

322
0:15:39.58,000 --> 0:15:41,000
and imagine if you knew what they were really like ...

323
0:15:43.66,000 --> 0:15:46,000
and imagine if they knew what you were really thinking

324
0:15:47.66,000 --> 0:15:48,000
and what were really like.

325
0:15:49.98,000 --> 0:15:51,000
It would certainly clear things up a lot

326
0:15:52.58,000 --> 0:15:54,000
and make your operations together more effective.

327
0:15:55.46,000 --> 0:15:57,000
I think it will improve your relationships.

328
0:15:58.42,000 --> 0:16:01,000
Now imagine that you can have algorithms

329
0:16:01.74,000 --> 0:16:04,000
that will help you gather all of that information

330
0:16:05.58,000 --> 0:16:09,000
and even help you make decisions in an idea-meritocratic way.

331
0:16:12.46,000 --> 0:16:16,000
This sort of radical transparency is coming at you

332
0:16:16.82,000 --> 0:16:17,000
and it is going to affect your life.

333
0:16:19.42,000 --> 0:16:21,000
And in my opinion,

334
0:16:21.5,000 --> 0:16:22,000
it's going to be wonderful.

335
0:16:22.86,000 --> 0:16:24,000
So I hope it is as wonderful for you

336
0:16:25.22,000 --> 0:16:26,000
as it is for me.

337
0:16:26.98,000 --> 0:16:27,000
Thank you very much.

338
0:16:28.26,000 --> 0:16:32,000
(Applause)

