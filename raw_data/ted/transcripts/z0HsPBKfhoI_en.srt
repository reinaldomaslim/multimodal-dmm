1
0:00:12.58,000 --> 0:00:15,000
When I was a kid, I was the quintessential nerd.

2
0:00:17.14,000 --> 0:00:19,000
I think some of you were, too.

3
0:00:19.34,000 --> 0:00:2,000
(Laughter)

4
0:00:20.58,000 --> 0:00:23,000
And you, sir, who laughed the loudest, you probably still are.

5
0:00:23.82,000 --> 0:00:25,000
(Laughter)

6
0:00:26.1,000 --> 0:00:29,000
I grew up in a small town in the dusty plains of north Texas,

7
0:00:29.62,000 --> 0:00:32,000
the son of a sheriff who was the son of a pastor.

8
0:00:32.98,000 --> 0:00:33,000
Getting into trouble was not an option.

9
0:00:35.86,000 --> 0:00:38,000
And so I started reading calculus books for fun.

10
0:00:39.14,000 --> 0:00:4,000
(Laughter)

11
0:00:40.7,000 --> 0:00:41,000
You did, too.

12
0:00:42.42,000 --> 0:00:45,000
That led me to building a laser and a computer and model rockets,

13
0:00:46.18,000 --> 0:00:49,000
and that led me to making rocket fuel in my bedroom.

14
0:00:49.78,000 --> 0:00:52,000
Now, in scientific terms,

15
0:00:53.46,000 --> 0:00:56,000
we call this a very bad idea.

16
0:00:56.74,000 --> 0:00:57,000
(Laughter)

17
0:00:57.98,000 --> 0:00:59,000
Around that same time,

18
0:01:00.18,000 --> 0:01:03,000
Stanley Kubrick's "2001: A Space Odyssey" came to the theaters,

19
0:01:03.42,000 --> 0:01:05,000
and my life was forever changed.

20
0:01:06.1,000 --> 0:01:08,000
I loved everything about that movie,

21
0:01:08.18,000 --> 0:01:1,000
especially the HAL 9000.

22
0:01:10.74,000 --> 0:01:12,000
Now, HAL was a sentient computer

23
0:01:12.82,000 --> 0:01:14,000
designed to guide the Discovery spacecraft

24
0:01:15.3,000 --> 0:01:17,000
from the Earth to Jupiter.

25
0:01:17.86,000 --> 0:01:19,000
HAL was also a flawed character,

26
0:01:19.94,000 --> 0:01:23,000
for in the end he chose to value the mission over human life.

27
0:01:24.66,000 --> 0:01:26,000
Now, HAL was a fictional character,

28
0:01:26.78,000 --> 0:01:28,000
but nonetheless he speaks to our fears,

29
0:01:29.46,000 --> 0:01:31,000
our fears of being subjugated

30
0:01:31.58,000 --> 0:01:34,000
by some unfeeling, artificial intelligence

31
0:01:34.62,000 --> 0:01:35,000
who is indifferent to our humanity.

32
0:01:37.7,000 --> 0:01:39,000
I believe that such fears are unfounded.

33
0:01:40.3,000 --> 0:01:42,000
Indeed, we stand at a remarkable time

34
0:01:43.02,000 --> 0:01:44,000
in human history,

35
0:01:44.58,000 --> 0:01:48,000
where, driven by refusal to accept the limits of our bodies and our minds,

36
0:01:49.58,000 --> 0:01:5,000
we are building machines

37
0:01:51.3,000 --> 0:01:54,000
of exquisite, beautiful complexity and grace

38
0:01:54.94,000 --> 0:01:56,000
that will extend the human experience

39
0:01:57.02,000 --> 0:01:58,000
in ways beyond our imagining.

40
0:01:59.54,000 --> 0:02:01,000
After a career that led me from the Air Force Academy

41
0:02:02.14,000 --> 0:02:03,000
to Space Command to now,

42
0:02:04.1,000 --> 0:02:05,000
I became a systems engineer,

43
0:02:05.82,000 --> 0:02:07,000
and recently I was drawn into an engineering problem

44
0:02:08.58,000 --> 0:02:1,000
associated with NASA's mission to Mars.

45
0:02:11.18,000 --> 0:02:13,000
Now, in space flights to the Moon,

46
0:02:13.7,000 --> 0:02:16,000
we can rely upon mission control in Houston

47
0:02:16.86,000 --> 0:02:17,000
to watch over all aspects of a flight.

48
0:02:18.86,000 --> 0:02:21,000
However, Mars is 200 times further away,

49
0:02:22.42,000 --> 0:02:25,000
and as a result it takes on average 13 minutes

50
0:02:25.66,000 --> 0:02:28,000
for a signal to travel from the Earth to Mars.

51
0:02:28.82,000 --> 0:02:31,000
If there's trouble, there's not enough time.

52
0:02:32.66,000 --> 0:02:34,000
And so a reasonable engineering solution

53
0:02:35.18,000 --> 0:02:37,000
calls for us to put mission control

54
0:02:37.78,000 --> 0:02:4,000
inside the walls of the Orion spacecraft.

55
0:02:40.82,000 --> 0:02:42,000
Another fascinating idea in the mission profile

56
0:02:43.74,000 --> 0:02:45,000
places humanoid robots on the surface of Mars

57
0:02:46.66,000 --> 0:02:47,000
before the humans themselves arrive,

58
0:02:48.54,000 --> 0:02:49,000
first to build facilities

59
0:02:50.22,000 --> 0:02:53,000
and later to serve as collaborative members of the science team.

60
0:02:55.22,000 --> 0:02:57,000
Now, as I looked at this from an engineering perspective,

61
0:02:57.98,000 --> 0:03:,000
it became very clear to me that what I needed to architect

62
0:03:01.18,000 --> 0:03:03,000
was a smart, collaborative,

63
0:03:03.38,000 --> 0:03:05,000
socially intelligent artificial intelligence.

64
0:03:05.78,000 --> 0:03:09,000
In other words, I needed to build something very much like a HAL

65
0:03:10.1,000 --> 0:03:12,000
but without the homicidal tendencies.

66
0:03:12.54,000 --> 0:03:13,000
(Laughter)

67
0:03:14.74,000 --> 0:03:15,000
Let's pause for a moment.

68
0:03:16.58,000 --> 0:03:19,000
Is it really possible to build an artificial intelligence like that?

69
0:03:20.5,000 --> 0:03:21,000
Actually, it is.

70
0:03:21.98,000 --> 0:03:22,000
In many ways,

71
0:03:23.26,000 --> 0:03:24,000
this is a hard engineering problem

72
0:03:25.26,000 --> 0:03:26,000
with elements of AI,

73
0:03:26.74,000 --> 0:03:3,000
not some wet hair ball of an AI problem that needs to be engineered.

74
0:03:31.46,000 --> 0:03:33,000
To paraphrase Alan Turing,

75
0:03:34.14,000 --> 0:03:36,000
I'm not interested in building a sentient machine.

76
0:03:36.54,000 --> 0:03:37,000
I'm not building a HAL.

77
0:03:38.14,000 --> 0:03:4,000
All I'm after is a simple brain,

78
0:03:40.58,000 --> 0:03:43,000
something that offers the illusion of intelligence.

79
0:03:44.82,000 --> 0:03:47,000
The art and the science of computing have come a long way

80
0:03:47.98,000 --> 0:03:48,000
since HAL was onscreen,

81
0:03:49.5,000 --> 0:03:52,000
and I'd imagine if his inventor Dr. Chandra were here today,

82
0:03:52.74,000 --> 0:03:54,000
he'd have a whole lot of questions for us.

83
0:03:55.1,000 --> 0:03:57,000
Is it really possible for us

84
0:03:57.22,000 --> 0:04:01,000
to take a system of millions upon millions of devices,

85
0:04:01.26,000 --> 0:04:02,000
to read in their data streams,

86
0:04:02.74,000 --> 0:04:04,000
to predict their failures and act in advance?

87
0:04:05.02,000 --> 0:04:06,000
Yes.

88
0:04:06.26,000 --> 0:04:09,000
Can we build systems that converse with humans in natural language?

89
0:04:09.46,000 --> 0:04:1,000
Yes.

90
0:04:10.7,000 --> 0:04:12,000
Can we build systems that recognize objects, identify emotions,

91
0:04:13.7,000 --> 0:04:16,000
emote themselves, play games and even read lips?

92
0:04:17.1,000 --> 0:04:18,000
Yes.

93
0:04:18.34,000 --> 0:04:2,000
Can we build a system that sets goals,

94
0:04:20.5,000 --> 0:04:23,000
that carries out plans against those goals and learns along the way?

95
0:04:24.14,000 --> 0:04:25,000
Yes.

96
0:04:25.38,000 --> 0:04:28,000
Can we build systems that have a theory of mind?

97
0:04:28.74,000 --> 0:04:29,000
This we are learning to do.

98
0:04:30.26,000 --> 0:04:33,000
Can we build systems that have an ethical and moral foundation?

99
0:04:34.3,000 --> 0:04:36,000
This we must learn how to do.

100
0:04:37.18,000 --> 0:04:38,000
So let's accept for a moment

101
0:04:38.58,000 --> 0:04:4,000
that it's possible to build such an artificial intelligence

102
0:04:41.5,000 --> 0:04:43,000
for this kind of mission and others.

103
0:04:43.66,000 --> 0:04:45,000
The next question you must ask yourself is,

104
0:04:46.22,000 --> 0:04:47,000
should we fear it?

105
0:04:47.7,000 --> 0:04:48,000
Now, every new technology

106
0:04:49.7,000 --> 0:04:51,000
brings with it some measure of trepidation.

107
0:04:52.62,000 --> 0:04:53,000
When we first saw cars,

108
0:04:54.34,000 --> 0:04:58,000
people lamented that we would see the destruction of the family.

109
0:04:58.38,000 --> 0:05:,000
When we first saw telephones come in,

110
0:05:01.1,000 --> 0:05:03,000
people were worried it would destroy all civil conversation.

111
0:05:04.02,000 --> 0:05:07,000
At a point in time we saw the written word become pervasive,

112
0:05:07.98,000 --> 0:05:09,000
people thought we would lose our ability to memorize.

113
0:05:10.5,000 --> 0:05:12,000
These things are all true to a degree,

114
0:05:12.58,000 --> 0:05:14,000
but it's also the case that these technologies

115
0:05:15.02,000 --> 0:05:18,000
brought to us things that extended the human experience

116
0:05:18.42,000 --> 0:05:19,000
in some profound ways.

117
0:05:21.66,000 --> 0:05:23,000
So let's take this a little further.

118
0:05:24.94,000 --> 0:05:28,000
I do not fear the creation of an AI like this,

119
0:05:29.7,000 --> 0:05:32,000
because it will eventually embody some of our values.

120
0:05:33.54,000 --> 0:05:36,000
Consider this: building a cognitive system is fundamentally different

121
0:05:37.06,000 --> 0:05:4,000
than building a traditional software-intensive system of the past.

122
0:05:40.38,000 --> 0:05:42,000
We don't program them. We teach them.

123
0:05:42.86,000 --> 0:05:44,000
In order to teach a system how to recognize flowers,

124
0:05:45.54,000 --> 0:05:48,000
I show it thousands of flowers of the kinds I like.

125
0:05:48.58,000 --> 0:05:5,000
In order to teach a system how to play a game --

126
0:05:50.86,000 --> 0:05:51,000
Well, I would. You would, too.

127
0:05:54.42,000 --> 0:05:56,000
I like flowers. Come on.

128
0:05:57.26,000 --> 0:05:59,000
To teach a system how to play a game like Go,

129
0:06:00.14,000 --> 0:06:02,000
I'd have it play thousands of games of Go,

130
0:06:02.22,000 --> 0:06:03,000
but in the process I also teach it

131
0:06:03.9,000 --> 0:06:05,000
how to discern a good game from a bad game.

132
0:06:06.34,000 --> 0:06:09,000
If I want to create an artificially intelligent legal assistant,

133
0:06:10.06,000 --> 0:06:11,000
I will teach it some corpus of law

134
0:06:11.86,000 --> 0:06:13,000
but at the same time I am fusing with it

135
0:06:14.74,000 --> 0:06:16,000
the sense of mercy and justice that is part of that law.

136
0:06:18.38,000 --> 0:06:2,000
In scientific terms, this is what we call ground truth,

137
0:06:21.38,000 --> 0:06:23,000
and here's the important point:

138
0:06:23.42,000 --> 0:06:24,000
in producing these machines,

139
0:06:24.9,000 --> 0:06:27,000
we are therefore teaching them a sense of our values.

140
0:06:28.34,000 --> 0:06:31,000
To that end, I trust an artificial intelligence

141
0:06:31.5,000 --> 0:06:34,000
the same, if not more, as a human who is well-trained.

142
0:06:35.9,000 --> 0:06:36,000
But, you may ask,

143
0:06:37.14,000 --> 0:06:39,000
what about rogue agents,

144
0:06:39.78,000 --> 0:06:42,000
some well-funded nongovernment organization?

145
0:06:43.14,000 --> 0:06:46,000
I do not fear an artificial intelligence in the hand of a lone wolf.

146
0:06:46.98,000 --> 0:06:5,000
Clearly, we cannot protect ourselves against all random acts of violence,

147
0:06:51.54,000 --> 0:06:53,000
but the reality is such a system

148
0:06:53.7,000 --> 0:06:56,000
requires substantial training and subtle training

149
0:06:56.82,000 --> 0:06:58,000
far beyond the resources of an individual.

150
0:06:59.14,000 --> 0:07:,000
And furthermore,

151
0:07:00.38,000 --> 0:07:03,000
it's far more than just injecting an internet virus to the world,

152
0:07:03.66,000 --> 0:07:06,000
where you push a button, all of a sudden it's in a million places

153
0:07:06.78,000 --> 0:07:08,000
and laptops start blowing up all over the place.

154
0:07:09.26,000 --> 0:07:11,000
Now, these kinds of substances are much larger,

155
0:07:12.1,000 --> 0:07:13,000
and we'll certainly see them coming.

156
0:07:14.34,000 --> 0:07:17,000
Do I fear that such an artificial intelligence

157
0:07:17.42,000 --> 0:07:18,000
might threaten all of humanity?

158
0:07:20.1,000 --> 0:07:24,000
If you look at movies such as "The Matrix," "Metropolis,"

159
0:07:24.5,000 --> 0:07:27,000
"The Terminator," shows such as "Westworld,"

160
0:07:27.7,000 --> 0:07:29,000
they all speak of this kind of fear.

161
0:07:29.86,000 --> 0:07:33,000
Indeed, in the book "Superintelligence" by the philosopher Nick Bostrom,

162
0:07:34.18,000 --> 0:07:35,000
he picks up on this theme

163
0:07:35.74,000 --> 0:07:39,000
and observes that a superintelligence might not only be dangerous,

164
0:07:39.78,000 --> 0:07:42,000
it could represent an existential threat to all of humanity.

165
0:07:43.66,000 --> 0:07:45,000
Dr. Bostrom's basic argument

166
0:07:45.9,000 --> 0:07:47,000
is that such systems will eventually

167
0:07:48.66,000 --> 0:07:51,000
have such an insatiable thirst for information

168
0:07:51.94,000 --> 0:07:53,000
that they will perhaps learn how to learn

169
0:07:54.86,000 --> 0:07:56,000
and eventually discover that they may have goals

170
0:07:57.5,000 --> 0:07:59,000
that are contrary to human needs.

171
0:07:59.82,000 --> 0:08:,000
Dr. Bostrom has a number of followers.

172
0:08:01.7,000 --> 0:08:05,000
He is supported by people such as Elon Musk and Stephen Hawking.

173
0:08:06.7,000 --> 0:08:08,000
With all due respect

174
0:08:09.98,000 --> 0:08:11,000
to these brilliant minds,

175
0:08:12.02,000 --> 0:08:14,000
I believe that they are fundamentally wrong.

176
0:08:14.3,000 --> 0:08:17,000
Now, there are a lot of pieces of Dr. Bostrom's argument to unpack,

177
0:08:17.5,000 --> 0:08:19,000
and I don't have time to unpack them all,

178
0:08:19.66,000 --> 0:08:21,000
but very briefly, consider this:

179
0:08:22.38,000 --> 0:08:25,000
super knowing is very different than super doing.

180
0:08:26.14,000 --> 0:08:27,000
HAL was a threat to the Discovery crew

181
0:08:28.06,000 --> 0:08:32,000
only insofar as HAL commanded all aspects of the Discovery.

182
0:08:32.5,000 --> 0:08:34,000
So it would have to be with a superintelligence.

183
0:08:35.02,000 --> 0:08:37,000
It would have to have dominion over all of our world.

184
0:08:37.54,000 --> 0:08:39,000
This is the stuff of Skynet from the movie "The Terminator"

185
0:08:40.38,000 --> 0:08:41,000
in which we had a superintelligence

186
0:08:42.26,000 --> 0:08:43,000
that commanded human will,

187
0:08:43.66,000 --> 0:08:46,000
that directed every device that was in every corner of the world.

188
0:08:47.54,000 --> 0:08:48,000
Practically speaking,

189
0:08:49.02,000 --> 0:08:51,000
it ain't gonna happen.

190
0:08:51.14,000 --> 0:08:54,000
We are not building AIs that control the weather,

191
0:08:54.22,000 --> 0:08:55,000
that direct the tides,

192
0:08:55.58,000 --> 0:08:58,000
that command us capricious, chaotic humans.

193
0:08:58.98,000 --> 0:09:01,000
And furthermore, if such an artificial intelligence existed,

194
0:09:02.9,000 --> 0:09:04,000
it would have to compete with human economies,

195
0:09:05.86,000 --> 0:09:07,000
and thereby compete for resources with us.

196
0:09:09.02,000 --> 0:09:1,000
And in the end --

197
0:09:10.26,000 --> 0:09:11,000
don't tell Siri this --

198
0:09:12.26,000 --> 0:09:13,000
we can always unplug them.

199
0:09:13.66,000 --> 0:09:15,000
(Laughter)

200
0:09:17.18,000 --> 0:09:19,000
We are on an incredible journey

201
0:09:19.66,000 --> 0:09:21,000
of coevolution with our machines.

202
0:09:22.18,000 --> 0:09:24,000
The humans we are today

203
0:09:24.7,000 --> 0:09:26,000
are not the humans we will be then.

204
0:09:27.26,000 --> 0:09:3,000
To worry now about the rise of a superintelligence

205
0:09:30.42,000 --> 0:09:33,000
is in many ways a dangerous distraction

206
0:09:33.5,000 --> 0:09:35,000
because the rise of computing itself

207
0:09:35.86,000 --> 0:09:38,000
brings to us a number of human and societal issues

208
0:09:38.9,000 --> 0:09:39,000
to which we must now attend.

209
0:09:41.18,000 --> 0:09:43,000
How shall I best organize society

210
0:09:44.02,000 --> 0:09:46,000
when the need for human labor diminishes?

211
0:09:46.38,000 --> 0:09:49,000
How can I bring understanding and education throughout the globe

212
0:09:50.22,000 --> 0:09:51,000
and still respect our differences?

213
0:09:52.02,000 --> 0:09:56,000
How might I extend and enhance human life through cognitive healthcare?

214
0:09:56.3,000 --> 0:09:58,000
How might I use computing

215
0:09:59.18,000 --> 0:10:,000
to help take us to the stars?

216
0:10:01.58,000 --> 0:10:03,000
And that's the exciting thing.

217
0:10:04.22,000 --> 0:10:06,000
The opportunities to use computing

218
0:10:06.58,000 --> 0:10:07,000
to advance the human experience

219
0:10:08.14,000 --> 0:10:09,000
are within our reach,

220
0:10:09.58,000 --> 0:10:1,000
here and now,

221
0:10:11.46,000 --> 0:10:12,000
and we are just beginning.

222
0:10:14.1,000 --> 0:10:15,000
Thank you very much.

223
0:10:15.34,000 --> 0:10:19,000
(Applause)

