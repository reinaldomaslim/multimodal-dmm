1
0:00:12.777,000 --> 0:00:13,000
I'm James.

2
0:00:13.984,000 --> 0:00:14,000
I'm a writer and artist,

3
0:00:15.694,000 --> 0:00:17,000
and I make work about technology.

4
0:00:18.454,000 --> 0:00:21,000
I do things like draw life-size outlines of military drones

5
0:00:22.389,000 --> 0:00:23,000
in city streets around the world,

6
0:00:24.241,000 --> 0:00:26,000
so that people can start to think and get their heads around

7
0:00:27.215,000 --> 0:00:3,000
these really quite hard-to-see and hard-to-think-about technologies.

8
0:00:31.494,000 --> 0:00:34,000
I make things like neural networks that predict the results of elections

9
0:00:35.354,000 --> 0:00:36,000
based on weather reports,

10
0:00:37.115,000 --> 0:00:38,000
because I'm intrigued about

11
0:00:38.453,000 --> 0:00:41,000
what the actual possibilities of these weird new technologies are.

12
0:00:43.405,000 --> 0:00:45,000
Last year, I built my own self-driving car.

13
0:00:45.855,000 --> 0:00:47,000
But because I don't really trust technology,

14
0:00:48.405,000 --> 0:00:49,000
I also designed a trap for it.

15
0:00:50.777,000 --> 0:00:51,000
(Laughter)

16
0:00:51.887,000 --> 0:00:55,000
And I do these things mostly because I find them completely fascinating,

17
0:00:56.209,000 --> 0:00:58,000
but also because I think when we talk about technology,

18
0:00:58.835,000 --> 0:01:,000
we're largely talking about ourselves

19
0:01:01.478,000 --> 0:01:03,000
and the way that we understand the world.

20
0:01:03.801,000 --> 0:01:05,000
So here's a story about technology.

21
0:01:07.52,000 --> 0:01:09,000
This is a "surprise egg" video.

22
0:01:10.374,000 --> 0:01:13,000
It's basically a video of someone opening up loads of chocolate eggs

23
0:01:13.746,000 --> 0:01:15,000
and showing the toys inside to the viewer.

24
0:01:16.461,000 --> 0:01:18,000
That's it. That's all it does for seven long minutes.

25
0:01:19.428,000 --> 0:01:22,000
And I want you to notice two things about this.

26
0:01:22.503,000 --> 0:01:26,000
First of all, this video has 30 million views.

27
0:01:26.601,000 --> 0:01:27,000
(Laughter)

28
0:01:28.376,000 --> 0:01:29,000
And the other thing is,

29
0:01:29.566,000 --> 0:01:32,000
it comes from a channel that has 6.3 million subscribers,

30
0:01:33.459,000 --> 0:01:35,000
that has a total of eight billion views,

31
0:01:36.163,000 --> 0:01:39,000
and it's all just more videos like this --

32
0:01:40.256,000 --> 0:01:43,000
30 million people watching a guy opening up these eggs.

33
0:01:44.188,000 --> 0:01:48,000
It sounds pretty weird, but if you search for "surprise eggs" on YouTube,

34
0:01:48.693,000 --> 0:01:51,000
it'll tell you there's 10 million of these videos,

35
0:01:52.24,000 --> 0:01:53,000
and I think that's an undercount.

36
0:01:53.921,000 --> 0:01:54,000
I think there's way, way more of these.

37
0:01:55.842,000 --> 0:01:57,000
If you keep searching, they're endless.

38
0:01:58.108,000 --> 0:02:,000
There's millions and millions of these videos

39
0:02:00.291,000 --> 0:02:03,000
in increasingly baroque combinations of brands and materials,

40
0:02:03.769,000 --> 0:02:06,000
and there's more and more of them being uploaded every single day.

41
0:02:07.639,000 --> 0:02:1,000
Like, this is a strange world. Right?

42
0:02:11.174,000 --> 0:02:14,000
But the thing is, it's not adults who are watching these videos.

43
0:02:14.581,000 --> 0:02:16,000
It's kids, small children.

44
0:02:17.526,000 --> 0:02:19,000
These videos are like crack for little kids.

45
0:02:19.704,000 --> 0:02:21,000
There's something about the repetition,

46
0:02:21.803,000 --> 0:02:23,000
the constant little dopamine hit of the reveal,

47
0:02:24.295,000 --> 0:02:25,000
that completely hooks them in.

48
0:02:26.185,000 --> 0:02:3,000
And little kids watch these videos over and over and over again,

49
0:02:31.018,000 --> 0:02:33,000
and they do it for hours and hours and hours.

50
0:02:33.369,000 --> 0:02:35,000
And if you try and take the screen away from them,

51
0:02:35.749,000 --> 0:02:36,000
they'll scream and scream and scream.

52
0:02:37.555,000 --> 0:02:38,000
If you don't believe me --

53
0:02:38.841,000 --> 0:02:4,000
and I've already seen people in the audience nodding --

54
0:02:41.472,000 --> 0:02:44,000
if you don't believe me, find someone with small children and ask them,

55
0:02:44.887,000 --> 0:02:46,000
and they'll know about the surprise egg videos.

56
0:02:47.251,000 --> 0:02:49,000
So this is where we start.

57
0:02:49.345,000 --> 0:02:52,000
It's 2018, and someone, or lots of people,

58
0:02:53.011,000 --> 0:02:56,000
are using the same mechanism that, like, Facebook and Instagram are using

59
0:02:56.976,000 --> 0:02:57,000
to get you to keep checking that app,

60
0:02:58.989,000 --> 0:03:01,000
and they're using it on YouTube to hack the brains of very small children

61
0:03:02.998,000 --> 0:03:03,000
in return for advertising revenue.

62
0:03:06.346,000 --> 0:03:08,000
At least, I hope that's what they're doing.

63
0:03:08.371,000 --> 0:03:09,000
I hope that's what they're doing it for,

64
0:03:10.35,000 --> 0:03:15,000
because there's easier ways of making ad revenue on YouTube.

65
0:03:15.682,000 --> 0:03:17,000
You can just make stuff up or steal stuff.

66
0:03:18.038,000 --> 0:03:2,000
So if you search for really popular kids' cartoons

67
0:03:20.697,000 --> 0:03:21,000
like "Peppa Pig" or "Paw Patrol,"

68
0:03:22.375,000 --> 0:03:25,000
you'll find there's millions and millions of these online as well.

69
0:03:25.546,000 --> 0:03:28,000
Of course, most of them aren't posted by the original content creators.

70
0:03:28.922,000 --> 0:03:3,000
They come from loads and loads of different random accounts,

71
0:03:31.945,000 --> 0:03:33,000
and it's impossible to know who's posting them

72
0:03:34.209,000 --> 0:03:35,000
or what their motives might be.

73
0:03:36.428,000 --> 0:03:37,000
Does that sound kind of familiar?

74
0:03:38.382,000 --> 0:03:39,000
Because it's exactly the same mechanism

75
0:03:40.386,000 --> 0:03:42,000
that's happening across most of our digital services,

76
0:03:43.01,000 --> 0:03:46,000
where it's impossible to know where this information is coming from.

77
0:03:46.241,000 --> 0:03:47,000
It's basically fake news for kids,

78
0:03:48.094,000 --> 0:03:5,000
and we're training them from birth

79
0:03:50.279,000 --> 0:03:52,000
to click on the very first link that comes along,

80
0:03:52.809,000 --> 0:03:53,000
regardless of what the source is.

81
0:03:54.786,000 --> 0:03:56,000
That's doesn't seem like a terribly good idea.

82
0:03:58.399,000 --> 0:04:,000
Here's another thing that's really big on kids' YouTube.

83
0:04:01.133,000 --> 0:04:02,000
This is called the "Finger Family Song."

84
0:04:03.085,000 --> 0:04:05,000
I just heard someone groan in the audience.

85
0:04:05.127,000 --> 0:04:06,000
This is the "Finger Family Song."

86
0:04:06.775,000 --> 0:04:07,000
This is the very first one I could find.

87
0:04:08.729,000 --> 0:04:1,000
It's from 2007, and it only has 200,000 views,

88
0:04:11.582,000 --> 0:04:12,000
which is, like, nothing in this game.

89
0:04:13.582,000 --> 0:04:15,000
But it has this insanely earwormy tune,

90
0:04:16.458,000 --> 0:04:17,000
which I'm not going to play to you,

91
0:04:18.164,000 --> 0:04:2,000
because it will sear itself into your brain

92
0:04:20.196,000 --> 0:04:22,000
in the same way that it seared itself into mine,

93
0:04:22.615,000 --> 0:04:23,000
and I'm not going to do that to you.

94
0:04:24.409,000 --> 0:04:25,000
But like the surprise eggs,

95
0:04:25.777,000 --> 0:04:27,000
it's got inside kids' heads

96
0:04:27.965,000 --> 0:04:28,000
and addicted them to it.

97
0:04:29.596,000 --> 0:04:31,000
So within a few years, these finger family videos

98
0:04:32.151,000 --> 0:04:33,000
start appearing everywhere,

99
0:04:33.478,000 --> 0:04:35,000
and you get versions in different languages

100
0:04:35.531,000 --> 0:04:37,000
with popular kids' cartoons using food

101
0:04:37.676,000 --> 0:04:39,000
or, frankly, using whatever kind of animation elements

102
0:04:40.25,000 --> 0:04:42,000
you seem to have lying around.

103
0:04:43.002,000 --> 0:04:48,000
And once again, there are millions and millions and millions of these videos

104
0:04:48.223,000 --> 0:04:51,000
available online in all of these kind of insane combinations.

105
0:04:51.682,000 --> 0:04:53,000
And the more time you start to spend with them,

106
0:04:53.934,000 --> 0:04:56,000
the crazier and crazier you start to feel that you might be.

107
0:04:57.652,000 --> 0:05:,000
And that's where I kind of launched into this,

108
0:05:01.009,000 --> 0:05:04,000
that feeling of deep strangeness and deep lack of understanding

109
0:05:04.699,000 --> 0:05:08,000
of how this thing was constructed that seems to be presented around me.

110
0:05:08.898,000 --> 0:05:11,000
Because it's impossible to know where these things are coming from.

111
0:05:12.089,000 --> 0:05:13,000
Like, who is making them?

112
0:05:13.354,000 --> 0:05:16,000
Some of them appear to be made of teams of professional animators.

113
0:05:16.521,000 --> 0:05:18,000
Some of them are just randomly assembled by software.

114
0:05:19.427,000 --> 0:05:23,000
Some of them are quite wholesome-looking young kids' entertainers.

115
0:05:23.704,000 --> 0:05:24,000
And some of them are from people

116
0:05:25.28,000 --> 0:05:28,000
who really clearly shouldn't be around children at all.

117
0:05:28.311,000 --> 0:05:29,000
(Laughter)

118
0:05:30.987,000 --> 0:05:34,000
And once again, this impossibility of figuring out who's making this stuff --

119
0:05:35.651,000 --> 0:05:36,000
like, this is a bot?

120
0:05:36.831,000 --> 0:05:38,000
Is this a person? Is this a troll?

121
0:05:39.502,000 --> 0:05:41,000
What does it mean that we can't tell the difference

122
0:05:41.908,000 --> 0:05:42,000
between these things anymore?

123
0:05:43.515,000 --> 0:05:47,000
And again, doesn't that uncertainty feel kind of familiar right now?

124
0:05:50.145,000 --> 0:05:52,000
So the main way people get views on their videos --

125
0:05:52.749,000 --> 0:05:53,000
and remember, views mean money --

126
0:05:54.48,000 --> 0:05:58,000
is that they stuff the titles of these videos with these popular terms.

127
0:05:59.246,000 --> 0:06:,000
So you take, like, "surprise eggs"

128
0:06:00.957,000 --> 0:06:02,000
and then you add "Paw Patrol," "Easter egg,"

129
0:06:03.047,000 --> 0:06:04,000
or whatever these things are,

130
0:06:04.464,000 --> 0:06:06,000
all of these words from other popular videos into your title,

131
0:06:07.381,000 --> 0:06:09,000
until you end up with this kind of meaningless mash of language

132
0:06:10.373,000 --> 0:06:12,000
that doesn't make sense to humans at all.

133
0:06:12.895,000 --> 0:06:15,000
Because of course it's only really tiny kids who are watching your video,

134
0:06:16.465,000 --> 0:06:17,000
and what the hell do they know?

135
0:06:18.316,000 --> 0:06:21,000
Your real audience for this stuff is software.

136
0:06:21.347,000 --> 0:06:22,000
It's the algorithms.

137
0:06:22.527,000 --> 0:06:23,000
It's the software that YouTube uses

138
0:06:24.406,000 --> 0:06:26,000
to select which videos are like other videos,

139
0:06:26.913,000 --> 0:06:28,000
to make them popular, to make them recommended.

140
0:06:29.18,000 --> 0:06:32,000
And that's why you end up with this kind of completely meaningless mash,

141
0:06:32.665,000 --> 0:06:34,000
both of title and of content.

142
0:06:35.792,000 --> 0:06:36,000
But the thing is, you have to remember,

143
0:06:37.71,000 --> 0:06:41,000
there really are still people within this algorithmically optimized system,

144
0:06:42.212,000 --> 0:06:44,000
people who are kind of increasingly forced to act out

145
0:06:45.026,000 --> 0:06:48,000
these increasingly bizarre combinations of words,

146
0:06:48.116,000 --> 0:06:53,000
like a desperate improvisation artist responding to the combined screams

147
0:06:53.313,000 --> 0:06:55,000
of a million toddlers at once.

148
0:06:57.168,000 --> 0:06:59,000
There are real people trapped within these systems,

149
0:06:59.66,000 --> 0:07:03,000
and that's the other deeply strange thing about this algorithmically driven culture,

150
0:07:03.739,000 --> 0:07:04,000
because even if you're human,

151
0:07:05.144,000 --> 0:07:07,000
you have to end up behaving like a machine

152
0:07:07.313,000 --> 0:07:08,000
just to survive.

153
0:07:09.137,000 --> 0:07:11,000
And also, on the other side of the screen,

154
0:07:11.261,000 --> 0:07:13,000
there still are these little kids watching this stuff,

155
0:07:14.232,000 --> 0:07:18,000
stuck, their full attention grabbed by these weird mechanisms.

156
0:07:18.768,000 --> 0:07:2,000
And most of these kids are too small to even use a website.

157
0:07:21.59,000 --> 0:07:24,000
They're just kind of hammering on the screen with their little hands.

158
0:07:24.89,000 --> 0:07:25,000
And so there's autoplay,

159
0:07:26.131,000 --> 0:07:29,000
where it just keeps playing these videos over and over and over in a loop,

160
0:07:29.734,000 --> 0:07:31,000
endlessly for hours and hours at a time.

161
0:07:31.817,000 --> 0:07:33,000
And there's so much weirdness in the system now

162
0:07:34.684,000 --> 0:07:37,000
that autoplay takes you to some pretty strange places.

163
0:07:37.717,000 --> 0:07:39,000
This is how, within a dozen steps,

164
0:07:40.229,000 --> 0:07:43,000
you can go from a cute video of a counting train

165
0:07:43.411,000 --> 0:07:45,000
to masturbating Mickey Mouse.

166
0:07:46.529,000 --> 0:07:48,000
Yeah. I'm sorry about that.

167
0:07:48.841,000 --> 0:07:49,000
This does get worse.

168
0:07:50.565,000 --> 0:07:51,000
This is what happens

169
0:07:51.871,000 --> 0:07:54,000
when all of these different keywords,

170
0:07:54.981,000 --> 0:07:56,000
all these different pieces of attention,

171
0:07:57.466,000 --> 0:07:59,000
this desperate generation of content,

172
0:08:00.297,000 --> 0:08:02,000
all comes together into a single place.

173
0:08:03.871,000 --> 0:08:07,000
This is where all those deeply weird keywords come home to roost.

174
0:08:08.367,000 --> 0:08:1,000
You cross-breed the finger family video

175
0:08:10.782,000 --> 0:08:12,000
with some live-action superhero stuff,

176
0:08:12.894,000 --> 0:08:15,000
you add in some weird, trollish in-jokes or something,

177
0:08:16.174,000 --> 0:08:19,000
and suddenly, you come to a very weird place indeed.

178
0:08:19.564,000 --> 0:08:21,000
The stuff that tends to upset parents

179
0:08:21.701,000 --> 0:08:24,000
is the stuff that has kind of violent or sexual content, right?

180
0:08:25.056,000 --> 0:08:27,000
Children's cartoons getting assaulted,

181
0:08:27.902,000 --> 0:08:29,000
getting killed,

182
0:08:29.944,000 --> 0:08:32,000
weird pranks that actually genuinely terrify children.

183
0:08:33.311,000 --> 0:08:36,000
What you have is software pulling in all of these different influences

184
0:08:37.01,000 --> 0:08:39,000
to automatically generate kids' worst nightmares.

185
0:08:39.995,000 --> 0:08:41,000
And this stuff really, really does affect small children.

186
0:08:42.72,000 --> 0:08:44,000
Parents report their children being traumatized,

187
0:08:45.61,000 --> 0:08:46,000
becoming afraid of the dark,

188
0:08:47.026,000 --> 0:08:5,000
becoming afraid of their favorite cartoon characters.

189
0:08:50.524,000 --> 0:08:53,000
If you take one thing away from this, it's that if you have small children,

190
0:08:54.159,000 --> 0:08:55,000
keep them the hell away from YouTube.

191
0:08:56.743,000 --> 0:08:59,000
(Applause)

192
0:09:02.504,000 --> 0:09:05,000
But the other thing, the thing that really gets to me about this,

193
0:09:05.624,000 --> 0:09:09,000
is that I'm not sure we even really understand how we got to this point.

194
0:09:10.951,000 --> 0:09:12,000
We've taken all of this influence, all of these things,

195
0:09:13.906,000 --> 0:09:15,000
and munged them together in a way that no one really intended.

196
0:09:16.883,000 --> 0:09:19,000
And yet, this is also the way that we're building the entire world.

197
0:09:20.063,000 --> 0:09:21,000
We're taking all of this data,

198
0:09:21.86,000 --> 0:09:22,000
a lot of it bad data,

199
0:09:23.331,000 --> 0:09:26,000
a lot of historical data full of prejudice,

200
0:09:26.384,000 --> 0:09:28,000
full of all of our worst impulses of history,

201
0:09:29.245,000 --> 0:09:31,000
and we're building that into huge data sets

202
0:09:31.318,000 --> 0:09:32,000
and then we're automating it.

203
0:09:32.765,000 --> 0:09:35,000
And we're munging it together into things like credit reports,

204
0:09:36.291,000 --> 0:09:37,000
into insurance premiums,

205
0:09:37.949,000 --> 0:09:39,000
into things like predictive policing systems,

206
0:09:40.666,000 --> 0:09:41,000
into sentencing guidelines.

207
0:09:42.452,000 --> 0:09:44,000
This is the way we're actually constructing the world today

208
0:09:45.297,000 --> 0:09:46,000
out of this data.

209
0:09:46.472,000 --> 0:09:47,000
And I don't know what's worse,

210
0:09:48.194,000 --> 0:09:51,000
that we built a system that seems to be entirely optimized

211
0:09:51.446,000 --> 0:09:53,000
for the absolute worst aspects of human behavior,

212
0:09:54.278,000 --> 0:09:56,000
or that we seem to have done it by accident,

213
0:09:56.727,000 --> 0:09:58,000
without even realizing that we were doing it,

214
0:09:58.958,000 --> 0:10:01,000
because we didn't really understand the systems that we were building,

215
0:10:02.364,000 --> 0:10:05,000
and we didn't really understand how to do anything differently with it.

216
0:10:06.769,000 --> 0:10:09,000
There's a couple of things I think that really seem to be driving this

217
0:10:10.158,000 --> 0:10:11,000
most fully on YouTube,

218
0:10:11.371,000 --> 0:10:12,000
and the first of those is advertising,

219
0:10:13.222,000 --> 0:10:15,000
which is the monetization of attention

220
0:10:16.083,000 --> 0:10:19,000
without any real other variables at work,

221
0:10:19.243,000 --> 0:10:22,000
any care for the people who are actually developing this content,

222
0:10:23.152,000 --> 0:10:26,000
the centralization of the power, the separation of those things.

223
0:10:26.812,000 --> 0:10:29,000
And I think however you feel about the use of advertising

224
0:10:29.98,000 --> 0:10:3,000
to kind of support stuff,

225
0:10:31.242,000 --> 0:10:34,000
the sight of grown men in diapers rolling around in the sand

226
0:10:34.333,000 --> 0:10:36,000
in the hope that an algorithm that they don't really understand

227
0:10:37.34,000 --> 0:10:38,000
will give them money for it

228
0:10:38.679,000 --> 0:10:4,000
suggests that this probably isn't the thing

229
0:10:40.74,000 --> 0:10:42,000
that we should be basing our society and culture upon,

230
0:10:43.327,000 --> 0:10:45,000
and the way in which we should be funding it.

231
0:10:45.511,000 --> 0:10:48,000
And the other thing that's kind of the major driver of this is automation,

232
0:10:49.054,000 --> 0:10:51,000
which is the deployment of all of this technology

233
0:10:51.407,000 --> 0:10:53,000
as soon as it arrives, without any kind of oversight,

234
0:10:53.952,000 --> 0:10:54,000
and then once it's out there,

235
0:10:55.388,000 --> 0:10:58,000
kind of throwing up our hands and going, "Hey, it's not us, it's the technology."

236
0:10:59.255,000 --> 0:11:,000
Like, "We're not involved in it."

237
0:11:00.921,000 --> 0:11:01,000
That's not really good enough,

238
0:11:02.712,000 --> 0:11:04,000
because this stuff isn't just algorithmically governed,

239
0:11:05.446,000 --> 0:11:07,000
it's also algorithmically policed.

240
0:11:07.968,000 --> 0:11:09,000
When YouTube first started to pay attention to this,

241
0:11:10.84,000 --> 0:11:12,000
the first thing they said they'd do about it

242
0:11:12.951,000 --> 0:11:14,000
was that they'd deploy better machine learning algorithms

243
0:11:15.67,000 --> 0:11:16,000
to moderate the content.

244
0:11:17.023,000 --> 0:11:2,000
Well, machine learning, as any expert in it will tell you,

245
0:11:20.532,000 --> 0:11:21,000
is basically what we've started to call

246
0:11:22.452,000 --> 0:11:24,000
software that we don't really understand how it works.

247
0:11:25.064,000 --> 0:11:28,000
And I think we have enough of that already.

248
0:11:29.071,000 --> 0:11:32,000
We shouldn't be leaving this stuff up to AI to decide

249
0:11:32.261,000 --> 0:11:33,000
what's appropriate or not,

250
0:11:33.536,000 --> 0:11:34,000
because we know what happens.

251
0:11:34.996,000 --> 0:11:35,000
It'll start censoring other things.

252
0:11:36.708,000 --> 0:11:37,000
It'll start censoring queer content.

253
0:11:38.515,000 --> 0:11:4,000
It'll start censoring legitimate public speech.

254
0:11:40.776,000 --> 0:11:41,000
What's allowed in these discourses,

255
0:11:42.725,000 --> 0:11:45,000
it shouldn't be something that's left up to unaccountable systems.

256
0:11:45.846,000 --> 0:11:47,000
It's part of a discussion all of us should be having.

257
0:11:48.817,000 --> 0:11:49,000
But I'd leave a reminder

258
0:11:50.149,000 --> 0:11:52,000
that the alternative isn't very pleasant, either.

259
0:11:52.926,000 --> 0:11:53,000
YouTube also announced recently

260
0:11:54.485,000 --> 0:11:56,000
that they're going to release a version of their kids' app

261
0:11:57.276,000 --> 0:11:59,000
that would be entirely moderated by humans.

262
0:12:00.134,000 --> 0:12:03,000
Facebook -- Zuckerberg said much the same thing at Congress,

263
0:12:03.776,000 --> 0:12:05,000
when pressed about how they were going to moderate their stuff.

264
0:12:06.787,000 --> 0:12:07,000
He said they'd have humans doing it.

265
0:12:08.558,000 --> 0:12:09,000
And what that really means is,

266
0:12:10.041,000 --> 0:12:13,000
instead of having toddlers being the first person to see this stuff,

267
0:12:13.288,000 --> 0:12:15,000
you're going to have underpaid, precarious contract workers

268
0:12:16.1,000 --> 0:12:17,000
without proper mental health support

269
0:12:17.85,000 --> 0:12:18,000
being damaged by it as well.

270
0:12:19.25,000 --> 0:12:2,000
(Laughter)

271
0:12:20.37,000 --> 0:12:22,000
And I think we can all do quite a lot better than that.

272
0:12:22.995,000 --> 0:12:24,000
(Applause)

273
0:12:26.068,000 --> 0:12:3,000
The thought, I think, that brings those two things together, really, for me,

274
0:12:30.705,000 --> 0:12:31,000
is agency.

275
0:12:32.149,000 --> 0:12:35,000
It's like, how much do we really understand -- by agency, I mean:

276
0:12:35.33,000 --> 0:12:39,000
how we know how to act in our own best interests.

277
0:12:39.744,000 --> 0:12:4,000
Which -- it's almost impossible to do

278
0:12:41.555,000 --> 0:12:44,000
in these systems that we don't really fully understand.

279
0:12:45.064,000 --> 0:12:48,000
Inequality of power always leads to violence.

280
0:12:48.159,000 --> 0:12:49,000
And we can see inside these systems

281
0:12:49.868,000 --> 0:12:51,000
that inequality of understanding does the same thing.

282
0:12:52.503,000 --> 0:12:55,000
If there's one thing that we can do to start to improve these systems,

283
0:12:56.306,000 --> 0:12:58,000
it's to make them more legible to the people who use them,

284
0:12:59.048,000 --> 0:13:01,000
so that all of us have a common understanding

285
0:13:01.268,000 --> 0:13:02,000
of what's actually going on here.

286
0:13:03.97,000 --> 0:13:05,000
The thing, though, I think most about these systems

287
0:13:06.962,000 --> 0:13:09,000
is that this isn't, as I hope I've explained, really about YouTube.

288
0:13:10.843,000 --> 0:13:11,000
It's about everything.

289
0:13:12.179,000 --> 0:13:14,000
These issues of accountability and agency,

290
0:13:14.647,000 --> 0:13:16,000
of opacity and complexity,

291
0:13:16.896,000 --> 0:13:19,000
of the violence and exploitation that inherently results

292
0:13:20.097,000 --> 0:13:22,000
from the concentration of power in a few hands --

293
0:13:22.915,000 --> 0:13:24,000
these are much, much larger issues.

294
0:13:26.395,000 --> 0:13:29,000
And they're issues not just of YouTube and not just of technology in general,

295
0:13:30.106,000 --> 0:13:31,000
and they're not even new.

296
0:13:31.395,000 --> 0:13:32,000
They've been with us for ages.

297
0:13:32.88,000 --> 0:13:36,000
But we finally built this system, this global system, the internet,

298
0:13:37.294,000 --> 0:13:4,000
that's actually showing them to us in this extraordinary way,

299
0:13:40.337,000 --> 0:13:41,000
making them undeniable.

300
0:13:41.908,000 --> 0:13:43,000
Technology has this extraordinary capacity

301
0:13:44.752,000 --> 0:13:47,000
to both instantiate and continue

302
0:13:48.749,000 --> 0:13:52,000
all of our most extraordinary, often hidden desires and biases

303
0:13:53.021,000 --> 0:13:54,000
and encoding them into the world,

304
0:13:54.911,000 --> 0:13:57,000
but it also writes them down so that we can see them,

305
0:13:58.409,000 --> 0:14:01,000
so that we can't pretend they don't exist anymore.

306
0:14:01.763,000 --> 0:14:05,000
We need to stop thinking about technology as a solution to all of our problems,

307
0:14:06.106,000 --> 0:14:09,000
but think of it as a guide to what those problems actually are,

308
0:14:09.887,000 --> 0:14:11,000
so we can start thinking about them properly

309
0:14:12.055,000 --> 0:14:13,000
and start to address them.

310
0:14:13.845,000 --> 0:14:14,000
Thank you very much.

311
0:14:15.204,000 --> 0:14:2,000
(Applause)

312
0:14:21.733,000 --> 0:14:22,000
Thank you.

313
0:14:22.945,000 --> 0:14:24,000
(Applause)

314
0:14:28.839,000 --> 0:14:31,000
Helen Walters: James, thank you for coming and giving us that talk.

315
0:14:32.041,000 --> 0:14:33,000
So it's interesting:

316
0:14:33.254,000 --> 0:14:36,000
when you think about the films where the robotic overlords take over,

317
0:14:36.773,000 --> 0:14:39,000
it's all a bit more glamorous than what you're describing.

318
0:14:40.076,000 --> 0:14:43,000
But I wonder -- in those films, you have the resistance mounting.

319
0:14:43.849,000 --> 0:14:46,000
Is there a resistance mounting towards this stuff?

320
0:14:47.089,000 --> 0:14:5,000
Do you see any positive signs, green shoots of resistance?

321
0:14:52.507,000 --> 0:14:54,000
James Bridle: I don't know about direct resistance,

322
0:14:54.947,000 --> 0:14:56,000
because I think this stuff is super long-term.

323
0:14:57.235,000 --> 0:14:59,000
I think it's baked into culture in really deep ways.

324
0:14:59.769,000 --> 0:15:01,000
A friend of mine, Eleanor Saitta, always says

325
0:15:01.935,000 --> 0:15:04,000
that any technological problems of sufficient scale and scope

326
0:15:05.568,000 --> 0:15:07,000
are political problems first of all.

327
0:15:07.859,000 --> 0:15:09,000
So all of these things we're working to address within this

328
0:15:10.668,000 --> 0:15:13,000
are not going to be addressed just by building the technology better,

329
0:15:13.966,000 --> 0:15:16,000
but actually by changing the society that's producing these technologies.

330
0:15:17.454,000 --> 0:15:2,000
So no, right now, I think we've got a hell of a long way to go.

331
0:15:20.505,000 --> 0:15:21,000
But as I said, I think by unpacking them,

332
0:15:22.515,000 --> 0:15:24,000
by explaining them, by talking about them super honestly,

333
0:15:25.236,000 --> 0:15:27,000
we can actually start to at least begin that process.

334
0:15:27.765,000 --> 0:15:3,000
HW: And so when you talk about legibility and digital literacy,

335
0:15:31.351,000 --> 0:15:32,000
I find it difficult to imagine

336
0:15:32.966,000 --> 0:15:35,000
that we need to place the burden of digital literacy on users themselves.

337
0:15:36.67,000 --> 0:15:4,000
But whose responsibility is education in this new world?

338
0:15:41.256,000 --> 0:15:44,000
JB: Again, I think this responsibility is kind of up to all of us,

339
0:15:44.892,000 --> 0:15:46,000
that everything we do, everything we build, everything we make,

340
0:15:47.9,000 --> 0:15:5,000
needs to be made in a consensual discussion

341
0:15:51.616,000 --> 0:15:52,000
with everyone who's avoiding it;

342
0:15:53.58,000 --> 0:15:57,000
that we're not building systems intended to trick and surprise people

343
0:15:57.945,000 --> 0:15:59,000
into doing the right thing,

344
0:16:00.269,000 --> 0:16:03,000
but that they're actually involved in every step in educating them,

345
0:16:03.529,000 --> 0:16:05,000
because each of these systems is educational.

346
0:16:05.831,000 --> 0:16:08,000
That's what I'm hopeful about, about even this really grim stuff,

347
0:16:08.957,000 --> 0:16:1,000
that if you can take it and look at it properly,

348
0:16:11.243,000 --> 0:16:13,000
it's actually in itself a piece of education

349
0:16:13.356,000 --> 0:16:16,000
that allows you to start seeing how complex systems come together and work

350
0:16:17.142,000 --> 0:16:2,000
and maybe be able to apply that knowledge elsewhere in the world.

351
0:16:20.667,000 --> 0:16:22,000
HW: James, it's such an important discussion,

352
0:16:22.806,000 --> 0:16:25,000
and I know many people here are really open and prepared to have it,

353
0:16:26.057,000 --> 0:16:27,000
so thanks for starting off our morning.

354
0:16:27.94,000 --> 0:16:28,000
JB: Thanks very much. Cheers.

355
0:16:29.364,000 --> 0:16:3,000
(Applause)

