1
0:00:,000 --> 0:00:07,000
Traducteur: MACQUERON CORENTIN Relecteur: Morgane Quilfen

2
0:00:12.956,000 --> 0:00:14,000
Je vais vous parler d'une erreur de jugement

3
0:00:15.24,000 --> 0:00:16,000
que beaucoup d'entre nous font.

4
0:00:17.48,000 --> 0:00:2,000
Il s'agit de l'incapacité à détecter un certain type de danger.

5
0:00:21.36,000 --> 0:00:22,000
Je vais décrire un scénario

6
0:00:23.12,000 --> 0:00:26,000
que je pense à la fois terrifiant

7
0:00:26.4,000 --> 0:00:27,000
et pourtant susceptible d'arriver,

8
0:00:28.84,000 --> 0:00:29,000
et ce genre de combinaison

9
0:00:30.52,000 --> 0:00:31,000
n'est pas une bonne chose.

10
0:00:32.08,000 --> 0:00:35,000
Pourtant, au lieu d'avoir peur, la plupart d'entre vous penseront

11
0:00:35.156,000 --> 0:00:36,000
que c'est plutôt cool.

12
0:00:37.2,000 --> 0:00:39,000
Je vais décrire comment les avancées que nous faisons

13
0:00:40.2,000 --> 0:00:41,000
dans l'intelligence artificielle

14
0:00:42,000 --> 0:00:43,000
pourraient finir par nous détruire.

15
0:00:43.806,000 --> 0:00:46,000
Il semble difficile de voir comment cela pourrait ne pas nous détruire

16
0:00:47.28,000 --> 0:00:48,000
ou nous aider à nous détruire.

17
0:00:49.4,000 --> 0:00:5,000
Pourtant, si vous me ressemblez un peu,

18
0:00:51.28,000 --> 0:00:53,000
vous allez trouver amusant de réfléchir à ces choses-là.

19
0:00:53.96,000 --> 0:00:56,000
Et cette façon de voir les choses est une partie du problème.

20
0:00:57.36,000 --> 0:00:58,000
Ce sentiment devrait vous inquiéter.

21
0:00:59.92,000 --> 0:01:01,000
Et si je devais vous convaincre, avec cette présentation,

22
0:01:02.626,000 --> 0:01:05,000
que nous allons probablement souffrir d'une famine généralisée,

23
0:01:06.04,000 --> 0:01:09,000
soit à cause du climat ou de toute autre catastrophe,

24
0:01:09.12,000 --> 0:01:12,000
et que vos petits-enfants, ou leurs petits-enfants,

25
0:01:12.56,000 --> 0:01:13,000
ont de grandes chances de vivre ainsi,

26
0:01:15.2,000 --> 0:01:16,000
vous ne penseriez pas :

27
0:01:17.44,000 --> 0:01:18,000
« Intéressant.

28
0:01:18.8,000 --> 0:01:19,000
Cool, ce TED. »

29
0:01:21.2,000 --> 0:01:22,000
La famine, ce n'est pas amusant.

30
0:01:23.8,000 --> 0:01:26,000
La mort en science-fiction, en revanche, est un concept amusant

31
0:01:27.2,000 --> 0:01:3,000
et ce qui m'inquiète le plus à propos du développement de l'IA en ce moment

32
0:01:31.2,000 --> 0:01:35,000
est le fait que nous semblons incapables de nous faire une idée

33
0:01:35.32,000 --> 0:01:36,000
des dangers qui nous attendent.

34
0:01:37.16,000 --> 0:01:4,000
Je n'ai pas de réponse, alors que je suis ici, à vous en parler.

35
0:01:42.12,000 --> 0:01:44,000
C'est comme si nous étions face à deux portes.

36
0:01:44.84,000 --> 0:01:45,000
Derrière la porte n°1,

37
0:01:46.12,000 --> 0:01:49,000
nous arrêtons nos progrès dans le développement de nos machines.

38
0:01:49.44,000 --> 0:01:53,000
Hardware et software stagnent, pour une raison ou pour une autre.

39
0:01:53.48,000 --> 0:01:56,000
Essayez d'imaginer ce qui pourrait arrêter ce développement.

40
0:01:56.567,000 --> 0:01:58,000
Étant donnée l'importance de l'intelligence

41
0:01:58.816,000 --> 0:01:59,000
et de l'automatisation

42
0:02:00.297,000 --> 0:02:03,000
nous allons continuer à améliorer notre technologie si nous le pouvons.

43
0:02:05.2,000 --> 0:02:06,000
Qu'est-ce qui nous en empêcherait ?

44
0:02:07.8,000 --> 0:02:08,000
Une guerre nucléaire mondiale ?

45
0:02:11,000 --> 0:02:12,000
Une pandémie globale ?

46
0:02:14.32,000 --> 0:02:15,000
Un impact d'astéroïde ?

47
0:02:17.64,000 --> 0:02:19,000
Justin Bieber président des États-Unis ?

48
0:02:20.24,000 --> 0:02:22,000
(Rires)

49
0:02:24.76,000 --> 0:02:27,000
Il faudrait que la civilisation telle que nous la connaissons

50
0:02:27.932,000 --> 0:02:28,000
soit détruite.

51
0:02:29.39,000 --> 0:02:33,000
Il faudrait vraiment imaginer quelque chose de terrible

52
0:02:33.68,000 --> 0:02:36,000
pour que nous arrêtions de développer notre technologie

53
0:02:37.04,000 --> 0:02:38,000
totalement,

54
0:02:38.28,000 --> 0:02:4,000
génération après génération.

55
0:02:40.32,000 --> 0:02:42,000
Par définition, ce serait la pire chose

56
0:02:42.48,000 --> 0:02:44,000
qui serait jamais arrivée à l'humanité.

57
0:02:44.52,000 --> 0:02:45,000
Donc la seule alternative,

58
0:02:45.84,000 --> 0:02:47,000
et c'est ce qui se trouve derrière la porte n°2,

59
0:02:48.2,000 --> 0:02:51,000
est que nous continuions d'améliorer l'intelligence de nos machines

60
0:02:51.36,000 --> 0:02:52,000
année après année.

61
0:02:53.72,000 --> 0:02:56,000
À un moment, nous allons construire des machines plus intelligentes que nous,

62
0:02:58.08,000 --> 0:03:,000
et ces machines plus intelligentes que nous

63
0:03:00.72,000 --> 0:03:01,000
vont commencer à s'améliorer elles-mêmes.

64
0:03:02.72,000 --> 0:03:04,000
Il y a le risque, théorisé par le mathématicien I.J. Good,

65
0:03:05.48,000 --> 0:03:06,000
d'une « explosion d'intelligence »,

66
0:03:07.28,000 --> 0:03:09,000
un processus qui pourrait nous échapper.

67
0:03:10.12,000 --> 0:03:12,000
Ce phénomène est souvent caricaturé, comme ici,

68
0:03:12.96,000 --> 0:03:15,000
avec des armées de robots malveillants

69
0:03:16.2,000 --> 0:03:17,000
qui nous attaqueraient.

70
0:03:17.48,000 --> 0:03:19,000
Mais ce n'est pas le scénario le plus probable.

71
0:03:20.2,000 --> 0:03:24,000
Nos machines ne vont probablement pas devenir spontanément malveillantes.

72
0:03:25.08,000 --> 0:03:27,000
Le risque est que nous construisions des machines

73
0:03:27.72,000 --> 0:03:29,000
tellement plus compétentes que nous

74
0:03:29.8,000 --> 0:03:32,000
que la moindre divergence d'intérêt entre elles et nous

75
0:03:33.6,000 --> 0:03:34,000
pourrait nous détruire.

76
0:03:35.96,000 --> 0:03:37,000
Pensez aux fourmis.

77
0:03:38.6,000 --> 0:03:39,000
Nous ne les haïssons pas.

78
0:03:40.28,000 --> 0:03:42,000
Nous ne cherchons pas à les écraser.

79
0:03:42.36,000 --> 0:03:44,000
Nous faisons même parfois l'effort de les éviter.

80
0:03:44.76,000 --> 0:03:46,000
Nous les enjambons.

81
0:03:46.8,000 --> 0:03:48,000
Mais dès que leur présence

82
0:03:48.96,000 --> 0:03:5,000
nous gêne vraiment,

83
0:03:51.48,000 --> 0:03:53,000
par exemple si nous voulons construire un bâtiment,

84
0:03:53.981,000 --> 0:03:54,000
nous les exterminons sans scrupule.

85
0:03:56.48,000 --> 0:03:58,000
Le risque est que nous construisions un jour des machines

86
0:03:59.44,000 --> 0:04:01,000
qui, conscientes ou non,

87
0:04:02.2,000 --> 0:04:04,000
nous traiteraient avec la même indifférence.

88
0:04:05.76,000 --> 0:04:07,000
Cela peut vous sembler capillotracté.

89
0:04:09.36,000 --> 0:04:15,000
Je parie que certains d'entre vous doutent qu'une super IA soit possible,

90
0:04:15.72,000 --> 0:04:16,000
et encore moins inévitable.

91
0:04:17.4,000 --> 0:04:2,000
Mais dans ce cas vous devez réfuter une des hypothèses suivantes.

92
0:04:21.044,000 --> 0:04:22,000
Et il n'y en a que trois.

93
0:04:23.8,000 --> 0:04:27,000
L'intelligence est le traitement de l'information dans un système physique.

94
0:04:29.32,000 --> 0:04:31,000
En fait, c'est un peu plus qu'une simple hypothèse.

95
0:04:31.959,000 --> 0:04:34,000
Nous avons déjà construit des machines vaguement intelligentes,

96
0:04:35.44,000 --> 0:04:37,000
et nombre d'entre elles font déjà preuve

97
0:04:37.48,000 --> 0:04:39,000
d'une intelligence surhumaine.

98
0:04:40.84,000 --> 0:04:42,000
Et nous savons que la matière

99
0:04:43.44,000 --> 0:04:45,000
peut produire « l'intelligence générale »,

100
0:04:46.08,000 --> 0:04:49,000
la capacité de réfléchir de manière croisée,

101
0:04:49.76,000 --> 0:04:52,000
parce que nos cerveaux y sont parvenus, n'est-ce pas ?

102
0:04:52.92,000 --> 0:04:55,000
Après tout, il n'y a que des atomes là-dedans,

103
0:04:56.88,000 --> 0:05:,000
et tant que nous continuerons à construire des systèmes d'atomes

104
0:05:01.4,000 --> 0:05:03,000
qui feront montre de plus en plus d'intelligence,

105
0:05:04.12,000 --> 0:05:06,000
nous parviendrons, à moins d'être interrompus,

106
0:05:06.68,000 --> 0:05:09,000
nous parviendrons à implanter une « intelligence générale »

107
0:05:10.08,000 --> 0:05:11,000
au cœur de nos machines.

108
0:05:11.4,000 --> 0:05:14,000
Il est crucial de comprendre que la vitesse n'est pas le problème,

109
0:05:15.08,000 --> 0:05:18,000
car n'importe quelle vitesse est suffisante pour aller au bout.

110
0:05:18.28,000 --> 0:05:2,000
Nous n'avons pas besoin de la loi de Moore

111
0:05:20.448,000 --> 0:05:21,000
ni d'un progrès exponentiel.

112
0:05:22.08,000 --> 0:05:23,000
Il suffit de continuer à avancer.

113
0:05:25.48,000 --> 0:05:27,000
La deuxième hypothèse est que nous allons continuer.

114
0:05:29,000 --> 0:05:31,000
Nous continuerons d'améliorer nos machines intelligentes.

115
0:05:33,000 --> 0:05:37,000
Et, vue la valeur de l'intelligence --

116
0:05:37.4,000 --> 0:05:4,000
l'intelligence est la source de tout ce qui compte

117
0:05:40.96,000 --> 0:05:42,000
ou alors elle doit protéger tout ce qui compte.

118
0:05:43.76,000 --> 0:05:45,000
C'est notre plus importante ressource.

119
0:05:46.04,000 --> 0:05:47,000
Donc nous allons continuer.

120
0:05:47.6,000 --> 0:05:5,000
Nous avons des problèmes que nous devons absolument résoudre.

121
0:05:50.96,000 --> 0:05:53,000
Nous voulons vaincre les maladies comme Alzheimer ou le cancer.

122
0:05:54.96,000 --> 0:05:55,000
Nous voulons comprendre l'économie.

123
0:05:56.84,000 --> 0:05:58,000
Nous voulons améliorer le climat.

124
0:05:58.92,000 --> 0:06:,000
Donc nous allons continuer, si nous le pouvons.

125
0:06:01.2,000 --> 0:06:04,000
Le train a déjà quitté la gare et il n'y a pas de frein.

126
0:06:05.88,000 --> 0:06:1,000
Et enfin, l'humain ne se trouve pas au sommet de l'intelligence,

127
0:06:11.36,000 --> 0:06:12,000
nous en sommes même très loin.

128
0:06:13.64,000 --> 0:06:14,000
Et c'est vraiment le point crucial.

129
0:06:15.56,000 --> 0:06:17,000
C'est ce qui rend notre situation si précaire

130
0:06:18,000 --> 0:06:22,000
et ce qui fait que notre compréhension des risques n'est pas fiable.

131
0:06:23.12,000 --> 0:06:25,000
Prenons l'humain le plus intelligent qui ait jamais vécu.

132
0:06:26.64,000 --> 0:06:29,000
Beaucoup de gens penseraient à John von Neumann.

133
0:06:30.08,000 --> 0:06:33,000
L'impression que von Neumann laissait aux gens,

134
0:06:33.44,000 --> 0:06:37,000
y compris les plus brillants mathématiciens et physiciens de son temps,

135
0:06:37.52,000 --> 0:06:38,000
est plutôt bien documentée.

136
0:06:39.48,000 --> 0:06:41,000
Si ne serait-ce que la moitié de ce que l'on dit sur lui

137
0:06:42.223,000 --> 0:06:42,000
est à moitié vraie,

138
0:06:43.135,000 --> 0:06:44,000
il n'y a aucun doute qu'il soit

139
0:06:44.669,000 --> 0:06:46,000
l'un des plus brillants esprits qui aient existé.

140
0:06:47,000 --> 0:06:49,000
Considérons le spectre de l'intelligence.

141
0:06:50.32,000 --> 0:06:51,000
Voici John von Neumann.

142
0:06:53.56,000 --> 0:06:54,000
Et nous, nous sommes ici.

143
0:06:56.12,000 --> 0:06:57,000
Et là nous avons un poulet.

144
0:06:57.44,000 --> 0:06:58,000
(Rires)

145
0:06:59.4,000 --> 0:07:,000
Pardon, un poulet.

146
0:07:00.64,000 --> 0:07:01,000
(Rires)

147
0:07:01.92,000 --> 0:07:04,000
Pas besoin de rendre cette présentation plus déprimante qu'elle ne l'est déjà.

148
0:07:05.68,000 --> 0:07:06,000
(Rires)

149
0:07:08.339,000 --> 0:07:11,000
Il semble néanmoins plus que probable que le spectre de l'intelligence

150
0:07:11.84,000 --> 0:07:14,000
s'étende beaucoup plus loin que nous ne pouvons le concevoir,

151
0:07:15.88,000 --> 0:07:18,000
Si nous construisons des machines plus intelligentes que nous,

152
0:07:19.12,000 --> 0:07:21,000
elles vont très probablement explorer ce spectre,

153
0:07:21.466,000 --> 0:07:22,000
plus que nous ne pouvons l'imaginer,

154
0:07:23.32,000 --> 0:07:26,000
et elles nous surpasseront, plus que nous ne pouvons l'imaginer.

155
0:07:27,000 --> 0:07:31,000
Il est important de noter que c'est vrai par la seule vertu de la vitesse.

156
0:07:31.36,000 --> 0:07:36,000
Imaginez que nous construisions une IA super intelligente

157
0:07:36.44,000 --> 0:07:39,000
qui ne soit pas plus intelligente qu'une équipe moyenne de chercheurs

158
0:07:39.92,000 --> 0:07:41,000
de Stanford ou du MIT.

159
0:07:42.24,000 --> 0:07:44,000
L'électronique va environ un million de fois plus vite

160
0:07:45.24,000 --> 0:07:46,000
que la biochimie,

161
0:07:46.52,000 --> 0:07:49,000
de sorte que cette machine penserait un million de fois plus vite

162
0:07:49.68,000 --> 0:07:5,000
que les humains qui l'auraient créée.

163
0:07:51.52,000 --> 0:07:52,000
En une semaine,

164
0:07:53.2,000 --> 0:07:57,000
cette machine réaliserait le même travail qu'une équipe d'humains en 20 000 ans,

165
0:07:58.4,000 --> 0:07:59,000
semaine après semaine.

166
0:08:01.64,000 --> 0:08:04,000
Comment pourrions-nous comprendre, et encore moins limiter,

167
0:08:04.76,000 --> 0:08:06,000
un esprit qui irait à une telle vitesse ?

168
0:08:08.84,000 --> 0:08:1,000
L'autre chose inquiétante, franchement,

169
0:08:11,000 --> 0:08:15,000
est la suivante : imaginez le meilleur des cas.

170
0:08:16,000 --> 0:08:2,000
Imaginez que nous concevions une IA super intelligente

171
0:08:20.2,000 --> 0:08:21,000
qui ne soit pas dangereuse.

172
0:08:21.6,000 --> 0:08:24,000
Nous trouvons la parfaite solution du premier coup.

173
0:08:24.88,000 --> 0:08:26,000
Comme si nous avions un oracle à disposition

174
0:08:27.12,000 --> 0:08:29,000
se comportant exactement comme prévu.

175
0:08:29.16,000 --> 0:08:32,000
Cette machine nous éviterait toute forme de travail.

176
0:08:33.68,000 --> 0:08:35,000
Elle concevrait la machine qui ferait la machine

177
0:08:36.133,000 --> 0:08:37,000
qui pourrait faire n'importe quoi,

178
0:08:37.92,000 --> 0:08:38,000
alimentée par le soleil,

179
0:08:39.4,000 --> 0:08:41,000
plus ou moins pour le seul coût des matières premières.

180
0:08:42.12,000 --> 0:08:45,000
On parle là de la fin de la pénibilité humaine.

181
0:08:45.4,000 --> 0:08:47,000
Nous parlons aussi de la fin du travail intellectuel.

182
0:08:49.2,000 --> 0:08:52,000
Que feraient des singes comme nous dans une pareille situation ?

183
0:08:52.28,000 --> 0:08:56,000
Nous serions libres de jouer au frisbee et de nous faire des massages.

184
0:08:57.84,000 --> 0:08:59,000
Avec un peu de LSD et quelques fringues douteuses,

185
0:09:00.72,000 --> 0:09:02,000
le monde entier serait comme le Burning Man.

186
0:09:02.92,000 --> 0:09:03,000
(Rires)

187
0:09:06.32,000 --> 0:09:08,000
Bon, tout ça peut sembler plutôt sympa,

188
0:09:09.28,000 --> 0:09:11,000
mais demandez-vous ce qui arriverait

189
0:09:11.68,000 --> 0:09:13,000
avec notre système politique et économique actuel ?

190
0:09:14.44,000 --> 0:09:16,000
Nous serions les témoins

191
0:09:16.88,000 --> 0:09:2,000
d'une inégalité des richesses et d'un taux de chômage

192
0:09:21.04,000 --> 0:09:22,000
encore jamais vus.

193
0:09:22.56,000 --> 0:09:24,000
Sans la volonté de mettre ces nouvelles richesses

194
0:09:25.2,000 --> 0:09:26,000
au service de toute l'humanité,

195
0:09:27.64,000 --> 0:09:3,000
quelques milliardaires feraient les couvertures des revues commerciales

196
0:09:31.28,000 --> 0:09:33,000
alors que le reste du monde mourrait de faim.

197
0:09:34.32,000 --> 0:09:36,000
Et que feraient les Russes ou les Chinois

198
0:09:36.64,000 --> 0:09:38,000
s'ils apprenaient qu'une société de la Silicon Valley

199
0:09:39.28,000 --> 0:09:41,000
était sur le point de créer une IA super intelligente ?

200
0:09:42.04,000 --> 0:09:44,000
Cette machine serait capable de faire la guerre,

201
0:09:44.92,000 --> 0:09:46,000
qu'elle soit physique ou numérique,

202
0:09:47.16,000 --> 0:09:48,000
avec une puissance jamais vue.

203
0:09:50.12,000 --> 0:09:51,000
Le vainqueur emporterait toute la mise.

204
0:09:52,000 --> 0:09:55,000
Avoir six mois d'avance dans une telle compétition

205
0:09:55.16,000 --> 0:09:57,000
revient à avoir 500 000 ans d'avance,

206
0:09:57.96,000 --> 0:09:58,000
au minimum.

207
0:09:59.48,000 --> 0:10:03,000
Il est donc possible que la moindre rumeur de ce type de percée

208
0:10:04.24,000 --> 0:10:06,000
pourrait tous nous rendre totalement dingues.

209
0:10:06.64,000 --> 0:10:08,000
L'une des choses les plus effrayantes,

210
0:10:09.56,000 --> 0:10:11,000
de mon point de vue,

211
0:10:12.36,000 --> 0:10:16,000
sont les choses que les chercheurs en IA disent

212
0:10:16.68,000 --> 0:10:17,000
quand ils veulent nous rassurer.

213
0:10:19,000 --> 0:10:22,000
On nous dit généralement que nous avons encore le temps.

214
0:10:22.48,000 --> 0:10:24,000
C'est pour plus tard, vous pensez bien.

215
0:10:24.56,000 --> 0:10:26,000
C'est probablement pour dans 50 ou 100 ans.

216
0:10:27.72,000 --> 0:10:28,000
Un chercheur a dit :

217
0:10:29,000 --> 0:10:31,000
« S'inquiéter de l'IA, c'est comme s'inquiéter

218
0:10:31.146,000 --> 0:10:32,000
de la surpopulation sur Mars. »

219
0:10:34.116,000 --> 0:10:35,000
La Silicon Valley sait parfois

220
0:10:35.76,000 --> 0:10:37,000
se montrer condescendante.

221
0:10:38.16,000 --> 0:10:39,000
(Rires)

222
0:10:39.52,000 --> 0:10:4,000
Personne ne semble se rendre compte

223
0:10:41.44,000 --> 0:10:43,000
que dire qu'on a le temps

224
0:10:44.08,000 --> 0:10:46,000
est totalement fallacieux dans ce contexte.

225
0:10:46.68,000 --> 0:10:49,000
Si l'intelligence n'est que le traitement de l'information

226
0:10:49.96,000 --> 0:10:51,000
et si l'on continue d'améliorer nos machines,

227
0:10:52.64,000 --> 0:10:54,000
alors nous produirons une forme de super intelligence.

228
0:10:56.32,000 --> 0:10:59,000
Et nous n'avons pas la moindre idée du temps qu'il nous faudra

229
0:11:,000 --> 0:11:02,000
pour trouver comment le faire sans risques.

230
0:11:04.2,000 --> 0:11:05,000
Je vais me répéter.

231
0:11:05.52,000 --> 0:11:08,000
Nous n'avons pas la moindre idée du temps qu'il nous faudra

232
0:11:09.36,000 --> 0:11:11,000
pour trouver comment le faire sans risques.

233
0:11:12.92,000 --> 0:11:15,000
Au cas où vous n'auriez pas remarqué, 50 ans, ce n'est pas grand chose.

234
0:11:16.4,000 --> 0:11:18,000
Voici 50 ans, en mois.

235
0:11:18.88,000 --> 0:11:19,000
Voici l'existence de l'iPhone.

236
0:11:21.44,000 --> 0:11:23,000
Voici la durée couverte par « Les Simpsons ».

237
0:11:24.68,000 --> 0:11:26,000
Cinquante ans, cela ne laisse guère de temps

238
0:11:27.08,000 --> 0:11:3,000
pour se préparer à l'un des plus grands défis de tous les temps.

239
0:11:31.64,000 --> 0:11:35,000
Une fois encore, nous semblons incapables de nous préparer

240
0:11:35.68,000 --> 0:11:37,000
à ce qui, selon toute probabilité, va arriver.

241
0:11:38.4,000 --> 0:11:41,000
L'informaticien Stuart Russel propose une belle analogie à ce sujet.

242
0:11:42.4,000 --> 0:11:46,000
Il dit : « Imaginez que nous recevions un message des aliens

243
0:11:47.32,000 --> 0:11:48,000
qui dirait :

244
0:11:49.04,000 --> 0:11:5,000
' Habitants de la Terre,

245
0:11:50.6,000 --> 0:11:52,000
nous arriverons chez vous dans 50 ans.

246
0:11:53.8,000 --> 0:11:54,000
Soyez prêts. '

247
0:11:55.4,000 --> 0:11:59,000
Et on ne ferait que regarder le compte à rebours ?

248
0:11:59.68,000 --> 0:12:02,000
Non, nous nous sentirions un peu plus concernés que ça. »

249
0:12:04.68,000 --> 0:12:05,000
Une autre raison de ne pas s'inquiéter

250
0:12:06.56,000 --> 0:12:09,000
serait que ces machines partageraient notre sens des valeurs

251
0:12:09.6,000 --> 0:12:11,000
parce qu'elles seraient des extensions de nous-mêmes.

252
0:12:12.24,000 --> 0:12:13,000
Elles seraient greffées sur nos cerveaux,

253
0:12:14.236,000 --> 0:12:16,000
nous serions leur système limbique.

254
0:12:17.12,000 --> 0:12:18,000
Réfléchissez un peu

255
0:12:18.56,000 --> 0:12:21,000
que le moyen le plus sûr,

256
0:12:21.76,000 --> 0:12:22,000
recommandé,

257
0:12:23.12,000 --> 0:12:26,000
serait de brancher cette technologie directement sur nos cerveaux.

258
0:12:26.6,000 --> 0:12:29,000
Cela peut sembler être la meilleure option,

259
0:12:3,000 --> 0:12:33,000
mais généralement, on essaie d'être sûr de son coup

260
0:12:33.08,000 --> 0:12:36,000
avant de s'enfoncer quelque chose dans le cerveau.

261
0:12:36.76,000 --> 0:12:38,000
(Rires)

262
0:12:38.8,000 --> 0:12:41,000
Le vrai problème est que simplement concevoir

263
0:12:42.77,000 --> 0:12:43,000
une IA super intelligente

264
0:12:44.16,000 --> 0:12:45,000
semble plus facile

265
0:12:45.89,000 --> 0:12:46,000
que de concevoir une IA super intelligente

266
0:12:47.856,000 --> 0:12:48,000
tout en maîtrisant les neurosciences

267
0:12:49.6,000 --> 0:12:51,000
pour connecter cette IA à nos cerveaux.

268
0:12:52.8,000 --> 0:12:55,000
Si l'on tient compte du fait que les entreprises et gouvernements

269
0:12:56,000 --> 0:12:59,000
se sentent probablement en concurrence,

270
0:12:59.68,000 --> 0:13:02,000
et, puisque remporter cette course revient à conquérir le monde,

271
0:13:02.96,000 --> 0:13:04,000
pourvu que vous ne le détruisiez pas juste après,

272
0:13:05.44,000 --> 0:13:07,000
il semble probable que l'on commence

273
0:13:08.08,000 --> 0:13:09,000
par le plus facile.

274
0:13:10.56,000 --> 0:13:12,000
Malheureusement, je n'ai pas la solution de ce problème,

275
0:13:13.34,000 --> 0:13:15,000
à part recommander que nous soyons plus nombreux à y réfléchir.

276
0:13:16.296,000 --> 0:13:18,000
Il nous faudrait une sorte de Projet Manhattan

277
0:13:18.486,000 --> 0:13:2,000
à propos de l'intelligence artificielle.

278
0:13:20.52,000 --> 0:13:22,000
Pas pour la concevoir, car nous y parviendrons,

279
0:13:23.28,000 --> 0:13:26,000
mais pour réfléchir à comment éviter une course aux armements

280
0:13:26.64,000 --> 0:13:29,000
et pour la concevoir d'une façon en accord avec nos intérêts.

281
0:13:30.16,000 --> 0:13:32,000
Quand vous parlez d'une IA super intelligente

282
0:13:32.32,000 --> 0:13:34,000
qui pourrait se modifier elle-même,

283
0:13:34.6,000 --> 0:13:38,000
il semblerait que nous n'ayons qu'une seule chance de faire les choses bien

284
0:13:39.24,000 --> 0:13:41,000
et encore, il nous faudra gérer

285
0:13:41.32,000 --> 0:13:44,000
les conséquences politiques et économiques.

286
0:13:45.76,000 --> 0:13:47,000
Mais à partir du moment où nous admettons

287
0:13:47.84,000 --> 0:13:51,000
que le traitement de l'information est la source de l'intelligence,

288
0:13:52.72,000 --> 0:13:56,000
qu'un système de calcul est la base de l'intelligence,

289
0:13:58.36,000 --> 0:14:01,000
et que nous admettons que nous allons continuer d'améliorer ce type de système

290
0:14:03.28,000 --> 0:14:05,000
et que nous admettons que l'horizon

291
0:14:05.46,000 --> 0:14:06,000
de l'intelligence dépasse totalement

292
0:14:07.447,000 --> 0:14:08,000
ce que nous savons aujourd'hui,

293
0:14:10.12,000 --> 0:14:11,000
nous devons admettre

294
0:14:11.53,000 --> 0:14:14,000
que nous sommes engagés dans la conception d'un pseudo-dieu.

295
0:14:15.4,000 --> 0:14:16,000
Il nous faut donc vérifier

296
0:14:17,000 --> 0:14:19,000
que c'est un dieu compatible avec notre survie.

297
0:14:20.12,000 --> 0:14:21,000
Merci beaucoup.

298
0:14:21.68,000 --> 0:14:26,000
(Applaudissements)

