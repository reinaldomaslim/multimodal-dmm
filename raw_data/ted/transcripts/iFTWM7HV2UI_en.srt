1
0:00:12.76,000 --> 0:00:15,000
So when people voice fears of artificial intelligence,

2
0:00:16.32,000 --> 0:00:19,000
very often, they invoke images of humanoid robots run amok.

3
0:00:20.32,000 --> 0:00:21,000
You know? Terminator?

4
0:00:22.4,000 --> 0:00:24,000
You know, that might be something to consider,

5
0:00:24.76,000 --> 0:00:25,000
but that's a distant threat.

6
0:00:26.64,000 --> 0:00:29,000
Or, we fret about digital surveillance

7
0:00:30.12,000 --> 0:00:31,000
with metaphors from the past.

8
0:00:31.92,000 --> 0:00:33,000
"1984," George Orwell's "1984,"

9
0:00:34.6,000 --> 0:00:36,000
it's hitting the bestseller lists again.

10
0:00:37.96,000 --> 0:00:38,000
It's a great book,

11
0:00:39.4,000 --> 0:00:42,000
but it's not the correct dystopia for the 21st century.

12
0:00:44.08,000 --> 0:00:45,000
What we need to fear most

13
0:00:45.52,000 --> 0:00:49,000
is not what artificial intelligence will do to us on its own,

14
0:00:50.32,000 --> 0:00:54,000
but how the people in power will use artificial intelligence

15
0:00:55.08,000 --> 0:00:57,000
to control us and to manipulate us

16
0:00:57.92,000 --> 0:01:,000
in novel, sometimes hidden,

17
0:01:01.08,000 --> 0:01:04,000
subtle and unexpected ways.

18
0:01:04.12,000 --> 0:01:05,000
Much of the technology

19
0:01:06,000 --> 0:01:1,000
that threatens our freedom and our dignity in the near-term future

20
0:01:10.36,000 --> 0:01:11,000
is being developed by companies

21
0:01:12.24,000 --> 0:01:16,000
in the business of capturing and selling our data and our attention

22
0:01:17.2,000 --> 0:01:19,000
to advertisers and others:

23
0:01:19.48,000 --> 0:01:22,000
Facebook, Google, Amazon,

24
0:01:22.92,000 --> 0:01:23,000
Alibaba, Tencent.

25
0:01:26.04,000 --> 0:01:31,000
Now, artificial intelligence has started bolstering their business as well.

26
0:01:31.56,000 --> 0:01:33,000
And it may seem like artificial intelligence

27
0:01:33.68,000 --> 0:01:35,000
is just the next thing after online ads.

28
0:01:36.56,000 --> 0:01:37,000
It's not.

29
0:01:37.8,000 --> 0:01:39,000
It's a jump in category.

30
0:01:40.28,000 --> 0:01:42,000
It's a whole different world,

31
0:01:42.88,000 --> 0:01:44,000
and it has great potential.

32
0:01:45.52,000 --> 0:01:51,000
It could accelerate our understanding of many areas of study and research.

33
0:01:53.12,000 --> 0:01:56,000
But to paraphrase a famous Hollywood philosopher,

34
0:01:56.64,000 --> 0:01:59,000
"With prodigious potential comes prodigious risk."

35
0:02:01.12,000 --> 0:02:04,000
Now let's look at a basic fact of our digital lives, online ads.

36
0:02:05.08,000 --> 0:02:07,000
Right? We kind of dismiss them.

37
0:02:08,000 --> 0:02:09,000
They seem crude, ineffective.

38
0:02:1,000 --> 0:02:14,000
We've all had the experience of being followed on the web

39
0:02:14.28,000 --> 0:02:16,000
by an ad based on something we searched or read.

40
0:02:17.08,000 --> 0:02:18,000
You know, you look up a pair of boots

41
0:02:18.96,000 --> 0:02:21,000
and for a week, those boots are following you around everywhere you go.

42
0:02:22.36,000 --> 0:02:25,000
Even after you succumb and buy them, they're still following you around.

43
0:02:26.04,000 --> 0:02:29,000
We're kind of inured to that kind of basic, cheap manipulation.

44
0:02:29.08,000 --> 0:02:32,000
We roll our eyes and we think, "You know what? These things don't work."

45
0:02:33.72,000 --> 0:02:35,000
Except, online,

46
0:02:35.84,000 --> 0:02:38,000
the digital technologies are not just ads.

47
0:02:40.24,000 --> 0:02:43,000
Now, to understand that, let's think of a physical world example.

48
0:02:43.84,000 --> 0:02:47,000
You know how, at the checkout counters at supermarkets, near the cashier,

49
0:02:48.52,000 --> 0:02:51,000
there's candy and gum at the eye level of kids?

50
0:02:52.8,000 --> 0:02:55,000
That's designed to make them whine at their parents

51
0:02:56.32,000 --> 0:02:59,000
just as the parents are about to sort of check out.

52
0:03:00.04,000 --> 0:03:02,000
Now, that's a persuasion architecture.

53
0:03:03.16,000 --> 0:03:06,000
It's not nice, but it kind of works.

54
0:03:06.28,000 --> 0:03:08,000
That's why you see it in every supermarket.

55
0:03:08.72,000 --> 0:03:09,000
Now, in the physical world,

56
0:03:10.44,000 --> 0:03:12,000
such persuasion architectures are kind of limited,

57
0:03:12.96,000 --> 0:03:16,000
because you can only put so many things by the cashier. Right?

58
0:03:17.8,000 --> 0:03:21,000
And the candy and gum, it's the same for everyone,

59
0:03:22.12,000 --> 0:03:23,000
even though it mostly works

60
0:03:23.6,000 --> 0:03:27,000
only for people who have whiny little humans beside them.

61
0:03:29.16,000 --> 0:03:32,000
In the physical world, we live with those limitations.

62
0:03:34.28,000 --> 0:03:35,000
In the digital world, though,

63
0:03:36.24,000 --> 0:03:4,000
persuasion architectures can be built at the scale of billions

64
0:03:41.84,000 --> 0:03:44,000
and they can target, infer, understand

65
0:03:45.72,000 --> 0:03:47,000
and be deployed at individuals

66
0:03:48.64,000 --> 0:03:49,000
one by one

67
0:03:49.88,000 --> 0:03:51,000
by figuring out your weaknesses,

68
0:03:52.04,000 --> 0:03:57,000
and they can be sent to everyone's phone private screen,

69
0:03:57.68,000 --> 0:03:59,000
so it's not visible to us.

70
0:03:59.96,000 --> 0:04:,000
And that's different.

71
0:04:01.24,000 --> 0:04:04,000
And that's just one of the basic things that artificial intelligence can do.

72
0:04:04.84,000 --> 0:04:05,000
Now, let's take an example.

73
0:04:06.2,000 --> 0:04:08,000
Let's say you want to sell plane tickets to Vegas. Right?

74
0:04:08.92,000 --> 0:04:11,000
So in the old world, you could think of some demographics to target

75
0:04:12.44,000 --> 0:04:14,000
based on experience and what you can guess.

76
0:04:15.56,000 --> 0:04:17,000
You might try to advertise to, oh,

77
0:04:18.4,000 --> 0:04:2,000
men between the ages of 25 and 35,

78
0:04:20.92,000 --> 0:04:23,000
or people who have a high limit on their credit card,

79
0:04:24.88,000 --> 0:04:25,000
or retired couples. Right?

80
0:04:26.28,000 --> 0:04:27,000
That's what you would do in the past.

81
0:04:28.12,000 --> 0:04:3,000
With big data and machine learning,

82
0:04:31.04,000 --> 0:04:32,000
that's not how it works anymore.

83
0:04:33.32,000 --> 0:04:35,000
So to imagine that,

84
0:04:35.52,000 --> 0:04:38,000
think of all the data that Facebook has on you:

85
0:04:39.4,000 --> 0:04:41,000
every status update you ever typed,

86
0:04:41.96,000 --> 0:04:43,000
every Messenger conversation,

87
0:04:44,000 --> 0:04:45,000
every place you logged in from,

88
0:04:48.4,000 --> 0:04:51,000
all your photographs that you uploaded there.

89
0:04:51.6,000 --> 0:04:54,000
If you start typing something and change your mind and delete it,

90
0:04:55.4,000 --> 0:04:58,000
Facebook keeps those and analyzes them, too.

91
0:04:59.16,000 --> 0:05:02,000
Increasingly, it tries to match you with your offline data.

92
0:05:03.12,000 --> 0:05:06,000
It also purchases a lot of data from data brokers.

93
0:05:06.32,000 --> 0:05:09,000
It could be everything from your financial records

94
0:05:09.76,000 --> 0:05:11,000
to a good chunk of your browsing history.

95
0:05:12.36,000 --> 0:05:17,000
Right? In the US, such data is routinely collected,

96
0:05:17.8,000 --> 0:05:18,000
collated and sold.

97
0:05:20.32,000 --> 0:05:22,000
In Europe, they have tougher rules.

98
0:05:23.68,000 --> 0:05:25,000
So what happens then is,

99
0:05:26.92,000 --> 0:05:3,000
by churning through all that data, these machine-learning algorithms --

100
0:05:30.96,000 --> 0:05:32,000
that's why they're called learning algorithms --

101
0:05:33.88,000 --> 0:05:37,000
they learn to understand the characteristics of people

102
0:05:38,000 --> 0:05:4,000
who purchased tickets to Vegas before.

103
0:05:41.76,000 --> 0:05:44,000
When they learn this from existing data,

104
0:05:45.32,000 --> 0:05:48,000
they also learn how to apply this to new people.

105
0:05:49.16,000 --> 0:05:52,000
So if they're presented with a new person,

106
0:05:52.24,000 --> 0:05:56,000
they can classify whether that person is likely to buy a ticket to Vegas or not.

107
0:05:57.72,000 --> 0:06:02,000
Fine. You're thinking, an offer to buy tickets to Vegas.

108
0:06:03.2,000 --> 0:06:04,000
I can ignore that.

109
0:06:04.68,000 --> 0:06:06,000
But the problem isn't that.

110
0:06:06.92,000 --> 0:06:07,000
The problem is,

111
0:06:08.52,000 --> 0:06:12,000
we no longer really understand how these complex algorithms work.

112
0:06:12.68,000 --> 0:06:15,000
We don't understand how they're doing this categorization.

113
0:06:16.16,000 --> 0:06:2,000
It's giant matrices, thousands of rows and columns,

114
0:06:20.6,000 --> 0:06:21,000
maybe millions of rows and columns,

115
0:06:23.32,000 --> 0:06:25,000
and not the programmers

116
0:06:26.76,000 --> 0:06:27,000
and not anybody who looks at it,

117
0:06:29.44,000 --> 0:06:3,000
even if you have all the data,

118
0:06:30.96,000 --> 0:06:34,000
understands anymore how exactly it's operating

119
0:06:35.6,000 --> 0:06:38,000
any more than you'd know what I was thinking right now

120
0:06:39.4,000 --> 0:06:42,000
if you were shown a cross section of my brain.

121
0:06:44.36,000 --> 0:06:46,000
It's like we're not programming anymore,

122
0:06:46.96,000 --> 0:06:5,000
we're growing intelligence that we don't truly understand.

123
0:06:52.52,000 --> 0:06:55,000
And these things only work if there's an enormous amount of data,

124
0:06:56.52,000 --> 0:07:01,000
so they also encourage deep surveillance on all of us

125
0:07:01.64,000 --> 0:07:03,000
so that the machine learning algorithms can work.

126
0:07:04,000 --> 0:07:07,000
That's why Facebook wants to collect all the data it can about you.

127
0:07:07.2,000 --> 0:07:08,000
The algorithms work better.

128
0:07:08.8,000 --> 0:07:1,000
So let's push that Vegas example a bit.

129
0:07:11.52,000 --> 0:07:14,000
What if the system that we do not understand

130
0:07:16.2,000 --> 0:07:21,000
was picking up that it's easier to sell Vegas tickets

131
0:07:21.36,000 --> 0:07:24,000
to people who are bipolar and about to enter the manic phase.

132
0:07:25.64,000 --> 0:07:29,000
Such people tend to become overspenders, compulsive gamblers.

133
0:07:31.28,000 --> 0:07:35,000
They could do this, and you'd have no clue that's what they were picking up on.

134
0:07:35.76,000 --> 0:07:38,000
I gave this example to a bunch of computer scientists once

135
0:07:39.4,000 --> 0:07:41,000
and afterwards, one of them came up to me.

136
0:07:41.48,000 --> 0:07:44,000
He was troubled and he said, "That's why I couldn't publish it."

137
0:07:45.6,000 --> 0:07:46,000
I was like, "Couldn't publish what?"

138
0:07:47.8,000 --> 0:07:52,000
He had tried to see whether you can indeed figure out the onset of mania

139
0:07:53.68,000 --> 0:07:56,000
from social media posts before clinical symptoms,

140
0:07:56.92,000 --> 0:07:57,000
and it had worked,

141
0:07:58.72,000 --> 0:08:,000
and it had worked very well,

142
0:08:00.8,000 --> 0:08:04,000
and he had no idea how it worked or what it was picking up on.

143
0:08:06.84,000 --> 0:08:1,000
Now, the problem isn't solved if he doesn't publish it,

144
0:08:11.28,000 --> 0:08:12,000
because there are already companies

145
0:08:13.2,000 --> 0:08:15,000
that are developing this kind of technology,

146
0:08:15.76,000 --> 0:08:17,000
and a lot of the stuff is just off the shelf.

147
0:08:19.24,000 --> 0:08:21,000
This is not very difficult anymore.

148
0:08:21.84,000 --> 0:08:24,000
Do you ever go on YouTube meaning to watch one video

149
0:08:25.32,000 --> 0:08:27,000
and an hour later you've watched 27?

150
0:08:28.76,000 --> 0:08:3,000
You know how YouTube has this column on the right

151
0:08:31.28,000 --> 0:08:33,000
that says, "Up next"

152
0:08:33.52,000 --> 0:08:34,000
and it autoplays something?

153
0:08:35.36,000 --> 0:08:36,000
It's an algorithm

154
0:08:36.6,000 --> 0:08:39,000
picking what it thinks that you might be interested in

155
0:08:40.24,000 --> 0:08:41,000
and maybe not find on your own.

156
0:08:41.8,000 --> 0:08:42,000
It's not a human editor.

157
0:08:43.08,000 --> 0:08:44,000
It's what algorithms do.

158
0:08:44.52,000 --> 0:08:48,000
It picks up on what you have watched and what people like you have watched,

159
0:08:49.28,000 --> 0:08:53,000
and infers that that must be what you're interested in,

160
0:08:53.52,000 --> 0:08:54,000
what you want more of,

161
0:08:54.799,000 --> 0:08:55,000
and just shows you more.

162
0:08:56.159,000 --> 0:08:58,000
It sounds like a benign and useful feature,

163
0:08:59.28,000 --> 0:09:,000
except when it isn't.

164
0:09:01.64,000 --> 0:09:07,000
So in 2016, I attended rallies of then-candidate Donald Trump

165
0:09:09.84,000 --> 0:09:12,000
to study as a scholar the movement supporting him.

166
0:09:13.2,000 --> 0:09:16,000
I study social movements, so I was studying it, too.

167
0:09:16.68,000 --> 0:09:19,000
And then I wanted to write something about one of his rallies,

168
0:09:20.04,000 --> 0:09:21,000
so I watched it a few times on YouTube.

169
0:09:23.24,000 --> 0:09:26,000
YouTube started recommending to me

170
0:09:26.36,000 --> 0:09:3,000
and autoplaying to me white supremacist videos

171
0:09:30.64,000 --> 0:09:32,000
in increasing order of extremism.

172
0:09:33.32,000 --> 0:09:34,000
If I watched one,

173
0:09:35.16,000 --> 0:09:37,000
it served up one even more extreme

174
0:09:38.16,000 --> 0:09:39,000
and autoplayed that one, too.

175
0:09:40.32,000 --> 0:09:44,000
If you watch Hillary Clinton or Bernie Sanders content,

176
0:09:44.88,000 --> 0:09:48,000
YouTube recommends and autoplays conspiracy left,

177
0:09:49.6,000 --> 0:09:5,000
and it goes downhill from there.

178
0:09:52.48,000 --> 0:09:55,000
Well, you might be thinking, this is politics, but it's not.

179
0:09:55.56,000 --> 0:09:56,000
This isn't about politics.

180
0:09:56.84,000 --> 0:09:59,000
This is just the algorithm figuring out human behavior.

181
0:09:59.96,000 --> 0:10:03,000
I once watched a video about vegetarianism on YouTube

182
0:10:04.76,000 --> 0:10:08,000
and YouTube recommended and autoplayed a video about being vegan.

183
0:10:09.72,000 --> 0:10:12,000
It's like you're never hardcore enough for YouTube.

184
0:10:12.76,000 --> 0:10:13,000
(Laughter)

185
0:10:14.36,000 --> 0:10:15,000
So what's going on?

186
0:10:16.52,000 --> 0:10:19,000
Now, YouTube's algorithm is proprietary,

187
0:10:20.08,000 --> 0:10:22,000
but here's what I think is going on.

188
0:10:23.36,000 --> 0:10:25,000
The algorithm has figured out

189
0:10:25.48,000 --> 0:10:28,000
that if you can entice people

190
0:10:29.2,000 --> 0:10:32,000
into thinking that you can show them something more hardcore,

191
0:10:32.96,000 --> 0:10:34,000
they're more likely to stay on the site

192
0:10:35.4,000 --> 0:10:39,000
watching video after video going down that rabbit hole

193
0:10:39.84,000 --> 0:10:4,000
while Google serves them ads.

194
0:10:43.76,000 --> 0:10:46,000
Now, with nobody minding the ethics of the store,

195
0:10:47.72,000 --> 0:10:51,000
these sites can profile people

196
0:10:53.68,000 --> 0:10:54,000
who are Jew haters,

197
0:10:56.36,000 --> 0:10:58,000
who think that Jews are parasites

198
0:11:00.32,000 --> 0:11:04,000
and who have such explicit anti-Semitic content,

199
0:11:06.08,000 --> 0:11:08,000
and let you target them with ads.

200
0:11:09.2,000 --> 0:11:12,000
They can also mobilize algorithms

201
0:11:12.76,000 --> 0:11:15,000
to find for you look-alike audiences,

202
0:11:15.92,000 --> 0:11:2,000
people who do not have such explicit anti-Semitic content on their profile

203
0:11:21.52,000 --> 0:11:27,000
but who the algorithm detects may be susceptible to such messages,

204
0:11:27.72,000 --> 0:11:28,000
and lets you target them with ads, too.

205
0:11:30.68,000 --> 0:11:32,000
Now, this may sound like an implausible example,

206
0:11:33.44,000 --> 0:11:34,000
but this is real.

207
0:11:35.48,000 --> 0:11:37,000
ProPublica investigated this

208
0:11:37.64,000 --> 0:11:4,000
and found that you can indeed do this on Facebook,

209
0:11:41.28,000 --> 0:11:43,000
and Facebook helpfully offered up suggestions

210
0:11:43.72,000 --> 0:11:44,000
on how to broaden that audience.

211
0:11:46.72,000 --> 0:11:49,000
BuzzFeed tried it for Google, and very quickly they found,

212
0:11:49.76,000 --> 0:11:5,000
yep, you can do it on Google, too.

213
0:11:51.52,000 --> 0:11:52,000
And it wasn't even expensive.

214
0:11:53.24,000 --> 0:11:57,000
The ProPublica reporter spent about 30 dollars

215
0:11:57.68,000 --> 0:11:59,000
to target this category.

216
0:12:02.6,000 --> 0:12:07,000
So last year, Donald Trump's social media manager disclosed

217
0:12:07.92,000 --> 0:12:12,000
that they were using Facebook dark posts to demobilize people,

218
0:12:13.28,000 --> 0:12:14,000
not to persuade them,

219
0:12:14.68,000 --> 0:12:16,000
but to convince them not to vote at all.

220
0:12:18.52,000 --> 0:12:21,000
And to do that, they targeted specifically,

221
0:12:22.12,000 --> 0:12:25,000
for example, African-American men in key cities like Philadelphia,

222
0:12:26.04,000 --> 0:12:28,000
and I'm going to read exactly what he said.

223
0:12:28.52,000 --> 0:12:29,000
I'm quoting.

224
0:12:29.76,000 --> 0:12:32,000
They were using "nonpublic posts

225
0:12:32.8,000 --> 0:12:34,000
whose viewership the campaign controls

226
0:12:35,000 --> 0:12:38,000
so that only the people we want to see it see it.

227
0:12:38.8,000 --> 0:12:39,000
We modeled this.

228
0:12:40.04,000 --> 0:12:44,000
It will dramatically affect her ability to turn these people out."

229
0:12:45.72,000 --> 0:12:47,000
What's in those dark posts?

230
0:12:48.48,000 --> 0:12:49,000
We have no idea.

231
0:12:50.16,000 --> 0:12:51,000
Facebook won't tell us.

232
0:12:52.48,000 --> 0:12:56,000
So Facebook also algorithmically arranges the posts

233
0:12:56.88,000 --> 0:12:59,000
that your friends put on Facebook, or the pages you follow.

234
0:13:00.64,000 --> 0:13:02,000
It doesn't show you everything chronologically.

235
0:13:02.88,000 --> 0:13:06,000
It puts the order in the way that the algorithm thinks will entice you

236
0:13:07.72,000 --> 0:13:08,000
to stay on the site longer.

237
0:13:11.04,000 --> 0:13:14,000
Now, so this has a lot of consequences.

238
0:13:14.44,000 --> 0:13:17,000
You may be thinking somebody is snubbing you on Facebook.

239
0:13:18.8,000 --> 0:13:21,000
The algorithm may never be showing your post to them.

240
0:13:22.08,000 --> 0:13:27,000
The algorithm is prioritizing some of them and burying the others.

241
0:13:29.32,000 --> 0:13:3,000
Experiments show

242
0:13:30.64,000 --> 0:13:34,000
that what the algorithm picks to show you can affect your emotions.

243
0:13:36.6,000 --> 0:13:37,000
But that's not all.

244
0:13:38.28,000 --> 0:13:4,000
It also affects political behavior.

245
0:13:41.36,000 --> 0:13:45,000
So in 2010, in the midterm elections,

246
0:13:46.04,000 --> 0:13:51,000
Facebook did an experiment on 61 million people in the US

247
0:13:51.96,000 --> 0:13:52,000
that was disclosed after the fact.

248
0:13:53.88,000 --> 0:13:56,000
So some people were shown, "Today is election day,"

249
0:13:57.32,000 --> 0:13:58,000
the simpler one,

250
0:13:58.72,000 --> 0:14:01,000
and some people were shown the one with that tiny tweak

251
0:14:02.64,000 --> 0:14:04,000
with those little thumbnails

252
0:14:04.76,000 --> 0:14:06,000
of your friends who clicked on "I voted."

253
0:14:09,000 --> 0:14:1,000
This simple tweak.

254
0:14:11.52,000 --> 0:14:15,000
OK? So the pictures were the only change,

255
0:14:15.84,000 --> 0:14:18,000
and that post shown just once

256
0:14:19.12,000 --> 0:14:25,000
turned out an additional 340,000 voters

257
0:14:25.2,000 --> 0:14:26,000
in that election,

258
0:14:26.92,000 --> 0:14:27,000
according to this research

259
0:14:28.64,000 --> 0:14:3,000
as confirmed by the voter rolls.

260
0:14:32.92,000 --> 0:14:33,000
A fluke? No.

261
0:14:34.6,000 --> 0:14:39,000
Because in 2012, they repeated the same experiment.

262
0:14:40.84,000 --> 0:14:41,000
And that time,

263
0:14:42.6,000 --> 0:14:45,000
that civic message shown just once

264
0:14:45.92,000 --> 0:14:49,000
turned out an additional 270,000 voters.

265
0:14:51.16,000 --> 0:14:56,000
For reference, the 2016 US presidential election

266
0:14:56.4,000 --> 0:14:59,000
was decided by about 100,000 votes.

267
0:15:01.36,000 --> 0:15:05,000
Now, Facebook can also very easily infer what your politics are,

268
0:15:06.12,000 --> 0:15:08,000
even if you've never disclosed them on the site.

269
0:15:08.4,000 --> 0:15:1,000
Right? These algorithms can do that quite easily.

270
0:15:11.96,000 --> 0:15:14,000
What if a platform with that kind of power

271
0:15:15.88,000 --> 0:15:2,000
decides to turn out supporters of one candidate over the other?

272
0:15:21.68,000 --> 0:15:23,000
How would we even know about it?

273
0:15:25.56,000 --> 0:15:29,000
Now, we started from someplace seemingly innocuous --

274
0:15:29.72,000 --> 0:15:31,000
online adds following us around --

275
0:15:31.96,000 --> 0:15:32,000
and we've landed someplace else.

276
0:15:35.48,000 --> 0:15:37,000
As a public and as citizens,

277
0:15:37.96,000 --> 0:15:4,000
we no longer know if we're seeing the same information

278
0:15:41.4,000 --> 0:15:42,000
or what anybody else is seeing,

279
0:15:43.68,000 --> 0:15:45,000
and without a common basis of information,

280
0:15:46.28,000 --> 0:15:47,000
little by little,

281
0:15:47.92,000 --> 0:15:5,000
public debate is becoming impossible,

282
0:15:51.16,000 --> 0:15:53,000
and we're just at the beginning stages of this.

283
0:15:54.16,000 --> 0:15:57,000
These algorithms can quite easily infer

284
0:15:57.64,000 --> 0:16:,000
things like your people's ethnicity,

285
0:16:00.92,000 --> 0:16:02,000
religious and political views, personality traits,

286
0:16:03.28,000 --> 0:16:06,000
intelligence, happiness, use of addictive substances,

287
0:16:06.68,000 --> 0:16:09,000
parental separation, age and genders,

288
0:16:09.84,000 --> 0:16:1,000
just from Facebook likes.

289
0:16:13.44,000 --> 0:16:17,000
These algorithms can identify protesters

290
0:16:17.52,000 --> 0:16:19,000
even if their faces are partially concealed.

291
0:16:21.72,000 --> 0:16:27,000
These algorithms may be able to detect people's sexual orientation

292
0:16:28.36,000 --> 0:16:31,000
just from their dating profile pictures.

293
0:16:33.56,000 --> 0:16:35,000
Now, these are probabilistic guesses,

294
0:16:36.2,000 --> 0:16:38,000
so they're not going to be 100 percent right,

295
0:16:39.12,000 --> 0:16:43,000
but I don't see the powerful resisting the temptation to use these technologies

296
0:16:44.04,000 --> 0:16:46,000
just because there are some false positives,

297
0:16:46.24,000 --> 0:16:49,000
which will of course create a whole other layer of problems.

298
0:16:49.52,000 --> 0:16:51,000
Imagine what a state can do

299
0:16:52.48,000 --> 0:16:55,000
with the immense amount of data it has on its citizens.

300
0:16:56.68,000 --> 0:17:,000
China is already using face detection technology

301
0:17:01.48,000 --> 0:17:03,000
to identify and arrest people.

302
0:17:05.28,000 --> 0:17:07,000
And here's the tragedy:

303
0:17:07.44,000 --> 0:17:12,000
we're building this infrastructure of surveillance authoritarianism

304
0:17:13,000 --> 0:17:15,000
merely to get people to click on ads.

305
0:17:17.24,000 --> 0:17:19,000
And this won't be Orwell's authoritarianism.

306
0:17:19.839,000 --> 0:17:2,000
This isn't "1984."

307
0:17:21.76,000 --> 0:17:25,000
Now, if authoritarianism is using overt fear to terrorize us,

308
0:17:26.359,000 --> 0:17:28,000
we'll all be scared, but we'll know it,

309
0:17:29.28,000 --> 0:17:31,000
we'll hate it and we'll resist it.

310
0:17:32.88,000 --> 0:17:36,000
But if the people in power are using these algorithms

311
0:17:37.319,000 --> 0:17:4,000
to quietly watch us,

312
0:17:40.72,000 --> 0:17:42,000
to judge us and to nudge us,

313
0:17:43.72,000 --> 0:17:47,000
to predict and identify the troublemakers and the rebels,

314
0:17:47.92,000 --> 0:17:5,000
to deploy persuasion architectures at scale

315
0:17:51.84,000 --> 0:17:55,000
and to manipulate individuals one by one

316
0:17:56,000 --> 0:18:01,000
using their personal, individual weaknesses and vulnerabilities,

317
0:18:02.72,000 --> 0:18:04,000
and if they're doing it at scale

318
0:18:06.08,000 --> 0:18:07,000
through our private screens

319
0:18:07.84,000 --> 0:18:08,000
so that we don't even know

320
0:18:09.52,000 --> 0:18:11,000
what our fellow citizens and neighbors are seeing,

321
0:18:13.56,000 --> 0:18:17,000
that authoritarianism will envelop us like a spider's web

322
0:18:18.4,000 --> 0:18:2,000
and we may not even know we're in it.

323
0:18:22.44,000 --> 0:18:24,000
So Facebook's market capitalization

324
0:18:25.4,000 --> 0:18:28,000
is approaching half a trillion dollars.

325
0:18:28.72,000 --> 0:18:31,000
It's because it works great as a persuasion architecture.

326
0:18:33.76,000 --> 0:18:35,000
But the structure of that architecture

327
0:18:36.6,000 --> 0:18:39,000
is the same whether you're selling shoes

328
0:18:39.84,000 --> 0:18:41,000
or whether you're selling politics.

329
0:18:42.36,000 --> 0:18:45,000
The algorithms do not know the difference.

330
0:18:46.24,000 --> 0:18:49,000
The same algorithms set loose upon us

331
0:18:49.56,000 --> 0:18:52,000
to make us more pliable for ads

332
0:18:52.76,000 --> 0:18:58,000
are also organizing our political, personal and social information flows,

333
0:18:59.52,000 --> 0:19:,000
and that's what's got to change.

334
0:19:02.24,000 --> 0:19:04,000
Now, don't get me wrong,

335
0:19:04.56,000 --> 0:19:07,000
we use digital platforms because they provide us with great value.

336
0:19:09.12,000 --> 0:19:12,000
I use Facebook to keep in touch with friends and family around the world.

337
0:19:14,000 --> 0:19:19,000
I've written about how crucial social media is for social movements.

338
0:19:19.8,000 --> 0:19:22,000
I have studied how these technologies can be used

339
0:19:22.84,000 --> 0:19:24,000
to circumvent censorship around the world.

340
0:19:27.28,000 --> 0:19:33,000
But it's not that the people who run, you know, Facebook or Google

341
0:19:33.72,000 --> 0:19:35,000
are maliciously and deliberately trying

342
0:19:36.44,000 --> 0:19:4,000
to make the country or the world more polarized

343
0:19:40.92,000 --> 0:19:41,000
and encourage extremism.

344
0:19:43.44,000 --> 0:19:46,000
I read the many well-intentioned statements

345
0:19:47.44,000 --> 0:19:5,000
that these people put out.

346
0:19:51.6,000 --> 0:19:57,000
But it's not the intent or the statements people in technology make that matter,

347
0:19:57.68,000 --> 0:20:,000
it's the structures and business models they're building.

348
0:20:02.36,000 --> 0:20:04,000
And that's the core of the problem.

349
0:20:04.48,000 --> 0:20:08,000
Either Facebook is a giant con of half a trillion dollars

350
0:20:10.2,000 --> 0:20:11,000
and ads don't work on the site,

351
0:20:12.12,000 --> 0:20:14,000
it doesn't work as a persuasion architecture,

352
0:20:14.84,000 --> 0:20:18,000
or its power of influence is of great concern.

353
0:20:20.56,000 --> 0:20:21,000
It's either one or the other.

354
0:20:22.36,000 --> 0:20:23,000
It's similar for Google, too.

355
0:20:24.88,000 --> 0:20:26,000
So what can we do?

356
0:20:27.36,000 --> 0:20:28,000
This needs to change.

357
0:20:29.32,000 --> 0:20:31,000
Now, I can't offer a simple recipe,

358
0:20:31.92,000 --> 0:20:33,000
because we need to restructure

359
0:20:34.2,000 --> 0:20:37,000
the whole way our digital technology operates.

360
0:20:37.24,000 --> 0:20:41,000
Everything from the way technology is developed

361
0:20:41.36,000 --> 0:20:44,000
to the way the incentives, economic and otherwise,

362
0:20:45.24,000 --> 0:20:47,000
are built into the system.

363
0:20:48.48,000 --> 0:20:51,000
We have to face and try to deal with

364
0:20:51.96,000 --> 0:20:55,000
the lack of transparency created by the proprietary algorithms,

365
0:20:56.64,000 --> 0:20:59,000
the structural challenge of machine learning's opacity,

366
0:21:00.48,000 --> 0:21:03,000
all this indiscriminate data that's being collected about us.

367
0:21:05,000 --> 0:21:07,000
We have a big task in front of us.

368
0:21:08.16,000 --> 0:21:1,000
We have to mobilize our technology,

369
0:21:11.76,000 --> 0:21:12,000
our creativity

370
0:21:13.36,000 --> 0:21:14,000
and yes, our politics

371
0:21:16.24,000 --> 0:21:18,000
so that we can build artificial intelligence

372
0:21:18.92,000 --> 0:21:21,000
that supports us in our human goals

373
0:21:22.8,000 --> 0:21:25,000
but that is also constrained by our human values.

374
0:21:27.6,000 --> 0:21:29,000
And I understand this won't be easy.

375
0:21:30.36,000 --> 0:21:33,000
We might not even easily agree on what those terms mean.

376
0:21:34.92,000 --> 0:21:36,000
But if we take seriously

377
0:21:38.24,000 --> 0:21:43,000
how these systems that we depend on for so much operate,

378
0:21:44.24,000 --> 0:21:48,000
I don't see how we can postpone this conversation anymore.

379
0:21:49.2,000 --> 0:21:51,000
These structures

380
0:21:51.76,000 --> 0:21:55,000
are organizing how we function

381
0:21:55.88,000 --> 0:21:57,000
and they're controlling

382
0:21:58.2,000 --> 0:22:,000
what we can and we cannot do.

383
0:22:00.84,000 --> 0:22:02,000
And many of these ad-financed platforms,

384
0:22:03.32,000 --> 0:22:04,000
they boast that they're free.

385
0:22:04.92,000 --> 0:22:08,000
In this context, it means that we are the product that's being sold.

386
0:22:10.84,000 --> 0:22:12,000
We need a digital economy

387
0:22:13.6,000 --> 0:22:16,000
where our data and our attention

388
0:22:17.12,000 --> 0:22:22,000
is not for sale to the highest-bidding authoritarian or demagogue.

389
0:22:23.16,000 --> 0:22:26,000
(Applause)

390
0:22:30.48,000 --> 0:22:33,000
So to go back to that Hollywood paraphrase,

391
0:22:33.76,000 --> 0:22:36,000
we do want the prodigious potential

392
0:22:37.52,000 --> 0:22:4,000
of artificial intelligence and digital technology to blossom,

393
0:22:41.4,000 --> 0:22:45,000
but for that, we must face this prodigious menace,

394
0:22:46.36,000 --> 0:22:47,000
open-eyed and now.

395
0:22:48.32,000 --> 0:22:49,000
Thank you.

396
0:22:49.56,000 --> 0:22:53,000
(Applause)

