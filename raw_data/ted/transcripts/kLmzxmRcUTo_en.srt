1
0:00:25,000 --> 0:00:27,000
As other speakers have said, it's a rather daunting experience --

2
0:00:27,000 --> 0:00:3,000
a particularly daunting experience -- to be speaking in front of this audience.

3
0:00:3,000 --> 0:00:33,000
But unlike the other speakers, I'm not going to tell you about

4
0:00:33,000 --> 0:00:35,000
the mysteries of the universe, or the wonders of evolution,

5
0:00:35,000 --> 0:00:39,000
or the really clever, innovative ways people are attacking

6
0:00:39,000 --> 0:00:41,000
the major inequalities in our world.

7
0:00:41,000 --> 0:00:46,000
Or even the challenges of nation-states in the modern global economy.

8
0:00:46,000 --> 0:00:5,000
My brief, as you've just heard, is to tell you about statistics --

9
0:00:5,000 --> 0:00:53,000
and, to be more precise, to tell you some exciting things about statistics.

10
0:00:53,000 --> 0:00:54,000
And that's --

11
0:00:54,000 --> 0:00:55,000
(Laughter)

12
0:00:55,000 --> 0:00:57,000
-- that's rather more challenging

13
0:00:57,000 --> 0:00:59,000
than all the speakers before me and all the ones coming after me.

14
0:00:59,000 --> 0:01:,000
(Laughter)

15
0:01:01,000 --> 0:01:06,000
One of my senior colleagues told me, when I was a youngster in this profession,

16
0:01:06,000 --> 0:01:1,000
rather proudly, that statisticians were people who liked figures

17
0:01:1,000 --> 0:01:13,000
but didn't have the personality skills to become accountants.

18
0:01:13,000 --> 0:01:15,000
(Laughter)

19
0:01:15,000 --> 0:01:18,000
And there's another in-joke among statisticians, and that's,

20
0:01:18,000 --> 0:01:21,000
"How do you tell the introverted statistician from the extroverted statistician?"

21
0:01:21,000 --> 0:01:23,000
To which the answer is,

22
0:01:23,000 --> 0:01:28,000
"The extroverted statistician's the one who looks at the other person's shoes."

23
0:01:28,000 --> 0:01:31,000
(Laughter)

24
0:01:31,000 --> 0:01:36,000
But I want to tell you something useful -- and here it is, so concentrate now.

25
0:01:36,000 --> 0:01:39,000
This evening, there's a reception in the University's Museum of Natural History.

26
0:01:39,000 --> 0:01:41,000
And it's a wonderful setting, as I hope you'll find,

27
0:01:41,000 --> 0:01:46,000
and a great icon to the best of the Victorian tradition.

28
0:01:46,000 --> 0:01:51,000
It's very unlikely -- in this special setting, and this collection of people --

29
0:01:51,000 --> 0:01:54,000
but you might just find yourself talking to someone you'd rather wish that you weren't.

30
0:01:54,000 --> 0:01:56,000
So here's what you do.

31
0:01:56,000 --> 0:02:,000
When they say to you, "What do you do?" -- you say, "I'm a statistician."

32
0:02:,000 --> 0:02:01,000
(Laughter)

33
0:02:01,000 --> 0:02:05,000
Well, except they've been pre-warned now, and they'll know you're making it up.

34
0:02:05,000 --> 0:02:07,000
And then one of two things will happen.

35
0:02:07,000 --> 0:02:09,000
They'll either discover their long-lost cousin in the other corner of the room

36
0:02:09,000 --> 0:02:11,000
and run over and talk to them.

37
0:02:11,000 --> 0:02:14,000
Or they'll suddenly become parched and/or hungry -- and often both --

38
0:02:14,000 --> 0:02:16,000
and sprint off for a drink and some food.

39
0:02:16,000 --> 0:02:2,000
And you'll be left in peace to talk to the person you really want to talk to.

40
0:02:2,000 --> 0:02:23,000
It's one of the challenges in our profession to try and explain what we do.

41
0:02:23,000 --> 0:02:28,000
We're not top on people's lists for dinner party guests and conversations and so on.

42
0:02:28,000 --> 0:02:3,000
And it's something I've never really found a good way of doing.

43
0:02:3,000 --> 0:02:33,000
But my wife -- who was then my girlfriend --

44
0:02:33,000 --> 0:02:36,000
managed it much better than I've ever been able to.

45
0:02:36,000 --> 0:02:39,000
Many years ago, when we first started going out, she was working for the BBC in Britain,

46
0:02:39,000 --> 0:02:41,000
and I was, at that stage, working in America.

47
0:02:41,000 --> 0:02:43,000
I was coming back to visit her.

48
0:02:43,000 --> 0:02:49,000
She told this to one of her colleagues, who said, "Well, what does your boyfriend do?"

49
0:02:49,000 --> 0:02:51,000
Sarah thought quite hard about the things I'd explained --

50
0:02:51,000 --> 0:02:55,000
and she concentrated, in those days, on listening.

51
0:02:55,000 --> 0:02:57,000
(Laughter)

52
0:02:58,000 --> 0:03:,000
Don't tell her I said that.

53
0:03:,000 --> 0:03:04,000
And she was thinking about the work I did developing mathematical models

54
0:03:04,000 --> 0:03:07,000
for understanding evolution and modern genetics.

55
0:03:07,000 --> 0:03:1,000
So when her colleague said, "What does he do?"

56
0:03:1,000 --> 0:03:14,000
She paused and said, "He models things."

57
0:03:14,000 --> 0:03:15,000
(Laughter)

58
0:03:15,000 --> 0:03:19,000
Well, her colleague suddenly got much more interested than I had any right to expect

59
0:03:19,000 --> 0:03:22,000
and went on and said, "What does he model?"

60
0:03:22,000 --> 0:03:25,000
Well, Sarah thought a little bit more about my work and said, "Genes."

61
0:03:25,000 --> 0:03:29,000
(Laughter)

62
0:03:29,000 --> 0:03:31,000
"He models genes."

63
0:03:31,000 --> 0:03:35,000
That is my first love, and that's what I'll tell you a little bit about.

64
0:03:35,000 --> 0:03:39,000
What I want to do more generally is to get you thinking about

65
0:03:39,000 --> 0:03:42,000
the place of uncertainty and randomness and chance in our world,

66
0:03:42,000 --> 0:03:47,000
and how we react to that, and how well we do or don't think about it.

67
0:03:47,000 --> 0:03:49,000
So you've had a pretty easy time up till now --

68
0:03:49,000 --> 0:03:51,000
a few laughs, and all that kind of thing -- in the talks to date.

69
0:03:51,000 --> 0:03:54,000
You've got to think, and I'm going to ask you some questions.

70
0:03:54,000 --> 0:03:56,000
So here's the scene for the first question I'm going to ask you.

71
0:03:56,000 --> 0:03:59,000
Can you imagine tossing a coin successively?

72
0:03:59,000 --> 0:04:02,000
And for some reason -- which shall remain rather vague --

73
0:04:02,000 --> 0:04:04,000
we're interested in a particular pattern.

74
0:04:04,000 --> 0:04:07,000
Here's one -- a head, followed by a tail, followed by a tail.

75
0:04:07,000 --> 0:04:1,000
So suppose we toss a coin repeatedly.

76
0:04:1,000 --> 0:04:15,000
Then the pattern, head-tail-tail, that we've suddenly become fixated with happens here.

77
0:04:15,000 --> 0:04:19,000
And you can count: one, two, three, four, five, six, seven, eight, nine, 10 --

78
0:04:19,000 --> 0:04:21,000
it happens after the 10th toss.

79
0:04:21,000 --> 0:04:24,000
So you might think there are more interesting things to do, but humor me for the moment.

80
0:04:24,000 --> 0:04:28,000
Imagine this half of the audience each get out coins, and they toss them

81
0:04:28,000 --> 0:04:31,000
until they first see the pattern head-tail-tail.

82
0:04:31,000 --> 0:04:33,000
The first time they do it, maybe it happens after the 10th toss, as here.

83
0:04:33,000 --> 0:04:35,000
The second time, maybe it's after the fourth toss.

84
0:04:35,000 --> 0:04:37,000
The next time, after the 15th toss.

85
0:04:37,000 --> 0:04:4,000
So you do that lots and lots of times, and you average those numbers.

86
0:04:4,000 --> 0:04:43,000
That's what I want this side to think about.

87
0:04:43,000 --> 0:04:45,000
The other half of the audience doesn't like head-tail-tail --

88
0:04:45,000 --> 0:04:48,000
they think, for deep cultural reasons, that's boring --

89
0:04:48,000 --> 0:04:51,000
and they're much more interested in a different pattern -- head-tail-head.

90
0:04:51,000 --> 0:04:54,000
So, on this side, you get out your coins, and you toss and toss and toss.

91
0:04:54,000 --> 0:04:57,000
And you count the number of times until the pattern head-tail-head appears

92
0:04:57,000 --> 0:05:,000
and you average them. OK?

93
0:05:,000 --> 0:05:02,000
So on this side, you've got a number --

94
0:05:02,000 --> 0:05:04,000
you've done it lots of times, so you get it accurately --

95
0:05:04,000 --> 0:05:07,000
which is the average number of tosses until head-tail-tail.

96
0:05:07,000 --> 0:05:11,000
On this side, you've got a number -- the average number of tosses until head-tail-head.

97
0:05:11,000 --> 0:05:13,000
So here's a deep mathematical fact --

98
0:05:13,000 --> 0:05:16,000
if you've got two numbers, one of three things must be true.

99
0:05:16,000 --> 0:05:19,000
Either they're the same, or this one's bigger than this one,

100
0:05:19,000 --> 0:05:2,000
or this one's bigger than that one.

101
0:05:2,000 --> 0:05:23,000
So what's going on here?

102
0:05:23,000 --> 0:05:25,000
So you've all got to think about this, and you've all got to vote --

103
0:05:25,000 --> 0:05:26,000
and we're not moving on.

104
0:05:26,000 --> 0:05:28,000
And I don't want to end up in the two-minute silence

105
0:05:28,000 --> 0:05:32,000
to give you more time to think about it, until everyone's expressed a view. OK.

106
0:05:32,000 --> 0:05:36,000
So what you want to do is compare the average number of tosses until we first see

107
0:05:36,000 --> 0:05:4,000
head-tail-head with the average number of tosses until we first see head-tail-tail.

108
0:05:41,000 --> 0:05:43,000
Who thinks that A is true --

109
0:05:43,000 --> 0:05:47,000
that, on average, it'll take longer to see head-tail-head than head-tail-tail?

110
0:05:47,000 --> 0:05:5,000
Who thinks that B is true -- that on average, they're the same?

111
0:05:51,000 --> 0:05:53,000
Who thinks that C is true -- that, on average, it'll take less time

112
0:05:53,000 --> 0:05:56,000
to see head-tail-head than head-tail-tail?

113
0:05:57,000 --> 0:06:,000
OK, who hasn't voted yet? Because that's really naughty -- I said you had to.

114
0:06:,000 --> 0:06:01,000
(Laughter)

115
0:06:02,000 --> 0:06:05,000
OK. So most people think B is true.

116
0:06:05,000 --> 0:06:08,000
And you might be relieved to know even rather distinguished mathematicians think that.

117
0:06:08,000 --> 0:06:12,000
It's not. A is true here.

118
0:06:12,000 --> 0:06:14,000
It takes longer, on average.

119
0:06:14,000 --> 0:06:16,000
In fact, the average number of tosses till head-tail-head is 10

120
0:06:16,000 --> 0:06:21,000
and the average number of tosses until head-tail-tail is eight.

121
0:06:21,000 --> 0:06:23,000
How could that be?

122
0:06:24,000 --> 0:06:27,000
Anything different about the two patterns?

123
0:06:3,000 --> 0:06:35,000
There is. Head-tail-head overlaps itself.

124
0:06:35,000 --> 0:06:39,000
If you went head-tail-head-tail-head, you can cunningly get two occurrences

125
0:06:39,000 --> 0:06:42,000
of the pattern in only five tosses.

126
0:06:42,000 --> 0:06:44,000
You can't do that with head-tail-tail.

127
0:06:44,000 --> 0:06:46,000
That turns out to be important.

128
0:06:46,000 --> 0:06:48,000
There are two ways of thinking about this.

129
0:06:48,000 --> 0:06:5,000
I'll give you one of them.

130
0:06:5,000 --> 0:06:52,000
So imagine -- let's suppose we're doing it.

131
0:06:52,000 --> 0:06:54,000
On this side -- remember, you're excited about head-tail-tail;

132
0:06:54,000 --> 0:06:56,000
you're excited about head-tail-head.

133
0:06:56,000 --> 0:06:59,000
We start tossing a coin, and we get a head --

134
0:06:59,000 --> 0:07:,000
and you start sitting on the edge of your seat

135
0:07:,000 --> 0:07:05,000
because something great and wonderful, or awesome, might be about to happen.

136
0:07:05,000 --> 0:07:07,000
The next toss is a tail -- you get really excited.

137
0:07:07,000 --> 0:07:11,000
The champagne's on ice just next to you; you've got the glasses chilled to celebrate.

138
0:07:11,000 --> 0:07:13,000
You're waiting with bated breath for the final toss.

139
0:07:13,000 --> 0:07:15,000
And if it comes down a head, that's great.

140
0:07:15,000 --> 0:07:17,000
You're done, and you celebrate.

141
0:07:17,000 --> 0:07:19,000
If it's a tail -- well, rather disappointedly, you put the glasses away

142
0:07:19,000 --> 0:07:21,000
and put the champagne back.

143
0:07:21,000 --> 0:07:24,000
And you keep tossing, to wait for the next head, to get excited.

144
0:07:25,000 --> 0:07:27,000
On this side, there's a different experience.

145
0:07:27,000 --> 0:07:3,000
It's the same for the first two parts of the sequence.

146
0:07:3,000 --> 0:07:32,000
You're a little bit excited with the first head --

147
0:07:32,000 --> 0:07:34,000
you get rather more excited with the next tail.

148
0:07:34,000 --> 0:07:36,000
Then you toss the coin.

149
0:07:36,000 --> 0:07:39,000
If it's a tail, you crack open the champagne.

150
0:07:39,000 --> 0:07:41,000
If it's a head you're disappointed,

151
0:07:41,000 --> 0:07:44,000
but you're still a third of the way to your pattern again.

152
0:07:44,000 --> 0:07:48,000
And that's an informal way of presenting it -- that's why there's a difference.

153
0:07:48,000 --> 0:07:5,000
Another way of thinking about it --

154
0:07:5,000 --> 0:07:52,000
if we tossed a coin eight million times,

155
0:07:52,000 --> 0:07:54,000
then we'd expect a million head-tail-heads

156
0:07:54,000 --> 0:08:01,000
and a million head-tail-tails -- but the head-tail-heads could occur in clumps.

157
0:08:01,000 --> 0:08:03,000
So if you want to put a million things down amongst eight million positions

158
0:08:03,000 --> 0:08:08,000
and you can have some of them overlapping, the clumps will be further apart.

159
0:08:08,000 --> 0:08:1,000
It's another way of getting the intuition.

160
0:08:1,000 --> 0:08:12,000
What's the point I want to make?

161
0:08:12,000 --> 0:08:16,000
It's a very, very simple example, an easily stated question in probability,

162
0:08:16,000 --> 0:08:19,000
which every -- you're in good company -- everybody gets wrong.

163
0:08:19,000 --> 0:08:23,000
This is my little diversion into my real passion, which is genetics.

164
0:08:23,000 --> 0:08:26,000
There's a connection between head-tail-heads and head-tail-tails in genetics,

165
0:08:26,000 --> 0:08:29,000
and it's the following.

166
0:08:29,000 --> 0:08:32,000
When you toss a coin, you get a sequence of heads and tails.

167
0:08:32,000 --> 0:08:35,000
When you look at DNA, there's a sequence of not two things -- heads and tails --

168
0:08:35,000 --> 0:08:38,000
but four letters -- As, Gs, Cs and Ts.

169
0:08:38,000 --> 0:08:41,000
And there are little chemical scissors, called restriction enzymes

170
0:08:41,000 --> 0:08:43,000
which cut DNA whenever they see particular patterns.

171
0:08:43,000 --> 0:08:47,000
And they're an enormously useful tool in modern molecular biology.

172
0:08:48,000 --> 0:08:51,000
And instead of asking the question, "How long until I see a head-tail-head?" --

173
0:08:51,000 --> 0:08:54,000
you can ask, "How big will the chunks be when I use a restriction enzyme

174
0:08:54,000 --> 0:08:58,000
which cuts whenever it sees G-A-A-G, for example?

175
0:08:58,000 --> 0:09:,000
How long will those chunks be?"

176
0:09:,000 --> 0:09:05,000
That's a rather trivial connection between probability and genetics.

177
0:09:05,000 --> 0:09:08,000
There's a much deeper connection, which I don't have time to go into

178
0:09:08,000 --> 0:09:11,000
and that is that modern genetics is a really exciting area of science.

179
0:09:11,000 --> 0:09:15,000
And we'll hear some talks later in the conference specifically about that.

180
0:09:15,000 --> 0:09:19,000
But it turns out that unlocking the secrets in the information generated by modern

181
0:09:19,000 --> 0:09:24,000
experimental technologies, a key part of that has to do with fairly sophisticated --

182
0:09:24,000 --> 0:09:27,000
you'll be relieved to know that I do something useful in my day job,

183
0:09:27,000 --> 0:09:29,000
rather more sophisticated than the head-tail-head story --

184
0:09:29,000 --> 0:09:33,000
but quite sophisticated computer modelings and mathematical modelings

185
0:09:33,000 --> 0:09:35,000
and modern statistical techniques.

186
0:09:35,000 --> 0:09:38,000
And I will give you two little snippets -- two examples --

187
0:09:38,000 --> 0:09:41,000
of projects we're involved in in my group in Oxford,

188
0:09:41,000 --> 0:09:43,000
both of which I think are rather exciting.

189
0:09:43,000 --> 0:09:45,000
You know about the Human Genome Project.

190
0:09:45,000 --> 0:09:49,000
That was a project which aimed to read one copy of the human genome.

191
0:09:51,000 --> 0:09:53,000
The natural thing to do after you've done that --

192
0:09:53,000 --> 0:09:55,000
and that's what this project, the International HapMap Project,

193
0:09:55,000 --> 0:10:,000
which is a collaboration between labs in five or six different countries.

194
0:10:,000 --> 0:10:04,000
Think of the Human Genome Project as learning what we've got in common,

195
0:10:04,000 --> 0:10:06,000
and the HapMap Project is trying to understand

196
0:10:06,000 --> 0:10:08,000
where there are differences between different people.

197
0:10:08,000 --> 0:10:1,000
Why do we care about that?

198
0:10:1,000 --> 0:10:12,000
Well, there are lots of reasons.

199
0:10:12,000 --> 0:10:16,000
The most pressing one is that we want to understand how some differences

200
0:10:16,000 --> 0:10:2,000
make some people susceptible to one disease -- type-2 diabetes, for example --

201
0:10:2,000 --> 0:10:25,000
and other differences make people more susceptible to heart disease,

202
0:10:25,000 --> 0:10:27,000
or stroke, or autism and so on.

203
0:10:27,000 --> 0:10:29,000
That's one big project.

204
0:10:29,000 --> 0:10:31,000
There's a second big project,

205
0:10:31,000 --> 0:10:33,000
recently funded by the Wellcome Trust in this country,

206
0:10:33,000 --> 0:10:35,000
involving very large studies --

207
0:10:35,000 --> 0:10:38,000
thousands of individuals, with each of eight different diseases,

208
0:10:38,000 --> 0:10:42,000
common diseases like type-1 and type-2 diabetes, and coronary heart disease,

209
0:10:42,000 --> 0:10:46,000
bipolar disease and so on -- to try and understand the genetics.

210
0:10:46,000 --> 0:10:49,000
To try and understand what it is about genetic differences that causes the diseases.

211
0:10:49,000 --> 0:10:51,000
Why do we want to do that?

212
0:10:51,000 --> 0:10:54,000
Because we understand very little about most human diseases.

213
0:10:54,000 --> 0:10:56,000
We don't know what causes them.

214
0:10:56,000 --> 0:10:58,000
And if we can get in at the bottom and understand the genetics,

215
0:10:58,000 --> 0:11:01,000
we'll have a window on the way the disease works,

216
0:11:01,000 --> 0:11:03,000
and a whole new way about thinking about disease therapies

217
0:11:03,000 --> 0:11:06,000
and preventative treatment and so on.

218
0:11:06,000 --> 0:11:09,000
So that's, as I said, the little diversion on my main love.

219
0:11:09,000 --> 0:11:14,000
Back to some of the more mundane issues of thinking about uncertainty.

220
0:11:14,000 --> 0:11:16,000
Here's another quiz for you --

221
0:11:16,000 --> 0:11:18,000
now suppose we've got a test for a disease

222
0:11:18,000 --> 0:11:2,000
which isn't infallible, but it's pretty good.

223
0:11:2,000 --> 0:11:23,000
It gets it right 99 percent of the time.

224
0:11:23,000 --> 0:11:26,000
And I take one of you, or I take someone off the street,

225
0:11:26,000 --> 0:11:28,000
and I test them for the disease in question.

226
0:11:28,000 --> 0:11:32,000
Let's suppose there's a test for HIV -- the virus that causes AIDS --

227
0:11:32,000 --> 0:11:35,000
and the test says the person has the disease.

228
0:11:35,000 --> 0:11:38,000
What's the chance that they do?

229
0:11:38,000 --> 0:11:4,000
The test gets it right 99 percent of the time.

230
0:11:4,000 --> 0:11:44,000
So a natural answer is 99 percent.

231
0:11:44,000 --> 0:11:46,000
Who likes that answer?

232
0:11:46,000 --> 0:11:47,000
Come on -- everyone's got to get involved.

233
0:11:47,000 --> 0:11:49,000
Don't think you don't trust me anymore.

234
0:11:49,000 --> 0:11:5,000
(Laughter)

235
0:11:5,000 --> 0:11:53,000
Well, you're right to be a bit skeptical, because that's not the answer.

236
0:11:53,000 --> 0:11:55,000
That's what you might think.

237
0:11:55,000 --> 0:11:58,000
It's not the answer, and it's not because it's only part of the story.

238
0:11:58,000 --> 0:12:01,000
It actually depends on how common or how rare the disease is.

239
0:12:01,000 --> 0:12:03,000
So let me try and illustrate that.

240
0:12:03,000 --> 0:12:07,000
Here's a little caricature of a million individuals.

241
0:12:07,000 --> 0:12:1,000
So let's think about a disease that affects --

242
0:12:1,000 --> 0:12:12,000
it's pretty rare, it affects one person in 10,000.

243
0:12:12,000 --> 0:12:15,000
Amongst these million individuals, most of them are healthy

244
0:12:15,000 --> 0:12:17,000
and some of them will have the disease.

245
0:12:17,000 --> 0:12:2,000
And in fact, if this is the prevalence of the disease,

246
0:12:2,000 --> 0:12:23,000
about 100 will have the disease and the rest won't.

247
0:12:23,000 --> 0:12:25,000
So now suppose we test them all.

248
0:12:25,000 --> 0:12:27,000
What happens?

249
0:12:27,000 --> 0:12:29,000
Well, amongst the 100 who do have the disease,

250
0:12:29,000 --> 0:12:34,000
the test will get it right 99 percent of the time, and 99 will test positive.

251
0:12:34,000 --> 0:12:36,000
Amongst all these other people who don't have the disease,

252
0:12:36,000 --> 0:12:39,000
the test will get it right 99 percent of the time.

253
0:12:39,000 --> 0:12:41,000
It'll only get it wrong one percent of the time.

254
0:12:41,000 --> 0:12:45,000
But there are so many of them that there'll be an enormous number of false positives.

255
0:12:45,000 --> 0:12:47,000
Put that another way --

256
0:12:47,000 --> 0:12:52,000
of all of them who test positive -- so here they are, the individuals involved --

257
0:12:52,000 --> 0:12:57,000
less than one in 100 actually have the disease.

258
0:12:57,000 --> 0:13:01,000
So even though we think the test is accurate, the important part of the story is

259
0:13:01,000 --> 0:13:04,000
there's another bit of information we need.

260
0:13:04,000 --> 0:13:06,000
Here's the key intuition.

261
0:13:07,000 --> 0:13:1,000
What we have to do, once we know the test is positive,

262
0:13:1,000 --> 0:13:16,000
is to weigh up the plausibility, or the likelihood, of two competing explanations.

263
0:13:16,000 --> 0:13:19,000
Each of those explanations has a likely bit and an unlikely bit.

264
0:13:19,000 --> 0:13:22,000
One explanation is that the person doesn't have the disease --

265
0:13:22,000 --> 0:13:25,000
that's overwhelmingly likely, if you pick someone at random --

266
0:13:25,000 --> 0:13:28,000
but the test gets it wrong, which is unlikely.

267
0:13:29,000 --> 0:13:32,000
The other explanation is that the person does have the disease -- that's unlikely --

268
0:13:32,000 --> 0:13:35,000
but the test gets it right, which is likely.

269
0:13:35,000 --> 0:13:37,000
And the number we end up with --

270
0:13:37,000 --> 0:13:4,000
that number which is a little bit less than one in 100 --

271
0:13:4,000 --> 0:13:46,000
is to do with how likely one of those explanations is relative to the other.

272
0:13:46,000 --> 0:13:48,000
Each of them taken together is unlikely.

273
0:13:49,000 --> 0:13:52,000
Here's a more topical example of exactly the same thing.

274
0:13:52,000 --> 0:13:56,000
Those of you in Britain will know about what's become rather a celebrated case

275
0:13:56,000 --> 0:14:01,000
of a woman called Sally Clark, who had two babies who died suddenly.

276
0:14:01,000 --> 0:14:05,000
And initially, it was thought that they died of what's known informally as "cot death,"

277
0:14:05,000 --> 0:14:08,000
and more formally as "Sudden Infant Death Syndrome."

278
0:14:08,000 --> 0:14:1,000
For various reasons, she was later charged with murder.

279
0:14:1,000 --> 0:14:14,000
And at the trial, her trial, a very distinguished pediatrician gave evidence

280
0:14:14,000 --> 0:14:19,000
that the chance of two cot deaths, innocent deaths, in a family like hers --

281
0:14:19,000 --> 0:14:25,000
which was professional and non-smoking -- was one in 73 million.

282
0:14:26,000 --> 0:14:29,000
To cut a long story short, she was convicted at the time.

283
0:14:29,000 --> 0:14:34,000
Later, and fairly recently, acquitted on appeal -- in fact, on the second appeal.

284
0:14:34,000 --> 0:14:38,000
And just to set it in context, you can imagine how awful it is for someone

285
0:14:38,000 --> 0:14:41,000
to have lost one child, and then two, if they're innocent,

286
0:14:41,000 --> 0:14:43,000
to be convicted of murdering them.

287
0:14:43,000 --> 0:14:45,000
To be put through the stress of the trial, convicted of murdering them --

288
0:14:45,000 --> 0:14:48,000
and to spend time in a women's prison, where all the other prisoners

289
0:14:48,000 --> 0:14:53,000
think you killed your children -- is a really awful thing to happen to someone.

290
0:14:53,000 --> 0:14:58,000
And it happened in large part here because the expert got the statistics

291
0:14:58,000 --> 0:15:01,000
horribly wrong, in two different ways.

292
0:15:01,000 --> 0:15:05,000
So where did he get the one in 73 million number?

293
0:15:05,000 --> 0:15:08,000
He looked at some research, which said the chance of one cot death in a family

294
0:15:08,000 --> 0:15:13,000
like Sally Clark's is about one in 8,500.

295
0:15:13,000 --> 0:15:17,000
So he said, "I'll assume that if you have one cot death in a family,

296
0:15:17,000 --> 0:15:21,000
the chance of a second child dying from cot death aren't changed."

297
0:15:21,000 --> 0:15:24,000
So that's what statisticians would call an assumption of independence.

298
0:15:24,000 --> 0:15:26,000
It's like saying, "If you toss a coin and get a head the first time,

299
0:15:26,000 --> 0:15:29,000
that won't affect the chance of getting a head the second time."

300
0:15:29,000 --> 0:15:34,000
So if you toss a coin twice, the chance of getting a head twice are a half --

301
0:15:34,000 --> 0:15:37,000
that's the chance the first time -- times a half -- the chance a second time.

302
0:15:37,000 --> 0:15:39,000
So he said, "Here,

303
0:15:39,000 --> 0:15:43,000
I'll assume that these events are independent.

304
0:15:43,000 --> 0:15:45,000
When you multiply 8,500 together twice,

305
0:15:45,000 --> 0:15:47,000
you get about 73 million."

306
0:15:47,000 --> 0:15:49,000
And none of this was stated to the court as an assumption

307
0:15:49,000 --> 0:15:51,000
or presented to the jury that way.

308
0:15:52,000 --> 0:15:55,000
Unfortunately here -- and, really, regrettably --

309
0:15:55,000 --> 0:15:59,000
first of all, in a situation like this you'd have to verify it empirically.

310
0:15:59,000 --> 0:16:01,000
And secondly, it's palpably false.

311
0:16:02,000 --> 0:16:07,000
There are lots and lots of things that we don't know about sudden infant deaths.

312
0:16:07,000 --> 0:16:1,000
It might well be that there are environmental factors that we're not aware of,

313
0:16:1,000 --> 0:16:12,000
and it's pretty likely to be the case that there are

314
0:16:12,000 --> 0:16:14,000
genetic factors we're not aware of.

315
0:16:14,000 --> 0:16:17,000
So if a family suffers from one cot death, you'd put them in a high-risk group.

316
0:16:17,000 --> 0:16:19,000
They've probably got these environmental risk factors

317
0:16:19,000 --> 0:16:22,000
and/or genetic risk factors we don't know about.

318
0:16:22,000 --> 0:16:25,000
And to argue, then, that the chance of a second death is as if you didn't know

319
0:16:25,000 --> 0:16:28,000
that information is really silly.

320
0:16:28,000 --> 0:16:32,000
It's worse than silly -- it's really bad science.

321
0:16:32,000 --> 0:16:37,000
Nonetheless, that's how it was presented, and at trial nobody even argued it.

322
0:16:37,000 --> 0:16:39,000
That's the first problem.

323
0:16:39,000 --> 0:16:43,000
The second problem is, what does the number of one in 73 million mean?

324
0:16:43,000 --> 0:16:45,000
So after Sally Clark was convicted --

325
0:16:45,000 --> 0:16:49,000
you can imagine, it made rather a splash in the press --

326
0:16:49,000 --> 0:16:56,000
one of the journalists from one of Britain's more reputable newspapers wrote that

327
0:16:56,000 --> 0:16:58,000
what the expert had said was,

328
0:16:58,000 --> 0:17:03,000
"The chance that she was innocent was one in 73 million."

329
0:17:03,000 --> 0:17:05,000
Now, that's a logical error.

330
0:17:05,000 --> 0:17:08,000
It's exactly the same logical error as the logical error of thinking that

331
0:17:08,000 --> 0:17:1,000
after the disease test, which is 99 percent accurate,

332
0:17:1,000 --> 0:17:14,000
the chance of having the disease is 99 percent.

333
0:17:14,000 --> 0:17:18,000
In the disease example, we had to bear in mind two things,

334
0:17:18,000 --> 0:17:22,000
one of which was the possibility that the test got it right or not.

335
0:17:22,000 --> 0:17:26,000
And the other one was the chance, a priori, that the person had the disease or not.

336
0:17:26,000 --> 0:17:29,000
It's exactly the same in this context.

337
0:17:29,000 --> 0:17:33,000
There are two things involved -- two parts to the explanation.

338
0:17:33,000 --> 0:17:37,000
We want to know how likely, or relatively how likely, two different explanations are.

339
0:17:37,000 --> 0:17:4,000
One of them is that Sally Clark was innocent --

340
0:17:4,000 --> 0:17:42,000
which is, a priori, overwhelmingly likely --

341
0:17:42,000 --> 0:17:45,000
most mothers don't kill their children.

342
0:17:45,000 --> 0:17:47,000
And the second part of the explanation

343
0:17:47,000 --> 0:17:5,000
is that she suffered an incredibly unlikely event.

344
0:17:5,000 --> 0:17:54,000
Not as unlikely as one in 73 million, but nonetheless rather unlikely.

345
0:17:54,000 --> 0:17:56,000
The other explanation is that she was guilty.

346
0:17:56,000 --> 0:17:58,000
Now, we probably think a priori that's unlikely.

347
0:17:58,000 --> 0:18:01,000
And we certainly should think in the context of a criminal trial

348
0:18:01,000 --> 0:18:04,000
that that's unlikely, because of the presumption of innocence.

349
0:18:04,000 --> 0:18:08,000
And then if she were trying to kill the children, she succeeded.

350
0:18:08,000 --> 0:18:12,000
So the chance that she's innocent isn't one in 73 million.

351
0:18:12,000 --> 0:18:14,000
We don't know what it is.

352
0:18:14,000 --> 0:18:18,000
It has to do with weighing up the strength of the other evidence against her

353
0:18:18,000 --> 0:18:2,000
and the statistical evidence.

354
0:18:2,000 --> 0:18:22,000
We know the children died.

355
0:18:22,000 --> 0:18:26,000
What matters is how likely or unlikely, relative to each other,

356
0:18:26,000 --> 0:18:28,000
the two explanations are.

357
0:18:28,000 --> 0:18:3,000
And they're both implausible.

358
0:18:31,000 --> 0:18:35,000
There's a situation where errors in statistics had really profound

359
0:18:35,000 --> 0:18:38,000
and really unfortunate consequences.

360
0:18:38,000 --> 0:18:4,000
In fact, there are two other women who were convicted on the basis of the

361
0:18:4,000 --> 0:18:44,000
evidence of this pediatrician, who have subsequently been released on appeal.

362
0:18:44,000 --> 0:18:46,000
Many cases were reviewed.

363
0:18:46,000 --> 0:18:5,000
And it's particularly topical because he's currently facing a disrepute charge

364
0:18:5,000 --> 0:18:53,000
at Britain's General Medical Council.

365
0:18:53,000 --> 0:18:57,000
So just to conclude -- what are the take-home messages from this?

366
0:18:57,000 --> 0:19:01,000
Well, we know that randomness and uncertainty and chance

367
0:19:01,000 --> 0:19:04,000
are very much a part of our everyday life.

368
0:19:04,000 --> 0:19:09,000
It's also true -- and, although, you, as a collective, are very special in many ways,

369
0:19:09,000 --> 0:19:13,000
you're completely typical in not getting the examples I gave right.

370
0:19:13,000 --> 0:19:16,000
It's very well documented that people get things wrong.

371
0:19:16,000 --> 0:19:19,000
They make errors of logic in reasoning with uncertainty.

372
0:19:2,000 --> 0:19:22,000
We can cope with the subtleties of language brilliantly --

373
0:19:22,000 --> 0:19:25,000
and there are interesting evolutionary questions about how we got here.

374
0:19:25,000 --> 0:19:28,000
We are not good at reasoning with uncertainty.

375
0:19:28,000 --> 0:19:3,000
That's an issue in our everyday lives.

376
0:19:3,000 --> 0:19:33,000
As you've heard from many of the talks, statistics underpins an enormous amount

377
0:19:33,000 --> 0:19:36,000
of research in science -- in social science, in medicine

378
0:19:36,000 --> 0:19:38,000
and indeed, quite a lot of industry.

379
0:19:38,000 --> 0:19:42,000
All of quality control, which has had a major impact on industrial processing,

380
0:19:42,000 --> 0:19:44,000
is underpinned by statistics.

381
0:19:44,000 --> 0:19:46,000
It's something we're bad at doing.

382
0:19:46,000 --> 0:19:49,000
At the very least, we should recognize that, and we tend not to.

383
0:19:49,000 --> 0:19:53,000
To go back to the legal context, at the Sally Clark trial

384
0:19:53,000 --> 0:19:57,000
all of the lawyers just accepted what the expert said.

385
0:19:57,000 --> 0:19:59,000
So if a pediatrician had come out and said to a jury,

386
0:19:59,000 --> 0:20:02,000
"I know how to build bridges. I've built one down the road.

387
0:20:02,000 --> 0:20:04,000
Please drive your car home over it,"

388
0:20:04,000 --> 0:20:06,000
they would have said, "Well, pediatricians don't know how to build bridges.

389
0:20:06,000 --> 0:20:08,000
That's what engineers do."

390
0:20:08,000 --> 0:20:11,000
On the other hand, he came out and effectively said, or implied,

391
0:20:11,000 --> 0:20:14,000
"I know how to reason with uncertainty. I know how to do statistics."

392
0:20:14,000 --> 0:20:17,000
And everyone said, "Well, that's fine. He's an expert."

393
0:20:17,000 --> 0:20:2,000
So we need to understand where our competence is and isn't.

394
0:20:2,000 --> 0:20:24,000
Exactly the same kinds of issues arose in the early days of DNA profiling,

395
0:20:24,000 --> 0:20:28,000
when scientists, and lawyers and in some cases judges,

396
0:20:28,000 --> 0:20:31,000
routinely misrepresented evidence.

397
0:20:32,000 --> 0:20:35,000
Usually -- one hopes -- innocently, but misrepresented evidence.

398
0:20:35,000 --> 0:20:4,000
Forensic scientists said, "The chance that this guy's innocent is one in three million."

399
0:20:4,000 --> 0:20:42,000
Even if you believe the number, just like the 73 million to one,

400
0:20:42,000 --> 0:20:44,000
that's not what it meant.

401
0:20:44,000 --> 0:20:46,000
And there have been celebrated appeal cases

402
0:20:46,000 --> 0:20:48,000
in Britain and elsewhere because of that.

403
0:20:48,000 --> 0:20:51,000
And just to finish in the context of the legal system.

404
0:20:51,000 --> 0:20:55,000
It's all very well to say, "Let's do our best to present the evidence."

405
0:20:55,000 --> 0:20:58,000
But more and more, in cases of DNA profiling -- this is another one --

406
0:20:58,000 --> 0:21:01,000
we expect juries, who are ordinary people --

407
0:21:01,000 --> 0:21:03,000
and it's documented they're very bad at this --

408
0:21:03,000 --> 0:21:07,000
we expect juries to be able to cope with the sorts of reasoning that goes on.

409
0:21:07,000 --> 0:21:12,000
In other spheres of life, if people argued -- well, except possibly for politics --

410
0:21:12,000 --> 0:21:14,000
but in other spheres of life, if people argued illogically,

411
0:21:14,000 --> 0:21:16,000
we'd say that's not a good thing.

412
0:21:16,000 --> 0:21:2,000
We sort of expect it of politicians and don't hope for much more.

413
0:21:2,000 --> 0:21:23,000
In the case of uncertainty, we get it wrong all the time --

414
0:21:23,000 --> 0:21:25,000
and at the very least, we should be aware of that,

415
0:21:25,000 --> 0:21:27,000
and ideally, we might try and do something about it.

416
0:21:27,000 --> 0:21:28,000
Thanks very much.

