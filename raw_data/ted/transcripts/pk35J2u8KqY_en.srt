1
0:00:12.705,000 --> 0:00:13,000
In ancient Greece,

2
0:00:15.256,000 --> 0:00:18,000
when anyone from slaves to soldiers, poets and politicians,

3
0:00:19.223,000 --> 0:00:23,000
needed to make a big decision on life's most important questions,

4
0:00:23.251,000 --> 0:00:24,000
like, "Should I get married?"

5
0:00:24.666,000 --> 0:00:25,000
or "Should we embark on this voyage?"

6
0:00:26.547,000 --> 0:00:28,000
or "Should our army advance into this territory?"

7
0:00:29.499,000 --> 0:00:31,000
they all consulted the oracle.

8
0:00:32.84,000 --> 0:00:33,000
So this is how it worked:

9
0:00:34.304,000 --> 0:00:37,000
you would bring her a question and you would get on your knees,

10
0:00:37.44,000 --> 0:00:38,000
and then she would go into this trance.

11
0:00:39.335,000 --> 0:00:4,000
It would take a couple of days,

12
0:00:40.908,000 --> 0:00:42,000
and then eventually she would come out of it,

13
0:00:43.095,000 --> 0:00:45,000
giving you her predictions as your answer.

14
0:00:46.73,000 --> 0:00:48,000
From the oracle bones of ancient China

15
0:00:49.32,000 --> 0:00:51,000
to ancient Greece to Mayan calendars,

16
0:00:51.689,000 --> 0:00:53,000
people have craved for prophecy

17
0:00:54.009,000 --> 0:00:57,000
in order to find out what's going to happen next.

18
0:00:58.336,000 --> 0:01:01,000
And that's because we all want to make the right decision.

19
0:01:01.599,000 --> 0:01:02,000
We don't want to miss something.

20
0:01:03.712,000 --> 0:01:04,000
The future is scary,

21
0:01:05.479,000 --> 0:01:07,000
so it's much nicer knowing that we can make a decision

22
0:01:08.22,000 --> 0:01:09,000
with some assurance of the outcome.

23
0:01:10.899,000 --> 0:01:11,000
Well, we have a new oracle,

24
0:01:12.534,000 --> 0:01:14,000
and it's name is big data,

25
0:01:14.703,000 --> 0:01:17,000
or we call it "Watson" or "deep learning" or "neural net."

26
0:01:19.16,000 --> 0:01:23,000
And these are the kinds of questions we ask of our oracle now,

27
0:01:23.196,000 --> 0:01:26,000
like, "What's the most efficient way to ship these phones

28
0:01:27.142,000 --> 0:01:28,000
from China to Sweden?"

29
0:01:28.989,000 --> 0:01:29,000
Or, "What are the odds

30
0:01:30.813,000 --> 0:01:33,000
of my child being born with a genetic disorder?"

31
0:01:34.772,000 --> 0:01:37,000
Or, "What are the sales volume we can predict for this product?"

32
0:01:39.928,000 --> 0:01:43,000
I have a dog. Her name is Elle, and she hates the rain.

33
0:01:43.999,000 --> 0:01:46,000
And I have tried everything to untrain her.

34
0:01:47.329,000 --> 0:01:49,000
But because I have failed at this,

35
0:01:50.124,000 --> 0:01:53,000
I also have to consult an oracle, called Dark Sky,

36
0:01:53.434,000 --> 0:01:54,000
every time before we go on a walk,

37
0:01:55.093,000 --> 0:01:58,000
for very accurate weather predictions in the next 10 minutes.

38
0:02:01.355,000 --> 0:02:02,000
She's so sweet.

39
0:02:03.647,000 --> 0:02:08,000
So because of all of this, our oracle is a $122 billion industry.

40
0:02:09.826,000 --> 0:02:12,000
Now, despite the size of this industry,

41
0:02:13.226,000 --> 0:02:15,000
the returns are surprisingly low.

42
0:02:16.162,000 --> 0:02:18,000
Investing in big data is easy,

43
0:02:18.68,000 --> 0:02:19,000
but using it is hard.

44
0:02:21.801,000 --> 0:02:25,000
Over 73 percent of big data projects aren't even profitable,

45
0:02:25.865,000 --> 0:02:27,000
and I have executives coming up to me saying,

46
0:02:28.32,000 --> 0:02:29,000
"We're experiencing the same thing.

47
0:02:30.133,000 --> 0:02:31,000
We invested in some big data system,

48
0:02:31.91,000 --> 0:02:33,000
and our employees aren't making better decisions.

49
0:02:34.902,000 --> 0:02:37,000
And they're certainly not coming up with more breakthrough ideas."

50
0:02:38.734,000 --> 0:02:41,000
So this is all really interesting to me,

51
0:02:41.942,000 --> 0:02:43,000
because I'm a technology ethnographer.

52
0:02:44.45,000 --> 0:02:46,000
I study and I advise companies

53
0:02:47.038,000 --> 0:02:49,000
on the patterns of how people use technology,

54
0:02:49.545,000 --> 0:02:51,000
and one of my interest areas is data.

55
0:02:52.247,000 --> 0:02:57,000
So why is having more data not helping us make better decisions,

56
0:02:57.464,000 --> 0:02:59,000
especially for companies who have all these resources

57
0:03:00.271,000 --> 0:03:01,000
to invest in these big data systems?

58
0:03:02.031,000 --> 0:03:04,000
Why isn't it getting any easier for them?

59
0:03:05.81,000 --> 0:03:07,000
So, I've witnessed the struggle firsthand.

60
0:03:09.194,000 --> 0:03:12,000
In 2009, I started a research position with Nokia.

61
0:03:13.052,000 --> 0:03:14,000
And at the time,

62
0:03:14.234,000 --> 0:03:17,000
Nokia was one of the largest cell phone companies in the world,

63
0:03:17.416,000 --> 0:03:2,000
dominating emerging markets like China, Mexico and India --

64
0:03:20.642,000 --> 0:03:22,000
all places where I had done a lot of research

65
0:03:23.168,000 --> 0:03:25,000
on how low-income people use technology.

66
0:03:25.868,000 --> 0:03:27,000
And I spent a lot of extra time in China

67
0:03:28.222,000 --> 0:03:3,000
getting to know the informal economy.

68
0:03:30.838,000 --> 0:03:32,000
So I did things like working as a street vendor

69
0:03:33.263,000 --> 0:03:35,000
selling dumplings to construction workers.

70
0:03:35.861,000 --> 0:03:36,000
Or I did fieldwork,

71
0:03:37.243,000 --> 0:03:39,000
spending nights and days in internet cafés,

72
0:03:40.225,000 --> 0:03:42,000
hanging out with Chinese youth, so I could understand

73
0:03:42.795,000 --> 0:03:44,000
how they were using games and mobile phones

74
0:03:45.103,000 --> 0:03:48,000
and using it between moving from the rural areas to the cities.

75
0:03:50.155,000 --> 0:03:53,000
Through all of this qualitative evidence that I was gathering,

76
0:03:54.106,000 --> 0:03:56,000
I was starting to see so clearly

77
0:03:56.954,000 --> 0:04:,000
that a big change was about to happen among low-income Chinese people.

78
0:04:02.84,000 --> 0:04:06,000
Even though they were surrounded by advertisements for luxury products

79
0:04:07.231,000 --> 0:04:1,000
like fancy toilets -- who wouldn't want one? --

80
0:04:10.75,000 --> 0:04:12,000
and apartments and cars,

81
0:04:13.664,000 --> 0:04:14,000
through my conversations with them,

82
0:04:15.508,000 --> 0:04:18,000
I found out that the ads the actually enticed them the most

83
0:04:19.373,000 --> 0:04:2,000
were the ones for iPhones,

84
0:04:21.393,000 --> 0:04:24,000
promising them this entry into this high-tech life.

85
0:04:25.289,000 --> 0:04:28,000
And even when I was living with them in urban slums like this one,

86
0:04:28.476,000 --> 0:04:3,000
I saw people investing over half of their monthly income

87
0:04:31.496,000 --> 0:04:32,000
into buying a phone,

88
0:04:33.143,000 --> 0:04:35,000
and increasingly, they were "shanzhai,"

89
0:04:35.469,000 --> 0:04:38,000
which are affordable knock-offs of iPhones and other brands.

90
0:04:40.123,000 --> 0:04:41,000
They're very usable.

91
0:04:42.71,000 --> 0:04:43,000
Does the job.

92
0:04:44.57,000 --> 0:04:49,000
And after years of living with migrants and working with them

93
0:04:50.383,000 --> 0:04:53,000
and just really doing everything that they were doing,

94
0:04:53.841,000 --> 0:04:56,000
I started piecing all these data points together --

95
0:04:57.462,000 --> 0:05:,000
from the things that seem random, like me selling dumplings,

96
0:05:00.609,000 --> 0:05:01,000
to the things that were more obvious,

97
0:05:02.437,000 --> 0:05:05,000
like tracking how much they were spending on their cell phone bills.

98
0:05:05.693,000 --> 0:05:07,000
And I was able to create this much more holistic picture

99
0:05:08.356,000 --> 0:05:09,000
of what was happening.

100
0:05:09.536,000 --> 0:05:1,000
And that's when I started to realize

101
0:05:11.282,000 --> 0:05:14,000
that even the poorest in China would want a smartphone,

102
0:05:14.815,000 --> 0:05:18,000
and that they would do almost anything to get their hands on one.

103
0:05:20.893,000 --> 0:05:22,000
You have to keep in mind,

104
0:05:23.321,000 --> 0:05:26,000
iPhones had just come out, it was 2009,

105
0:05:26.429,000 --> 0:05:27,000
so this was, like, eight years ago,

106
0:05:28.338,000 --> 0:05:3,000
and Androids had just started looking like iPhones.

107
0:05:30.799,000 --> 0:05:32,000
And a lot of very smart and realistic people said,

108
0:05:33.33,000 --> 0:05:35,000
"Those smartphones -- that's just a fad.

109
0:05:36.063,000 --> 0:05:38,000
Who wants to carry around these heavy things

110
0:05:39.083,000 --> 0:05:42,000
where batteries drain quickly and they break every time you drop them?"

111
0:05:44.613,000 --> 0:05:45,000
But I had a lot of data,

112
0:05:45.838,000 --> 0:05:47,000
and I was very confident about my insights,

113
0:05:48.122,000 --> 0:05:5,000
so I was very excited to share them with Nokia.

114
0:05:53.152,000 --> 0:05:55,000
But Nokia was not convinced,

115
0:05:55.693,000 --> 0:05:57,000
because it wasn't big data.

116
0:05:58.842,000 --> 0:06:,000
They said, "We have millions of data points,

117
0:06:01.27,000 --> 0:06:05,000
and we don't see any indicators of anyone wanting to buy a smartphone,

118
0:06:05.541,000 --> 0:06:09,000
and your data set of 100, as diverse as it is, is too weak

119
0:06:09.953,000 --> 0:06:1,000
for us to even take seriously."

120
0:06:12.728,000 --> 0:06:13,000
And I said, "Nokia, you're right.

121
0:06:14.357,000 --> 0:06:15,000
Of course you wouldn't see this,

122
0:06:15.941,000 --> 0:06:18,000
because you're sending out surveys assuming that people don't know

123
0:06:19.336,000 --> 0:06:2,000
what a smartphone is,

124
0:06:20.519,000 --> 0:06:22,000
so of course you're not going to get any data back

125
0:06:22.909,000 --> 0:06:24,000
about people wanting to buy a smartphone in two years.

126
0:06:25.505,000 --> 0:06:27,000
Your surveys, your methods have been designed

127
0:06:27.647,000 --> 0:06:29,000
to optimize an existing business model,

128
0:06:29.693,000 --> 0:06:31,000
and I'm looking at these emergent human dynamics

129
0:06:32.325,000 --> 0:06:33,000
that haven't happened yet.

130
0:06:33.703,000 --> 0:06:35,000
We're looking outside of market dynamics

131
0:06:36.165,000 --> 0:06:37,000
so that we can get ahead of it."

132
0:06:39.193,000 --> 0:06:41,000
Well, you know what happened to Nokia?

133
0:06:41.461,000 --> 0:06:43,000
Their business fell off a cliff.

134
0:06:44.611,000 --> 0:06:47,000
This -- this is the cost of missing something.

135
0:06:48.983,000 --> 0:06:49,000
It was unfathomable.

136
0:06:51.823,000 --> 0:06:52,000
But Nokia's not alone.

137
0:06:54.078,000 --> 0:06:56,000
I see organizations throwing out data all the time

138
0:06:56.683,000 --> 0:06:58,000
because it didn't come from a quant model

139
0:06:59.268,000 --> 0:07:,000
or it doesn't fit in one.

140
0:07:02.039,000 --> 0:07:04,000
But it's not big data's fault.

141
0:07:04.762,000 --> 0:07:07,000
It's the way we use big data; it's our responsibility.

142
0:07:09.55,000 --> 0:07:1,000
Big data's reputation for success

143
0:07:11.485,000 --> 0:07:14,000
comes from quantifying very specific environments,

144
0:07:15.268,000 --> 0:07:19,000
like electricity power grids or delivery logistics or genetic code,

145
0:07:20.205,000 --> 0:07:24,000
when we're quantifying in systems that are more or less contained.

146
0:07:24.547,000 --> 0:07:26,000
But not all systems are as neatly contained.

147
0:07:27.54,000 --> 0:07:3,000
When you're quantifying and systems are more dynamic,

148
0:07:30.822,000 --> 0:07:33,000
especially systems that involve human beings,

149
0:07:34.645,000 --> 0:07:36,000
forces are complex and unpredictable,

150
0:07:37.095,000 --> 0:07:4,000
and these are things that we don't know how to model so well.

151
0:07:41.024,000 --> 0:07:43,000
Once you predict something about human behavior,

152
0:07:43.861,000 --> 0:07:44,000
new factors emerge,

153
0:07:45.74,000 --> 0:07:47,000
because conditions are constantly changing.

154
0:07:48.129,000 --> 0:07:49,000
That's why it's a never-ending cycle.

155
0:07:49.956,000 --> 0:07:5,000
You think you know something,

156
0:07:51.444,000 --> 0:07:53,000
and then something unknown enters the picture.

157
0:07:53.71,000 --> 0:07:56,000
And that's why just relying on big data alone

158
0:07:57.056,000 --> 0:07:59,000
increases the chance that we'll miss something,

159
0:07:59.929,000 --> 0:08:02,000
while giving us this illusion that we already know everything.

160
0:08:04.226,000 --> 0:08:07,000
And what makes it really hard to see this paradox

161
0:08:08.106,000 --> 0:08:1,000
and even wrap our brains around it

162
0:08:10.789,000 --> 0:08:13,000
is that we have this thing that I call the quantification bias,

163
0:08:14.504,000 --> 0:08:17,000
which is the unconscious belief of valuing the measurable

164
0:08:18.45,000 --> 0:08:19,000
over the immeasurable.

165
0:08:21.042,000 --> 0:08:24,000
And we often experience this at our work.

166
0:08:24.35,000 --> 0:08:26,000
Maybe we work alongside colleagues who are like this,

167
0:08:27.024,000 --> 0:08:29,000
or even our whole entire company may be like this,

168
0:08:29.476,000 --> 0:08:31,000
where people become so fixated on that number,

169
0:08:32.046,000 --> 0:08:34,000
that they can't see anything outside of it,

170
0:08:34.137,000 --> 0:08:37,000
even when you present them evidence right in front of their face.

171
0:08:38.943,000 --> 0:08:41,000
And this is a very appealing message,

172
0:08:42.338,000 --> 0:08:44,000
because there's nothing wrong with quantifying;

173
0:08:44.705,000 --> 0:08:45,000
it's actually very satisfying.

174
0:08:46.159,000 --> 0:08:5,000
I get a great sense of comfort from looking at an Excel spreadsheet,

175
0:08:50.545,000 --> 0:08:51,000
even very simple ones.

176
0:08:51.97,000 --> 0:08:52,000
(Laughter)

177
0:08:53.008,000 --> 0:08:54,000
It's just kind of like,

178
0:08:54.184,000 --> 0:08:57,000
"Yes! The formula worked. It's all OK. Everything is under control."

179
0:08:58.612,000 --> 0:09:,000
But the problem is

180
0:09:01.026,000 --> 0:09:03,000
that quantifying is addictive.

181
0:09:03.711,000 --> 0:09:04,000
And when we forget that

182
0:09:05.117,000 --> 0:09:08,000
and when we don't have something to kind of keep that in check,

183
0:09:08.179,000 --> 0:09:1,000
it's very easy to just throw out data

184
0:09:10.321,000 --> 0:09:12,000
because it can't be expressed as a numerical value.

185
0:09:13.063,000 --> 0:09:15,000
It's very easy just to slip into silver-bullet thinking,

186
0:09:16.008,000 --> 0:09:18,000
as if some simple solution existed.

187
0:09:19.42,000 --> 0:09:23,000
Because this is a great moment of danger for any organization,

188
0:09:23.506,000 --> 0:09:25,000
because oftentimes, the future we need to predict --

189
0:09:26.164,000 --> 0:09:28,000
it isn't in that haystack,

190
0:09:28.354,000 --> 0:09:3,000
but it's that tornado that's bearing down on us

191
0:09:30.916,000 --> 0:09:31,000
outside of the barn.

192
0:09:34.78,000 --> 0:09:36,000
There is no greater risk

193
0:09:37.13,000 --> 0:09:38,000
than being blind to the unknown.

194
0:09:38.82,000 --> 0:09:4,000
It can cause you to make the wrong decisions.

195
0:09:40.993,000 --> 0:09:41,000
It can cause you to miss something big.

196
0:09:43.554,000 --> 0:09:46,000
But we don't have to go down this path.

197
0:09:47.273,000 --> 0:09:5,000
It turns out that the oracle of ancient Greece

198
0:09:50.492,000 --> 0:09:53,000
holds the secret key that shows us the path forward.

199
0:09:55.474,000 --> 0:09:57,000
Now, recent geological research has shown

200
0:09:58.093,000 --> 0:10:01,000
that the Temple of Apollo, where the most famous oracle sat,

201
0:10:01.681,000 --> 0:10:04,000
was actually built over two earthquake faults.

202
0:10:04.789,000 --> 0:10:06,000
And these faults would release these petrochemical fumes

203
0:10:07.699,000 --> 0:10:08,000
from underneath the Earth's crust,

204
0:10:09.408,000 --> 0:10:12,000
and the oracle literally sat right above these faults,

205
0:10:13.298,000 --> 0:10:16,000
inhaling enormous amounts of ethylene gas, these fissures.

206
0:10:16.91,000 --> 0:10:17,000
(Laughter)

207
0:10:17.942,000 --> 0:10:18,000
It's true.

208
0:10:19.139,000 --> 0:10:2,000
(Laughter)

209
0:10:20.18,000 --> 0:10:23,000
It's all true, and that's what made her babble and hallucinate

210
0:10:23.713,000 --> 0:10:24,000
and go into this trance-like state.

211
0:10:25.461,000 --> 0:10:26,000
She was high as a kite!

212
0:10:27.255,000 --> 0:10:31,000
(Laughter)

213
0:10:31.74,000 --> 0:10:33,000
So how did anyone --

214
0:10:34.543,000 --> 0:10:37,000
How did anyone get any useful advice out of her

215
0:10:37.597,000 --> 0:10:38,000
in this state?

216
0:10:39.317,000 --> 0:10:41,000
Well, you see those people surrounding the oracle?

217
0:10:41.722,000 --> 0:10:42,000
You see those people holding her up,

218
0:10:43.625,000 --> 0:10:44,000
because she's, like, a little woozy?

219
0:10:45.366,000 --> 0:10:47,000
And you see that guy on your left-hand side

220
0:10:47.698,000 --> 0:10:48,000
holding the orange notebook?

221
0:10:49.925,000 --> 0:10:5,000
Well, those were the temple guides,

222
0:10:51.679,000 --> 0:10:54,000
and they worked hand in hand with the oracle.

223
0:10:55.904,000 --> 0:10:57,000
When inquisitors would come and get on their knees,

224
0:10:58.444,000 --> 0:11:,000
that's when the temple guides would get to work,

225
0:11:00.808,000 --> 0:11:01,000
because after they asked her questions,

226
0:11:02.696,000 --> 0:11:04,000
they would observe their emotional state,

227
0:11:04.721,000 --> 0:11:06,000
and then they would ask them follow-up questions,

228
0:11:07.069,000 --> 0:11:09,000
like, "Why do you want to know this prophecy? Who are you?

229
0:11:09.927,000 --> 0:11:11,000
What are you going to do with this information?"

230
0:11:12.215,000 --> 0:11:15,000
And then the temple guides would take this more ethnographic,

231
0:11:15.421,000 --> 0:11:17,000
this more qualitative information,

232
0:11:17.601,000 --> 0:11:19,000
and interpret the oracle's babblings.

233
0:11:21.248,000 --> 0:11:23,000
So the oracle didn't stand alone,

234
0:11:23.564,000 --> 0:11:25,000
and neither should our big data systems.

235
0:11:26.45,000 --> 0:11:27,000
Now to be clear,

236
0:11:27.635,000 --> 0:11:3,000
I'm not saying that big data systems are huffing ethylene gas,

237
0:11:31.118,000 --> 0:11:33,000
or that they're even giving invalid predictions.

238
0:11:33.495,000 --> 0:11:34,000
The total opposite.

239
0:11:34.68,000 --> 0:11:36,000
But what I am saying

240
0:11:36.772,000 --> 0:11:39,000
is that in the same way that the oracle needed her temple guides,

241
0:11:40.628,000 --> 0:11:42,000
our big data systems need them, too.

242
0:11:42.94,000 --> 0:11:46,000
They need people like ethnographers and user researchers

243
0:11:47.073,000 --> 0:11:49,000
who can gather what I call thick data.

244
0:11:50.322,000 --> 0:11:52,000
This is precious data from humans,

245
0:11:53.337,000 --> 0:11:57,000
like stories, emotions and interactions that cannot be quantified.

246
0:11:57.463,000 --> 0:11:59,000
It's the kind of data that I collected for Nokia

247
0:11:59.809,000 --> 0:12:01,000
that comes in in the form of a very small sample size,

248
0:12:02.502,000 --> 0:12:04,000
but delivers incredible depth of meaning.

249
0:12:05.481,000 --> 0:12:08,000
And what makes it so thick and meaty

250
0:12:10.265,000 --> 0:12:14,000
is the experience of understanding the human narrative.

251
0:12:14.318,000 --> 0:12:17,000
And that's what helps to see what's missing in our models.

252
0:12:18.671,000 --> 0:12:22,000
Thick data grounds our business questions in human questions,

253
0:12:22.74,000 --> 0:12:25,000
and that's why integrating big and thick data

254
0:12:26.326,000 --> 0:12:27,000
forms a more complete picture.

255
0:12:28.592,000 --> 0:12:3,000
Big data is able to offer insights at scale

256
0:12:31.497,000 --> 0:12:33,000
and leverage the best of machine intelligence,

257
0:12:34.168,000 --> 0:12:37,000
whereas thick data can help us rescue the context loss

258
0:12:37.764,000 --> 0:12:39,000
that comes from making big data usable,

259
0:12:39.886,000 --> 0:12:41,000
and leverage the best of human intelligence.

260
0:12:42.091,000 --> 0:12:45,000
And when you actually integrate the two, that's when things get really fun,

261
0:12:45.667,000 --> 0:12:47,000
because then you're no longer just working with data

262
0:12:48.127,000 --> 0:12:49,000
you've already collected.

263
0:12:49.347,000 --> 0:12:51,000
You get to also work with data that hasn't been collected.

264
0:12:52.108,000 --> 0:12:53,000
You get to ask questions about why:

265
0:12:53.851,000 --> 0:12:54,000
Why is this happening?

266
0:12:55.598,000 --> 0:12:56,000
Now, when Netflix did this,

267
0:12:57.001,000 --> 0:13:,000
they unlocked a whole new way to transform their business.

268
0:13:01.226,000 --> 0:13:04,000
Netflix is known for their really great recommendation algorithm,

269
0:13:05.206,000 --> 0:13:09,000
and they had this $1 million prize for anyone who could improve it.

270
0:13:10.027,000 --> 0:13:11,000
And there were winners.

271
0:13:12.075,000 --> 0:13:16,000
But Netflix discovered the improvements were only incremental.

272
0:13:17.224,000 --> 0:13:18,000
So to really find out what was going on,

273
0:13:19.212,000 --> 0:13:22,000
they hired an ethnographer, Grant McCracken,

274
0:13:22.977,000 --> 0:13:23,000
to gather thick data insights.

275
0:13:24.547,000 --> 0:13:27,000
And what he discovered was something that they hadn't seen initially

276
0:13:28.495,000 --> 0:13:29,000
in the quantitative data.

277
0:13:30.892,000 --> 0:13:32,000
He discovered that people loved to binge-watch.

278
0:13:33.644,000 --> 0:13:35,000
In fact, people didn't even feel guilty about it.

279
0:13:36.021,000 --> 0:13:37,000
They enjoyed it.

280
0:13:37.3,000 --> 0:13:38,000
(Laughter)

281
0:13:38.35,000 --> 0:13:4,000
So Netflix was like, "Oh. This is a new insight."

282
0:13:40.73,000 --> 0:13:41,000
So they went to their data science team,

283
0:13:42.692,000 --> 0:13:44,000
and they were able to scale this big data insight

284
0:13:45.034,000 --> 0:13:47,000
in with their quantitative data.

285
0:13:47.645,000 --> 0:13:5,000
And once they verified it and validated it,

286
0:13:50.839,000 --> 0:13:54,000
Netflix decided to do something very simple but impactful.

287
0:13:56.654,000 --> 0:14:02,000
They said, instead of offering the same show from different genres

288
0:14:03.17,000 --> 0:14:06,000
or more of the different shows from similar users,

289
0:14:07.082,000 --> 0:14:09,000
we'll just offer more of the same show.

290
0:14:09.66,000 --> 0:14:11,000
We'll make it easier for you to binge-watch.

291
0:14:11.789,000 --> 0:14:12,000
And they didn't stop there.

292
0:14:13.299,000 --> 0:14:14,000
They did all these things

293
0:14:14.797,000 --> 0:14:16,000
to redesign their entire viewer experience,

294
0:14:17.78,000 --> 0:14:18,000
to really encourage binge-watching.

295
0:14:20.05,000 --> 0:14:23,000
It's why people and friends disappear for whole weekends at a time,

296
0:14:23.315,000 --> 0:14:25,000
catching up on shows like "Master of None."

297
0:14:25.682,000 --> 0:14:29,000
By integrating big data and thick data, they not only improved their business,

298
0:14:29.879,000 --> 0:14:31,000
but they transformed how we consume media.

299
0:14:32.715,000 --> 0:14:36,000
And now their stocks are projected to double in the next few years.

300
0:14:38.1,000 --> 0:14:41,000
But this isn't just about watching more videos

301
0:14:41.954,000 --> 0:14:42,000
or selling more smartphones.

302
0:14:43.963,000 --> 0:14:47,000
For some, integrating thick data insights into the algorithm

303
0:14:48.037,000 --> 0:14:5,000
could mean life or death,

304
0:14:50.324,000 --> 0:14:52,000
especially for the marginalized.

305
0:14:53.558,000 --> 0:14:56,000
All around the country, police departments are using big data

306
0:14:57.016,000 --> 0:14:58,000
for predictive policing,

307
0:14:59.003,000 --> 0:15:02,000
to set bond amounts and sentencing recommendations

308
0:15:02.111,000 --> 0:15:05,000
in ways that reinforce existing biases.

309
0:15:06.116,000 --> 0:15:08,000
NSA's Skynet machine learning algorithm

310
0:15:08.563,000 --> 0:15:13,000
has possibly aided in the deaths of thousands of civilians in Pakistan

311
0:15:14.031,000 --> 0:15:16,000
from misreading cellular device metadata.

312
0:15:18.951,000 --> 0:15:21,000
As all of our lives become more automated,

313
0:15:22.378,000 --> 0:15:25,000
from automobiles to health insurance or to employment,

314
0:15:25.482,000 --> 0:15:27,000
it is likely that all of us

315
0:15:27.856,000 --> 0:15:29,000
will be impacted by the quantification bias.

316
0:15:32.792,000 --> 0:15:34,000
Now, the good news is that we've come a long way

317
0:15:35.437,000 --> 0:15:37,000
from huffing ethylene gas to make predictions.

318
0:15:37.911,000 --> 0:15:4,000
We have better tools, so let's just use them better.

319
0:15:41.005,000 --> 0:15:43,000
Let's integrate the big data with the thick data.

320
0:15:43.352,000 --> 0:15:45,000
Let's bring our temple guides with the oracles,

321
0:15:45.637,000 --> 0:15:48,000
and whether this work happens in companies or nonprofits

322
0:15:49.037,000 --> 0:15:51,000
or government or even in the software,

323
0:15:51.53,000 --> 0:15:52,000
all of it matters,

324
0:15:53.346,000 --> 0:15:56,000
because that means we're collectively committed

325
0:15:56.393,000 --> 0:15:58,000
to making better data,

326
0:15:58.608,000 --> 0:15:59,000
better algorithms, better outputs

327
0:16:00.468,000 --> 0:16:01,000
and better decisions.

328
0:16:02.135,000 --> 0:16:05,000
This is how we'll avoid missing that something.

329
0:16:07.042,000 --> 0:16:1,000
(Applause)

