1
0:00:,000 --> 0:00:07,000
Translator: Joseph Geni Reviewer: Morton Bast

2
0:00:15.931,000 --> 0:00:17,000
Hi. So, this chap here,

3
0:00:18.716,000 --> 0:00:2,000
he thinks he can tell you the future.

4
0:00:20.978,000 --> 0:00:21,000
His name is Nostradamus, although here the Sun have

5
0:00:22.957,000 --> 0:00:25,000
made him look a little bit like Sean Connery. (Laughter)

6
0:00:26.627,000 --> 0:00:28,000
And like most of you, I suspect, I don't really believe

7
0:00:29.53,000 --> 0:00:3,000
that people can see into the future.

8
0:00:30.638,000 --> 0:00:32,000
I don't believe in precognition, and every now and then,

9
0:00:33.342,000 --> 0:00:36,000
you hear that somebody has been able to predict something that happened in the future,

10
0:00:36.524,000 --> 0:00:39,000
and that's probably because it was a fluke, and we only

11
0:00:39.583,000 --> 0:00:41,000
hear about the flukes and about the freaks.

12
0:00:42.198,000 --> 0:00:46,000
We don't hear about all the times that people got stuff wrong.

13
0:00:46.277,000 --> 0:00:48,000
Now we expect that to happen with silly stories

14
0:00:48.437,000 --> 0:00:51,000
about precognition, but the problem is,

15
0:00:51.576,000 --> 0:00:54,000
we have exactly the same problem in academia

16
0:00:54.92,000 --> 0:00:58,000
and in medicine, and in this environment, it costs lives.

17
0:00:59.421,000 --> 0:01:02,000
So firstly, thinking just about precognition, as it turns out,

18
0:01:02.786,000 --> 0:01:04,000
just last year a researcher called Daryl Bem conducted

19
0:01:05.315,000 --> 0:01:06,000
a piece of research where he found evidence

20
0:01:07.151,000 --> 0:01:1,000
of precognitive powers in undergraduate students,

21
0:01:10.809,000 --> 0:01:12,000
and this was published in a peer-reviewed academic journal

22
0:01:13.383,000 --> 0:01:15,000
and most of the people who read this just said, "Okay, well,

23
0:01:15.664,000 --> 0:01:17,000
fair enough, but I think that's a fluke, that's a freak, because I know

24
0:01:17.845,000 --> 0:01:19,000
that if I did a study where I found no evidence

25
0:01:20.679,000 --> 0:01:22,000
that undergraduate students had precognitive powers,

26
0:01:23.191,000 --> 0:01:26,000
it probably wouldn't get published in a journal.

27
0:01:26.743,000 --> 0:01:28,000
And in fact, we know that that's true, because

28
0:01:29.598,000 --> 0:01:31,000
several different groups of research scientists tried

29
0:01:32.127,000 --> 0:01:35,000
to replicate the findings of this precognition study,

30
0:01:35.647,000 --> 0:01:37,000
and when they submitted it to the exact same journal,

31
0:01:38.282,000 --> 0:01:41,000
the journal said, "No, we're not interested in publishing

32
0:01:41.434,000 --> 0:01:45,000
replication. We're not interested in your negative data."

33
0:01:45.959,000 --> 0:01:47,000
So this is already evidence of how, in the academic

34
0:01:48.713,000 --> 0:01:52,000
literature, we will see a biased sample of the true picture

35
0:01:53.583,000 --> 0:01:56,000
of all of the scientific studies that have been conducted.

36
0:01:57.05,000 --> 0:02:01,000
But it doesn't just happen in the dry academic field of psychology.

37
0:02:01.479,000 --> 0:02:05,000
It also happens in, for example, cancer research.

38
0:02:05.846,000 --> 0:02:09,000
So in March, 2012, just one month ago, some researchers

39
0:02:09.923,000 --> 0:02:11,000
reported in the journal Nature how they had tried

40
0:02:12.819,000 --> 0:02:15,000
to replicate 53 different basic science studies looking at

41
0:02:16.665,000 --> 0:02:19,000
potential treatment targets in cancer,

42
0:02:20.22,000 --> 0:02:22,000
and out of those 53 studies, they were only able

43
0:02:22.858,000 --> 0:02:25,000
to successfully replicate six.

44
0:02:25.934,000 --> 0:02:29,000
Forty-seven out of those 53 were unreplicable.

45
0:02:30.267,000 --> 0:02:33,000
And they say in their discussion that this is very likely

46
0:02:34.18,000 --> 0:02:36,000
because freaks get published.

47
0:02:36.819,000 --> 0:02:38,000
People will do lots and lots and lots of different studies,

48
0:02:38.915,000 --> 0:02:4,000
and the occasions when it works they will publish,

49
0:02:41.035,000 --> 0:02:42,000
and the ones where it doesn't work they won't.

50
0:02:42.714,000 --> 0:02:45,000
And their first recommendation of how to fix this problem,

51
0:02:46.655,000 --> 0:02:49,000
because it is a problem, because it sends us all down blind alleys,

52
0:02:49.944,000 --> 0:02:5,000
their first recommendation of how to fix this problem

53
0:02:51.65,000 --> 0:02:54,000
is to make it easier to publish negative results in science,

54
0:02:55.043,000 --> 0:02:57,000
and to change the incentives so that scientists are

55
0:02:57.95,000 --> 0:03:01,000
encouraged to post more of their negative results in public.

56
0:03:02.302,000 --> 0:03:05,000
But it doesn't just happen in the very dry world

57
0:03:06.154,000 --> 0:03:09,000
of preclinical basic science cancer research.

58
0:03:10.005,000 --> 0:03:13,000
It also happens in the very real, flesh and blood

59
0:03:13.662,000 --> 0:03:16,000
of academic medicine. So in 1980,

60
0:03:17.253,000 --> 0:03:2,000
some researchers did a study on a drug called lorcainide,

61
0:03:20.261,000 --> 0:03:22,000
and this was an anti-arrhythmic drug,

62
0:03:22.592,000 --> 0:03:24,000
a drug that suppresses abnormal heart rhythms,

63
0:03:24.858,000 --> 0:03:26,000
and the idea was, after people have had a heart attack,

64
0:03:27.086,000 --> 0:03:28,000
they're quite likely to have abnormal heart rhythms,

65
0:03:28.623,000 --> 0:03:3,000
so if we give them a drug that suppresses abnormal heart

66
0:03:31,000 --> 0:03:34,000
rhythms, this will increase the chances of them surviving.

67
0:03:34.713,000 --> 0:03:37,000
Early on its development, they did a very small trial,

68
0:03:37.721,000 --> 0:03:38,000
just under a hundred patients.

69
0:03:39.365,000 --> 0:03:42,000
Fifty patients got lorcainide, and of those patients, 10 died.

70
0:03:43.017,000 --> 0:03:46,000
Another 50 patients got a dummy placebo sugar pill

71
0:03:46.06,000 --> 0:03:48,000
with no active ingredient, and only one of them died.

72
0:03:49.018,000 --> 0:03:51,000
So they rightly regarded this drug as a failure,

73
0:03:51.667,000 --> 0:03:53,000
and its commercial development was stopped, and because

74
0:03:54.536,000 --> 0:03:58,000
its commercial development was stopped, this trial was never published.

75
0:03:58.884,000 --> 0:04:03,000
Unfortunately, over the course of the next five, 10 years,

76
0:04:04.281,000 --> 0:04:07,000
other companies had the same idea about drugs that would

77
0:04:08.106,000 --> 0:04:1,000
prevent arrhythmias in people who have had heart attacks.

78
0:04:10.698,000 --> 0:04:11,000
These drugs were brought to market. They were prescribed

79
0:04:12.418,000 --> 0:04:15,000
very widely because heart attacks are a very common thing,

80
0:04:15.83,000 --> 0:04:18,000
and it took so long for us to find out that these drugs

81
0:04:19.673,000 --> 0:04:21,000
also caused an increased rate of death

82
0:04:22.584,000 --> 0:04:24,000
that before we detected that safety signal,

83
0:04:25.331,000 --> 0:04:31,000
over 100,000 people died unnecessarily in America

84
0:04:31.382,000 --> 0:04:34,000
from the prescription of anti-arrhythmic drugs.

85
0:04:34.833,000 --> 0:04:37,000
Now actually, in 1993,

86
0:04:38.431,000 --> 0:04:41,000
the researchers who did that 1980 study, that early study,

87
0:04:41.991,000 --> 0:04:44,000
published a mea culpa, an apology to the scientific community,

88
0:04:45.832,000 --> 0:04:48,000
in which they said, "When we carried out our study in 1980,

89
0:04:48.957,000 --> 0:04:49,000
we thought that the increased death rate that occurred

90
0:04:50.893,000 --> 0:04:53,000
in the lorcainide group was an effect of chance."

91
0:04:54.251,000 --> 0:04:56,000
The development of lorcainide was abandoned for commercial reasons,

92
0:04:56.283,000 --> 0:04:57,000
and this study was never published;

93
0:04:57.921,000 --> 0:04:59,000
it's now a good example of publication bias.

94
0:05:00.307,000 --> 0:05:01,000
That's the technical term for the phenomenon where

95
0:05:02.198,000 --> 0:05:06,000
unflattering data gets lost, gets unpublished, is left

96
0:05:06.436,000 --> 0:05:09,000
missing in action, and they say the results described here

97
0:05:09.807,000 --> 0:05:13,000
"might have provided an early warning of trouble ahead."

98
0:05:14.615,000 --> 0:05:17,000
Now these are stories from basic science.

99
0:05:17.828,000 --> 0:05:21,000
These are stories from 20, 30 years ago.

100
0:05:22.615,000 --> 0:05:25,000
The academic publishing environment is very different now.

101
0:05:25.762,000 --> 0:05:28,000
There are academic journals like "Trials," the open access journal,

102
0:05:29.757,000 --> 0:05:31,000
which will publish any trial conducted in humans

103
0:05:32.412,000 --> 0:05:35,000
regardless of whether it has a positive or a negative result.

104
0:05:35.715,000 --> 0:05:38,000
But this problem of negative results that go missing in action

105
0:05:39.684,000 --> 0:05:42,000
is still very prevalent. In fact it's so prevalent

106
0:05:43.243,000 --> 0:05:48,000
that it cuts to the core of evidence-based medicine.

107
0:05:49.094,000 --> 0:05:52,000
So this is a drug called reboxetine, and this is a drug

108
0:05:52.109,000 --> 0:05:54,000
that I myself have prescribed. It's an antidepressant.

109
0:05:54.644,000 --> 0:05:56,000
And I'm a very nerdy doctor, so I read all of the studies

110
0:05:57.18,000 --> 0:06:,000
that I could on this drug. I read the one study that was published

111
0:06:00.232,000 --> 0:06:02,000
that showed that reboxetine was better than placebo,

112
0:06:03.179,000 --> 0:06:04,000
and I read the other three studies that were published

113
0:06:05.043,000 --> 0:06:08,000
that showed that reboxetine was just as good as any other antidepressant,

114
0:06:08.614,000 --> 0:06:1,000
and because this patient hadn't done well on those other antidepressants,

115
0:06:10.801,000 --> 0:06:12,000
I thought, well, reboxetine is just as good. It's one to try.

116
0:06:13.267,000 --> 0:06:16,000
But it turned out that I was misled. In fact,

117
0:06:16.659,000 --> 0:06:18,000
seven trials were conducted comparing reboxetine

118
0:06:19.108,000 --> 0:06:21,000
against a dummy placebo sugar pill. One of them

119
0:06:21.82,000 --> 0:06:23,000
was positive and that was published, but six of them

120
0:06:24.132,000 --> 0:06:28,000
were negative and they were left unpublished.

121
0:06:28.18,000 --> 0:06:29,000
Three trials were published comparing reboxetine

122
0:06:29.919,000 --> 0:06:31,000
against other antidepressants in which reboxetine

123
0:06:32.145,000 --> 0:06:33,000
was just as good, and they were published,

124
0:06:33.938,000 --> 0:06:37,000
but three times as many patients' worth of data was collected

125
0:06:38.327,000 --> 0:06:39,000
which showed that reboxetine was worse than

126
0:06:40.198,000 --> 0:06:44,000
those other treatments, and those trials were not published.

127
0:06:44.899,000 --> 0:06:47,000
I felt misled.

128
0:06:48.658,000 --> 0:06:5,000
Now you might say, well, that's an extremely unusual example,

129
0:06:50.788,000 --> 0:06:52,000
and I wouldn't want to be guilty of the same kind of

130
0:06:52.796,000 --> 0:06:54,000
cherry-picking and selective referencing

131
0:06:55.777,000 --> 0:06:56,000
that I'm accusing other people of.

132
0:06:57.568,000 --> 0:06:58,000
But it turns out that this phenomenon of publication bias

133
0:06:59.452,000 --> 0:07:01,000
has actually been very, very well studied.

134
0:07:01.579,000 --> 0:07:03,000
So here is one example of how you approach it.

135
0:07:03.797,000 --> 0:07:05,000
The classic model is, you get a bunch of studies where

136
0:07:06.237,000 --> 0:07:08,000
you know that they've been conducted and completed,

137
0:07:08.422,000 --> 0:07:1,000
and then you go and see if they've been published anywhere

138
0:07:10.743,000 --> 0:07:12,000
in the academic literature. So this took all of the trials

139
0:07:13.606,000 --> 0:07:15,000
that had ever been conducted on antidepressants

140
0:07:15.76,000 --> 0:07:18,000
that were approved over a 15-year period by the FDA.

141
0:07:19.402,000 --> 0:07:22,000
They took all of the trials which were submitted to the FDA as part of the approval package.

142
0:07:23.158,000 --> 0:07:26,000
So that's not all of the trials that were ever conducted on these drugs,

143
0:07:26.358,000 --> 0:07:28,000
because we can never know if we have those,

144
0:07:28.456,000 --> 0:07:31,000
but it is the ones that were conducted in order to get the marketing authorization.

145
0:07:31.95,000 --> 0:07:33,000
And then they went to see if these trials had been published

146
0:07:34.299,000 --> 0:07:36,000
in the peer-reviewed academic literature. And this is what they found.

147
0:07:36.871,000 --> 0:07:39,000
It was pretty much a 50-50 split. Half of these trials

148
0:07:40.04,000 --> 0:07:43,000
were positive, half of them were negative, in reality.

149
0:07:43.637,000 --> 0:07:47,000
But when they went to look for these trials in the peer-reviewed academic literature,

150
0:07:48.378,000 --> 0:07:5,000
what they found was a very different picture.

151
0:07:50.612,000 --> 0:07:54,000
Only three of the negative trials were published,

152
0:07:54.984,000 --> 0:07:58,000
but all but one of the positive trials were published.

153
0:07:59.626,000 --> 0:08:02,000
Now if we just flick back and forth between those two,

154
0:08:03.387,000 --> 0:08:05,000
you can see what a staggering difference there was

155
0:08:05.981,000 --> 0:08:08,000
between reality and what doctors, patients,

156
0:08:09.431,000 --> 0:08:11,000
commissioners of health services, and academics

157
0:08:12.053,000 --> 0:08:15,000
were able to see in the peer-reviewed academic literature.

158
0:08:15.334,000 --> 0:08:19,000
We were misled, and this is a systematic flaw

159
0:08:19.788,000 --> 0:08:22,000
in the core of medicine.

160
0:08:23.118,000 --> 0:08:25,000
In fact, there have been so many studies conducted on

161
0:08:25.781,000 --> 0:08:28,000
publication bias now, over a hundred, that they've been

162
0:08:29.165,000 --> 0:08:32,000
collected in a systematic review, published in 2010,

163
0:08:32.359,000 --> 0:08:34,000
that took every single study on publication bias

164
0:08:35.125,000 --> 0:08:36,000
that they could find.

165
0:08:36.424,000 --> 0:08:38,000
Publication bias affects every field of medicine.

166
0:08:39.276,000 --> 0:08:43,000
About half of all trials, on average, go missing in action,

167
0:08:43.589,000 --> 0:08:46,000
and we know that positive findings are around twice as likely

168
0:08:46.647,000 --> 0:08:49,000
to be published as negative findings.

169
0:08:49.701,000 --> 0:08:53,000
This is a cancer at the core of evidence-based medicine.

170
0:08:53.762,000 --> 0:08:56,000
If I flipped a coin 100 times but then

171
0:08:57.633,000 --> 0:09:,000
withheld the results from you from half of those tosses,

172
0:09:00.892,000 --> 0:09:03,000
I could make it look as if I had a coin that always came up heads.

173
0:09:04.292,000 --> 0:09:05,000
But that wouldn't mean that I had a two-headed coin.

174
0:09:06.113,000 --> 0:09:07,000
That would mean that I was a chancer

175
0:09:07.825,000 --> 0:09:1,000
and you were an idiot for letting me get away with it. (Laughter)

176
0:09:10.939,000 --> 0:09:13,000
But this is exactly what we blindly tolerate

177
0:09:14.576,000 --> 0:09:17,000
in the whole of evidence-based medicine.

178
0:09:18.365,000 --> 0:09:22,000
And to me, this is research misconduct.

179
0:09:22.797,000 --> 0:09:24,000
If I conducted one study and I withheld

180
0:09:25.54,000 --> 0:09:28,000
half of the data points from that one study,

181
0:09:28.54,000 --> 0:09:32,000
you would rightly accuse me, essentially, of research fraud.

182
0:09:33.247,000 --> 0:09:35,000
And yet, for some reason, if somebody conducts

183
0:09:36.03,000 --> 0:09:4,000
10 studies but only publishes the five that give the result that they want,

184
0:09:40.588,000 --> 0:09:42,000
we don't consider that to be research misconduct.

185
0:09:43.376,000 --> 0:09:45,000
And when that responsibility is diffused between

186
0:09:45.943,000 --> 0:09:48,000
a whole network of researchers, academics,

187
0:09:49.104,000 --> 0:09:52,000
industry sponsors, journal editors, for some reason

188
0:09:52.632,000 --> 0:09:53,000
we find it more acceptable,

189
0:09:54.085,000 --> 0:09:57,000
but the effect on patients is damning.

190
0:09:57.76,000 --> 0:10:02,000
And this is happening right now, today.

191
0:10:02.778,000 --> 0:10:04,000
This is a drug called Tamiflu. Tamiflu is a drug

192
0:10:05.489,000 --> 0:10:07,000
which governments around the world have spent billions

193
0:10:08.085,000 --> 0:10:1,000
and billions of dollars on stockpiling,

194
0:10:10.657,000 --> 0:10:13,000
and we've stockpiled Tamiflu in panic,

195
0:10:13.805,000 --> 0:10:16,000
in the belief that it will reduce the rate of complications of influenza.

196
0:10:17.754,000 --> 0:10:19,000
Complications is a medical euphemism for pneumonia

197
0:10:20.438,000 --> 0:10:24,000
and death. (Laughter)

198
0:10:25.252,000 --> 0:10:28,000
Now when the Cochrane systematic reviewers

199
0:10:28.46,000 --> 0:10:3,000
were trying to collect together all of the data from all

200
0:10:30.985,000 --> 0:10:33,000
of the trials that had ever been conducted on whether Tamiflu actually did this or not,

201
0:10:34.633,000 --> 0:10:36,000
they found that several of those trials were unpublished.

202
0:10:37.584,000 --> 0:10:38,000
The results were unavailable to them.

203
0:10:39.426,000 --> 0:10:42,000
And when they started obtaining the writeups of those trials through various different means,

204
0:10:43.39,000 --> 0:10:44,000
through Freedom of Information Act requests, through

205
0:10:45.072,000 --> 0:10:49,000
harassing various different organizations, what they found was inconsistent.

206
0:10:49.881,000 --> 0:10:51,000
And when they tried to get a hold of the clinical study reports,

207
0:10:52.347,000 --> 0:10:55,000
the 10,000-page long documents that have

208
0:10:55.393,000 --> 0:10:58,000
the best possible rendition of the information,

209
0:10:58.993,000 --> 0:11:,000
they were told they weren't allowed to have them.

210
0:11:01.881,000 --> 0:11:03,000
And if you want to read the full correspondence

211
0:11:04.564,000 --> 0:11:07,000
and the excuses and the explanations given by the drug company,

212
0:11:07.854,000 --> 0:11:09,000
you can see that written up in this week's edition

213
0:11:10.571,000 --> 0:11:14,000
of PLOS Medicine.

214
0:11:14.938,000 --> 0:11:17,000
And the most staggering thing of all of this, to me,

215
0:11:18.797,000 --> 0:11:21,000
is that not only is this a problem, not only do we recognize

216
0:11:22.096,000 --> 0:11:26,000
that this is a problem, but we've had to suffer fake fixes.

217
0:11:26.291,000 --> 0:11:29,000
We've had people pretend that this is a problem that's been fixed.

218
0:11:29.349,000 --> 0:11:31,000
First of all, we had trials registers, and everybody said,

219
0:11:31.537,000 --> 0:11:34,000
oh, it's okay. We'll get everyone to register their trials, they'll post the protocol,

220
0:11:35.14,000 --> 0:11:37,000
they'll say what they're going to do before they do it,

221
0:11:37.164,000 --> 0:11:39,000
and then afterwards we'll be able to check and see if all the trials which

222
0:11:39.285,000 --> 0:11:41,000
have been conducted and completed have been published.

223
0:11:41.753,000 --> 0:11:43,000
But people didn't bother to use those registers.

224
0:11:43.949,000 --> 0:11:45,000
And so then the International Committee of Medical Journal Editors came along,

225
0:11:46.568,000 --> 0:11:47,000
and they said, oh, well, we will hold the line.

226
0:11:48.111,000 --> 0:11:5,000
We won't publish any journals, we won't publish any trials,

227
0:11:50.744,000 --> 0:11:52,000
unless they've been registered before they began.

228
0:11:53.426,000 --> 0:11:56,000
But they didn't hold the line. In 2008, a study was conducted

229
0:11:56.957,000 --> 0:11:59,000
which showed that half of all of trials published by journals

230
0:11:59.972,000 --> 0:12:01,000
edited by members of the ICMJE

231
0:12:02.639,000 --> 0:12:06,000
weren't properly registered, and a quarter of them weren't registered at all.

232
0:12:07.452,000 --> 0:12:09,000
And then finally, the FDA Amendment Act was passed

233
0:12:10.253,000 --> 0:12:12,000
a couple of years ago saying that everybody who conducts

234
0:12:12.602,000 --> 0:12:15,000
a trial must post the results of that trial within one year.

235
0:12:16.045,000 --> 0:12:2,000
And in the BMJ, in the first edition of January, 2012,

236
0:12:20.141,000 --> 0:12:22,000
you can see a study which looks to see if people kept

237
0:12:22.845,000 --> 0:12:25,000
to that ruling, and it turns out that only one in five

238
0:12:26.564,000 --> 0:12:28,000
have done so.

239
0:12:29.428,000 --> 0:12:32,000
This is a disaster.

240
0:12:32.711,000 --> 0:12:35,000
We cannot know the true effects of the medicines

241
0:12:36.275,000 --> 0:12:39,000
that we prescribe if we do not have access

242
0:12:39.491,000 --> 0:12:42,000
to all of the information.

243
0:12:42.671,000 --> 0:12:45,000
And this is not a difficult problem to fix.

244
0:12:46.63,000 --> 0:12:51,000
We need to force people to publish all trials

245
0:12:51.758,000 --> 0:12:53,000
conducted in humans, including the older trials,

246
0:12:54.729,000 --> 0:12:57,000
because the FDA Amendment Act only asks that you publish the trials conducted after 2008,

247
0:12:58.674,000 --> 0:13:,000
and I don't know what world it is in which we're only

248
0:13:01.287,000 --> 0:13:05,000
practicing medicine on the basis of trials that completed in the past two years.

249
0:13:05.743,000 --> 0:13:07,000
We need to publish all trials in humans,

250
0:13:07.848,000 --> 0:13:1,000
including the older trials, for all drugs in current use,

251
0:13:10.922,000 --> 0:13:12,000
and you need to tell everyone you know

252
0:13:13.838,000 --> 0:13:16,000
that this is a problem and that it has not been fixed.

253
0:13:17.28,000 --> 0:13:19,000
Thank you very much. (Applause)

254
0:13:20.231,000 --> 0:13:23,000
(Applause)

